{"id": "7a91bcf9-2752-40ea-b605-c7b6175046db", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.3 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Adaptive Swarm Gradient Descent (ASGD): Combines swarm intelligence and gradient estimation to explore and exploit search space efficiently.", "configspace": "", "generation": 0, "fitness": 0.8264407036439118, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.020. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8041324604654931, 0.8227266579918514, 0.8524629924743906], "final_y": [0.1412466306774247, 0.13112094863304602, 0.12789490696654837]}, "mutation_prompt": null}
{"id": "4fc3d2be-6753-4388-a1d3-3bcaa2882a26", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.3 * adaptive_factor\n            cognitive_coeff = 0.5 + 1.0 * adaptive_factor  # Adjusted personal coefficient\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with dynamic personal coefficient adjustment for improved exploration-exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.8566704298637485, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.043. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "7a91bcf9-2752-40ea-b605-c7b6175046db", "metadata": {"aucs": [0.9177579278946406, 0.831343488701037, 0.8209098729955682], "final_y": [0.11586510246012938, 0.12544547895396108, 0.1395194783377911]}, "mutation_prompt": null}
{"id": "2a7e6d8d-0a70-40a2-be13-917c12620472", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.3 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            if evaluations % 20 == 0:  # New line: adjust population size dynamically\n                self.population_size = max(5, int(self.population_size * adaptive_factor))  # New line: adjust\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent (E-ASGD) with dynamic adjustment of population size based on the convergence rate for improved exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.8392113854641531, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.011. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "7a91bcf9-2752-40ea-b605-c7b6175046db", "metadata": {"aucs": [0.8234894584999073, 0.8497889239482623, 0.8443557739442897], "final_y": [0.1365782450313504, 0.13012667127027566, 0.1329426009793241]}, "mutation_prompt": null}
{"id": "bc59b1b6-6a99-40c4-ad2c-e7b4d2c2c2c9", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.05\n                else:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent (EASGD): Incorporates dynamic local search and adaptive learning rates to improve convergence speed and solution quality.", "configspace": "", "generation": 1, "fitness": 0.8599947301548524, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.017. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "7a91bcf9-2752-40ea-b605-c7b6175046db", "metadata": {"aucs": [0.8481913227107263, 0.8844575345683234, 0.8473353331855074], "final_y": [0.13207120940454398, 0.11670179262143354, 0.11772488080787658]}, "mutation_prompt": null}
{"id": "42c55a4f-5065-44f2-aeda-1acb13187709", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.3 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5 + 0.5 * adaptive_factor  # Updated line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Adaptive Swarm Gradient Descent (ASGD): Combines swarm intelligence and gradient estimation with adaptive social coefficient for improved exploration and exploitation.", "configspace": "", "generation": 1, "fitness": 0.8219629993653088, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.024. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "7a91bcf9-2752-40ea-b605-c7b6175046db", "metadata": {"aucs": [0.7913387872343434, 0.8504782430751867, 0.8240719677863962], "final_y": [0.1491186094393402, 0.13090843191004375, 0.13431816210821446]}, "mutation_prompt": null}
{"id": "a96d8e4d-0c28-49e7-8ca9-6628917294de", "solution": "import numpy as np\n\nclass AdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        # Constraint on function evaluations\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.3 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5 + 0.5 * adaptive_factor  # Line modified\n            learning_rate = 0.1 + 0.9 * adaptive_factor  # Line added\n             \n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += learning_rate * self.velocity[i]  # Line modified\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                # Evaluate and update personal best\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n\n                # Update global best\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "AdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent (EASGD): Integrates adaptive learning rates and dynamic adjustment of social influence to improve solution convergence.", "configspace": "", "generation": 1, "fitness": 0.828414381055885, "feedback": "The algorithm AdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.038. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "7a91bcf9-2752-40ea-b605-c7b6175046db", "metadata": {"aucs": [0.80306077806339, 0.8000609336454476, 0.8821214314588173], "final_y": [0.1517526208159683, 0.14696360226018157, 0.11876821879264898]}, "mutation_prompt": null}
{"id": "25c9106f-60bd-4341-bfa6-51b87c23b7f2", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.velocity_clamp = 0.1 * (func.bounds.ub - func.bounds.lb)  # New line\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                self.velocity[i] = np.clip(self.velocity[i], -self.velocity_clamp, self.velocity_clamp)  # New line\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.05\n                else:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent (EASGD) with Velocity Clamping to prevent excessive movements and improve convergence stability.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "bc59b1b6-6a99-40c4-ad2c-e7b4d2c2c2c9", "metadata": {}, "mutation_prompt": null}
{"id": "0130d00e-f972-413c-82ea-e3219224b396", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.perturbation_factor = 0.05  # New parameter for stochastic perturbations\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                # Apply stochastic perturbation\n                perturbation = np.random.uniform(-self.perturbation_factor, self.perturbation_factor, self.dim)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i] + perturbation\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.05\n                else:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with Stochastic Perturbations (EASGD-SP): Integrates stochastic perturbations to escape local optima and improve exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8443867408675008, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.019. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "bc59b1b6-6a99-40c4-ad2c-e7b4d2c2c2c9", "metadata": {"aucs": [0.8562070631786411, 0.8599480341831545, 0.8170051252407067], "final_y": [0.12352914142107008, 0.12906483574745908, 0.145619464648715]}, "mutation_prompt": null}
{"id": "4a813966-d79b-46b0-b54c-658069179fa0", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = (1 - (evaluations / self.budget)) ** 2  # Adjusted adaptive factor\n            inertia_weight = 0.4 + 0.6 * adaptive_factor  # Adjusted inertia weight\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.05\n                else:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improved inertia weight and adaptive learning rate adjustments for enhanced convergence and solution quality.", "configspace": "", "generation": 2, "fitness": 0.8481193620613211, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.019. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "bc59b1b6-6a99-40c4-ad2c-e7b4d2c2c2c9", "metadata": {"aucs": [0.8255947290357933, 0.8463801649045113, 0.8723831922436589], "final_y": [0.13058356443247743, 0.12988951967357532, 0.125058586829479]}, "mutation_prompt": null}
{"id": "37fdeb5f-d0da-412c-8839-6fe2e3484fe8", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.1  # Slight increase in adaptive factor\n                else:\n                    self.adaptive_lr[i] *= 0.9  # Slight decrease in adaptive factor\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduced stochastic exploration strategy in the velocity update and refined adaptive learning rates for improved exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8585945728883614, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.023. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "bc59b1b6-6a99-40c4-ad2c-e7b4d2c2c2c9", "metadata": {"aucs": [0.868933440104417, 0.8798522943022462, 0.826997984258421], "final_y": [0.11555371982099627, 0.11526026781636611, 0.14046354719431475]}, "mutation_prompt": null}
{"id": "c7f53bc9-110a-4c0c-afca-aaf4858351f6", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                self.velocity[i] = (inertia_weight * (self.velocity[i] + \n                                     np.random.normal(0, 0.1, self.dim)) +   # Line modified\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]))\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.05\n                else:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhanced Adaptive Swarm Gradient Descent with Stochastic Perturbations (EASGD-SP): Introduces stochastic perturbations to diversify search and enhance global exploration capability.", "configspace": "", "generation": 2, "fitness": 0.8410476764657369, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.040. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "bc59b1b6-6a99-40c4-ad2c-e7b4d2c2c2c9", "metadata": {"aucs": [0.8133728825106327, 0.8127062302345398, 0.8970639166520381], "final_y": [0.1502629266469555, 0.1438652432285319, 0.1204051494559768]}, "mutation_prompt": null}
{"id": "24311b64-b749-4d7c-96d8-2993688891ed", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim) * adaptive_factor  # Change 1\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Change 2\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Change 3\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Incorporated dynamic stochastic factor scaling and modified adaptive learning rate adjustment to enhance convergence speed and accuracy.", "configspace": "", "generation": 3, "fitness": 0.8524945064242044, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.034. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "37fdeb5f-d0da-412c-8839-6fe2e3484fe8", "metadata": {"aucs": [0.8056392664734882, 0.867051394831582, 0.8847928579675433], "final_y": [0.1516268082455935, 0.1217609441741534, 0.12134747538065704]}, "mutation_prompt": null}
{"id": "89bbba3e-391d-4a26-a763-05c58e814d00", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.1  # Slight increase in adaptive factor\n                else:\n                    self.adaptive_lr[i] *= (0.9 - 0.4 * (1 - adaptive_factor)**2)  # Nonlinear decrease in adaptive factor\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Added a nonlinear decrease factor to the adaptive learning rate for enhanced balance between exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.863971662414626, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.009. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "37fdeb5f-d0da-412c-8839-6fe2e3484fe8", "metadata": {"aucs": [0.8552957438677725, 0.8609965172583356, 0.8756227261177698], "final_y": [0.1266842215716042, 0.12495870655288366, 0.11855774957469845]}, "mutation_prompt": null}
{"id": "7a6a3438-c0f4-4242-ad14-55b10093d25b", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1 * adaptive_factor, 0.1 * adaptive_factor, self.dim)  # Modified line\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.1  # Slight increase in adaptive factor\n                else:\n                    self.adaptive_lr[i] *= 0.9  # Slight decrease in adaptive factor\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduced a dynamic stochastic factor range to enhance exploration capabilities while maintaining stability.", "configspace": "", "generation": 3, "fitness": 0.8703065804258331, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.010. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "37fdeb5f-d0da-412c-8839-6fe2e3484fe8", "metadata": {"aucs": [0.869283976894504, 0.8581277046575574, 0.8835080597254378], "final_y": [0.12692927269586474, 0.11816271750760454, 0.11880663137319225]}, "mutation_prompt": null}
{"id": "4cd373d0-1208-4bd8-81d7-afebc9b93ca5", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                nonlinear_acceleration = np.sin(np.pi * evaluations / self.budget)  # Non-linear acceleration factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor) * nonlinear_acceleration\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.1\n                else:\n                    self.adaptive_lr[i] *= 0.9\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhanced velocity update introduces non-linear acceleration and dynamic adaptation to further improve exploration-exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.8682262717759658, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.026. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "37fdeb5f-d0da-412c-8839-6fe2e3484fe8", "metadata": {"aucs": [0.9054823356977617, 0.8488944519785641, 0.8503020276515719], "final_y": [0.11355366467582872, 0.13121442167337705, 0.13418483772488465]}, "mutation_prompt": null}
{"id": "6c576261-5e2f-4917-8564-ad9db1c0596d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 + 0.5 * np.sin(np.pi * adaptive_factor)  # Dynamic cognitive coefficient\n            social_coeff = 1.5 - 0.5 * np.cos(np.pi * adaptive_factor)    # Dynamic social coefficient\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.05 + 0.05 * adaptive_factor  # Non-linear adaptive increase\n                else:\n                    self.adaptive_lr[i] *= 0.95 - 0.05 * adaptive_factor  # Non-linear adaptive decrease\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhanced the balance between exploration and exploitation by introducing dynamic social and cognitive coefficients along with a non-linear adaptive learning rate.", "configspace": "", "generation": 3, "fitness": 0.8456219657011085, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.010. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "37fdeb5f-d0da-412c-8839-6fe2e3484fe8", "metadata": {"aucs": [0.8549725960559368, 0.8498629122843693, 0.8320303887630195], "final_y": [0.13250004104879765, 0.1281096647029757, 0.13538606559290745]}, "mutation_prompt": null}
{"id": "9343a11b-2e12-4c27-b2d6-fd4bc65b8b12", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a memory-enhanced adaptive velocity update and neighborhood learning to boost convergence and diversity.", "configspace": "", "generation": 4, "fitness": 0.8232910094857383, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.027. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "7a6a3438-c0f4-4242-ad14-55b10093d25b", "metadata": {"aucs": [0.861625972581855, 0.8062919022745663, 0.8019551536007936], "final_y": [0.1267203874181151, 0.14835724508446724, 0.14375870475374075]}, "mutation_prompt": null}
{"id": "7bab8721-07a0-453d-940e-40e83883793c", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * (1 + 0.5 * adaptive_factor)  # Modified line\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1 * adaptive_factor, 0.1 * adaptive_factor, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.1\n                else:\n                    self.adaptive_lr[i] *= 0.9\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduced an adaptive cognitive coefficient to balance exploration and exploitation dynamically.", "configspace": "", "generation": 4, "fitness": 0.7852391516790643, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.056. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "7a6a3438-c0f4-4242-ad14-55b10093d25b", "metadata": {"aucs": [0.8317238353587906, 0.7072148420554956, 0.8167787776229067], "final_y": [0.13259249523865435, 0.19384936890000293, 0.14436657287301635]}, "mutation_prompt": null}
{"id": "f1b18502-2c67-4dd4-81d9-4e01a6e0ff23", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1 * adaptive_factor, 0.1 * adaptive_factor, self.dim)\n                self.velocity[i] = (0.9 * inertia_weight * self.velocity[i] +  # Modified line\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.05  # Modified line\n                else:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improved swarm evolution by introducing a decaying velocity factor and slightly adjusting learning rates for better convergence.", "configspace": "", "generation": 4, "fitness": 0.8044148633309938, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.063. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "7a6a3438-c0f4-4242-ad14-55b10093d25b", "metadata": {"aucs": [0.862625657891652, 0.7175176131223844, 0.8331013189789451], "final_y": [0.12622685271683343, 0.18611419104793592, 0.14053402835334772]}, "mutation_prompt": null}
{"id": "909eaa9c-cc7d-474b-b43c-57f5f71fe5af", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1 * adaptive_factor * (1 - evaluations/self.budget), 0.1 * adaptive_factor * (1 - evaluations/self.budget), self.dim)  # Modified line\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.1  # Slight increase in adaptive factor\n                else:\n                    self.adaptive_lr[i] *= 0.9  # Slight decrease in adaptive factor\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Implemented a dynamic reduction of the stochastic factor impact as the budget is exhausted to enhance convergence precision.", "configspace": "", "generation": 4, "fitness": 0.7906907250816086, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.791 with standard deviation 0.061. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "7a6a3438-c0f4-4242-ad14-55b10093d25b", "metadata": {"aucs": [0.8527145165483649, 0.7078164268941238, 0.8115412318023374], "final_y": [0.12341563366272956, 0.1925139605684495, 0.14331931273888954]}, "mutation_prompt": null}
{"id": "18bc036f-2651-40c8-af9d-5c12f2ac134c", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.2 * adaptive_factor, 0.2 * adaptive_factor, self.dim)  # Changed line\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed line\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Changed line\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhanced adaptive swarm gradient descent with improved stochastic factor for better global exploration and convergence.", "configspace": "", "generation": 4, "fitness": 0.8022746487210557, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.046. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "7a6a3438-c0f4-4242-ad14-55b10093d25b", "metadata": {"aucs": [0.8477644162804245, 0.7392984207235163, 0.8197611091592264], "final_y": [0.1342325364545469, 0.1807942958816604, 0.1315539300684072]}, "mutation_prompt": null}
{"id": "7fce8304-967b-4d57-94ce-bffc66c281a2", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.7 * adaptive_factor  # Added diversity in influence\n            social_coeff = 1.3  # Reduced social influence\n            neighborhood_influence = 0.2  # New hybrid influence factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim) \n                neighbor_best = np.mean(swarm, axis=0)  # Neighborhood concept\n\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    neighborhood_influence * (neighbor_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i] * adaptive_factor)  # Dynamic memory decay\n\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.85\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Integrate a hybrid neighborhood influence and dynamic memory decay to enhance adaptability and convergence speed.", "configspace": "", "generation": 5, "fitness": 0.7939768258866174, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.006. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9343a11b-2e12-4c27-b2d6-fd4bc65b8b12", "metadata": {"aucs": [0.8017670956652667, 0.7879043930008147, 0.7922589889937707], "final_y": [0.14383444530497302, 0.15670916027594528, 0.1593666613503053]}, "mutation_prompt": null}
{"id": "36d4d7ea-9614-4def-b30f-e4d9c68283ad", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                decay_factor = 0.95  # Introduced decay factor\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2 \n                else:\n                    self.adaptive_lr[i] *= 0.85 * decay_factor  # Applied decay factor\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance the learning rate adaptation strategy by introducing a decay factor for increased stability and convergence.", "configspace": "", "generation": 5, "fitness": 0.7604754923171889, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.760 with standard deviation 0.073. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "9343a11b-2e12-4c27-b2d6-fd4bc65b8b12", "metadata": {"aucs": [0.7055794157642654, 0.7122810508518207, 0.8635660103354805], "final_y": [0.19606703314025076, 0.19167465615206059, 0.1324289440789833]}, "mutation_prompt": null}
{"id": "974161fd-c477-40ff-8ea2-b81c3a231621", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            decay_factor = 0.99 ** (evaluations / self.population_size)  # Added decay factor for velocity\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * decay_factor * self.velocity[i] +  # Applied decay factor\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance diversity by introducing a decay factor to the velocity update, reducing premature convergence.", "configspace": "", "generation": 5, "fitness": 0.7649084441007398, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.765 with standard deviation 0.039. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "9343a11b-2e12-4c27-b2d6-fd4bc65b8b12", "metadata": {"aucs": [0.749073963056033, 0.7274796713923493, 0.8181716978538369], "final_y": [0.16787681950247135, 0.18168663850214672, 0.14617635026016162]}, "mutation_prompt": null}
{"id": "59c35537-9eb7-4aa8-adb9-5333ca8b65e1", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.5 * adaptive_factor\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb + adaptive_factor * (ub - lb) * 0.1, ub - adaptive_factor * (ub - lb) * 0.1)  # Dynamic boundary adaptation\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic boundary adaptation mechanism to reduce convergence time and improve solution quality.", "configspace": "", "generation": 5, "fitness": 0.7727249540449944, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.047. And the mean value of best solutions found was 0.161 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "9343a11b-2e12-4c27-b2d6-fd4bc65b8b12", "metadata": {"aucs": [0.7824707632857826, 0.7111150006349222, 0.8245890982142785], "final_y": [0.15221141986661335, 0.19039889535997567, 0.13958117770074885]}, "mutation_prompt": null}
{"id": "7467ac04-a08f-492f-ba8e-918b7966b42e", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance convergence by implementing a decaying inertia weight to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.8025905541896954, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.060. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "9343a11b-2e12-4c27-b2d6-fd4bc65b8b12", "metadata": {"aucs": [0.8550658880942389, 0.7179903941536714, 0.8347153803211758], "final_y": [0.12905248943030845, 0.18530927877380732, 0.14143605188239905]}, "mutation_prompt": null}
{"id": "2a3cede6-ac7a-4f90-8a24-70221e1d82ae", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improve the convergence by adjusting the learning rate adaptation rate to enhance global and personal best updates.", "configspace": "", "generation": 6, "fitness": 0.8444040012398112, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.021. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "7467ac04-a08f-492f-ba8e-918b7966b42e", "metadata": {"aucs": [0.8651880227318639, 0.8157341570891922, 0.8522898238983774], "final_y": [0.12528812901127317, 0.13833616082016165, 0.12824814076583635]}, "mutation_prompt": null}
{"id": "0672e52c-9409-4e0b-bc76-ee85040466fe", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-adaptive_factor * 0.1, adaptive_factor * 0.1, self.dim)  # Adjusted random range with adaptive factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Implement stochastic factor adaptation for improved exploration-exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.8110768341957373, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.048. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "7467ac04-a08f-492f-ba8e-918b7966b42e", "metadata": {"aucs": [0.826862820570683, 0.7462721614940602, 0.8600955205224687], "final_y": [0.14165072701007642, 0.17088773977568195, 0.11806134017135361]}, "mutation_prompt": null}
{"id": "88505db6-2f70-4100-b220-03d431a46478", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 + 0.5 * adaptive_factor  # Introduced dynamic cognitive coefficient\n            social_coeff = 1.5 + 0.5 * (1 - adaptive_factor)  # Introduced dynamic social coefficient\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduced dynamic cognitive and social coefficients for enhanced adaptability in the swarm optimization process.", "configspace": "", "generation": 6, "fitness": 0.785056991058628, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.048. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "7467ac04-a08f-492f-ba8e-918b7966b42e", "metadata": {"aucs": [0.8039087011130368, 0.7188168752964988, 0.8324453967663485], "final_y": [0.14794859058692789, 0.18642177066999277, 0.1366744024972324]}, "mutation_prompt": null}
{"id": "5613e9dc-6744-41ec-878d-b3f97eeca63b", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.05 * self.memory[i])  # Refined memory integration\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improve solution quality by refining memory update to enhance exploration.", "configspace": "", "generation": 6, "fitness": 0.7954164084828719, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.041. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "7467ac04-a08f-492f-ba8e-918b7966b42e", "metadata": {"aucs": [0.8021339075566168, 0.7421830119286927, 0.8419323059633059], "final_y": [0.15448423275004963, 0.17276375760703366, 0.13325355064154176]}, "mutation_prompt": null}
{"id": "ce3bd09a-ad8b-4b51-8ab4-de65180b8f3d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5 * (1 + (evaluations / self.budget))  # Added dynamic adjustment to social coefficient\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance exploration by introducing dynamic cognitive and social coefficients based on iterations.", "configspace": "", "generation": 6, "fitness": 0.7813107224582542, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.055. And the mean value of best solutions found was 0.161 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "7467ac04-a08f-492f-ba8e-918b7966b42e", "metadata": {"aucs": [0.7362432527888224, 0.7496479749841478, 0.8580409396017925], "final_y": [0.1810424225954087, 0.17196857739009574, 0.12973445701012976]}, "mutation_prompt": null}
{"id": "f5ab66de-003c-48ac-836e-b0e2cefa178b", "solution": "# Description: Enhance convergence by employing a dynamically adjusted swarm size and adaptive velocity scaling.\n# Code:\nimport numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_size = 10 + 2 * int(np.sqrt(dim))\n        self.population_size = self.base_size\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            self.population_size = self.base_size + int((self.budget - evaluations) / self.budget * self.base_size)\n            inertia_weight = 0.8 - 0.5 * (evaluations / self.budget)\n            cognitive_coeff = 2.0 * (1 - evaluations / self.budget)\n            social_coeff = 2.0\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) + \n                                    stochastic_factor)\n                self.memory[i] = self.velocity[i]\n                velocity_scale = np.clip(np.linalg.norm(self.velocity[i]), 0, 1.5)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i] * velocity_scale\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] = min(1.2, self.adaptive_lr[i] * 1.2)\n                else:\n                    self.adaptive_lr[i] *= 0.9\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance convergence by employing a dynamically adjusted swarm size and adaptive velocity scaling.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 16 is out of bounds for axis 0 with size 16').", "error": "IndexError('index 16 is out of bounds for axis 0 with size 16')", "parent_id": "2a3cede6-ac7a-4f90-8a24-70221e1d82ae", "metadata": {}, "mutation_prompt": null}
{"id": "7614d29d-ab6f-45a7-b674-3a8708b3cc84", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)\n            cognitive_coeff = 1.5 * (adaptive_factor ** 2)  # Changed to nonlinear cognitive coefficient\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])\n                self.memory[i] = 0.5 * self.memory[i] + 0.5 * self.velocity[i]  # Improved memory update\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15\n                else:\n                    self.adaptive_lr[i] *= 0.85\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic memory update method and a nonlinear cognitive coefficient to improve convergence efficiency and adaptability.", "configspace": "", "generation": 7, "fitness": 0.7885135640319256, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.049. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "2a3cede6-ac7a-4f90-8a24-70221e1d82ae", "metadata": {"aucs": [0.7843090452052135, 0.7312681481200249, 0.8499634987705385], "final_y": [0.15928532458567857, 0.18051158777117648, 0.13828928316733358]}, "mutation_prompt": null}
{"id": "6b8a1864-e071-4070-b71e-630c1dd03760", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7  # Slightly increased social coefficient\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i])  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Further enhance convergence by increasing the social coefficient subtly during the optimization process.", "configspace": "", "generation": 7, "fitness": 0.7730039296351102, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.063. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "2a3cede6-ac7a-4f90-8a24-70221e1d82ae", "metadata": {"aucs": [0.7356968621279716, 0.7209869062990965, 0.8623280204782625], "final_y": [0.180509091022666, 0.18829798895982264, 0.12620662340101463]}, "mutation_prompt": null}
{"id": "8744980d-6479-4689-9a25-c338215f7585", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7  # Modified social coefficient for better exploration\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                velocity_perturbation = np.sin(self.velocity[i]) * 0.05  # Introduced non-linear perturbation\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i] + velocity_perturbation)  # Integrated memory\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance exploration by adjusting the social coefficient and introducing limited non-linear perturbation to velocity.", "configspace": "", "generation": 7, "fitness": 0.7688656948563574, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.053. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "2a3cede6-ac7a-4f90-8a24-70221e1d82ae", "metadata": {"aucs": [0.7625819021928678, 0.7070793854175443, 0.8369357969586599], "final_y": [0.1696206924575122, 0.19579310396031502, 0.1388535342648658]}, "mutation_prompt": null}
{"id": "af6d319e-520c-4aac-a0fc-076480378937", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                # Integrated memory and included dynamic adjustment factor\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)  # Dynamic adjustment factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance convergence by refining the velocity update to incorporate a dynamic adjustment factor for improved exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.7909750633009223, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.791 with standard deviation 0.037. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "2a3cede6-ac7a-4f90-8a24-70221e1d82ae", "metadata": {"aucs": [0.8096265538826232, 0.7386561034960879, 0.8246425325240556], "final_y": [0.14834115046622298, 0.17444786632423104, 0.12227762072733439]}, "mutation_prompt": null}
{"id": "cb914f1c-7fca-45c9-a569-1803e786b84d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)  # Adjusted random range\n                # Integrated memory and included dynamic adjustment factor\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)  # Dynamic adjustment factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance exploration by adjusting the stochastic factor range for improved diversity.", "configspace": "", "generation": 8, "fitness": 0.7911958081255799, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.791 with standard deviation 0.011. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "af6d319e-520c-4aac-a0fc-076480378937", "metadata": {"aucs": [0.7780311303547379, 0.7915006899904979, 0.8040556040315039], "final_y": [0.16143912100501023, 0.1604064509939186, 0.1523394403253907]}, "mutation_prompt": null}
{"id": "b69eb815-c3bb-4405-9254-9d28e7433c2e", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Modified inertia weight\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7  # Increased social coefficient\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)  # Larger random range\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)  # Changed dynamic adjustment factor range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor + \n                                    0.05 * self.memory[i]) * dynamic_adjustment  # Adjusted memory effect\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.8  # Adjusted adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce an adaptive diversification-convergence control mechanism to balance exploration and exploitation dynamically.", "configspace": "", "generation": 8, "fitness": 0.8240640256890419, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.013. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "af6d319e-520c-4aac-a0fc-076480378937", "metadata": {"aucs": [0.821865437728275, 0.8413549288075508, 0.8089717105312999], "final_y": [0.14714978992392658, 0.14048867952291288, 0.15050149402300395]}, "mutation_prompt": null}
{"id": "fc7b2d55-3492-481a-ab8f-9659e6c7be99", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                # Integrated memory and included dynamic adjustment factor\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)  # Dynamic adjustment factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.2 * self.memory[i]) * dynamic_adjustment  # Increase memory influence\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance the solution update by increasing the influence of the memory component, promoting diverse exploration.", "configspace": "", "generation": 8, "fitness": 0.8165942956045553, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.017. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "af6d319e-520c-4aac-a0fc-076480378937", "metadata": {"aucs": [0.8175818073817231, 0.7954930078596184, 0.8367080715723245], "final_y": [0.13799826802129367, 0.15022083867010583, 0.12323871506011985]}, "mutation_prompt": null}
{"id": "6c8d6784-b125-4ab9-a757-45a39172daf3", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.4 * (evaluations / self.budget)  # Changed the inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)  # Adjusted random range\n                # Integrated memory and included dynamic adjustment factor\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)  # Dynamic adjustment factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improve exploration by increasing the range of the stochastic factor in velocity update.", "configspace": "", "generation": 8, "fitness": 0.8221691983022756, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.023. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "af6d319e-520c-4aac-a0fc-076480378937", "metadata": {"aucs": [0.8529304847053576, 0.799552749286945, 0.8140243609145242], "final_y": [0.13486612448646296, 0.1403117875250316, 0.1480935427375698]}, "mutation_prompt": null}
{"id": "a2b35c15-7e3c-4a55-b206-d73e1493cbec", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))  # Added memory for velocity\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.9 - 0.5 * (evaluations / self.budget)  # Adjusted inertia weight calculation\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.5\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.05, 0.05, self.dim)  # Adjusted random range\n                # Integrated memory and included dynamic adjustment factor\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)  # Dynamic adjustment factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    stochastic_factor + 0.1 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]  # Update memory\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15  # Changed adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Decreased adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improve convergence by optimizing the inertia weighting to adaptively enhance particle exploration in early stages and exploitation in later stages.", "configspace": "", "generation": 8, "fitness": 0.8151637053792854, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.041. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "af6d319e-520c-4aac-a0fc-076480378937", "metadata": {"aucs": [0.84617742653386, 0.7578688780486628, 0.8414448115553337], "final_y": [0.12288169964277884, 0.1694887493246514, 0.1400385031856074]}, "mutation_prompt": null}
{"id": "5225988d-5e02-4445-9656-aaccd5de71eb", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Modified inertia weight\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7 * (1 - adaptive_factor)  # Adjusted social coefficient dynamically\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)  # Larger random range\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)  # Changed dynamic adjustment factor range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor + \n                                    0.05 * self.memory[i]) * dynamic_adjustment  # Adjusted memory effect\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.8  # Adjusted adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic adjustment to the social coefficient based on the current best value to improve solution convergence.", "configspace": "", "generation": 9, "fitness": 0.7805109865304076, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.043. And the mean value of best solutions found was 0.161 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "b69eb815-c3bb-4405-9254-9d28e7433c2e", "metadata": {"aucs": [0.8316938677232616, 0.7835884671052888, 0.7262506247626724], "final_y": [0.13170830749025264, 0.16393101392563547, 0.18689945739026959]}, "mutation_prompt": null}
{"id": "c4a92b83-2cd6-4ae5-9057-421617450907", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic exploration adjustment based on current performance improvement rate.", "configspace": "", "generation": 9, "fitness": 0.8057823557620377, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.011. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "b69eb815-c3bb-4405-9254-9d28e7433c2e", "metadata": {"aucs": [0.8028328699619827, 0.7944347407480101, 0.8200794565761201], "final_y": [0.1519702641302242, 0.15996216620574355, 0.15002637364065174]}, "mutation_prompt": null}
{"id": "7b6dac19-0402-49e0-ae72-6cf698ac2b97", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Modified inertia weight\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7  # Increased social coefficient\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)  # Larger random range\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)  # Changed dynamic adjustment factor range\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor + \n                                    0.05 * self.memory[i] * np.random.uniform(0.9, 1.1)) * dynamic_adjustment  # Adjusted memory effect\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.8  # Adjusted adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm adaptability by refining velocity update with dynamic stochastic memory influence.", "configspace": "", "generation": 9, "fitness": 0.7766385911689725, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.048. And the mean value of best solutions found was 0.164 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "b69eb815-c3bb-4405-9254-9d28e7433c2e", "metadata": {"aucs": [0.8379582175965588, 0.7716407458770922, 0.7203168100332663], "final_y": [0.1350617677303383, 0.16830920205166877, 0.18884153441534934]}, "mutation_prompt": null}
{"id": "c0834f5d-4642-44a8-977c-a3ec46ff5736", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adaptive inertia adjustment\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7\n\n            for i in range(self.population_size):\n                r1, r2, r3 = np.random.random(self.dim), np.random.random(self.dim), np.random.random(self.dim)\n                neighborhood_influence = 0.05 * np.mean(swarm, axis=0) - swarm[i]  # Neighborhood influence\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * np.random.uniform(-0.1, 0.1, self.dim) + \n                                    neighborhood_influence * r3 +  # Added neighborhood influence\n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.7  # Slightly more aggressive adaptation penalty\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance exploration-exploitation balance using dynamic neighborhood influence and adaptive inertia adjustment.", "configspace": "", "generation": 9, "fitness": 0.8003033892064652, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.017. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "b69eb815-c3bb-4405-9254-9d28e7433c2e", "metadata": {"aucs": [0.7814225478679724, 0.8220041713700444, 0.7974834483813782], "final_y": [0.1602129435282964, 0.1492683162517796, 0.15706743463472717]}, "mutation_prompt": null}
{"id": "b6d09f96-e1ba-4220-9eb1-93a8479714e8", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7  \n            exploration_weight = (0.5 * (1 - np.cos(np.pi * evaluations / self.budget))) + 0.2  # Updated exploration_weight calculation\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    exploration_weight * stochastic_factor +  # Applied updated exploration_weight\n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a nonlinear time-varying exploration weight to enhance dynamic adaptation.", "configspace": "", "generation": 9, "fitness": 0.7519370784521805, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.752 with standard deviation 0.052. And the mean value of best solutions found was 0.175 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "b69eb815-c3bb-4405-9254-9d28e7433c2e", "metadata": {"aucs": [0.8260158471480263, 0.7158590765405257, 0.7139363116679893], "final_y": [0.14425790225503154, 0.19194733897311422, 0.1900339325967143]}, "mutation_prompt": null}
{"id": "1a450717-a0f4-4d05-998f-8ba52a5f281f", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate +\n                                    0.1 * self.memory[i]) * dynamic_adjustment  # Modified line\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance dynamic adjustment of exploration by modifying velocity update.", "configspace": "", "generation": 10, "fitness": 0.8494437706280819, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.017. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "c4a92b83-2cd6-4ae5-9057-421617450907", "metadata": {"aucs": [0.8388568470317139, 0.8364417258349424, 0.8730327390175892], "final_y": [0.13832613217694167, 0.1391944802093853, 0.13070303856215626]}, "mutation_prompt": null}
{"id": "f5f61a5b-50c9-4010-a147-b12b8893f0b5", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.3, 0.9)  # Adjusted exploration weight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7\n\n            for i in range(self.population_size):\n                r1, r2, r3 = np.random.random(self.dim), np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                diversity_mutation = np.random.uniform(-0.05, 0.05, self.dim)  # New mutation strategy\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i])\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i] + diversity_mutation\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.3  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.7  # Reduced adaptation rate\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance adaptive learning rates and introduce a diversity-promoting mutation strategy to improve global exploration and convergence speed.", "configspace": "", "generation": 10, "fitness": 0.8166206991029007, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.036. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "c4a92b83-2cd6-4ae5-9057-421617450907", "metadata": {"aucs": [0.8194040756119702, 0.859552509389738, 0.7709055123069939], "final_y": [0.14565412771071906, 0.13028934562121997, 0.16823064196837811]}, "mutation_prompt": null}
{"id": "c831ed37-52fa-4783-b49c-0f2128fff50e", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7 + 0.3 * (evaluations / self.budget)  # Adjusted social coefficient\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Refine global influence by increasing the social coefficient dynamically as evaluations progress.", "configspace": "", "generation": 10, "fitness": 0.8279922828389826, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.037. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "c4a92b83-2cd6-4ae5-9057-421617450907", "metadata": {"aucs": [0.8049520661260389, 0.7982718414674486, 0.8807529409234608], "final_y": [0.14348127836496594, 0.1545050976043275, 0.1262930084806353]}, "mutation_prompt": null}
{"id": "77c37c51-e7e5-4770-b03e-22a90cc6732f", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor\n            social_coeff = 1.7 + 0.3 * np.exp(-np.var(personal_best_value))  # Dynamic social coefficient\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i] * 0.9  # Memory decay\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance convergence by introducing a dynamic memory decay and adaptive social coefficient scaling based on swarm diversity.", "configspace": "", "generation": 10, "fitness": 0.8592238236017403, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.024. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "c4a92b83-2cd6-4ae5-9057-421617450907", "metadata": {"aucs": [0.8314475151837513, 0.8899655241170424, 0.8562584315044272], "final_y": [0.14101786864149157, 0.11613986783688401, 0.1354024333451893]}, "mutation_prompt": null}
{"id": "2a923766-8cca-428a-8608-1413dace5c28", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n        self.synergy_factor = np.random.uniform(0.5, 1.5)  # New synergy factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor  # Modified\n            social_coeff = 1.7 * self.synergy_factor  # Modified\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a synergy factor and enhanced velocity update for better convergence control and solution quality.", "configspace": "", "generation": 10, "fitness": 0.859710557507786, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.039. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "c4a92b83-2cd6-4ae5-9057-421617450907", "metadata": {"aucs": [0.9068724417192295, 0.8611882761281188, 0.81107095467601], "final_y": [0.11621885905119234, 0.1267585190092213, 0.14962863829198336]}, "mutation_prompt": null}
{"id": "44087ace-5af5-4cb2-b415-5a123966c78d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.3, 0.9)  # Adjusted range\n        self.synergy_factor = np.random.uniform(0.7, 1.7)  # Adjusted range\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n        \n        chaos_map = np.random.uniform(0, 1)  # Initializing chaos map\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.5 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Adjusted weight\n            cognitive_coeff = 1.4 * adaptive_factor * self.synergy_factor  # Adjusted\n            social_coeff = 1.8 * self.synergy_factor  # Adjusted\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.15, 0.15, self.dim)  # Adjusted range\n                dynamic_adjustment = np.random.uniform(0.6, 1.4)  # Adjusted range\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                chaos_map = 4 * chaos_map * (1 - chaos_map)  # Logistic map for chaos\n                chaotic_exploration = self.exploration_weight * chaos_map\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    chaotic_exploration * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.3  # Adjusted\n                else:\n                    self.adaptive_lr[i] *= 0.7  # Adjusted\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Incorporate adaptive synergy and chaos-enhanced exploration for improved convergence and solution diversity.", "configspace": "", "generation": 11, "fitness": 0.7925691003775645, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.019. And the mean value of best solutions found was 0.157 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "2a923766-8cca-428a-8608-1413dace5c28", "metadata": {"aucs": [0.8097277501530379, 0.801489278350494, 0.7664902726291615], "final_y": [0.1488760823624128, 0.15252282291819874, 0.16981563070673023]}, "mutation_prompt": null}
{"id": "a0ae8cf9-bfa0-4926-b7f6-3eb91589248d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n        self.synergy_factor = np.random.uniform(0.5, 1.5)  # New synergy factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor  # Modified\n            social_coeff = 1.7 * self.synergy_factor  # Modified\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i] * 0.8  # Updated memory factor\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i] * 0.9 + personal_best[i] * 0.1  # Updated personal best strategy\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce synergy factor adjustment and memory-based personal best updates for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.8459587483430314, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.001. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "2a923766-8cca-428a-8608-1413dace5c28", "metadata": {"aucs": [0.8452000429914879, 0.8449003211793074, 0.8477758808582989], "final_y": [0.14004112012676861, 0.13704145413005575, 0.1406496701161455]}, "mutation_prompt": null}
{"id": "f604fe52-ba1a-47e4-b28d-8ddd4b7a0f9c", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n        self.synergy_factor = np.random.uniform(0.5, 1.5)  # New synergy factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)  # Modified\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor  # Modified\n            social_coeff = 1.7 * self.synergy_factor  # Modified\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic inertia weight adjustment to enhance convergence speed and solution exploration.", "configspace": "", "generation": 11, "fitness": 0.8589000488386143, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.026. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2a923766-8cca-428a-8608-1413dace5c28", "metadata": {"aucs": [0.8931573341532495, 0.8299456812053088, 0.8535971311572843], "final_y": [0.12339476387746617, 0.14297399536120214, 0.1337952445755738]}, "mutation_prompt": null}
{"id": "7bb79758-9834-4c72-81bb-c29ae7e3c69f", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.3, 0.9)  # Modified exploration weight\n        self.synergy_factor = np.random.uniform(0.5, 1.5)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                learning_factor = 0.05 * np.tanh(improvement_rate)  # New learning factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    learning_factor * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Adjusted velocity update to incorporate a learning factor and improved exploration weight for a more adaptive search.", "configspace": "", "generation": 11, "fitness": 0.8281086194327374, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.009. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "2a923766-8cca-428a-8608-1413dace5c28", "metadata": {"aucs": [0.8413074008012025, 0.820454593650246, 0.8225638638467634], "final_y": [0.13645165414356064, 0.1414055817434282, 0.14600748784794282]}, "mutation_prompt": null}
{"id": "bb5cfb3f-7979-4f7c-8076-eee43a4206f6", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n        self.synergy_factor = np.random.uniform(0.5, 1.5)  # New synergy factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            self.synergy_factor *= 1.01  # Dynamic adjustment of synergy factor\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor  # Modified\n            social_coeff = 1.7 * self.synergy_factor  # Modified\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic synergy factor adjustment to enhance convergence adaptability and solution precision.", "configspace": "", "generation": 11, "fitness": 0.8044550210907774, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.036. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "2a923766-8cca-428a-8608-1413dace5c28", "metadata": {"aucs": [0.8344222508262269, 0.7537396078927247, 0.8252032045533808], "final_y": [0.1364726652792928, 0.1725739986226581, 0.1451320436150384]}, "mutation_prompt": null}
{"id": "6f4df244-4fc0-44d0-ac6b-2b8745a8a230", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = np.random.uniform(0.5, 1.5)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n\n                distance_factor = np.linalg.norm(global_best - swarm[i]) / self.dim  # New distance factor\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate +\n                                    0.05 * self.memory[i] * distance_factor) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic acceleration factor based on distance to enhance local refinement and convergence.", "configspace": "", "generation": 12, "fitness": 0.8260027805495721, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.010. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "f604fe52-ba1a-47e4-b28d-8ddd4b7a0f9c", "metadata": {"aucs": [0.8302584531502252, 0.8121840279312396, 0.8355658605672518], "final_y": [0.1466900899741399, 0.1469157048974583, 0.13705466356382134]}, "mutation_prompt": null}
{"id": "4c92b604-a2f6-47a2-89bb-c6b420c389c7", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = np.random.uniform(0.5, 1.5)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                # Add random restart if stuck in local optima\n                if evaluations % (self.budget // 5) == 0:  # Change made here\n                    swarm[i] = np.random.uniform(lb, ub, self.dim)\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a random restart mechanism to avoid local optima and improve exploration capabilities.", "configspace": "", "generation": 12, "fitness": 0.8522103391054462, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.021. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "f604fe52-ba1a-47e4-b28d-8ddd4b7a0f9c", "metadata": {"aucs": [0.849231901811891, 0.879000812762007, 0.8283983027424403], "final_y": [0.13725937800550925, 0.12291455874577961, 0.14274581147242882]}, "mutation_prompt": null}
{"id": "e4583e37-b6eb-41d2-969d-c84a53a9ace7", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce an adaptive synergy factor and momentum adjustment to refine convergence and exploration.", "configspace": "", "generation": 12, "fitness": 0.8784431658408817, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.023. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "f604fe52-ba1a-47e4-b28d-8ddd4b7a0f9c", "metadata": {"aucs": [0.9106390719925392, 0.8628167454370772, 0.8618736800930286], "final_y": [0.11703945465576637, 0.13021479873799158, 0.12960211242614128]}, "mutation_prompt": null}
{"id": "8259bc16-5b95-4b5b-a2aa-b0f3d61228bd", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)  # Added exploration weight\n        self.synergy_factor = np.random.uniform(0.5, 1.5)  # New synergy factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.velocity[i] = np.clip(self.velocity[i], -0.1, 0.1)  # Dynamic velocity clamping\n                self.exploration_weight = 0.3 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adaptive weight\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance exploitation by introducing dynamic velocity clamping and adaptive exploration weight.", "configspace": "", "generation": 12, "fitness": 0.8309165975272944, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.009. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "f604fe52-ba1a-47e4-b28d-8ddd4b7a0f9c", "metadata": {"aucs": [0.8330883377102336, 0.8184249095442558, 0.8412365453273937], "final_y": [0.11506747744536394, 0.1474571436368738, 0.13600520239716263]}, "mutation_prompt": null}
{"id": "9e142c80-de07-4f8b-b40e-0599eea5b2b1", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = np.random.uniform(0.5, 1.5)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                blend_factor = np.random.uniform(0.3, 0.7)  # New line\n                combined_best = blend_factor * global_best + (1 - blend_factor) * personal_best[i]  # New line\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (combined_best - swarm[i]) +  # Modified line\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Incorporate a dynamic blending of global and personal bests to enhance solution diversity and convergence.", "configspace": "", "generation": 12, "fitness": 0.8670623035295907, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.008. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "f604fe52-ba1a-47e4-b28d-8ddd4b7a0f9c", "metadata": {"aucs": [0.8556013393433877, 0.8744368004029135, 0.8711487708424711], "final_y": [0.13264325426791357, 0.11927836727530705, 0.12637642282160344]}, "mutation_prompt": null}
{"id": "271e9959-37a6-4070-8697-a981b316ce76", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - (evaluations / self.budget) ** 1.5  # Non-linear decay\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Fine-tuned\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.1  # Slight adjustment\n                else:\n                    self.adaptive_lr[i] *= 0.85  # Slight adjustment\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a non-linear adaptive learning rate decay and fine-tune synergy factor dynamics to enhance convergence and adaptability.", "configspace": "", "generation": 13, "fitness": 0.8451353849036751, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.033. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "e4583e37-b6eb-41d2-969d-c84a53a9ace7", "metadata": {"aucs": [0.884922518888721, 0.8471441673495064, 0.8033394684727982], "final_y": [0.11540623956830487, 0.12951583407167078, 0.15073044964644844]}, "mutation_prompt": null}
{"id": "5c34e0c7-d756-41bd-8b8e-b5a8f563dcf7", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n            self.exploration_weight = 0.3 + 0.7 * (global_best_value / (1 + np.max(personal_best_value)))  # Updated line 1\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate +\n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce adaptive exploration weights to dynamically balance exploration and exploitation phases.", "configspace": "", "generation": 13, "fitness": 0.8469109340718197, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.027. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "e4583e37-b6eb-41d2-969d-c84a53a9ace7", "metadata": {"aucs": [0.8776023723801949, 0.8509406518803018, 0.8121897779549624], "final_y": [0.1264629958215301, 0.13606451246527018, 0.15098571711197928]}, "mutation_prompt": null}
{"id": "e88f603b-3a5f-492d-8511-941a188cc02e", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * adaptive_factor * self.synergy_factor  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm communication by integrating adaptive synergy in social influence for improved convergence.", "configspace": "", "generation": 13, "fitness": 0.8738766676174787, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.027. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "e4583e37-b6eb-41d2-969d-c84a53a9ace7", "metadata": {"aucs": [0.8802390738862764, 0.8384696604962616, 0.9029212684698982], "final_y": [0.12558456876108814, 0.14132296076734863, 0.11819877225762054]}, "mutation_prompt": null}
{"id": "6af5a238-b5f3-4e65-9a4f-b11c966e6371", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Changed line 1\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * 0.5 * (1 + np.cos(np.pi * evaluations / self.budget)) * stochastic_factor * improvement_rate +  # Changed line 2\n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance diversity by introducing oscillating exploration weights and adaptive inertia adjustment to improve convergence balance.", "configspace": "", "generation": 13, "fitness": 0.8653281521212036, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.027. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "e4583e37-b6eb-41d2-969d-c84a53a9ace7", "metadata": {"aucs": [0.8464550441682507, 0.8463943115731558, 0.9031351006222045], "final_y": [0.1403819352765967, 0.13701190527471374, 0.12012383473765775]}, "mutation_prompt": null}
{"id": "7d58baa6-a9bb-49ab-a748-4d3bfd7c236d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget * 1.5)  # Modified line\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1)  # Modified line\n            social_coeff = 1.7 * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic inertia weight scaling based on evaluations and a synergy factor adjustment for enhanced convergence.", "configspace": "", "generation": 13, "fitness": 0.8501670102581733, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.029. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "e4583e37-b6eb-41d2-969d-c84a53a9ace7", "metadata": {"aucs": [0.8752543414917481, 0.8096073715474823, 0.8656393177352895], "final_y": [0.12662346684027037, 0.15139981321528784, 0.1294846728847674]}, "mutation_prompt": null}
{"id": "1b0249ef-8e5b-4ae4-96c0-d09fd911ada5", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        # Change made here: Adjusted exploration weight initialization\n        self.exploration_weight = np.random.uniform(0.2, 0.8) * (0.8 + 0.2 * np.sin(np.pi))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * adaptive_factor * self.synergy_factor  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm communication by introducing a dynamic exploration weight adjustment for improved adaptability and convergence.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'EnhancedAdaptiveSwarmGradientDescent' object has no attribute 'synergy_factor'\").", "error": "AttributeError(\"'EnhancedAdaptiveSwarmGradientDescent' object has no attribute 'synergy_factor'\")", "parent_id": "e88f603b-3a5f-492d-8511-941a188cc02e", "metadata": {}, "mutation_prompt": null}
{"id": "3cdb6945-e206-4c12-928f-c5244cb68b53", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * adaptive_factor * self.synergy_factor  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * (dynamic_adjustment + 0.1 * improvement_rate) # Changed line\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm communication by integrating adaptive synergy with a dynamic velocity scaling factor for improved convergence.", "configspace": "", "generation": 14, "fitness": 0.8544647475689677, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.032. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "e88f603b-3a5f-492d-8511-941a188cc02e", "metadata": {"aucs": [0.8981137247836768, 0.8223175245298215, 0.8429629933934049], "final_y": [0.12002152065091876, 0.1463688950811196, 0.1389615455833786]}, "mutation_prompt": null}
{"id": "53860ccd-abd7-4e84-94f9-120a7cc76ff7", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.random.rand()  # Probabilistic inertia weight\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * adaptive_factor * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                    self.memory[i] *= 0.5  # Strategic memory resetting\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm efficiency by incorporating probabilistic inertia weight and strategic memory resetting to boost convergence without excessive exploitation.", "configspace": "", "generation": 14, "fitness": 0.8397505305031064, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.032. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "e88f603b-3a5f-492d-8511-941a188cc02e", "metadata": {"aucs": [0.8679982157913945, 0.8556217782237231, 0.7956315974942016], "final_y": [0.13020275005893478, 0.13181341595323615, 0.154779272093852]}, "mutation_prompt": null}
{"id": "668623e6-8058-45c6-96a1-5c856b2e7957", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.5, 0.5, (self.population_size, dim))  # Changed line\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.7 * (1.0 - adaptive_factor) * self.synergy_factor  # Changed line\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm exploration by refining memory initialization and synergy factor dynamic adjustment.", "configspace": "", "generation": 14, "fitness": 0.8597223214799697, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.018. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e88f603b-3a5f-492d-8511-941a188cc02e", "metadata": {"aucs": [0.8847526702432187, 0.8410665702099315, 0.853347723986759], "final_y": [0.12488293844994824, 0.13211174958197647, 0.130712742231052]}, "mutation_prompt": null}
{"id": "1588e75f-e024-4f62-9b0d-13a466f3da3c", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.9 * adaptive_factor * self.synergy_factor  # Modified line for increased synergy\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm adaptability by refining synergy dynamics for precise optimization.", "configspace": "", "generation": 14, "fitness": 0.8630419123804759, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.016. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "e88f603b-3a5f-492d-8511-941a188cc02e", "metadata": {"aucs": [0.8576893528776823, 0.8471640004616985, 0.8842723838020466], "final_y": [0.1335214882036404, 0.13505389000372003, 0.1257216073205918]}, "mutation_prompt": null}
{"id": "4f6c553e-2234-4ee1-aa4a-a4cff9978049", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.9 * adaptive_factor * self.synergy_factor  # Modified line for increased synergy\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    (self.exploration_weight + evaluations / self.budget) * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment  # Adjust exploration weight dynamically\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improve exploration by enhancing dynamic exploration weight adjustment in synergy dynamics.", "configspace": "", "generation": 15, "fitness": 0.8263544317059942, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.021. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "1588e75f-e024-4f62-9b0d-13a466f3da3c", "metadata": {"aucs": [0.8505541205289817, 0.8299975845474059, 0.7985115900415947], "final_y": [0.13741879569957638, 0.14252070524929328, 0.15587541528669802]}, "mutation_prompt": null}
{"id": "534b920d-b33e-4c23-8547-f9ced6052cba", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.9 * adaptive_factor * self.synergy_factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment * (1 + 0.1 * adaptive_factor)  # Modified line\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm adaptability by refining synergy dynamics for precise optimization, with improved dynamic velocity adjustment.", "configspace": "", "generation": 15, "fitness": 0.8440976600561155, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.035. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "1588e75f-e024-4f62-9b0d-13a466f3da3c", "metadata": {"aucs": [0.8902283010499165, 0.8379041321167945, 0.8041605470016353], "final_y": [0.12262233941154221, 0.13716960334619566, 0.1503912270720562]}, "mutation_prompt": null}
{"id": "0e4429ba-3e84-4f62-8403-892f634914ca", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 2.0 * adaptive_factor * self.synergy_factor  # Modified line for increased synergy\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.3  # Modified line for personalized learning adjustment\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm efficacy with nuanced synergy dynamics and personalized learning adjustments.", "configspace": "", "generation": 15, "fitness": 0.8444703926283071, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.031. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "1588e75f-e024-4f62-9b0d-13a466f3da3c", "metadata": {"aucs": [0.8867991462446765, 0.811367252220798, 0.8352447794194471], "final_y": [0.12477438418204978, 0.1496250245380839, 0.13943744333664243]}, "mutation_prompt": null}
{"id": "d80f3778-4864-4e90-b82f-373ef19d3138", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.6 + np.random.uniform(0, 0.9)  # Adjusted synergy factor initialization\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * self.synergy_factor\n            social_coeff = 1.9 * adaptive_factor * self.synergy_factor  # Modified line for increased synergy\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Augment swarm efficiency by refining synergy initialization for more precise optimization.", "configspace": "", "generation": 15, "fitness": 0.8461348491742067, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.022. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "1588e75f-e024-4f62-9b0d-13a466f3da3c", "metadata": {"aucs": [0.8767004947068928, 0.823417403609096, 0.8382866492066312], "final_y": [0.12886610983974112, 0.14515633805218098, 0.13997752239475425]}, "mutation_prompt": null}
{"id": "33bfd59d-5eaa-428d-9ddb-3c40160c5c93", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce dynamic cognitive and social synergy scaling to enhance convergence precision.", "configspace": "", "generation": 15, "fitness": 0.8515108250963929, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.014. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "1588e75f-e024-4f62-9b0d-13a466f3da3c", "metadata": {"aucs": [0.8708707522859067, 0.8453872494735557, 0.8382744735297165], "final_y": [0.13186859077653879, 0.13834491721574715, 0.13877696590266353]}, "mutation_prompt": null}
{"id": "3bcf6a2b-f357-47c0-81cd-13352f877ba5", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        # Modify this line to introduce a dynamic exploration weight\n        self.exploration_weight = np.random.uniform(0.2, 0.8) * (1 + 0.5 * np.sin(np.pi * np.arange(self.population_size) / self.budget))\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight[i] * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a dynamic exploration weight influenced by evaluations to enhance traversal of the search space.", "configspace": "", "generation": 16, "fitness": 0.8535767502248474, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.012. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "33bfd59d-5eaa-428d-9ddb-3c40160c5c93", "metadata": {"aucs": [0.8651166480842205, 0.8366274205923498, 0.8589861819979716], "final_y": [0.1279614977474285, 0.13432864475585415, 0.12725694578431268]}, "mutation_prompt": null}
{"id": "4bc27641-63af-4ae6-aad7-174f7c1e0b4b", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance convergence by incorporating an adaptive learning rate influenced by historical velocity trends.", "configspace": "", "generation": 16, "fitness": 0.8740390697867042, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.016. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "33bfd59d-5eaa-428d-9ddb-3c40160c5c93", "metadata": {"aucs": [0.8563735554963146, 0.8943835721994355, 0.8713600816643625], "final_y": [0.1352139128247295, 0.11999720099994793, 0.12990593920235205]}, "mutation_prompt": null}
{"id": "7b85c9a7-1f4b-45d4-a3e9-b2a05bd9f0f9", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * np.exp(-evaluations/self.budget) * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce nonlinear velocity scaling based on evaluations to improve convergence efficiency.", "configspace": "", "generation": 16, "fitness": 0.8457232032781624, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.017. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "33bfd59d-5eaa-428d-9ddb-3c40160c5c93", "metadata": {"aucs": [0.8403264665308237, 0.8691688028778357, 0.8276743404258278], "final_y": [0.13949789917660604, 0.12560316351750256, 0.13934783085897462]}, "mutation_prompt": null}
{"id": "a303da0b-1d09-4427-a464-c976c893da5d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.8, 1.2)  # Adjust dynamic range for better tuning\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.3  # Slightly increase learning rate adjustment for faster adaptation\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance convergence precision by optimizing update dynamics and parameter tuning.", "configspace": "", "generation": 16, "fitness": 0.8606925622580763, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.013. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "33bfd59d-5eaa-428d-9ddb-3c40160c5c93", "metadata": {"aucs": [0.8562140079570688, 0.8471206464831469, 0.8787430323340133], "final_y": [0.1347444025933221, 0.13482393166750817, 0.12383084095222097]}, "mutation_prompt": null}
{"id": "69239e10-763b-4577-ae0a-576f9dcf0b5f", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor +\n                                    0.05 * self.memory[i] - 0.01 * swarm[i]) * dynamic_adjustment\n                self.memory[i] = 0.9 * self.memory[i] + 0.1 * self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.15\n                else:\n                    self.adaptive_lr[i] *= 0.85\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce adaptive inertia and gradient-based memory enhancement to boost convergence efficiency.", "configspace": "", "generation": 16, "fitness": 0.8402851047043489, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.051. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "33bfd59d-5eaa-428d-9ddb-3c40160c5c93", "metadata": {"aucs": [0.8917195103868905, 0.7715683995623313, 0.857567404163825], "final_y": [0.12002490919652409, 0.16482949254706636, 0.13039835429464075]}, "mutation_prompt": null}
{"id": "8970c32b-d250-40e5-8802-910c91ea5091", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.sin(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance exploration and exploitation balance by dynamically adjusting synergy factors and incorporating a multi-mutation strategy.", "configspace": "", "generation": 17, "fitness": 0.8551724309740373, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.013. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "4bc27641-63af-4ae6-aad7-174f7c1e0b4b", "metadata": {"aucs": [0.8510649640382291, 0.8728828095635692, 0.8415695193203134], "final_y": [0.13701415198173483, 0.12540413088162072, 0.14038441945475]}, "mutation_prompt": null}
{"id": "21b01830-1114-45c9-a803-fc3f7c780cb4", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = np.sin(evaluations / self.budget * np.pi) * self.velocity[i]  # Change applied here\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce oscillating memory decay to enhance exploration and convergence balance.", "configspace": "", "generation": 17, "fitness": 0.8438531942417492, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.009. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "4bc27641-63af-4ae6-aad7-174f7c1e0b4b", "metadata": {"aucs": [0.8530866380424894, 0.8318624742962093, 0.846610470386549], "final_y": [0.13661191744983925, 0.14059592808517063, 0.13592126757998924]}, "mutation_prompt": null}
{"id": "38fbac84-3316-4b70-8147-5361ce8a6f11", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n        velocity_decay = 0.98  # Line 1 changed\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = (0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)) * velocity_decay  # Line 2 changed\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate +\n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n                elif np.linalg.norm(self.velocity[i]) < 0.5:  # Line 3 changed\n                    self.adaptive_lr[i] *= 1.05  # Line 4 changed\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improve convergence by introducing a velocity decay factor and a feedback loop for adaptive learning rates based on swarm stability.", "configspace": "", "generation": 17, "fitness": 0.8475364760192621, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.015. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "4bc27641-63af-4ae6-aad7-174f7c1e0b4b", "metadata": {"aucs": [0.8657787574116189, 0.830107949770401, 0.8467227208757664], "final_y": [0.12646886526472412, 0.14527899941729927, 0.13949260015765252]}, "mutation_prompt": null}
{"id": "8fe054ae-d2b2-4a68-a1d3-71e5e9ce045d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n                    self.synergy_factor = 0.5 + 0.5 * np.tanh(np.linalg.norm(global_best - personal_best[i]))  # Changed line\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm exploration by integrating a dynamic synergy factor based on velocity convergence.", "configspace": "", "generation": 17, "fitness": 0.8117395321998138, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.029. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "4bc27641-63af-4ae6-aad7-174f7c1e0b4b", "metadata": {"aucs": [0.8524022519798842, 0.7972779800386415, 0.7855383645809156], "final_y": [0.1355251536558264, 0.15224723852267208, 0.16038491968394475]}, "mutation_prompt": null}
{"id": "d734c38c-94a7-401a-82ed-f3c953e5b0bf", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (self.synergy_factor + 0.1 * np.sin(2 * np.pi * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.7, 1.3)\n                improvement_rate = (global_best_value - np.min(personal_best_value)) / (1 + evaluations)\n                velocity_magnitude = np.linalg.norm(self.velocity[i])  # Magnitude of velocity\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * (improvement_rate / (1 + velocity_magnitude)) + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if velocity_magnitude > 1.0:  # Adjust learning rate based on velocity magnitude\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance convergence by incorporating an adaptive learning rate and dynamic exploration informed by velocity magnitude trends.", "configspace": "", "generation": 17, "fitness": 0.825765319894046, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.042. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "4bc27641-63af-4ae6-aad7-174f7c1e0b4b", "metadata": {"aucs": [0.8802308145305342, 0.7781250137970794, 0.8189401313545246], "final_y": [0.1250824637033796, 0.1633414548499441, 0.14836026409549796]}, "mutation_prompt": null}
{"id": "031a0b0b-3158-40ec-9742-56ef94866952", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Changed from sin to cos\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))  # Changed from sin to cos\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce dynamic inertia weight and synergy factor modulation to enhance convergence performance.", "configspace": "", "generation": 18, "fitness": 0.851038429111799, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.023. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "8970c32b-d250-40e5-8802-910c91ea5091", "metadata": {"aucs": [0.8429903763071949, 0.8820403716463713, 0.8280845393818312], "final_y": [0.1406384588300088, 0.11973260930764362, 0.1456348166098389]}, "mutation_prompt": null}
{"id": "2260f541-dd5a-43e5-8c40-ec479a1604ac", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.7 + 0.2 * np.sin(np.pi * evaluations / self.budget)  # Changed\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.sin(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.04 * self.memory[i]) * dynamic_adjustment  # Changed\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm performance by refining synergy factors and improving velocity dynamics.", "configspace": "", "generation": 18, "fitness": 0.8339387064398194, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.015. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "8970c32b-d250-40e5-8802-910c91ea5091", "metadata": {"aucs": [0.8533223172765547, 0.8325876834076813, 0.8159061186352221], "final_y": [0.13540308487415498, 0.14220608847410565, 0.14572248745945604]}, "mutation_prompt": null}
{"id": "6e6430a9-f8dd-48fe-837c-b571e2abfff0", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.sin(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            self.synergy_factor *= 0.99  # Adaptive synergy factor\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] *= 0.9  # Memory decay\n                self.memory[i] += self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Refine exploration and exploitation balance by incorporating memory decay and adaptive synergy factor for improved convergence.", "configspace": "", "generation": 18, "fitness": 0.8133205641481149, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.013. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "8970c32b-d250-40e5-8802-910c91ea5091", "metadata": {"aucs": [0.8066711856347255, 0.8022226616912782, 0.8310678451183411], "final_y": [0.1528661533905602, 0.15056552534175616, 0.138682254526458]}, "mutation_prompt": null}
{"id": "34add5be-01ee-4b83-8929-ca801d9d3854", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.sin(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                momentum = 0.9 * self.velocity[i]\n                self.velocity[i] = (momentum + cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.3  # Adjusted learning rate multiplier\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Improve velocity update by introducing momentum and adjusting adaptive learning rates for enhanced convergence.", "configspace": "", "generation": 18, "fitness": 0.7901154448927925, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.030. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "8970c32b-d250-40e5-8802-910c91ea5091", "metadata": {"aucs": [0.8294784374585216, 0.7556383869314011, 0.7852295102884549], "final_y": [0.14659378602527473, 0.17327600296352796, 0.16027233059501644]}, "mutation_prompt": null}
{"id": "2fc09591-49a6-4413-82dd-4f35ac6d36e5", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.sin(np.pi * evaluations / self.budget)\n            # Updated: Introduce synergy learning modulation\n            synergy_learning = 1 + 0.1 * np.sin(2 * np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + synergy_learning * np.sin(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + synergy_learning * np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                # Updated: Incorporate memory-based velocity adaptation\n                velocity_memory = 0.1 * np.tanh(self.memory[i])\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    velocity_memory) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce adaptive synergy learning and memory-based velocity adaptation to enhance global convergence in the optimization process.", "configspace": "", "generation": 18, "fitness": 0.8215292207405861, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.038. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "8970c32b-d250-40e5-8802-910c91ea5091", "metadata": {"aucs": [0.8695594900737985, 0.8196994724097341, 0.7753286997382254], "final_y": [0.12909346774074315, 0.1427043575067687, 0.1653136804812374]}, "mutation_prompt": null}
{"id": "35347bfc-0966-4a19-ae07-be1456985a34", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            self.synergy_factor = 0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # Adjust synergy factor oscillation\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a variable synergy factor oscillation to enhance adaptability and convergence dynamics.", "configspace": "", "generation": 19, "fitness": 0.8048854660888249, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.038. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "031a0b0b-3158-40ec-9742-56ef94866952", "metadata": {"aucs": [0.8570652463672972, 0.7656944756298454, 0.7918966762693322], "final_y": [0.1314822786098453, 0.16811337113717062, 0.15748559295895515]}, "mutation_prompt": null}
{"id": "024d238a-e627-41b7-8d20-a6172f7d3073", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Changed from sin to cos\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))  # Changed from sin to cos\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget)**2)  # Introduced square to non-linearize\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce non-linear time-varying social coefficient for enhanced global search capability.", "configspace": "", "generation": 19, "fitness": 0.8134929845452682, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.039. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "031a0b0b-3158-40ec-9742-56ef94866952", "metadata": {"aucs": [0.8628334721060866, 0.8106676495841376, 0.7669778319455802], "final_y": [0.13284897895927328, 0.1483535369246486, 0.16891007118675916]}, "mutation_prompt": null}
{"id": "1443de62-41ba-4727-9c00-e9d1dbd55b56", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)  # Changed from sin to cos\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))  # Changed from sin to cos\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment / (1 + np.linalg.norm(self.velocity[i]))  # Normalized velocity update\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                # Adjust learning rate based on velocity trend\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce a normalized velocity update factor to enhance stability and convergence.", "configspace": "", "generation": 19, "fitness": 0.7942038037345397, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.055. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "031a0b0b-3158-40ec-9742-56ef94866952", "metadata": {"aucs": [0.8693444443795981, 0.7371500928330313, 0.7761168739909894], "final_y": [0.12131556567349222, 0.1803118477285417, 0.16622371414264592]}, "mutation_prompt": null}
{"id": "0df59c5b-eac4-49d1-a1e9-f4c6a1fc5eda", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                self.exploration_weight = 0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # Added line\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce adaptive exploration weight modulation to enhance global search adaptability.", "configspace": "", "generation": 19, "fitness": 0.8205241740138808, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.036. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "031a0b0b-3158-40ec-9742-56ef94866952", "metadata": {"aucs": [0.8543513114772074, 0.7712738301283909, 0.8359473804360439], "final_y": [0.13476610505829278, 0.1674623225689471, 0.14064462893952423]}, "mutation_prompt": null}
{"id": "caeabe72-c9c8-4b18-bd95-8c908d16ef1b", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 2.0)  # Changed upper bound from 1.5 to 2.0\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n\n                # Introduce self-adaptive mutation\n                mutation_factor = np.random.normal(0, 1, self.dim)\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate +\n                                    0.05 * self.memory[i] + 0.01 * mutation_factor) * dynamic_adjustment\n\n                self.memory[i] = self.velocity[i]\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce self-adaptive mutation and synergy-enhanced learning to optimize convergence and diversity in swarm-based optimization.", "configspace": "", "generation": 19, "fitness": 0.8103228638717243, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.012. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "031a0b0b-3158-40ec-9742-56ef94866952", "metadata": {"aucs": [0.7971942206449769, 0.8077496219848167, 0.8260247489853796], "final_y": [0.15785390397258836, 0.15463108103327527, 0.14327139803042455]}, "mutation_prompt": null}
{"id": "c4412a67-2e2c-43a2-9168-a2dfd3b65e67", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                local_search_factor = np.random.uniform(0.1, 0.3, self.dim)  # Added line\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.velocity[i] += local_search_factor * (personal_best[i] - swarm[i])  # Added line\n                self.memory[i] = self.velocity[i]\n                self.exploration_weight = 0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n            if evaluations % (self.budget // 10) == 0:  # Added line\n                np.random.shuffle(swarm)  # Added line\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm efficiency by incorporating local search and adaptive diversity scaling mechanisms.", "configspace": "", "generation": 20, "fitness": 0.8279532992107365, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.040. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "0df59c5b-eac4-49d1-a1e9-f4c6a1fc5eda", "metadata": {"aucs": [0.8836293092967531, 0.7936206688342481, 0.8066099195012081], "final_y": [0.12010000437215529, 0.15466879525657007, 0.14797761539038234]}, "mutation_prompt": null}
{"id": "bc0e5356-c8f0-4927-bd90-4af3975b4c44", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.55 + 0.35 * np.cos(np.pi * evaluations / self.budget)  # Modified line\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                self.exploration_weight = 0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce adaptive inertia to dynamically balance exploration and exploitation.", "configspace": "", "generation": 20, "fitness": 0.786539570373407, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.017. And the mean value of best solutions found was 0.162 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "0df59c5b-eac4-49d1-a1e9-f4c6a1fc5eda", "metadata": {"aucs": [0.8110738697031449, 0.7761327248846687, 0.7724121165324077], "final_y": [0.15313624763105038, 0.16501603919983565, 0.16656121224742237]}, "mutation_prompt": null}
{"id": "1ffc8d0c-5626-4e26-ad53-49b488e164fe", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.velocity[i] /= np.linalg.norm(self.velocity[i]) + 1e-8  # Normalizing velocity\n                self.memory[i] = self.velocity[i]\n                self.exploration_weight = 0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # Added line\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce velocity normalization to ensure stability in the exploration phase. ", "configspace": "", "generation": 20, "fitness": 0.8072030872866535, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.050. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "0df59c5b-eac4-49d1-a1e9-f4c6a1fc5eda", "metadata": {"aucs": [0.8327109427344805, 0.7378822445108155, 0.8510160746146647], "final_y": [0.12860281472104895, 0.18015652234886204, 0.13607584282987872]}, "mutation_prompt": null}
{"id": "4746abf4-6ff0-40a4-a5f8-dd81c2f891cc", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.05 * self.memory[i]) * dynamic_adjustment\n                self.memory[i] = self.velocity[i]\n                self.exploration_weight = 0.5 + 0.5 * np.sin(np.pi * evaluations / self.budget)  # Added line\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.2\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n                \n                self.velocity[i] *= 0.99  # Velocity damping\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Introduce velocity damping to improve convergence stability of swarm optimization.", "configspace": "", "generation": 20, "fitness": 0.8044630830141798, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.013. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "0df59c5b-eac4-49d1-a1e9-f4c6a1fc5eda", "metadata": {"aucs": [0.8214837764775671, 0.8033280265519862, 0.7885774460129862], "final_y": [0.14746154227385755, 0.1526315562131747, 0.15603706541654982]}, "mutation_prompt": null}
{"id": "695e2d58-a73a-434c-a2c4-377f5c82b35d", "solution": "import numpy as np\n\nclass EnhancedAdaptiveSwarmGradientDescent:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 + 2 * int(np.sqrt(dim))\n        self.velocity = np.zeros((self.population_size, dim))\n        self.adaptive_lr = np.ones(self.population_size)\n        self.memory = np.random.uniform(-0.1, 0.1, (self.population_size, dim))\n        self.exploration_weight = np.random.uniform(0.2, 0.8)\n        self.synergy_factor = 0.5 + np.random.uniform(0, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        personal_best = swarm.copy()\n        personal_best_value = np.array([func(x) for x in swarm])\n        global_best = personal_best[np.argmin(personal_best_value)]\n        global_best_value = np.min(personal_best_value)\n\n        evaluations = self.population_size\n\n        while evaluations < self.budget:\n            adaptive_factor = 1 - evaluations / self.budget\n            inertia_weight = 0.6 + 0.3 * np.cos(np.pi * evaluations / self.budget)\n            cognitive_coeff = 1.5 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n            social_coeff = 1.9 * adaptive_factor * (1 + np.cos(self.synergy_factor * evaluations / self.budget))\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.random(self.dim), np.random.random(self.dim)\n                stochastic_factor = np.random.uniform(-0.1, 0.1, self.dim)\n                dynamic_adjustment = np.random.uniform(0.5, 1.5)\n                improvement_rate = np.exp((global_best_value - np.min(personal_best_value)) / (1 + evaluations))\n                self.velocity[i] = (inertia_weight * self.velocity[i] +\n                                    cognitive_coeff * r1 * (personal_best[i] - swarm[i]) +\n                                    social_coeff * r2 * (global_best - swarm[i]) +\n                                    self.exploration_weight * stochastic_factor * improvement_rate + \n                                    0.1 * self.memory[i]) * dynamic_adjustment  # Increased memory impact\n                self.memory[i] = self.velocity[i]\n                self.exploration_weight = 0.6 + 0.4 * np.sin(np.pi * evaluations / self.budget)  # Adjusted oscillation range\n                swarm[i] += self.adaptive_lr[i] * self.velocity[i]\n                swarm[i] = np.clip(swarm[i], lb, ub)\n\n                f_value = func(swarm[i])\n                evaluations += 1\n                if f_value < personal_best_value[i]:\n                    personal_best[i] = swarm[i]\n                    personal_best_value[i] = f_value\n                    self.adaptive_lr[i] *= 1.25  # Increased adaptation rate\n                else:\n                    self.adaptive_lr[i] *= 0.8\n\n                if np.linalg.norm(self.velocity[i]) > 1.0:\n                    self.adaptive_lr[i] *= 0.95\n\n                if f_value < global_best_value:\n                    global_best = swarm[i]\n                    global_best_value = f_value\n\n                if evaluations >= self.budget:\n                    break\n\n        return global_best, global_best_value", "name": "EnhancedAdaptiveSwarmGradientDescent", "description": "Enhance swarm synergy and velocity adaptation to improve convergence efficiency.", "configspace": "", "generation": 20, "fitness": 0.7822962279844595, "feedback": "The algorithm EnhancedAdaptiveSwarmGradientDescent got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.019. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "0df59c5b-eac4-49d1-a1e9-f4c6a1fc5eda", "metadata": {"aucs": [0.7893447587314886, 0.8016040987654481, 0.7559398264564422], "final_y": [0.15871181227799358, 0.1502988270046144, 0.17226703254806552]}, "mutation_prompt": null}
