{"id": "f480eedb-bc61-4ab8-8483-df86a832349d", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_index = np.argmin(fitness)\n        best_solution = population[best_index]\n        best_fitness = fitness[best_index]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            # Differential Evolution-like Mutation\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + 0.8 * (x2 - x3)\n                mutant = np.clip(mutant, lb, ub)\n                \n                # Simulated Annealing-like Probabilistic Acceptance\n                trial = mutant if np.random.rand() < 0.8 else population[i]\n                trial_fitness = func(trial)\n                evaluations += 1\n                \n                if trial_fitness < fitness[i] or np.exp((fitness[i] - trial_fitness) / self.temperature) > np.random.rand():\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < best_fitness:\n                    best_solution = trial\n                    best_fitness = trial_fitness\n            \n            self.temperature *= self.cooling_rate\n        \n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A dynamic hybrid metaheuristic combining differential evolution and simulated annealing, leveraging adaptive strategy selection for effective search space exploration and exploitation.", "configspace": "", "generation": 0, "fitness": 0.7631479478855381, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.763 with standard deviation 0.028. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7257542627259739, 0.76890311738062, 0.7947864635500201], "final_y": [0.17995525605197993, 0.1702433386818767, 0.15284103542252336]}, "mutation_prompt": null}
{"id": "acd1bf1f-0609-4bff-8534-d51609a78e39", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Utilize an adaptive differential evolution strategy that dynamically adjusts mutation and crossover rates based on historical performance to efficiently explore and exploit the search space.", "configspace": "", "generation": 0, "fitness": 0.8041128805519834, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.020. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7764220521859247, 0.8124774164507056, 0.8234391730193198], "final_y": [0.14538697914360765, 0.14666894883862325, 0.13150850373624245]}, "mutation_prompt": null}
{"id": "a71efeca-d0db-4b60-a30e-e904da0b3232", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.temperature = 1.0\n        self.cooling_rate = 0.995  # Change 1\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_index = np.argmin(fitness)\n        best_solution = population[best_index]\n        best_fitness = fitness[best_index]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            # Differential Evolution-like Mutation\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                F = np.random.uniform(0.5, 1.0)  # Change 2\n                mutant = x1 + F * (x2 - x3)\n                mutant = np.clip(mutant, lb, ub)\n                \n                # Simulated Annealing-like Probabilistic Acceptance\n                trial = mutant if np.random.rand() < 0.8 else population[i]\n                trial_fitness = func(trial)\n                evaluations += 1\n                \n                if trial_fitness < fitness[i] or np.exp((fitness[i] - trial_fitness) / self.temperature) > np.random.rand():\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < best_fitness:\n                    best_solution = trial\n                    best_fitness = trial_fitness\n            \n            self.temperature *= self.cooling_rate  # Change 3\n        \n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A refined dynamic hybrid metaheuristic that enhances exploration by incorporating randomness into the mutation factor and scales temperature more gradually for better convergence.", "configspace": "", "generation": 1, "fitness": 0.7844672542286381, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.026. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "f480eedb-bc61-4ab8-8483-df86a832349d", "metadata": {"aucs": [0.7635380657328603, 0.76890311738062, 0.8209605795724342], "final_y": [0.14825345182773164, 0.1702433386818767, 0.14014632426775675]}, "mutation_prompt": null}
{"id": "758c8310-f5b7-4c35-812f-75df676abf3f", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.temperature = 1.0\n        self.cooling_rate = 0.99\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_index = np.argmin(fitness)\n        best_solution = population[best_index]\n        best_fitness = fitness[best_index]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            # Differential Evolution-like Mutation\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                # Change 1: Updated mutation strategy with control parameter\n                mutant = x1 + 0.9 * (x2 - x3) \n                mutant = np.clip(mutant, lb, ub)\n                \n                # Simulated Annealing-like Probabilistic Acceptance\n                # Change 2: Updated crossover probability for trial solution\n                trial = mutant if np.random.rand() < 0.9 else population[i]\n                trial_fitness = func(trial)\n                evaluations += 1\n                \n                if trial_fitness < fitness[i] or np.exp((fitness[i] - trial_fitness) / self.temperature) > np.random.rand():\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < best_fitness:\n                    best_solution = trial\n                    best_fitness = trial_fitness\n            \n            self.temperature *= self.cooling_rate\n        \n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved Hybrid Metaheuristic Optimizer with enhanced mutation and crossover strategy for better exploration-exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.7576870937771796, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.758 with standard deviation 0.008. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f480eedb-bc61-4ab8-8483-df86a832349d", "metadata": {"aucs": [0.7506693000748472, 0.76890311738062, 0.7534888638760713], "final_y": [0.17224578503556998, 0.1702433386818767, 0.1760141326302096]}, "mutation_prompt": null}
{"id": "f4bea5c6-53cd-489b-ae50-3610de5a55d2", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n            \n            # Adjust population size based on convergence\n            if len(self.history) > 10 and np.std(self.history[-10:]) < 0.001:\n                pop_size = max(5, int(0.9 * pop_size))  # Reduce population size\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improved Adaptive Differential Evolution with dynamic population size adjustment based on convergence progress.", "configspace": "", "generation": 1, "fitness": 0.8023837159376722, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.019. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "acd1bf1f-0609-4bff-8534-d51609a78e39", "metadata": {"aucs": [0.7786342223612244, 0.8030059718917031, 0.8255109535600895], "final_y": [0.1544977084146557, 0.1501193551858956, 0.13859715467445632]}, "mutation_prompt": null}
{"id": "05373a6b-9bd4-4399-bc91-4667a7d0eed1", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.temperature = 1.0\n        self.cooling_rate = 0.95  # Reduced cooling rate for slower temperature decrease\n        self.mutation_scale = 0.8\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        best_index = np.argmin(fitness)\n        best_solution = population[best_index]\n        best_fitness = fitness[best_index]\n        \n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            # Differential Evolution-like Mutation\n            for i in range(self.population_size):\n                indices = np.random.choice(self.population_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                self.mutation_scale = np.random.uniform(0.5, 1.0)  # Self-adaptive mutation scale\n                mutant = x1 + self.mutation_scale * (x2 - x3)\n                mutant = np.clip(mutant, lb, ub)\n                \n                # Simulated Annealing-like Probabilistic Acceptance\n                trial = mutant if np.random.rand() < 0.9 else population[i]  # Increased trial acceptance probability\n                trial_fitness = func(trial)\n                evaluations += 1\n                \n                if trial_fitness < fitness[i] or np.exp((fitness[i] - trial_fitness) / self.temperature) > np.random.rand():\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                \n                if trial_fitness < best_fitness:\n                    best_solution = trial\n                    best_fitness = trial_fitness\n            \n            self.temperature *= self.cooling_rate\n        \n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimization leveraging self-adaptive mutation scales and dynamic cooling for improved exploration-exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.796478115668157, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.016. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "f480eedb-bc61-4ab8-8483-df86a832349d", "metadata": {"aucs": [0.7949725923459444, 0.7773624410086708, 0.8170993136498557], "final_y": [0.1502368759229108, 0.16576403996236455, 0.14527234872025363]}, "mutation_prompt": null}
{"id": "a2b38d4c-3951-42fb-9010-8994641f2b25", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                pop_size = max(5 * self.dim, pop_size + int(np.sign(recent_perf[-1])))  # Changed line\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Incorporate an adaptive population size mechanism to improve search efficiency and convergence speed.", "configspace": "", "generation": 1, "fitness": 0.8076225803908675, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.021. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "acd1bf1f-0609-4bff-8534-d51609a78e39", "metadata": {"aucs": [0.7844604659115639, 0.8343857618988111, 0.8040215133622273], "final_y": [0.1412122752143754, 0.13364658718157596, 0.14312957727902442]}, "mutation_prompt": null}
{"id": "3e6d6281-58ea-4794-ad55-65fe78b23550", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)  # Modified line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Refine the adaptive strategy by slightly modifying the mutation factor F using a new coefficient to enhance convergence.", "configspace": "", "generation": 1, "fitness": 0.8159859413675349, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.014. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "acd1bf1f-0609-4bff-8534-d51609a78e39", "metadata": {"aucs": [0.7995259681347964, 0.8331236266648159, 0.8153082293029922], "final_y": [0.13169579171293289, 0.13374269485715728, 0.13052373948162777]}, "mutation_prompt": null}
{"id": "eb590aea-3649-42a6-a17a-64f089d740a5", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                pop_size = max(4 * self.dim, int(pop_size * (0.9 if recent_perf[-1] < 0 else 1.1)))\n                population = np.resize(population, (pop_size, self.dim))  # Resize population accordingly\n                fitness = np.resize(fitness, pop_size)  # Adjust fitness array size\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance Adaptive Differential Evolution by incorporating an adaptive population size based on convergence trends to better balance exploration and exploitation.", "configspace": "", "generation": 1, "fitness": 0.8061659415944114, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.004. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "acd1bf1f-0609-4bff-8534-d51609a78e39", "metadata": {"aucs": [0.8023986078045799, 0.8109342668142449, 0.8051649501644096], "final_y": [0.1407394995462834, 0.1423486212027858, 0.1392212559808691]}, "mutation_prompt": null}
{"id": "a0533044-1f0a-4035-b675-9e4ba38dc94f", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population using Latin Hypercube Sampling\n        population = lb + (ub - lb) * self.latin_hypercube_sampling(pop_size, self.dim)\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]\n    \n    def latin_hypercube_sampling(self, samples, dimensions):\n        # Latin Hypercube Sampling implementation\n        result = np.zeros((samples, dimensions))\n        for d in range(dimensions):\n            perm = np.random.permutation(samples) + 1\n            result[:, d] = (perm - np.random.rand(samples)) / samples\n        return result", "name": "AdaptiveDifferentialEvolution", "description": "Enhance population initialization by using a Latin Hypercube Sampling method to improve diversity and coverage of the search space.", "configspace": "", "generation": 1, "fitness": 0.8053917807483107, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.013. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "acd1bf1f-0609-4bff-8534-d51609a78e39", "metadata": {"aucs": [0.7878138142472574, 0.818479135882218, 0.8098823921154569], "final_y": [0.1350500074930926, 0.13933028588455454, 0.14472889183538162]}, "mutation_prompt": null}
{"id": "71307019-2e9d-4008-9f8c-689bf8fd6a2b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance the selection mechanism by introducing elitism to retain the best solutions from the population for robust performance improvement.", "configspace": "", "generation": 1, "fitness": 0.8096149609210382, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.013. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "acd1bf1f-0609-4bff-8534-d51609a78e39", "metadata": {"aucs": [0.7927752336687671, 0.8112356740571096, 0.8248339750372379], "final_y": [0.14177377418940262, 0.14405110777303076, 0.1343100447031207]}, "mutation_prompt": null}
{"id": "b795c412-6ff6-4cda-b76b-56d6a6020688", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                pop_size = int(np.clip(10 * self.dim * (1 + 0.1 * np.sign(recent_perf[-1])), self.dim, 20 * self.dim))  # Dynamic pop size adjustment\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance the Adaptive Differential Evolution by incorporating a dynamic population size strategy to improve exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.8025956573199924, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.004. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "acd1bf1f-0609-4bff-8534-d51609a78e39", "metadata": {"aucs": [0.7992966517280893, 0.8080526567557856, 0.8004376634761023], "final_y": [0.12715416699571402, 0.13591490762487712, 0.13591687364311145]}, "mutation_prompt": null}
{"id": "4bc152bf-ebd2-4b8d-8207-fc5c4b065fae", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic population size adaptation mechanism to balance exploration and exploitation by adjusting the population size based on recent performance trends.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "3e6d6281-58ea-4794-ad55-65fe78b23550", "metadata": {}, "mutation_prompt": null}
{"id": "b63c985d-4455-4453-8e35-5b27929a610d", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n\n            # Adaptive mutation factor\n            F_adapt = np.random.uniform(0.4, 0.9) if evals < self.budget / 2 else np.random.uniform(0.2, 0.6)\n\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F_adapt * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance diversification and convergence by dynamically adjusting mutation factors and optimizing selection pressure.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "71307019-2e9d-4008-9f8c-689bf8fd6a2b", "metadata": {}, "mutation_prompt": null}
{"id": "cbec4cdd-2c17-4fd2-8c40-0b748c0507d6", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                elite_rate = 0.1 if len(self.history) < 5 else 0.05 + 0.05 * (1.0 - np.sign(np.diff(self.history[-5:])[-1]))  # Dynamic adjustment\n                population[np.random.choice(np.argsort(fitness)[-int(elite_rate * pop_size):])] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance elitism by implementing a dynamic adjustment to the elitism rate based on recent performance trends to improve convergence.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "71307019-2e9d-4008-9f8c-689bf8fd6a2b", "metadata": {}, "mutation_prompt": null}
{"id": "093ba426-9c8c-47a9-93a4-de1516625352", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)  # Modified line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.05 * np.sign(recent_perf[-1])), 1.0)  # Modified line\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Modify the crossover probability CR during adaptation to enhance exploration and convergence.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "3e6d6281-58ea-4794-ad55-65fe78b23550", "metadata": {}, "mutation_prompt": null}
{"id": "7f6158e9-9678-41c9-8882-064c69fc670f", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.2 * np.sign(recent_perf[-1])), 1.0)  # Adjust CR dynamically based on recent performance\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Refine adaptation by dynamically adjusting the crossover probability (CR) based on recent performance history.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "71307019-2e9d-4008-9f8c-689bf8fd6a2b", "metadata": {}, "mutation_prompt": null}
{"id": "85897669-d1f1-4f58-b3f1-c1826b3d062c", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n            diversity = np.mean(np.std(population, axis=0))  # Measure population diversity\n            CR = max(0.1, 1 - diversity)  # Dynamically adjust CR based on diversity\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce dynamic scaling of crossover probability based on population diversity to enhance exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "71307019-2e9d-4008-9f8c-689bf8fd6a2b", "metadata": {}, "mutation_prompt": null}
{"id": "5e89d8ca-f0c3-41cf-9bdd-577578456ada", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                diversity_factor = np.std(population, axis=0).mean() / (ub - lb).mean()  # New line\n                F_dynamic = F * (1 + diversity_factor)  # New line\n                mutant = np.clip(a + 0.6 * F_dynamic * (b - c), lb, ub)  # Modified line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic adjustment of mutation factor F based on population diversity to enhance convergence.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "3e6d6281-58ea-4794-ad55-65fe78b23550", "metadata": {}, "mutation_prompt": null}
{"id": "ca3c52e2-8250-4a85-abd1-429750a8dd58", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n            # Dynamic population size adjustment\n            if evals % (self.budget // 10) == 0:\n                pop_size = max(4, pop_size + np.random.randint(-3, 4))\n                population = np.append(population, np.random.uniform(lb, ub, (pop_size - len(population), self.dim)), axis=0)\n                fitness = np.append(fitness, [func(ind) for ind in population[len(fitness):]])\n                \n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Integrate a dynamic population size adjustment mechanism to balance exploration and exploitation more effectively.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "71307019-2e9d-4008-9f8c-689bf8fd6a2b", "metadata": {}, "mutation_prompt": null}
{"id": "df5d19ad-d9f0-4a41-a832-f78cbddd730e", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n            \n            # Adaptive Population Sizing Start\n            if len(self.history) > 5 and np.all(np.diff(self.history[-5:]) > 0):\n                pop_size = max(4, pop_size - 1)\n                population = population[:pop_size]\n                fitness = fitness[:pop_size]\n            # Adaptive Population Sizing End\n            \n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                \n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive population sizing by reducing the population size based on convergence trends to improve the exploitation phase.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "71307019-2e9d-4008-9f8c-689bf8fd6a2b", "metadata": {}, "mutation_prompt": null}
{"id": "d3a91770-c476-4573-b16e-5e5e62b1e0c2", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n\n        evals = pop_size\n\n        # Evolutionary loop\n        while evals < self.budget:\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)  # Modified line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                evals += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                \n            # Dynamic population size adjustment\n            pop_size = max(4, int((self.budget - evals) / 100))  # New line\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce dynamic population size adjustment based on convergence metrics to enhance solution diversity and prevent premature convergence.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('negative dimensions are not allowed').", "error": "ValueError('negative dimensions are not allowed')", "parent_id": "3e6d6281-58ea-4794-ad55-65fe78b23550", "metadata": {}, "mutation_prompt": null}
{"id": "3e52b2db-e844-4060-9f56-122273490dc4", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)  # Fix: Ensure consistent array selection\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Refine mutation strategy by ensuring array selection for consistent length between operations.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "4bc152bf-ebd2-4b8d-8207-fc5c4b065fae", "metadata": {}, "mutation_prompt": null}
{"id": "2d154f7a-e4d0-4f8b-b8e9-dd8d7b51ebba", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a weighted averaging mechanism to adapt differential weight and crossover probability based on recent performance trends.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "4bc152bf-ebd2-4b8d-8207-fc5c4b065fae", "metadata": {}, "mutation_prompt": null}
{"id": "ef5bd6df-4c1a-4062-ba41-1065ecdd1121", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * (np.diff(self.history[-2:])[-1] > 0))))  # Correct array handling\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * (recent_perf[-1] > 0)), 1.0)  # Adaptation based on performance\n                CR = min(max(0.1, CR + 0.1 * (recent_perf[-1] > 0)), 1.0)  # Adaptation based on performance\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Refine the dynamic adaptation mechanism by ensuring correct array handling for population updates and adapting control parameters based on historical performance trends.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index -1 is out of bounds for axis 0 with size 0').", "error": "IndexError('index -1 is out of bounds for axis 0 with size 0')", "parent_id": "4bc152bf-ebd2-4b8d-8207-fc5c4b065fae", "metadata": {}, "mutation_prompt": null}
{"id": "853909d4-b93e-42d1-b02a-dffb0429b304", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F = 0.8 * F if np.diff(self.history[-2:]).sum() < 0 else 0.6 * F  # Adjust F dynamically\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Implement dynamic adjustment of the scaling factor to improve convergence and exploration balance.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "4bc152bf-ebd2-4b8d-8207-fc5c4b065fae", "metadata": {}, "mutation_prompt": null}
{"id": "e3891097-0543-468e-997d-f95ed5ccc612", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * (np.diff(self.history[-2:]).flatten()[0] > 0))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * (recent_perf[-1] > 0)), 1.0)\n                CR = min(max(0.1, CR + 0.1 * (recent_perf[-1] > 0)), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Adjust the mutation factor dynamically based on recent performance trends and fix an exception in the dynamic adaptation mechanism.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 0 is out of bounds for axis 0 with size 0').", "error": "IndexError('index 0 is out of bounds for axis 0 with size 0')", "parent_id": "4bc152bf-ebd2-4b8d-8207-fc5c4b065fae", "metadata": {}, "mutation_prompt": null}
{"id": "f017e6ca-3554-4df9-a22a-7b2d42b5e9eb", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n\n            # Competition-based adaptation for mutation factor\n            F_adapt = np.random.choice([0.4, 0.5, 0.6, 0.7, 0.8, 0.9], p=[0.1, 0.15, 0.2, 0.2, 0.15, 0.1])\n\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F_adapt * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce competition-based adaptation for mutation and crossover rates to enhance solution diversity and performance.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('probabilities do not sum to 1').", "error": "ValueError('probabilities do not sum to 1')", "parent_id": "b63c985d-4455-4453-8e35-5b27929a610d", "metadata": {}, "mutation_prompt": null}
{"id": "dc480356-b5b8-4a17-86f7-fdad4cf24e82", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)  # Ensure correct scaling\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Incorporate a safeguard in mutation to prevent potential TypeError by correctly calculating the `mutant` vector.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "4bc152bf-ebd2-4b8d-8207-fc5c4b065fae", "metadata": {}, "mutation_prompt": null}
{"id": "65c25c55-490d-4f72-83d8-1391a8632e80", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic population size adaptation mechanism to balance exploration and exploitation by adjusting the population size based on recent performance trends, with enhanced mutation strategy to avoid index error.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "4bc152bf-ebd2-4b8d-8207-fc5c4b065fae", "metadata": {}, "mutation_prompt": null}
{"id": "682357e4-b5ae-48bc-976f-246a472c1225", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(10, 10 * self.dim)  # Ensure pop_size is at least 10\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n\n            # Adaptive mutation factor\n            F_adapt = np.random.uniform(0.4, 0.9) if evals < self.budget / 2 else np.random.uniform(0.2, 0.6)\n\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F_adapt * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improve constraint handling by ensuring the adapted mutation factor F_adapt respects boundaries and adjust initialization for diversity.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "b63c985d-4455-4453-8e35-5b27929a610d", "metadata": {}, "mutation_prompt": null}
{"id": "6672d563-33f0-48d6-8159-d5f616628eea", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            best_idx = np.argmin(fitness)  # Elitism: Track the best solution\n\n            # Adaptive mutation factor\n            F_adapt = np.random.uniform(0.4, 0.9) if evals < self.budget / 2 else np.random.uniform(0.2, 0.6)\n\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F_adapt * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n            # Retain best individual using elitism\n            if fitness[best_idx] < fitness[np.argmin(fitness)]:\n                population[np.argmax(fitness)] = population[best_idx]\n                fitness[np.argmax(fitness)] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                CR = min(max(0.1, CR + 0.05 * np.sign(recent_perf[-1])), 1.0)  # Reduced CR adjustment factor\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance diversification and convergence by dynamically adjusting mutation factors, optimizing selection pressure, and improving adaptive strategies for crossover probability.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "b63c985d-4455-4453-8e35-5b27929a610d", "metadata": {}, "mutation_prompt": null}
{"id": "514e67ae-6d1e-4b3e-8886-066b60a80780", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.4, F + 0.1 * np.sign(recent_perf[-1])), 0.9)  # Adjust F range \n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Use adaptive population size and maintenance of historical performance trends to improve differential evolution efficiency.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "2d154f7a-e4d0-4f8b-b8e9-dd8d7b51ebba", "metadata": {}, "mutation_prompt": null}
{"id": "6883ce51-b3ff-4ed3-b4fa-ce064b91ba42", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(np.sum(recent_perf[-1]))), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(np.sum(recent_perf[-1]))), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance selection strategy by correcting index selection logic to prevent TypeError in dynamic population resizing.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "2d154f7a-e4d0-4f8b-b8e9-dd8d7b51ebba", "metadata": {}, "mutation_prompt": null}
{"id": "e3cb5199-9b36-48c3-8eda-15cc8b3b90bb", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improve array indexing by converting arrays to scalars to resolve TypeError.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "2d154f7a-e4d0-4f8b-b8e9-dd8d7b51ebba", "metadata": {}, "mutation_prompt": null}
{"id": "4cf26db9-83e9-4a39-84b0-be509520ac38", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)  # Fix: Ensure consistent array selection\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance the mutation strategy in Adaptive Differential Evolution by accurately managing array operations and ensuring effective adaptation based on historical performance.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "3e52b2db-e844-4060-9f56-122273490dc4", "metadata": {}, "mutation_prompt": null}
{"id": "5a55d0be-0681-4137-a7ed-9a47d30ef052", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            # Adjusted dynamic population adaptation for greater stability\n            pop_size = max(5, int(initial_pop_size * (0.95 + 0.05 * np.sign(np.diff(self.history[-2:])))))  \n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Utilize adaptive differential evolution with a dynamic population size adjustment strategy based on recent performance trends and mutation refinement.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "2d154f7a-e4d0-4f8b-b8e9-dd8d7b51ebba", "metadata": {}, "mutation_prompt": null}
{"id": "e982972e-45d7-44e8-aacd-e0a82e64f21d", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = [np.nan]  # Initialize with NaN for consistent array length\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improve error handling by ensuring historical performance data is initialized correctly to avoid indexing error.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('cannot convert float NaN to integer').", "error": "ValueError('cannot convert float NaN to integer')", "parent_id": "2d154f7a-e4d0-4f8b-b8e9-dd8d7b51ebba", "metadata": {}, "mutation_prompt": null}
{"id": "61d560cd-d5eb-447f-95bb-76f7e3f6a109", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)  # Ensure consistent array selection\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a scaling factor to improve precision in differential mutation to avoid conversion errors.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('only length-1 arrays can be converted to Python scalars').", "error": "TypeError('only length-1 arrays can be converted to Python scalars')", "parent_id": "3e52b2db-e844-4060-9f56-122273490dc4", "metadata": {}, "mutation_prompt": null}
{"id": "4d09fc1b-5748-45c0-b3aa-0e228b6e7f31", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (1.1 if len(self.history) < 2 else 0.9 + 0.1 * np.sign(np.diff(self.history[-2:])))))  # Fixed adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)  # Fix: Ensure consistent array selection\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)\n                CR = min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance population management by correcting the dynamic adaptation formula for pop_size.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 103 is out of bounds for axis 0 with size 100').", "error": "IndexError('index 103 is out of bounds for axis 0 with size 100')", "parent_id": "3e52b2db-e844-4060-9f56-122273490dc4", "metadata": {}, "mutation_prompt": null}
{"id": "8137d2c8-de97-4e0f-ba12-c403a37e3a1d", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce correct index handling in the dynamic adaptation of population size to prevent errors in the evolutionary loop.", "configspace": "", "generation": 4, "fitness": 0.8216379001176003, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.010. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "2d154f7a-e4d0-4f8b-b8e9-dd8d7b51ebba", "metadata": {"aucs": [0.8102152424667297, 0.8200282973545583, 0.834670160531513], "final_y": [0.13751970945152592, 0.1370097923974859, 0.12854268544059844]}, "mutation_prompt": null}
{"id": "db6de032-0247-4e08-8615-e01047f4311b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Use historical trend analysis to dynamically adapt population size and mutation parameters in Differential Evolution.", "configspace": "", "generation": 4, "fitness": 0.83345864594023, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.020. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "2d154f7a-e4d0-4f8b-b8e9-dd8d7b51ebba", "metadata": {"aucs": [0.8053844599602645, 0.8489936631731263, 0.8459978146872992], "final_y": [0.13636860007690388, 0.12746002149308533, 0.1290352235097456]}, "mutation_prompt": null}
{"id": "dcda5545-a7e5-4a75-8943-35828f2644af", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + np.tanh(self.history[-1] - self.history[-2]))  # Adaptive scaling for F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance Differential Evolution by introducing adaptive scaling for the differential weight and dynamic trial vector selection based on recent performance trends.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "db6de032-0247-4e08-8615-e01047f4311b", "metadata": {}, "mutation_prompt": null}
{"id": "47378183-2bca-4268-86a3-d1b83406d3d1", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance population diversity by adjusting mutation strategy based on convergence trends in Adaptive Differential Evolution.", "configspace": "", "generation": 5, "fitness": 0.8216909543217632, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.015. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "db6de032-0247-4e08-8615-e01047f4311b", "metadata": {"aucs": [0.8009852076061242, 0.8356658697150965, 0.8284217856440688], "final_y": [0.1399888692441461, 0.14127057411464772, 0.13781255477207843]}, "mutation_prompt": null}
{"id": "b7b7d9d5-c36e-4a0e-bedc-857ff9dc3c02", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.05 * np.sign(recent_perf[-1])), 1.0))  # Adjust CR adaptation\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic crossover probability adjustment based on historical trend analysis in Adaptive Differential Evolution.", "configspace": "", "generation": 5, "fitness": 0.8188023982541335, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.819 with standard deviation 0.003. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "db6de032-0247-4e08-8615-e01047f4311b", "metadata": {"aucs": [0.818166187144246, 0.8225538591322349, 0.8156871484859196], "final_y": [0.13808222267452397, 0.14030517797842368, 0.13230618396448945]}, "mutation_prompt": null}
{"id": "b8de532b-14a6-4869-a1f7-0653fd68fa4e", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive crossover probability based on population diversity to enhance exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.8339093379806405, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.019. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "8137d2c8-de97-4e0f-ba12-c403a37e3a1d", "metadata": {"aucs": [0.8124822309017127, 0.8299585948137911, 0.8592871882264178], "final_y": [0.13144444651674148, 0.13268220538296882, 0.12211438730967383]}, "mutation_prompt": null}
{"id": "06d0ec3b-1d31-403f-8371-3547358274a0", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, 0.8 * F + 0.1 * np.sign(recent_perf[-1])), 1.0))  # Changed line\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive scaling factor adjustment using exponential smoothing on historical performance data in Differential Evolution.", "configspace": "", "generation": 5, "fitness": 0.8072369599073218, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.013. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "db6de032-0247-4e08-8615-e01047f4311b", "metadata": {"aucs": [0.7920659168628316, 0.8237379884166135, 0.8059069744425206], "final_y": [0.13721428025771576, 0.1289400816763202, 0.13913603501243932]}, "mutation_prompt": null}
{"id": "9ebd6ab5-bce3-42b2-a7aa-1ef215392d93", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.95 + 0.05 * np.sign(np.diff(self.history[-2:])[0]))))  # Adjusted dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * F * (b - c), lb, ub)  # Adjusted mutation strategy\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Adjust dynamic adaptation logic and mutation strategy to enhance exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.8212998478733655, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.009. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "8137d2c8-de97-4e0f-ba12-c403a37e3a1d", "metadata": {"aucs": [0.8106218530553972, 0.8197847414595658, 0.8334929491051334], "final_y": [0.13504983874652887, 0.13161964361996814, 0.12964287745663883]}, "mutation_prompt": null}
{"id": "1104c646-57e5-4f38-bce8-5e4bf0424500", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.8 + 0.2 * np.tanh(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = F * (1 + np.random.rand() * 0.2)  # Adaptive mutation factor\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = np.clip(F + 0.1 * np.sign(recent_perf[-1]), 0.1, 1.0)\n                CR = np.clip(CR + 0.1 * np.sign(recent_perf[-1]), 0.1, 1.0)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance dynamic population size and parameter adaptation by leveraging adaptive learning rates and a refined mutation strategy.", "configspace": "", "generation": 5, "fitness": 0.8105995993154279, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.013. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "8137d2c8-de97-4e0f-ba12-c403a37e3a1d", "metadata": {"aucs": [0.7928904937328949, 0.8231280601405015, 0.8157802440728874], "final_y": [0.14038311194974118, 0.1329925252242945, 0.13203827692601455]}, "mutation_prompt": null}
{"id": "37681ee8-d720-4ca6-b883-a89c3c94bced", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0 + 0.1 * np.std(recent_perf)))  # Changed line to improve mutation factor adaptation\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance mutation factor adaptation based on fitness variability to increase convergence speed.", "configspace": "", "generation": 5, "fitness": 0.8144188355388783, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.013. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "8137d2c8-de97-4e0f-ba12-c403a37e3a1d", "metadata": {"aucs": [0.8042948872455319, 0.8325142487034076, 0.8064473706676952], "final_y": [0.13217920362757463, 0.1325360560071489, 0.13485802059512186]}, "mutation_prompt": null}
{"id": "013eabd0-ce7b-4df7-9b09-4a3600af8f0b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.15 * np.sign(recent_perf[-1])), 1.0))  # Slightly increase adaptation rate\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce dynamic differential weight adjustment based on historical performance trends to enhance convergence speed.", "configspace": "", "generation": 5, "fitness": 0.8126920766836992, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.016. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "8137d2c8-de97-4e0f-ba12-c403a37e3a1d", "metadata": {"aucs": [0.8048295383355103, 0.8348676716388552, 0.7983790200767324], "final_y": [0.1278194440147159, 0.13401208933557973, 0.1364621094537004]}, "mutation_prompt": null}
{"id": "e6726220-2d0d-48a2-b17e-0f70e841676c", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                modulation = np.var(fitness) / (np.mean(fitness) + 1e-7)  # Dynamic scaling\n                mutant = np.clip(a + modulation * F * (b - c), lb, ub)  # Adjusted F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Integrate a dynamic scaling factor based on fitness variance to improve exploration and exploitation balance in Differential Evolution.", "configspace": "", "generation": 5, "fitness": 0.8174261152949006, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.021. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "db6de032-0247-4e08-8615-e01047f4311b", "metadata": {"aucs": [0.7875175784038322, 0.8350141994854472, 0.8297465679954225], "final_y": [0.13847095996138092, 0.13264497400913777, 0.1303474504101101]}, "mutation_prompt": null}
{"id": "294d62d6-32bc-4217-a879-68d812f0c3ac", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        best_idx = np.argmin(fitness)\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i] or f_trial < fitness[best_idx]:  # Improved selection strategy\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best_idx = i\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improve selection by comparing trial vector with best-so-far individual in addition to its parent.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 84 is out of bounds for axis 0 with size 80').", "error": "IndexError('index 84 is out of bounds for axis 0 with size 80')", "parent_id": "47378183-2bca-4268-86a3-d1b83406d3d1", "metadata": {}, "mutation_prompt": null}
{"id": "791399d9-b991-483d-bbbd-9e3334eebe1a", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.6 * F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1]) * np.std(population, axis=0).mean()), 1.0))  # Modified line\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improve the adaptation strategy for F by incorporating both fitness trends and population diversity to balance exploration and exploitation.", "configspace": "", "generation": 6, "fitness": 0.8119674237776778, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.016. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "b8de532b-14a6-4869-a1f7-0653fd68fa4e", "metadata": {"aucs": [0.792222288872972, 0.8131657793626839, 0.8305142030973774], "final_y": [0.14968156791371423, 0.13492889217939996, 0.1369652141320491]}, "mutation_prompt": null}
{"id": "b77a6332-69c6-43b5-9c21-b6947605c4c1", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.9 * F * (b - c), lb, ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Fine-tune mutation strategy and incorporate elitism for improved convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 6, "fitness": 0.8152527948271434, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.006. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "b8de532b-14a6-4869-a1f7-0653fd68fa4e", "metadata": {"aucs": [0.8071174632553821, 0.822296103287518, 0.8163448179385301], "final_y": [0.1277788172473665, 0.12732662329358124, 0.13818883773634483]}, "mutation_prompt": null}
{"id": "c22e7597-fea4-4dd1-93b2-b6203e5889eb", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR * (0.9 + 0.1 * np.random.rand())  # Change: Stochastic element in CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce stochastic element in crossover probability to enhance exploration in Adaptive Differential Evolution.", "configspace": "", "generation": 6, "fitness": 0.8103833358519269, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.011. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "47378183-2bca-4268-86a3-d1b83406d3d1", "metadata": {"aucs": [0.8115455918523977, 0.8229463507415931, 0.7966580649617898], "final_y": [0.14182276963941376, 0.1306614012418561, 0.1365302114687622]}, "mutation_prompt": null}
{"id": "3f0a47c5-b83e-4490-b189-b99d8c8b7c36", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.95 + 0.05 * np.sign(np.diff(self.history[-2:]).sum()))))  # Change 1: Slightly increased pop_size adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.15 * np.sign(recent_perf[-1])), 1.0))  # Change 2: Increased adaptation rate for CR\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce dynamic scaling of population size and use an adaptive strategy for crossover probability to enhance diversity and convergence.", "configspace": "", "generation": 6, "fitness": 0.8073341851327559, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.003. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "47378183-2bca-4268-86a3-d1b83406d3d1", "metadata": {"aucs": [0.8106797789076599, 0.8073551591021706, 0.8039676173884374], "final_y": [0.14980105479608585, 0.13972990323083279, 0.14604971343701545]}, "mutation_prompt": null}
{"id": "7f16156e-150c-41ed-98ae-7138925b6dbd", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive crossover probability adjustment based on historical variance trends to balance exploration and exploitation.", "configspace": "", "generation": 6, "fitness": 0.8194633736111566, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.819 with standard deviation 0.016. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "47378183-2bca-4268-86a3-d1b83406d3d1", "metadata": {"aucs": [0.7991818647280494, 0.8212117943799015, 0.837996461725519], "final_y": [0.13775399446385694, 0.13459056214302767, 0.13040933535006816]}, "mutation_prompt": null}
{"id": "aa86206f-244c-4883-a31a-10522449c7d3", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                # Change 2: Apply recent performance trend to F_adapted\n                if len(self.history) > 5:\n                    recent_trend = np.mean(np.diff(self.history[-5:]))  # Calculate recent trend\n                    F_adapted *= 1 + 0.1 * np.sign(recent_trend)  # Adjust F based on trend\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive scaling factor variability based on recent performance trends to refine search efficiency.", "configspace": "", "generation": 6, "fitness": 0.8101090294754862, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.020. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "47378183-2bca-4268-86a3-d1b83406d3d1", "metadata": {"aucs": [0.782462847932056, 0.8210321386383174, 0.8268321018560852], "final_y": [0.14628415609415424, 0.1283239561015036, 0.14010471592304785]}, "mutation_prompt": null}
{"id": "7e5bcf23-1fd4-443b-8223-7415139f0b3d", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < max(0.1, min(0.9, CR * (0.8 + 0.4 * (fitness[i] > np.median(fitness)))))  # Modify CR based on individual success\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(recent_perf[-1])), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive crossover probability based on individual success to enhance convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 6, "fitness": 0.8098563458253188, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.002. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "47378183-2bca-4268-86a3-d1b83406d3d1", "metadata": {"aucs": [0.8126158744544888, 0.8084382956759382, 0.8085148673455292], "final_y": [0.13141788350844086, 0.14565983701785412, 0.13876583052580127]}, "mutation_prompt": null}
{"id": "10d3d9f7-38a5-46df-92e8-90b61ba788ce", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_dynamic = 0.5 + 0.3 * np.random.rand()  # Dynamic scaling factor for mutation\n                mutant = np.clip(a + F_dynamic * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance mutation strategy with a dynamic scaling factor to improve convergence speed.", "configspace": "", "generation": 6, "fitness": 0.810007645812358, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.021. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "b8de532b-14a6-4869-a1f7-0653fd68fa4e", "metadata": {"aucs": [0.7842224060193828, 0.8103338002688694, 0.8354667311488215], "final_y": [0.14634090925040644, 0.13862953064406713, 0.13901667076030877]}, "mutation_prompt": null}
{"id": "352b1b02-97e6-48a2-a2c7-fbc586d74426", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Adjust mutation factor dynamically based on recent performance trends to improve convergence speed and accuracy.", "configspace": "", "generation": 6, "fitness": 0.8112286847768958, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.013. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "b8de532b-14a6-4869-a1f7-0653fd68fa4e", "metadata": {"aucs": [0.7936667110667552, 0.8166724488977892, 0.8233468943661427], "final_y": [0.141773524646428, 0.1274055798288165, 0.13242701718451666]}, "mutation_prompt": null}
{"id": "949b49cc-7199-457e-bdf2-9767c76022a6", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 + int(0.1 * self.dim)  # Change 1: Dynamic adaptation of initial population size\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic adaptation of the initial population size based on problem dimensionality.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 8 is out of bounds for axis 0 with size 8').", "error": "IndexError('index 8 is out of bounds for axis 0 with size 8')", "parent_id": "7f16156e-150c-41ed-98ae-7138925b6dbd", "metadata": {}, "mutation_prompt": null}
{"id": "afc42131-028b-4980-8dd8-a0f31f969205", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = np.clip(F + 0.1 * np.sign(recent_perf[-1]), 0.1, 1.0)  # Adaptive F based on recent improvements\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce dynamic adjustment of differential weight (F) based on recent fitness improvements to enhance convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 7, "fitness": 0.8094894698092884, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.012. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "b77a6332-69c6-43b5-9c21-b6947605c4c1", "metadata": {"aucs": [0.8004285017639061, 0.8261163255069182, 0.8019235821570406], "final_y": [0.1252552622149814, 0.1257644963976069, 0.13900996319658077]}, "mutation_prompt": null}
{"id": "77533954-ef2c-4533-a42c-4c792fee282b", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.9 * F * (b - c), lb, ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(0.5 * (np.std(population, axis=0).mean() + np.mean(recent_perf)), 0.1, 1.0)  # Refined adaptive CR\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Refine the adaptive CR adjustment by incorporating both historical performance trends and population diversity to improve convergence.", "configspace": "", "generation": 7, "fitness": 0.8053686631405047, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.012. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "b77a6332-69c6-43b5-9c21-b6947605c4c1", "metadata": {"aucs": [0.7936718607399688, 0.8011395242167851, 0.8212946044647601], "final_y": [0.14098480271124536, 0.14894901098853142, 0.1324878998221951]}, "mutation_prompt": null}
{"id": "aacc9e2d-3323-447b-a67e-987f7cd2448c", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_variance = np.var(fitness)\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.8 + 0.2 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * F * (b - c), lb, ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < (CR + 0.1 * (pop_variance / (pop_variance + 1)))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive population size and dynamic crossover strategy based on fitness variance for enhanced exploration and convergence.", "configspace": "", "generation": 7, "fitness": 0.8119423262615805, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.005. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "b77a6332-69c6-43b5-9c21-b6947605c4c1", "metadata": {"aucs": [0.8063821581454337, 0.8177615852694535, 0.8116832353698541], "final_y": [0.13386754985798377, 0.13485910076276275, 0.13288127402064986]}, "mutation_prompt": null}
{"id": "ad500836-a4b8-4061-bfaf-57449d144dc2", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Diversity boost\n            if len(self.history) > 1 and self.history[-1] > self.history[-2]:\n                random_idx = np.random.randint(0, pop_size)\n                population[random_idx] = np.random.uniform(lb, ub, self.dim)\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce diversity boost by replacing a random individual from the population with a randomly generated new individual based on historical performance trends.", "configspace": "", "generation": 7, "fitness": 0.8038283694284107, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.005. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "7f16156e-150c-41ed-98ae-7138925b6dbd", "metadata": {"aucs": [0.8003046189788963, 0.8106914596022767, 0.8004890297040592], "final_y": [0.1354584377539857, 0.13961639425169448, 0.14239243736239093]}, "mutation_prompt": null}
{"id": "b6ce1a07-b83b-42f6-a545-0f7ffe839f90", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                dynamic_F = 0.5 + 0.4 * np.random.rand()  # Introduce dynamic scaling for mutation factor\n                mutant = np.clip(a + dynamic_F * (b - c), lb, ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic scaling factor for mutation to balance global and local search adaptively.", "configspace": "", "generation": 7, "fitness": 0.8029701549338396, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.024. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "b77a6332-69c6-43b5-9c21-b6947605c4c1", "metadata": {"aucs": [0.7716633292950783, 0.8071108750781587, 0.8301362604282818], "final_y": [0.15531406200959752, 0.13332006836136567, 0.13415388677659446]}, "mutation_prompt": null}
{"id": "8c98e9ae-399c-40c1-9afe-e8425c5e2ebb", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.9 * F * (b - c), lb, ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n            # Diversity preservation mechanism\n            unique_population = np.unique(population, axis=0)\n            if len(unique_population) < pop_size * 0.7:  # Maintain diversity\n                additional_individuals = np.random.uniform(lb, ub, (pop_size - len(unique_population), self.dim))\n                population = np.vstack((unique_population, additional_individuals))\n                fitness = np.array([func(ind) for ind in population])\n                evals += len(additional_individuals)\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(np.std(population, axis=0).mean(), 0.2, 1.0)  # Adjusted adaptive CR\n            \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a diversity preservation mechanism and adjust crossover probability dynamically to enhance convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 7, "fitness": 0.8031610930258228, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.011. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "b77a6332-69c6-43b5-9c21-b6947605c4c1", "metadata": {"aucs": [0.7963496563380896, 0.8189347278148229, 0.7941988949245558], "final_y": [0.13964359752673472, 0.1436390336238249, 0.14410650757997745]}, "mutation_prompt": null}
{"id": "05c09484-e56f-4a3b-a00c-9d73a77438c9", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce dynamic mutation scaling based on fitness improvement rate to enhance adaptability in Adaptive Differential Evolution.", "configspace": "", "generation": 7, "fitness": 0.8161555141865814, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.010. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "7f16156e-150c-41ed-98ae-7138925b6dbd", "metadata": {"aucs": [0.8033644908240336, 0.8290213401079253, 0.8160807116277851], "final_y": [0.13598445715687435, 0.1258519170503184, 0.1384040878006998]}, "mutation_prompt": null}
{"id": "8c6d83b2-2a80-4b51-b46b-c9d3d9bb5dd8", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 3: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 4: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 5: Re-evaluate fitness\n                no_improvement = 0  # Change 6: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance diversity by introducing a population reset mechanism based on stagnation detection and adapt mutation strategy dynamically.", "configspace": "", "generation": 7, "fitness": 0.8126661277447225, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.014. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "7f16156e-150c-41ed-98ae-7138925b6dbd", "metadata": {"aucs": [0.7938662709684434, 0.8152457227556431, 0.8288863895100811], "final_y": [0.1429437885945254, 0.13244454459329247, 0.13008490837402853]}, "mutation_prompt": null}
{"id": "fd961e24-6053-410b-a9b4-6131c3ef2dcf", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        best_idx = np.argmin(fitness)  # Track best individual\n        best_solution = population[best_idx].copy()  # Retain best solution\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            if len(self.history) > 1:\n                pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:])[0]))))  # Dynamic adaptation\n            else:\n                pop_size = initial_pop_size\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.9 * F * (b - c), lb, ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best_idx = i\n                        best_solution = population[best_idx].copy()  # Update best solution\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR = np.clip(np.std(population, axis=0).mean(), 0.1, 1.0)  # Adaptive CR based on diversity\n        \n        return best_solution", "name": "AdaptiveDifferentialEvolution", "description": "Introduce elitism by always retaining the best solution found so far to prevent regression in Adaptive Differential Evolution.", "configspace": "", "generation": 7, "fitness": 0.8101182154140579, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.014. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "b77a6332-69c6-43b5-9c21-b6947605c4c1", "metadata": {"aucs": [0.7911937762059138, 0.8246891744424972, 0.814471695593763], "final_y": [0.13834989116742347, 0.13532832204029066, 0.13763739367133843]}, "mutation_prompt": null}
{"id": "51513e70-6734-4951-967a-bca9faba428e", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c) + np.random.normal(0, 0.01, self.dim), lb, ub)  # Change 1: Add Gaussian mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce diversity by incorporating a Gaussian mutation with adaptive variance in Adaptive Differential Evolution.", "configspace": "", "generation": 8, "fitness": 0.8114235273344984, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.012. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "05c09484-e56f-4a3b-a00c-9d73a77438c9", "metadata": {"aucs": [0.7959226247982965, 0.8247847643196606, 0.8135631928855384], "final_y": [0.14291529018063986, 0.1320561126512224, 0.13039363087331746]}, "mutation_prompt": null}
{"id": "b8c97749-cf12-4a23-ba70-81e1f6362827", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 3: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 4: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 5: Re-evaluate fitness\n                no_improvement = 0  # Change 6: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Integrate a dynamic scaling factor for differential weight and crossover probability based on historical fitness variance to enhance convergence in Adaptive Differential Evolution.", "configspace": "", "generation": 8, "fitness": 0.7975896372983783, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.019. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "8c6d83b2-2a80-4b51-b46b-c9d3d9bb5dd8", "metadata": {"aucs": [0.7715691427568361, 0.8154754624784487, 0.8057243066598498], "final_y": [0.14080594437991223, 0.13987727540373096, 0.13343511537589214]}, "mutation_prompt": null}
{"id": "bd765b72-652e-440c-9d53-4897eea2bfcf", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Adjust F more dynamically\n                CR_variance = np.var(fitness)  # Change 1: Adjust CR based on current fitness variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))  # Change 2: Use current variance\n                \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive crossover probability adjustment based on fitness variance to enhance exploration in Adaptive Differential Evolution.", "configspace": "", "generation": 8, "fitness": 0.8021449806575367, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.012. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "05c09484-e56f-4a3b-a00c-9d73a77438c9", "metadata": {"aucs": [0.7869611100974436, 0.8036290926481862, 0.8158447392269803], "final_y": [0.14400043819523256, 0.14834359224796523, 0.13649318999162208]}, "mutation_prompt": null}
{"id": "88d7d4ae-c210-4343-8ebd-83c35f1420f3", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(np.diff(self.history[-5:]).sum())), 1.0))  # Change: Adapt CR\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive crossover probability based on fitness improvement rate to enhance exploration in Adaptive Differential Evolution.", "configspace": "", "generation": 8, "fitness": 0.8013078977755987, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.004. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "05c09484-e56f-4a3b-a00c-9d73a77438c9", "metadata": {"aucs": [0.8046374664279514, 0.8034850898179755, 0.7958011370808692], "final_y": [0.12871943536344566, 0.1451163546598424, 0.13924838467248524]}, "mutation_prompt": null}
{"id": "57feffb6-5607-4106-823a-f202848142fe", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.2 * np.sign(CR_variance)), 1.0))  # Adjust CR based on variance\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance exploration by dynamically adjusting crossover probability based on recent fitness variance in Adaptive Differential Evolution.", "configspace": "", "generation": 8, "fitness": 0.8139493133057463, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.004. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "05c09484-e56f-4a3b-a00c-9d73a77438c9", "metadata": {"aucs": [0.8085603079197377, 0.8148888820955142, 0.8183987499019869], "final_y": [0.1362755106745881, 0.14127440315566464, 0.13325853679069743]}, "mutation_prompt": null}
{"id": "732c0d7c-a381-459f-87e9-17f1ef17f361", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.3 * np.random.randn())  # Change 1: Increase variability to 0.3 for F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 3: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 4: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 5: Re-evaluate fitness\n                no_improvement = 0  # Change 6: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improve exploration by introducing a larger mutation variability based on the current fitness trend.", "configspace": "", "generation": 8, "fitness": 0.8118175269501604, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.017. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "8c6d83b2-2a80-4b51-b46b-c9d3d9bb5dd8", "metadata": {"aucs": [0.7960445692440755, 0.8357274131952956, 0.80368059841111], "final_y": [0.13819534649066312, 0.12540747975411948, 0.1497051917181521]}, "mutation_prompt": null}
{"id": "f5309f9e-70f8-4c20-b61c-d6c399cdd5c9", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight\n                levy_step = scipy.stats.levy.rvs(size=self.dim)\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance exploration by incorporating adaptive Levy flight-based mutation and dynamic crossover, ensuring robust convergence in varying landscapes.", "configspace": "", "generation": 8, "fitness": 0.8132534850618328, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.027. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "8c6d83b2-2a80-4b51-b46b-c9d3d9bb5dd8", "metadata": {"aucs": [0.7830031878243237, 0.8080453088761801, 0.8487119584849947], "final_y": [0.1567158442876544, 0.1403070749499118, 0.12361271454543354]}, "mutation_prompt": null}
{"id": "d7ded3b5-15e0-4235-87f4-cb5785b93123", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(np.mean(recent_perf))), 1.0))  # Change: Adapt CR based on recent performance mean\n                \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 3: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 4: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 5: Re-evaluate fitness\n                no_improvement = 0  # Change 6: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive crossover rate scaling based on recent fitness improvement trends.", "configspace": "", "generation": 8, "fitness": 0.8050101091566431, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.006. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "8c6d83b2-2a80-4b51-b46b-c9d3d9bb5dd8", "metadata": {"aucs": [0.8061434175951498, 0.8118590432420516, 0.7970278666327278], "final_y": [0.13866761517627235, 0.1425211495759321, 0.14392554676344482]}, "mutation_prompt": null}
{"id": "7d7bc80e-774c-451f-882d-1851624e6c17", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size_variance = np.var(fitness) // 2  # Adaptive change: scale size based on fitness variance\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum() + pop_size_variance))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                no_improvement = 0  # Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive population size scaling based on recent fitness variance to enhance exploration.", "configspace": "", "generation": 8, "fitness": 0.8093840674824687, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.023. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "8c6d83b2-2a80-4b51-b46b-c9d3d9bb5dd8", "metadata": {"aucs": [0.7767316041250001, 0.8264077236869759, 0.8250128746354304], "final_y": [0.1320690348062924, 0.13841171017216214, 0.13410454913251135]}, "mutation_prompt": null}
{"id": "1af96ade-23fa-463f-a93c-463d66b94789", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)  # Change 2: Use F_adapted instead of F\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 3: Population reset condition\n                variance_adj = int(np.var(fitness) * 5)  # Change 4: Adjust population size based on variance\n                new_size = initial_pop_size - variance_adj  # Change 5: New population size\n                population = np.random.uniform(lb, ub, (new_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                no_improvement = 0  # Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive population scaling based on historical fitness variance to improve convergence stability.", "configspace": "", "generation": 8, "fitness": 0.8051524329380744, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.009. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "8c6d83b2-2a80-4b51-b46b-c9d3d9bb5dd8", "metadata": {"aucs": [0.797947781716511, 0.817208359082952, 0.8003011580147602], "final_y": [0.14078582280178698, 0.14459977313769146, 0.14222259988645014]}, "mutation_prompt": null}
{"id": "87401284-7ed3-416f-bb67-7da0d45cef84", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight with dynamic scaling\n                levy_step = scipy.stats.levy.rvs(size=self.dim) * np.random.uniform(0.1, 0.5)  # Change: dynamic scaling factor\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic scaling factor in Levy flight mutation for enhanced exploitation in local search.", "configspace": "", "generation": 9, "fitness": 0.8236116868625843, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.018. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "f5309f9e-70f8-4c20-b61c-d6c399cdd5c9", "metadata": {"aucs": [0.8008198841565191, 0.8440209711713146, 0.8259942052599195], "final_y": [0.1351846700637147, 0.12234174383067808, 0.13829847126883077]}, "mutation_prompt": null}
{"id": "170b2a78-766e-46e7-8366-b9d3901f93e0", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim\n        F = 0.5\n        CR = 0.9\n        \n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0\n        \n        # Chaotic mapping initialization\n        chaotic_seq = np.random.rand(self.dim)\n        \n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            F_adapt = 0.1 + chaotic_seq.mean() * 0.4  # Self-adaptive F (Change 1)\n            \n            for i in range(pop_size):\n                levy_step = scipy.stats.levy.rvs(size=self.dim)\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                chaotic_seq = 4 * chaotic_seq * (1 - chaotic_seq)  # Logistic map (Change 2)\n\n                # Chaotic mutation step (Change 3)\n                chaotic_factor = 1.0 + 0.5 * (chaotic_seq - 0.5)\n                mutant = np.clip(a + F_adapt * chaotic_factor * levy_step * (b - c), lb, ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0\n                else:\n                    no_improvement += 1\n            \n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            if no_improvement > pop_size // 2:\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                no_improvement = 0\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance robustness and exploration by integrating a self-adaptive control parameter mechanism and chaotic mapping into the mutation strategy.", "configspace": "", "generation": 9, "fitness": 0.7903206537465319, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.022. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "f5309f9e-70f8-4c20-b61c-d6c399cdd5c9", "metadata": {"aucs": [0.7591975631334793, 0.8096934273621198, 0.8020709707439966], "final_y": [0.16305072449038938, 0.13764695513209857, 0.13271267738452697]}, "mutation_prompt": null}
{"id": "1a5023f4-6595-4080-8cc7-3f21149fdaaf", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            best_idx = np.argmin(fitness)  # Identify the best individual\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Elitism: Carry over the best individual\n            population[0] = population[best_idx]\n            fitness[0] = fitness[best_idx]\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.2 * np.sign(CR_variance)), 1.0))  # Adjust CR based on variance\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a simple elitism strategy by retaining the best individual from the previous generation to improve convergence performance.", "configspace": "", "generation": 9, "fitness": 0.8145231042570686, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.007. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "57feffb6-5607-4106-823a-f202848142fe", "metadata": {"aucs": [0.8084108149854289, 0.8246081075789767, 0.8105503902068002], "final_y": [0.1347664054054064, 0.13665637416571563, 0.14152148739434423]}, "mutation_prompt": null}
{"id": "fc17e09f-9735-43a6-b06a-fb74340db5c8", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.1 * np.random.randn())  # Change 1: Add variability to F\n                learning_factor = 1 + 0.1 * np.tanh(np.mean(fitness) - fitness[i])  # New line: Introduce learning factor\n                mutant = np.clip(a + F_adapted * learning_factor * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.2 * np.sign(CR_variance)), 1.0))  # Adjust CR based on variance\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improve exploration by introducing a learning factor to adaptively control the scale of mutation in Adaptive Differential Evolution.", "configspace": "", "generation": 9, "fitness": 0.8122210979383047, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.009. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "57feffb6-5607-4106-823a-f202848142fe", "metadata": {"aucs": [0.8044552017501051, 0.8251050898531475, 0.8071030022116612], "final_y": [0.13793124649492294, 0.14091747972432445, 0.1276836933031671]}, "mutation_prompt": null}
{"id": "5b740359-6373-4e93-909f-2df3ed4121d3", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight\n                levy_step = scipy.stats.levy.rvs(size=self.dim)\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation or low diversity\n            if no_improvement > pop_size // 2 or np.std(fitness) < 1e-3:  # Change: Added diversity-based reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a random reset mechanism based on fitness diversity to improve exploration and prevent premature convergence. ", "configspace": "", "generation": 9, "fitness": 0.8029491988055993, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.019. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "f5309f9e-70f8-4c20-b61c-d6c399cdd5c9", "metadata": {"aucs": [0.7790739528840803, 0.8246556467367663, 0.8051179967959512], "final_y": [0.15085869845776734, 0.13292373570311233, 0.14961891509455338]}, "mutation_prompt": null}
{"id": "7ae0cfe7-ae70-42e3-8d9f-cb4c45b67521", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight\n                levy_step = scipy.stats.levy.rvs(size=self.dim)\n                levy_scale = 1.0 / (1.0 + fitness[i])  # Adjusted line: Inverse fitness-based Levy step scale\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_scale * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Improve exploration by adjusting Levy flight step scale using inverse fitness-based feedback for better adaptability.", "configspace": "", "generation": 9, "fitness": 0.7991418278735143, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.025. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "f5309f9e-70f8-4c20-b61c-d6c399cdd5c9", "metadata": {"aucs": [0.7660202090988658, 0.827319944685819, 0.8040853298358581], "final_y": [0.15196700930628548, 0.13656579849621442, 0.14204255758987394]}, "mutation_prompt": null}
{"id": "4ca92b95-6219-42ae-96a6-1afb30344af3", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight\n                levy_step = scipy.stats.levy.rvs(size=self.dim)\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Adjust F based on recent performance change\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic adjustment to F based on the fitness improvement rate to enhance adaptability and convergence speed.", "configspace": "", "generation": 9, "fitness": 0.7988714855502455, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.022. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "f5309f9e-70f8-4c20-b61c-d6c399cdd5c9", "metadata": {"aucs": [0.7684598294656755, 0.8145160926718862, 0.8136385345131745], "final_y": [0.1589123171461534, 0.14127875623304675, 0.1385252594597115]}, "mutation_prompt": null}
{"id": "244c4693-5bca-47fa-a74b-657836e5213c", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight\n                levy_scale = 0.5 * (1 + np.sign(np.diff(self.history[-2:]).sum()))  # Dynamic scaling factor\n                levy_step = levy_scale * scipy.stats.levy.rvs(size=self.dim)\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a dynamic scaling factor for Levy step size based on fitness improvement history to enhance exploration.", "configspace": "", "generation": 9, "fitness": 0.8146114163655356, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.029. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "f5309f9e-70f8-4c20-b61c-d6c399cdd5c9", "metadata": {"aucs": [0.77472821248935, 0.8270364729064524, 0.8420695637008042], "final_y": [0.15728465465072172, 0.12835514212272536, 0.12736643394800784]}, "mutation_prompt": null}
{"id": "a4f9dacf-455c-4dc4-9f0f-0dba88289964", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.15 * np.random.randn())  # Change 1: Add more variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.2 * np.sign(CR_variance)), 1.0))  # Adjust CR based on variance\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Refine exploration by implementing adaptive differential weight variance based on fitness improvement trends in Adaptive Differential Evolution.", "configspace": "", "generation": 9, "fitness": 0.8224425489519698, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.006. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "57feffb6-5607-4106-823a-f202848142fe", "metadata": {"aucs": [0.819026875073321, 0.8304912206346609, 0.8178095511479277], "final_y": [0.12597279981502219, 0.13144980272384432, 0.13657112432779128]}, "mutation_prompt": null}
{"id": "0ab39280-1aed-49d5-a08c-17f26a5e67b0", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            fitness_diversity = np.std(fitness)  # New measure: Fitness diversity\n            for i in range(pop_size):\n                # Mutation using Levy flight\n                levy_step = scipy.stats.levy.rvs(size=self.dim)\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                dynamic_F = F + (fitness_diversity / np.mean(fitness)) * 0.1  # Adjust F based on diversity\n                mutant = np.clip(a + dynamic_F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation with dynamic F\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Incorporate fitness diversity measure to dynamically adjust mutation scale in Adaptive Differential Evolution for enhanced exploration.", "configspace": "", "generation": 9, "fitness": 0.7966167978005014, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.034. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "f5309f9e-70f8-4c20-b61c-d6c399cdd5c9", "metadata": {"aucs": [0.7484553752382418, 0.8193592050325609, 0.8220358131307015], "final_y": [0.15120994799811605, 0.12270493250076941, 0.12853633499966322]}, "mutation_prompt": null}
{"id": "9707b383-cf80-4b29-8744-8c6a57a67704", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.15 * np.random.randn())  # Change 1: Add more variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.2 * np.sign(CR_variance)), 1.0))  # Adjust CR based on variance\n                CR = min(1.0, CR + 0.05 * np.sign(recent_perf[-1]))  # Change: Adjust CR based on recent performance feedback\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance adaptive strategy by integrating fitness improvement feedback to dynamically adjust both mutation factor and crossover rate.", "configspace": "", "generation": 10, "fitness": 0.8007011349169962, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.004. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "a4f9dacf-455c-4dc4-9f0f-0dba88289964", "metadata": {"aucs": [0.7960041228777047, 0.8061535480259999, 0.799945733847284], "final_y": [0.1419187255192077, 0.1481692205469044, 0.14042767249716925]}, "mutation_prompt": null}
{"id": "5a6fb068-557f-4a02-857b-a6145fda9370", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.15 * np.random.randn())  # Change 1: Add more variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.2 * np.sign(CR_variance)), 1.0))  # Adjust CR based on variance\n                CR = min(1.0, CR * (1 + 0.1 * np.sign(np.mean(recent_perf))))  # Updated line: Adapt CR based on recent performance trends\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive crossover probability based on historical improvement trends to enhance balancing exploration and exploitation.", "configspace": "", "generation": 10, "fitness": 0.808003888895659, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.023. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "a4f9dacf-455c-4dc4-9f0f-0dba88289964", "metadata": {"aucs": [0.7874474107607413, 0.8396211468238465, 0.796943109102389], "final_y": [0.14022073614081398, 0.13748959674121686, 0.1489822061209991]}, "mutation_prompt": null}
{"id": "06b18d49-f376-409b-96a8-067d151260a1", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim\n        F = 0.5\n        CR = 0.9\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            # Change 1: Adaptive population size based on fitness diversity\n            fitness_diversity = np.std(fitness) / np.mean(fitness)\n            pop_size = max(5, int(initial_pop_size * (0.8 + 0.2 * fitness_diversity)))\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            \n            for i in range(pop_size):\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.15 * np.random.randn())\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                # Change 2: Adjust F more dynamically\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.2 * np.sign(CR_variance)), 1.0)) \n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce an adaptive population size based on fitness diversity to enhance exploration and prevent premature convergence.", "configspace": "", "generation": 10, "fitness": 0.8154334401575257, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.006. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "a4f9dacf-455c-4dc4-9f0f-0dba88289964", "metadata": {"aucs": [0.819843448998141, 0.8197071719812847, 0.8067496994931518], "final_y": [0.13526693222976416, 0.1378984509095117, 0.14561244600283563]}, "mutation_prompt": null}
{"id": "4112e9da-9492-4bd6-833f-e159aef19663", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight with dynamic scaling\n                recent_fitness_change = np.abs(np.diff(self.history[-5:]).mean()) if len(self.history) > 5 else 0.1\n                levy_step = scipy.stats.levy.rvs(size=self.dim) * (0.1 + 0.4 * recent_fitness_change)  # Change: dynamic scaling based on fitness change\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Adjust Levy flight to use dynamic exponent for step scaling based on recent fitness changes.", "configspace": "", "generation": 10, "fitness": 0.8097536448647862, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.011. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "87401284-7ed3-416f-bb67-7da0d45cef84", "metadata": {"aucs": [0.7954949081591544, 0.8112898324584326, 0.8224761939767717], "final_y": [0.1357310151412089, 0.14760604757968476, 0.1360864125083726]}, "mutation_prompt": null}
{"id": "410fdf69-0129-4b20-a583-7551846ee58b", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight with dynamic scaling\n                levy_step = scipy.stats.levy.rvs(size=self.dim) * np.random.uniform(0.1, 0.5)  # Change: dynamic scaling factor\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                # Change 1: Introducing self-adaptive mechanism for F\n                F = 0.9 * F + 0.1 * np.random.rand()  \n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce a self-adaptive mechanism for the differential weight F to enhance convergence in varied landscapes.", "configspace": "", "generation": 10, "fitness": 0.8110418505983338, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.011. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "87401284-7ed3-416f-bb67-7da0d45cef84", "metadata": {"aucs": [0.7985808022895496, 0.8099041013535162, 0.8246406481519354], "final_y": [0.1454738923317196, 0.14599674117538697, 0.12803197565323088]}, "mutation_prompt": null}
{"id": "783ab25e-0bf0-4505-8651-b42ab71bc6a0", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight with dynamic scaling\n                levy_step = scipy.stats.levy.rvs(size=self.dim) * np.random.uniform(0.1, 0.5)  # Change: dynamic scaling factor\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 1: Levy flight mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            fitness_variance_mod = 1 + np.var(fitness) * 0.01  # Change 6: Add variance-based scaling modification\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0)) * fitness_variance_mod\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance adaptive scaling factors by integrating a fitness variance-based modifier to boost convergence stability.", "configspace": "", "generation": 10, "fitness": 0.8021987505951071, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.010. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "87401284-7ed3-416f-bb67-7da0d45cef84", "metadata": {"aucs": [0.7899544967667188, 0.8143593298563706, 0.8022824251622319], "final_y": [0.1502149825479383, 0.1430369490052209, 0.1405964028668384]}, "mutation_prompt": null}
{"id": "aa1d3af7-5b22-47e4-9754-327d8916609c", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight with dynamic scaling and adaptive noise\n                levy_step = scipy.stats.levy.rvs(size=self.dim) * np.random.uniform(0.1, 0.5)\n                noise = np.random.normal(0, 0.1, self.dim) * (1 - np.tanh(0.01 * no_improvement))  # Change: adaptive noise\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * (levy_step + noise) * (b - c), lb, ub)  # Change: incorporate noise\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                no_improvement = 0\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive noise in mutation to enhance diversity and escape local optima in Adaptive Differential Evolution.", "configspace": "", "generation": 10, "fitness": 0.810225103144103, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.005. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "87401284-7ed3-416f-bb67-7da0d45cef84", "metadata": {"aucs": [0.8090345740513909, 0.817088215020703, 0.8045525203602151], "final_y": [0.1457641800749978, 0.1384941340473098, 0.13652160388381374]}, "mutation_prompt": null}
{"id": "73825073-5264-428f-a43a-6db7fd7e6e41", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 12 * self.dim  # Change 1: Increase initial population size\n        F = 0.6  # Change 2: Increase base differential weight\n        CR = 0.8  # Change 3: Adjust base crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.8 + 0.2 * np.sign(np.diff(self.history[-2:]).sum()))))  # Change 4: Adjust dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.2 * np.random.randn())  # Change 5: Increase variability to F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.15 * np.sign(recent_perf[-1])))  # Change 6: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.25 * np.sign(CR_variance)), 1.0))  # Change 7: Alter CR adjustment formula\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance exploration and exploitation balance by integrating dynamic mutation scaling with adaptive crossover strategies in Adaptive Differential Evolution.", "configspace": "", "generation": 10, "fitness": 0.8102955928896467, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.002. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "a4f9dacf-455c-4dc4-9f0f-0dba88289964", "metadata": {"aucs": [0.8098232053532521, 0.8129704971721845, 0.8080930761435035], "final_y": [0.1244058052578858, 0.13979863458310926, 0.13611128227713254]}, "mutation_prompt": null}
{"id": "755341df-4311-4b9d-aa1f-7488c9320091", "solution": "import numpy as np\nimport scipy.stats\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n        \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        no_improvement = 0  # Track stagnation\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.9 + 0.1 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation using Levy flight with dynamic scaling\n                levy_step = scipy.stats.levy.rvs(size=self.dim) * np.random.uniform(0.1, 0.7)  # Change 1: increased upper bound for dynamic scaling factor\n                a, b, c = population[np.random.choice(pop_size, 3, replace=False)]\n                mutant = np.clip(a + F * levy_step * (b - c), lb, ub)  # Change 2: Adjusted levy_step scaling\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    no_improvement = 0  # Reset stagnation counter\n                else:\n                    no_improvement += 1  # Increment stagnation counter\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = 0.5 * (F + min(max(0.1, F + 0.1 * np.sign(recent_perf[-1])), 1.0))\n                CR_variance = np.var(self.history[-5:])  # Adjust based on variance\n                CR = 0.7 * (CR + min(max(0.1, CR + 0.1 * np.sign(CR_variance)), 1.0))  # Change 3: Adjusted CR update factor\n            \n            # Introduce population reset on stagnation\n            if no_improvement > pop_size // 2:  # Change 2: Population reset condition\n                population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))  # Change 3: Reset population\n                fitness = np.array([func(ind) for ind in population])  # Change 4: Re-evaluate fitness\n                no_improvement = 0  # Change 5: Reset stagnation counter after reset\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Enhance selection pressure through dynamic mutation scaling and adaptive crossover probability adjustment.", "configspace": "", "generation": 10, "fitness": 0.8066828412853333, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.008. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "87401284-7ed3-416f-bb67-7da0d45cef84", "metadata": {"aucs": [0.7967671428438776, 0.8157739155705698, 0.8075074654415527], "final_y": [0.1425856504995302, 0.14021770628821106, 0.1419288463985654]}, "mutation_prompt": null}
{"id": "5134c33d-a512-4a18-be16-042896e896c2", "solution": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []  # Store historical performance\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        initial_pop_size = 10 * self.dim  # A common heuristic choice\n        F = 0.5  # Differential weight\n        CR = 0.9  # Crossover probability\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, (initial_pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.history.append(np.mean(fitness))\n        \n        evals = initial_pop_size\n        \n        # Evolutionary loop\n        while evals < self.budget:\n            pop_size = max(5, int(initial_pop_size * (0.8 + 0.2 * np.sign(np.diff(self.history[-2:]).sum()))))  # Dynamic adaptation\n            population = population[:pop_size]\n            fitness = fitness[:pop_size]\n            for i in range(pop_size):\n                # Mutation and Crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adapted = F * (1 + 0.25 * np.random.randn())  # Change 1: Increase variability in F\n                mutant = np.clip(a + F_adapted * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                evals += 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n            \n            # Adaptation based on history\n            mean_fitness = np.mean(fitness)\n            self.history.append(mean_fitness)\n            if len(self.history) > 5:\n                recent_perf = np.diff(self.history[-5:])\n                F = max(0.1, min(1.0, F + 0.1 * np.sign(recent_perf[-1])))  # Change 2: Adjust F more dynamically\n                CR_variance = np.var(self.history[-5:])\n                CR = 0.5 * (CR + min(max(0.1, CR + 0.2 * np.sign(CR_variance)), 1.0))  # Adjust CR based on variance\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "AdaptiveDifferentialEvolution", "description": "Introduce adaptive mutation scaling and population resizing based on recent fitness improvements and diversity, enhancing search effectiveness.", "configspace": "", "generation": 10, "fitness": 0.8121910872865344, "feedback": "The algorithm AdaptiveDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.022. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "a4f9dacf-455c-4dc4-9f0f-0dba88289964", "metadata": {"aucs": [0.7855998958945629, 0.8390786045739589, 0.8118947613910819], "final_y": [0.13824576284415468, 0.13021721615572124, 0.1305993258250968]}, "mutation_prompt": null}
