{"id": "01b0fbd9-8317-4805-b66d-046d704f3d58", "solution": "import numpy as np\n\nclass PeriodicityEnhancedDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Hyperparameters\n        pop_size = 20\n        F = 0.8  # Differential weight\n        Cr = 0.9  # Crossover probability\n        periodicity_weight = 0.1\n        local_search_prob = 0.3\n\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        num_evaluations = pop_size\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Mutation\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Periodicity enforcement\n            if periodicity_weight > 0:\n                for i in range(pop_size):\n                    new_sol = self.enforce_periodicity(population[i], periodicity_weight)\n                    new_fitness = func(new_sol)\n                    num_evaluations += 1\n                    if new_fitness < fitness[i]:\n                        population[i] = new_sol\n                        fitness[i] = new_fitness\n\n            # Local search\n            if np.random.rand() < local_search_prob:\n                for i in range(pop_size):\n                    new_sol = self.local_search(population[i], func)\n                    new_fitness = func(new_sol)\n                    num_evaluations += 1\n                    if new_fitness < fitness[i]:\n                        population[i] = new_sol\n                        fitness[i] = new_fitness\n\n            if num_evaluations >= self.budget:\n                break\n\n        best_index = np.argmin(fitness)\n        return population[best_index]\n\n    def enforce_periodicity(self, solution, weight):\n        # Transform solution to emphasize periodicity\n        avg_period = np.mean(solution)\n        return solution + weight * (avg_period - solution)\n\n    def local_search(self, solution, func):\n        # Simple gradient-based local search (e.g., tweak or BFGS)\n        epsilon = 1e-4\n        best_sol = solution.copy()\n        best_fitness = func(best_sol)\n        for i in range(len(solution)):\n            perturb = np.zeros_like(solution)\n            perturb[i] = epsilon\n            trial_sol = np.clip(best_sol + perturb, func.bounds.lb, func.bounds.ub)\n            trial_fitness = func(trial_sol)\n            if trial_fitness < best_fitness:\n                best_sol = trial_sol\n                best_fitness = trial_fitness\n        return best_sol", "name": "PeriodicityEnhancedDE", "description": "A hybrid evolutionary algorithm combining Differential Evolution with periodicity-enhanced local search to exploit constructive interference for multilayer photonic structure optimization.", "configspace": "", "generation": 0, "fitness": 0.8412301509812815, "feedback": "The algorithm PeriodicityEnhancedDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.025. And the mean value of best solutions found was 0.175 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8221462250355323, 0.8251458455011746, 0.8763983824071374], "final_y": [0.17340720604848536, 0.18616872526643247, 0.1657309905054284]}, "mutation_prompt": null}
{"id": "a89d7c60-ddc8-4ac0-90bb-28eaca873036", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid global-local metaheuristic that combines Quasi-Oppositional Differential Evolution with a local refinement step to efficiently explore and exploit the search space, encouraging periodicity to maximize performance in multilayered photonic structures.", "configspace": "", "generation": 0, "fitness": 0.9155412972650888, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.072. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9386380574182427, 0.9903484247744382, 0.8176374096025857], "final_y": [0.1818785898207873, 0.16485690895689153, 0.2280454717545254]}, "mutation_prompt": null}
{"id": "bb7e319a-6377-450c-90c9-1b7707d157c4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def adaptive_mutation(self, bounds, F_min=0.4, F_max=0.9):\n        return np.random.uniform(F_min, F_max)\n\n    def differential_evolution_step(self, population, bounds, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            F = self.adaptive_mutation(bounds)\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        def periodicity_enforcing(obj):\n            penalty = np.cos(2 * np.pi * np.arange(self.dim) / self.dim)\n            return obj + 0.1 * np.sum((best - penalty) ** 2)\n        \n        result = minimize(lambda x: func(x) + periodicity_enforcing(x), best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced hybrid optimization algorithm combining Quasi-Oppositional Differential Evolution with adaptive mutation and periodicity-enforcing local refinement to efficiently navigate and exploit complex search spaces.  ", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The user-provided objective function must return a scalar value.').", "error": "ValueError('The user-provided objective function must return a scalar value.')", "parent_id": "a89d7c60-ddc8-4ac0-90bb-28eaca873036", "metadata": {}, "mutation_prompt": null}
{"id": "101efcba-11cb-4f87-9762-33c98f0f02d9", "solution": "import numpy as np\n\nclass PeriodicityEnhancedDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Hyperparameters\n        pop_size = 20\n        F = 0.8  # Differential weight\n        Cr = 0.9  # Crossover probability\n        periodicity_weight = 0.1\n        local_search_prob = 0.3\n\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        num_evaluations = pop_size\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Mutation\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Periodicity enforcement\n            if periodicity_weight > 0:\n                periodicity_weight = periodicity_weight * (1 - num_evaluations/self.budget)  # Dynamic adjustment\n                for i in range(pop_size):\n                    new_sol = self.enforce_periodicity(population[i], periodicity_weight)\n                    new_fitness = func(new_sol)\n                    num_evaluations += 1\n                    if new_fitness < fitness[i]:\n                        population[i] = new_sol\n                        fitness[i] = new_fitness\n\n            # Local search\n            if np.random.rand() < local_search_prob:\n                for i in range(pop_size):\n                    new_sol = self.local_search(population[i], func)\n                    new_fitness = func(new_sol)\n                    num_evaluations += 1\n                    if new_fitness < fitness[i]:\n                        population[i] = new_sol\n                        fitness[i] = new_fitness\n\n            if num_evaluations >= self.budget:\n                break\n\n        best_index = np.argmin(fitness)\n        return population[best_index]\n\n    def enforce_periodicity(self, solution, weight):\n        avg_period = np.mean(solution)\n        return solution + weight * (avg_period - solution)\n\n    def local_search(self, solution, func):\n        epsilon = 1e-4\n        best_sol = solution.copy()\n        best_fitness = func(best_sol)\n        for i in range(len(solution)):\n            perturb = np.zeros_like(solution)\n            perturb[i] = epsilon\n            trial_sol = np.clip(best_sol + perturb, func.bounds.lb, func.bounds.ub)\n            trial_fitness = func(trial_sol)\n            if trial_fitness < best_fitness:\n                best_sol = trial_sol\n                best_fitness = trial_fitness\n        return best_sol", "name": "PeriodicityEnhancedDE", "description": "Enhanced Differential Evolution with dynamic periodicity adaptation and improved local search for multilayer photonic optimization.", "configspace": "", "generation": 1, "fitness": 0.8316771842578596, "feedback": "The algorithm PeriodicityEnhancedDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.035. And the mean value of best solutions found was 0.187 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "01b0fbd9-8317-4805-b66d-046d704f3d58", "metadata": {"aucs": [0.7896845103521837, 0.8743974963818083, 0.8309495460395868], "final_y": [0.1837379838365778, 0.18109293123978032, 0.19625070944194034]}, "mutation_prompt": null}
{"id": "2d3a1855-4e7e-459c-a46f-e67b8fb2db95", "solution": "import numpy as np\n\nclass PeriodicityEnhancedDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Hyperparameters\n        pop_size = 20\n        F = 0.8  # Differential weight\n        Cr = 0.9  # Crossover probability\n        periodicity_weight = 0.1\n        local_search_prob = 0.3\n\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        num_evaluations = pop_size\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Mutation\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_adaptive = F * (0.5 + np.random.rand() * 0.5)  # Adaptive mutation\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Periodicity enforcement\n            if periodicity_weight > 0:\n                for i in range(pop_size):\n                    new_sol = self.enforce_periodicity(population[i], periodicity_weight)\n                    new_fitness = func(new_sol)\n                    num_evaluations += 1\n                    if new_fitness < fitness[i]:\n                        population[i] = new_sol\n                        fitness[i] = new_fitness\n\n            # Local search\n            if np.random.rand() < local_search_prob:\n                for i in range(pop_size):\n                    new_sol = self.local_search(population[i], func)\n                    new_fitness = func(new_sol)\n                    num_evaluations += 1\n                    if new_fitness < fitness[i]:\n                        population[i] = new_sol\n                        fitness[i] = new_fitness\n\n            if num_evaluations >= self.budget:\n                break\n\n        best_index = np.argmin(fitness)\n        return population[best_index]\n\n    def enforce_periodicity(self, solution, weight):\n        # Transform solution to emphasize periodicity\n        avg_period = np.mean(solution)\n        return solution + weight * (avg_period - solution)\n\n    def local_search(self, solution, func):\n        # Simple gradient-based local search (e.g., tweak or BFGS)\n        epsilon = 1e-4\n        best_sol = solution.copy()\n        best_fitness = func(best_sol)\n        for i in range(len(solution)):\n            perturb = np.zeros_like(solution)\n            perturb[i] = epsilon\n            trial_sol = np.clip(best_sol + perturb, func.bounds.lb, func.bounds.ub)\n            trial_fitness = func(trial_sol)\n            if trial_fitness < best_fitness:\n                best_sol = trial_sol\n                best_fitness = trial_fitness\n        return best_sol", "name": "PeriodicityEnhancedDE", "description": "A refined differential evolution algorithm integrating periodicity constraints and adaptive mutation to enhance solution diversity and precision for photonic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.8508080423694274, "feedback": "The algorithm PeriodicityEnhancedDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.025. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "01b0fbd9-8317-4805-b66d-046d704f3d58", "metadata": {"aucs": [0.8759556680297553, 0.8602643816629137, 0.8162040774156132], "final_y": [0.1698844105586791, 0.18488803068226778, 0.18157245824595247]}, "mutation_prompt": null}
{"id": "0be697ac-90a6-47d7-b134-040d7aa4fe7d", "solution": "import numpy as np\n\nclass PeriodicityEnhancedDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Hyperparameters\n        pop_size = 20\n        F = 0.8  # Differential weight\n        Cr = 0.9  # Crossover probability\n        periodicity_weight = 0.1\n        local_search_prob = 0.3\n\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        \n        num_evaluations = pop_size\n\n        while num_evaluations < self.budget:\n            for i in range(pop_size):\n                # Mutation\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Evaluate trial solution\n                trial_fitness = func(trial)\n                num_evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Periodicity enforcement\n            if periodicity_weight > 0:\n                for i in range(pop_size):\n                    new_sol = self.enforce_periodicity(population[i], periodicity_weight)\n                    new_fitness = func(new_sol)\n                    num_evaluations += 1\n                    if new_fitness < fitness[i]:\n                        population[i] = new_sol\n                        fitness[i] = new_fitness\n\n            # Local search\n            if np.random.rand() < local_search_prob:\n                for i in range(pop_size):\n                    new_sol = self.local_search(population[i], func)\n                    new_fitness = func(new_sol)\n                    num_evaluations += 1\n                    if new_fitness < fitness[i]:\n                        population[i] = new_sol\n                        fitness[i] = new_fitness\n\n            if num_evaluations >= self.budget:\n                break\n\n        best_index = np.argmin(fitness)\n        return population[best_index]\n\n    def enforce_periodicity(self, solution, weight):\n        # Transform solution to emphasize periodicity\n        avg_period = np.mean(solution)\n        return solution + weight * (avg_period - solution)\n\n    def local_search(self, solution, func):\n        # Simple stochastic gradient-based local search\n        best_sol = solution.copy()\n        best_fitness = func(best_sol)\n        lr = 0.01  # Learning rate for stochastic gradient\n        for i in range(len(solution)):\n            perturb = np.random.normal(0, 1, size=solution.shape) * lr\n            trial_sol = np.clip(best_sol + perturb, func.bounds.lb, func.bounds.ub)\n            trial_fitness = func(trial_sol)\n            if trial_fitness < best_fitness:\n                best_sol = trial_sol\n                best_fitness = trial_fitness\n        return best_sol", "name": "PeriodicityEnhancedDE", "description": "Enhanced the local search with stochastic gradient estimates to improve fine-tuning near promising regions.", "configspace": "", "generation": 1, "fitness": 0.780549851439567, "feedback": "The algorithm PeriodicityEnhancedDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.026. And the mean value of best solutions found was 0.190 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "01b0fbd9-8317-4805-b66d-046d704f3d58", "metadata": {"aucs": [0.816941887919808, 0.7685159327321375, 0.7561917336667556], "final_y": [0.16943903796764936, 0.21082785397305637, 0.19018772597447386]}, "mutation_prompt": null}
{"id": "1347dc4b-33a5-4b91-8092-df4f838975f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "An improved Hybrid Metaheuristic Optimizer that utilizes adaptive mutation scaling in Differential Evolution to enhance exploration and exploitation balance for optimizing multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": 0.9811417942113453, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a89d7c60-ddc8-4ac0-90bb-28eaca873036", "metadata": {"aucs": [0.992647904769902, 0.9734024763011592, 0.977375001562975], "final_y": [0.1648562662413121, 0.16485935700743914, 0.16485599234934833]}, "mutation_prompt": null}
{"id": "f91e86b1-9de3-4e2e-9bdb-935eb81e1a54", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        # Calculate adjusted CR based on budget usage for better balance\n        adjusted_CR = CR * (1 - self.budget_used / self.budget)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < adjusted_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A refined hybrid metaheuristic optimizer that enhances exploration by adjusting the crossover probability in Differential Evolution based on the current budget usage to balance exploration and exploitation effectively.", "configspace": "", "generation": 1, "fitness": 0.9557107942236147, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.956 with standard deviation 0.024. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "a89d7c60-ddc8-4ac0-90bb-28eaca873036", "metadata": {"aucs": [0.9386388675102363, 0.9387046473979268, 0.9897888677626813], "final_y": [0.18187838877676676, 0.1818781175277726, 0.1648561324783273]}, "mutation_prompt": null}
{"id": "9de06334-f175-4fd9-80a5-da01b7067110", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) + 0.1 * np.var(ind) for ind in population])  # Enhanced fitness calculation\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced fitness calculation with weighted periodicity to improve exploration in multilayered photonic structures.", "configspace": "", "generation": 1, "fitness": 0.974111042036372, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "a89d7c60-ddc8-4ac0-90bb-28eaca873036", "metadata": {"aucs": [0.9895189257176825, 0.9387045712359303, 0.9941096291555034], "final_y": [0.16485698844931207, 0.1818781359941185, 0.16485637912284856]}, "mutation_prompt": null}
{"id": "46d357b4-ee0a-4017-8498-6be3a8ce7eda", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F * (1.0 - self.budget_used / self.budget)  # Adaptive mutation rate\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = np.clip(trial_population, bounds.lb, bounds.ub)  # Periodicity constraint applied\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with adaptive mutation rate and periodicity constraint injection to improve exploration and exploitation for multilayered photonic structures.", "configspace": "", "generation": 1, "fitness": 0.9266432851406351, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.927 with standard deviation 0.053. And the mean value of best solutions found was 0.184 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "a89d7c60-ddc8-4ac0-90bb-28eaca873036", "metadata": {"aucs": [0.920394143423582, 0.8654244738573761, 0.9941112381409474], "final_y": [0.18813093984386953, 0.20044530312782305, 0.16485590886064172]}, "mutation_prompt": null}
{"id": "dc865593-a35a-46aa-9478-a085be91a1b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds, F=0.8)  # Dynamic adjustment of F\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local exploration by adjusting DE parameters dynamically based on population diversity.", "configspace": "", "generation": 1, "fitness": 0.8918969335955742, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.105. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "a89d7c60-ddc8-4ac0-90bb-28eaca873036", "metadata": {"aucs": [0.9386370929245783, 0.9903490404142519, 0.7467046674478925], "final_y": [0.1818784984610451, 0.1648564779603977, 0.25781019097329216]}, "mutation_prompt": null}
{"id": "7335ad11-4ced-48a7-b964-3d0c9b795d0a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n\n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n\n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F + np.random.normal(0, 0.1)  # Adaptive scaling factor\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n\n        return new_population\n\n    def periodicity_based_mutation(self, population, bounds):\n        for ind in population:\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                ind += np.sin(np.linspace(0, np.pi, self.dim)) * (bounds.ub - bounds.lb) * 0.1\n                ind = np.clip(ind, bounds.lb, bounds.ub)\n        return population\n\n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n\n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = self.periodicity_based_mutation(trial_population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n\n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n\n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced global-local metaheuristic with adaptive differential evolution and periodicity-based mutation for improved performance in multilayered photonic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.9913028221359129, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a89d7c60-ddc8-4ac0-90bb-28eaca873036", "metadata": {"aucs": [0.9894493036147427, 0.9903487608052247, 0.9941104019877716], "final_y": [0.1648558115517903, 0.16485657043260904, 0.1648561452768451]}, "mutation_prompt": null}
{"id": "844ef518-b414-460d-9605-f19fbd0c6a38", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        perturbation = np.random.normal(0, 0.05, self.dim)\n        perturbed_best = np.clip(best + perturbation, bounds.lb, bounds.ub)\n        result = minimize(func, perturbed_best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with stochastic gradient boosting in local search for optimizing multilayer photonic structures.", "configspace": "", "generation": 2, "fitness": 0.9553538139330945, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "1347dc4b-33a5-4b91-8092-df4f838975f4", "metadata": {"aucs": [0.9386203571062502, 0.9903462575182735, 0.93709482717476], "final_y": [0.18187816947959612, 0.1648579151925491, 0.18188029398467043]}, "mutation_prompt": null}
{"id": "fdfa37fd-91c0-4db9-b740-54db1a92199d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            adaptive_CR = CR * (1 - (self.budget_used / self.budget))  # Adaptive crossover rate\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with adaptive crossover rate to improve exploration-exploitation balance in multilayer photonic structure optimization.", "configspace": "", "generation": 2, "fitness": 0.9779275584815096, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1347dc4b-33a5-4b91-8092-df4f838975f4", "metadata": {"aucs": [0.9857170907235477, 0.9903462575182735, 0.9577193272027076], "final_y": [0.1648580646447484, 0.1648579151925491, 0.16485580022712099]}, "mutation_prompt": null}
{"id": "010e1a80-ebb9-4645-9822-8b2d0647f92d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            adaptive_CR = CR * (1 - (self.budget_used / self.budget))  # Adaptive recombination rate\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < adaptive_CR   # Use adaptive_CR for crossover\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved Hybrid Metaheuristic Optimizer utilizing adaptive recombination rate in Differential Evolution to enhance balance between exploration and exploitation for optimizing multilayer photonic structures.", "configspace": "", "generation": 2, "fitness": 0.9558969735918731, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.956 with standard deviation 0.024. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "1347dc4b-33a5-4b91-8092-df4f838975f4", "metadata": {"aucs": [0.9386398531054141, 0.9903462575182735, 0.9387048101519319], "final_y": [0.18187814040019112, 0.1648579151925491, 0.18187825222698006]}, "mutation_prompt": null}
{"id": "0786b702-f802-4fd5-beac-21cb2a91a84b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 2 < self.budget:  # Change: Increase local search frequency\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced HybridMetaheuristicOptimizer with improved usage of local search to refine solutions more frequently for optimizing multilayer photonic structures.", "configspace": "", "generation": 2, "fitness": 0.9435094193268908, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.010. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "1347dc4b-33a5-4b91-8092-df4f838975f4", "metadata": {"aucs": [0.9386370536474822, 0.9343044588704466, 0.957586745462744], "final_y": [0.18187913559185476, 0.164856982554346, 0.1648559432282043]}, "mutation_prompt": null}
{"id": "6409ea40-4994-4e15-8db4-163b9f17acc3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            # Introduce periodicity in mutation: force every alternate layer to be similar\n            trial[::2] = np.mean(trial[::2])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced a periodicity-preserving mutation strategy to enhance constructive interference in photonic structures.", "configspace": "", "generation": 2, "fitness": 0.9315238399553712, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.932 with standard deviation 0.084. And the mean value of best solutions found was 0.186 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "1347dc4b-33a5-4b91-8092-df4f838975f4", "metadata": {"aucs": [0.9878916791063789, 0.8125704444598166, 0.994109396299918], "final_y": [0.16485577884818348, 0.22804547814401077, 0.1648562209023159]}, "mutation_prompt": null}
{"id": "8e9694dc-5d11-42b7-8ac7-e9219efb335d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation\n            if np.random.rand() < 0.2:  # 20% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer integrating periodic pattern mutation to better exploit constructive interference properties in multilayer photonic structures.  ", "configspace": "", "generation": 2, "fitness": 0.983144109291903, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1347dc4b-33a5-4b91-8092-df4f838975f4", "metadata": {"aucs": [0.9966288356888163, 0.9903487479554776, 0.9624547442314153], "final_y": [0.16485577194000478, 0.16485657221772265, 0.16486096487968815]}, "mutation_prompt": null}
{"id": "754bb3de-430b-4768-b37f-53d0e7f9f993", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            adaptive_CR = CR * (self.budget_used / self.budget)  # Dynamic crossover rate\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Integrated dynamic crossover rate adjustment in Differential Evolution to optimize balance between exploration and exploitation.", "configspace": "", "generation": 2, "fitness": 0.8838244403681023, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.884 with standard deviation 0.097. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "1347dc4b-33a5-4b91-8092-df4f838975f4", "metadata": {"aucs": [0.966338548912629, 0.9373381790991103, 0.7477965930925679], "final_y": [0.1648590477784826, 0.18187830869658161, 0.25781042917478414]}, "mutation_prompt": null}
{"id": "494e4ba6-1a8d-48e8-870a-1b696d258134", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n\n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n\n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F + np.random.normal(0, 0.1)  # Adaptive scaling factor\n            CR_adaptive = np.clip(CR + np.random.normal(0, 0.1), 0, 1)  # Adaptive crossover rate\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_adaptive\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n\n        return new_population\n\n    def periodicity_based_mutation(self, population, bounds):\n        for ind in population:\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                ind += np.sin(np.linspace(0, np.pi, self.dim)) * (bounds.ub - bounds.lb) * 0.1\n                ind = np.clip(ind, bounds.lb, bounds.ub)\n        return population\n\n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n\n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = self.periodicity_based_mutation(trial_population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n\n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n\n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced HybridMetaheuristicOptimizer using adaptive crossover rates in Differential Evolution for improved exploration and exploitation balance in multilayer photonic structure optimization.", "configspace": "", "generation": 2, "fitness": 0.9089612127850261, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.909 with standard deviation 0.072. And the mean value of best solutions found was 0.194 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "7335ad11-4ced-48a7-b964-3d0c9b795d0a", "metadata": {"aucs": [0.9201305498620345, 0.9913262635714876, 0.8154268249215562], "final_y": [0.1881310501913741, 0.16485621498794778, 0.22804589195438218]}, "mutation_prompt": null}
{"id": "e25378fb-4a03-4486-9d24-b322a94e759b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n\n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n\n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F + np.random.normal(0, 0.1)  # Adaptive scaling factor\n            CR_adaptive = np.clip(CR + np.random.normal(0, 0.05), 0.1, 1.0)  # Adaptive crossover rate\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_adaptive\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n\n        return new_population\n\n    def periodicity_based_mutation(self, population, bounds):\n        for ind in population:\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                ind += np.sin(np.linspace(0, np.pi, self.dim)) * (bounds.ub - bounds.lb) * 0.1\n                ind = np.clip(ind, bounds.lb, bounds.ub)\n        return population\n\n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n\n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = self.periodicity_based_mutation(trial_population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n\n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n\n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced global-local metaheuristic with adaptive differential evolution and periodicity-based mutation, now incorporating adaptive crossover rates for improved search flexibility in multilayered photonic structure optimization.", "configspace": "", "generation": 2, "fitness": 0.9745995040004461, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "7335ad11-4ced-48a7-b964-3d0c9b795d0a", "metadata": {"aucs": [0.9386374911203686, 0.9903481650461075, 0.9948128558348622], "final_y": [0.18187855981046708, 0.1648565164323461, 0.1648558823209001]}, "mutation_prompt": null}
{"id": "fd6dbcc2-ab3c-4809-b302-f6f2500e7fee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n\n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n\n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F + np.random.normal(0, 0.1)  # Adaptive scaling factor\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n\n        return new_population\n\n    def periodicity_based_mutation(self, population, bounds):\n        for ind in population:\n            if np.random.rand() < 0.5:  # Increased to 50% chance to apply periodic mutation\n                ind += np.sin(np.linspace(0, 2 * np.pi, self.dim)) * (bounds.ub - bounds.lb) * 0.15  # Adjusted amplitude\n                ind = np.clip(ind, bounds.lb, bounds.ub)\n        return population\n\n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T, options={'maxiter': 50})  # Set max iterations\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n\n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = self.periodicity_based_mutation(trial_population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n\n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n\n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved Hybrid Metaheuristic Optimizer with enhanced periodic mutation and adaptive local search for better performance in multilayer photonic structure optimization.", "configspace": "", "generation": 2, "fitness": 0.9809742696015814, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7335ad11-4ced-48a7-b964-3d0c9b795d0a", "metadata": {"aucs": [0.9584656024831972, 0.9903473775043795, 0.9941098288171676], "final_y": [0.16485581305555885, 0.16485682980550842, 0.16485588445303212]}, "mutation_prompt": null}
{"id": "c546ab98-0adf-48d2-a5b5-d169b26c5ba6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - 0.5 * (self.budget_used / self.budget))  # Refined adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            if np.random.rand() < 0.25:  # Increased chance for periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Randomized period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 12  # Slightly increased population for better exploration\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with adaptive periodic pattern mutation and targeted local exploration for optimizing multilayer photonic structures.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "8e9694dc-5d11-42b7-8ac7-e9219efb335d", "metadata": {}, "mutation_prompt": null}
{"id": "2ab2fa49-b15d-4b3d-9ae3-ea387cd57afe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Adaptive periodic mutation rate\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search with adaptive periodic mutation rate to better exploit multi-layer periodicity for improved optimization.", "configspace": "", "generation": 3, "fitness": 0.9911543684144047, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e9694dc-5d11-42b7-8ac7-e9219efb335d", "metadata": {"aucs": [0.989815649276637, 0.9903482074999352, 0.9932992484666422], "final_y": [0.1648557719047422, 0.16485671061720453, 0.1648560606207724]}, "mutation_prompt": null}
{"id": "5c360989-1142-41e5-af0e-61b0a12ae64d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget**2))  # Enhanced mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation\n            if np.random.rand() < 0.2:  # 20% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive mutation scaling by incorporating budget efficiency to improve exploration-exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.8673901014802027, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.125. And the mean value of best solutions found was 0.212 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "8e9694dc-5d11-42b7-8ac7-e9219efb335d", "metadata": {"aucs": [0.9873001994115275, 0.9205094500986297, 0.6943606549304506], "final_y": [0.16485577190469802, 0.18813079949988565, 0.2827467642603374]}, "mutation_prompt": null}
{"id": "05c6cd51-f0c4-46c2-a021-9156ae819044", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with adaptive crossover and improved periodic mutation integration for optimal multilayer photonic structure design.", "configspace": "", "generation": 3, "fitness": 0.9885256121861214, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e9694dc-5d11-42b7-8ac7-e9219efb335d", "metadata": {"aucs": [0.9845169929842256, 0.9907068583144291, 0.9903529852597095], "final_y": [0.16485577262763929, 0.1648562844748559, 0.1648559682719576]}, "mutation_prompt": null}
{"id": "8598c555-59c9-41c8-8f0f-06f292b30814", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n\n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n\n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F + np.random.normal(0, 0.1)  # Adaptive scaling factor\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n\n        return new_population\n\n    def periodicity_based_mutation(self, population, bounds):\n        for ind in population:\n            if np.random.rand() < 0.3 + 0.2 * np.sin(self.budget_used / 10.0):  # Adaptive mutation frequency\n                ind += np.sin(np.linspace(0, 2 * np.pi, self.dim)) * (bounds.ub - bounds.lb) * 0.15  # Adjusted amplitude\n                ind = np.clip(ind, bounds.lb, bounds.ub)\n        return population\n\n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T, options={'maxiter': 30})  # Adjusted max iterations\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n\n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = self.periodicity_based_mutation(trial_population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n\n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n\n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Optimized Hybrid Metaheuristic Optimizer with adaptive periodic pattern mutation frequency and enhanced local search termination.", "configspace": "", "generation": 3, "fitness": 0.9647657143679184, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.021. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "fd6dbcc2-ab3c-4809-b302-f6f2500e7fee", "metadata": {"aucs": [0.9389629301614539, 0.9903479196460694, 0.9649862932962315], "final_y": [0.1818781553358827, 0.16485687301472318, 0.1648576207991218]}, "mutation_prompt": null}
{"id": "1464f9d3-927b-4789-8ec2-2460d2c331a9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n\n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n\n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F + np.random.normal(0, 0.1)  # Adaptive scaling factor\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n\n        return new_population\n\n    def periodicity_based_mutation(self, population, bounds):\n        amplitude = 0.1 + 0.05 * np.random.rand()  # Adaptive amplitude\n        for ind in population:\n            if np.random.rand() < 0.5:  # Increased to 50% chance to apply periodic mutation\n                ind += np.sin(np.linspace(0, 2 * np.pi, self.dim)) * (bounds.ub - bounds.lb) * amplitude\n                ind = np.clip(ind, bounds.lb, bounds.ub)\n        return population\n\n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T, options={'maxiter': 100})  # Increased max iterations\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n\n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = self.periodicity_based_mutation(trial_population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n\n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n\n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Further refined Hybrid Metaheuristic Optimizer by introducing adaptive periodic mutation amplitude and improved local search strategy.", "configspace": "", "generation": 3, "fitness": 0.8892429807140356, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.102. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "fd6dbcc2-ab3c-4809-b302-f6f2500e7fee", "metadata": {"aucs": [0.9386376727527922, 0.9812949739499591, 0.7477962954393558], "final_y": [0.18187817824722097, 0.1648560377826035, 0.25781033200969394]}, "mutation_prompt": null}
{"id": "f91cf92e-a70f-43cc-86b9-602b4c7d008e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Adaptive periodic mutation\n            periodic_prob = 0.1 + 0.1 * (self.budget_used / self.budget)\n            if np.random.rand() < periodic_prob:  # Adaptive chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refined Enhanced Hybrid Metaheuristic Optimizer with adaptive periodic mutation scaling to better balance exploration and exploitation in optimization of multilayer photonic structures.", "configspace": "", "generation": 3, "fitness": 0.9715722020738906, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.023. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "8e9694dc-5d11-42b7-8ac7-e9219efb335d", "metadata": {"aucs": [0.9855645315269982, 0.9903485984989804, 0.9388034761956933], "final_y": [0.16485577237892435, 0.16485650915033545, 0.18188108399445901]}, "mutation_prompt": null}
{"id": "cfa6d192-db26-438e-bd85-8d95b0511755", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation\n            if np.random.rand() < 0.2:  # 20% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period][::-1], self.dim // period)[:self.dim]  # Lateral inversion\n\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced version of the HybridMetaheuristicOptimizer incorporating lateral inversion of periodic patterns to further exploit constructive interference.", "configspace": "", "generation": 3, "fitness": 0.9456508793087534, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.946 with standard deviation 0.049. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "8e9694dc-5d11-42b7-8ac7-e9219efb335d", "metadata": {"aucs": [0.9650311082451796, 0.8778117062741996, 0.9941098234068808], "final_y": [0.16485599469943812, 0.18187822583124846, 0.1648560774173402]}, "mutation_prompt": null}
{"id": "3d4cece8-ddea-483b-b86f-c3e35f16e0ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n\n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n\n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F + np.random.normal(0, 0.1)  # Adaptive scaling factor\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n\n        return new_population\n\n    def periodicity_based_mutation(self, population, bounds):\n        for ind in population:\n            if np.random.rand() < 0.6:  # Increased to 60% chance to apply periodic mutation\n                amplitude = np.random.uniform(0.1, 0.2)  # Adaptive amplitude\n                ind += np.sin(np.linspace(0, 2 * np.pi, self.dim)) * (bounds.ub - bounds.lb) * amplitude\n                ind = np.clip(ind, bounds.lb, bounds.ub)\n        return population\n\n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T, options={'maxiter': 50})  # Set max iterations\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 12  # Increased population size\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n\n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = self.periodicity_based_mutation(trial_population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n\n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n\n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refined Hybrid Metaheuristic Optimizer with increased population diversity and adaptive periodic mutation amplitude for improved optimization of multilayer photonic structures.", "configspace": "", "generation": 3, "fitness": 0.9139991061861706, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.914 with standard deviation 0.074. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "fd6dbcc2-ab3c-4809-b302-f6f2500e7fee", "metadata": {"aucs": [0.938198837432619, 0.990347045202537, 0.813451435923356], "final_y": [0.18187920300647287, 0.16485735675772561, 0.22804557173776452]}, "mutation_prompt": null}
{"id": "01ff8a8a-dffc-4887-b4ee-5dfbf782e153", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n\n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n\n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F + np.random.normal(0, 0.1)  # Adaptive scaling factor\n            mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n\n        return new_population\n\n    def periodicity_based_mutation(self, population, bounds):\n        for ind in population:\n            if np.random.rand() < 0.5:  # Increased to 50% chance to apply periodic mutation\n                ind += np.sin(np.linspace(0, 2 * np.pi, self.dim)) * (bounds.ub - bounds.lb) * 0.17  # Adjusted amplitude to 0.17\n                ind = np.clip(ind, bounds.lb, bounds.ub)\n        return population\n\n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T, options={'maxiter': 50})  # Set max iterations\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n\n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_population = self.periodicity_based_mutation(trial_population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n\n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n\n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n\n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced the periodicity-based mutation by increasing the mutation amplitude slightly to further exploit constructive interference.", "configspace": "", "generation": 3, "fitness": 0.8905861866618335, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.074. And the mean value of best solutions found was 0.184 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "fd6dbcc2-ab3c-4809-b302-f6f2500e7fee", "metadata": {"aucs": [0.8276240471559556, 0.8500249306058683, 0.9941095822236768], "final_y": [0.20044503639860478, 0.18813117370715748, 0.16485591748882444]}, "mutation_prompt": null}
{"id": "0d8c18c5-1ccb-4df3-9a30-4756f7f9fede", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n          \n            if np.random.rand() < 0.5:  # Increased chance for periodic mutation\n                period = self.dim // 4 + np.random.randint(-1, 2)\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + np.random.beta(0.5, 0.5))  # Altered adaptive crossover\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial + 0.01 * np.random.normal(size=self.dim)  # Noise handling\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with dual-phase adaptation by integrating dynamic crossover/mutation adaptations and noise handling to refine solutions.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "05c6cd51-f0c4-46c2-a021-9156ae819044", "metadata": {}, "mutation_prompt": null}
{"id": "0c92a9f0-9b03-476e-8828-0d42a817ffac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with adaptive periodic crossover mechanism and targeted local refinement to improve reflectivity optimization in multilayer photonic structures.", "configspace": "", "generation": 4, "fitness": 0.9914640006857578, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05c6cd51-f0c4-46c2-a021-9156ae819044", "metadata": {"aucs": [0.9923943012729349, 0.9903502886756901, 0.9916474121086487], "final_y": [0.1648557719047029, 0.1648558095974051, 0.1648557721075099]}, "mutation_prompt": null}
{"id": "62eea5e5-3ddc-4eaa-a46f-842ad9946bd8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            \n            # Adaptive periodic mutation rate\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Self-adaptive crossover rate\n            adaptive_CR = CR * (1 - (self.budget_used / self.budget))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        # Adaptive local search intensification\n        iter_count = 5 + self.budget_used // (self.budget // 10)\n        result = minimize(func, best, method='L-BFGS-B', options={'maxiter': iter_count}, bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer incorporating adaptive local intensification and self-adaptive crossover rates for improved exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.9888373127605211, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ab2fa49-b15d-4b3d-9ae3-ea387cd57afe", "metadata": {"aucs": [0.9873349245250568, 0.9854557649573908, 0.9937212487991154], "final_y": [0.1648558347128175, 0.16485577190469847, 0.164855771926934]}, "mutation_prompt": null}
{"id": "e36c948f-98dd-4d74-a1f6-efe9df24a6a9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Adaptive periodic mutation rate\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Modified adaptive periodic crossover\n            crossover = np.random.rand(self.dim) < (CR * (1 - (self.budget_used / self.budget)))\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer leveraging adaptive periodic crossover for improved reflectivity in photonic structure design.", "configspace": "", "generation": 4, "fitness": 0.9845876937836641, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ab2fa49-b15d-4b3d-9ae3-ea387cd57afe", "metadata": {"aucs": [0.9890599647845921, 0.9748317953926333, 0.9898713211737666], "final_y": [0.1648557719047169, 0.1648557719123943, 0.1648557757014979]}, "mutation_prompt": null}
{"id": "bce74dfe-a25c-41a6-81f5-a9bee67c2196", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Improved adaptive periodic mutation rate\n            if np.random.rand() < 0.4:  # 40% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T, options={'maxiter': 150})\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with improved adaptive periodic mutation and more efficient local search incorporation for multilayer photonic structures.", "configspace": "", "generation": 4, "fitness": 0.98467517813241, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ab2fa49-b15d-4b3d-9ae3-ea387cd57afe", "metadata": {"aucs": [0.9887696915429486, 0.9903512812459825, 0.9749045616082989], "final_y": [0.16485577190470102, 0.16485577190610345, 0.16485577333135004]}, "mutation_prompt": null}
{"id": "c4f0227a-1141-4e74-ab2c-ebd94d2ec3cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Adaptive periodic mutation rate\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            crossover = np.random.rand(self.dim) < CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10 + int(5 * (1 - (self.budget_used / self.budget)))  # Dynamic population size scaling\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced dynamic population size scaling to enhance exploration and exploitation balance in the optimization process.", "configspace": "", "generation": 4, "fitness": 0.9864131616779522, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ab2fa49-b15d-4b3d-9ae3-ea387cd57afe", "metadata": {"aucs": [0.9853914441823268, 0.9822008266732916, 0.9916472141782386], "final_y": [0.1648557719046978, 0.164855773963122, 0.16485577190470124]}, "mutation_prompt": null}
{"id": "8c7fa6f1-b83b-44fc-a566-c29a437919eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Increase the chance to apply periodic mutation\n            if np.random.rand() < 0.5:  # Increased to 50% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with improved periodic mutation rate for optimal multilayer photonic structure design.", "configspace": "", "generation": 4, "fitness": 0.991074162934694, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05c6cd51-f0c4-46c2-a021-9156ae819044", "metadata": {"aucs": [0.988665725814061, 0.9891461846731342, 0.9954105783168866], "final_y": [0.1648557719047118, 0.16485581070712674, 0.1648557736936399]}, "mutation_prompt": null}
{"id": "5b7548f5-c9e5-41e6-8e9f-22ce035c859d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Adaptive periodic mutation rate\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())  # Dynamic crossover rate\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search with dynamic crossover rate in DE and adaptive periodic mutation to better exploit multi-layer periodicity for improved optimization.", "configspace": "", "generation": 4, "fitness": 0.9851569646090077, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ab2fa49-b15d-4b3d-9ae3-ea387cd57afe", "metadata": {"aucs": [0.9857561666920012, 0.9898696729417165, 0.9798450541933056], "final_y": [0.164855771904737, 0.16485586293955157, 0.1648557769830583]}, "mutation_prompt": null}
{"id": "74e303f6-d365-4d76-bf8f-7ae5d438943c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Dynamically adjust periodic mutation probability\n            prob_periodic_mutation = 0.3 * (1 - (self.budget_used / self.budget))\n            if np.random.rand() < prob_periodic_mutation:\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved adaptive mutation strategy by dynamically adjusting periodic mutation probability based on budget usage, enhancing solution quality.", "configspace": "", "generation": 4, "fitness": 0.9840841151911168, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05c6cd51-f0c4-46c2-a021-9156ae819044", "metadata": {"aucs": [0.9891643650995943, 0.9714408467355065, 0.9916471337382496], "final_y": [0.16485577190541512, 0.1648557719046988, 0.16485577190568212]}, "mutation_prompt": null}
{"id": "4e4527c9-5d89-490d-9723-6ae0249b7815", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Dynamic periodic mutation integration\n            periodic_chance = 0.3 + 0.2 * np.sin(2 * np.pi * self.budget_used / self.budget)\n            if np.random.rand() < periodic_chance:  # Adjusted mutation chance\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            # Adaptive local search frequency\n            if self.budget_used + 1 < self.budget and np.random.rand() < 0.5:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved convergence through adaptive local search frequency and dynamic periodic mutation for enhanced photonic structure optimization.", "configspace": "", "generation": 4, "fitness": 0.984127116498088, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05c6cd51-f0c4-46c2-a021-9156ae819044", "metadata": {"aucs": [0.9899070556266081, 0.9859882869229062, 0.9764860069447497], "final_y": [0.16485577190595424, 0.16485668691702482, 0.1648557719123842]}, "mutation_prompt": null}
{"id": "b3f7d14b-ae74-4878-af4e-8df05eb183d2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance local refinement step by dynamically adjusting the search space bounds based on population variance to optimize convergence.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "0c92a9f0-9b03-476e-8828-0d42a817ffac", "metadata": {}, "mutation_prompt": null}
{"id": "4169e50b-d950-4f4d-8199-d8fedda67a33", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Enhanced periodic pattern mutation\n            if np.random.rand() < 0.35:  # 35% chance to apply periodic mutation\n                period = self.dim // 5\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid optimizer with dynamic exploration-exploitation balance and enhanced periodic crossover adapted to multilayer photonic structures optimization.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "0c92a9f0-9b03-476e-8828-0d42a817ffac", "metadata": {}, "mutation_prompt": null}
{"id": "5d43ba5b-c417-4166-b207-b807c0d6a2ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        # Added Gaussian noise for refinement in local search\n        noisy_best = best + np.random.normal(0, 0.01, size=self.dim)\n        result = minimize(func, noisy_best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with adaptive periodic crossover and Gaussian noise refinement for improved multilayer photonic structure optimization.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "0c92a9f0-9b03-476e-8828-0d42a817ffac", "metadata": {}, "mutation_prompt": null}
{"id": "393c8d87-872f-498d-92e8-21c17700fd87", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Increase the chance to apply periodic mutation\n            if np.random.rand() < 0.5:  # Increased to 50% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with dynamic adaptive mutation rate for optimal multilayer photonic structure design.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "8c7fa6f1-b83b-44fc-a566-c29a437919eb", "metadata": {}, "mutation_prompt": null}
{"id": "56e5b85b-5280-4966-9759-d90c8dbf5574", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / (2 * self.budget)))  # Fine-tuned adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Increase the chance to apply periodic mutation\n            if np.random.rand() < 0.5:  # Increased to 50% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with fine-tuned adaptive F scaling to improve optimization in multilayer photonic structures.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "8c7fa6f1-b83b-44fc-a566-c29a437919eb", "metadata": {}, "mutation_prompt": null}
{"id": "09033b60-7368-435b-b73b-97965d8c5782", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Increase the chance to apply periodic mutation\n            if np.random.rand() < 0.5:  # Increased to 50% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        pop_size = max(5, int(10 * (1 - self.budget_used / self.budget)))  # Dynamic population adjustment\n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with dynamic population size adjustment for improved exploration and exploitation balance in optimizing multilayer photonic structures.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "8c7fa6f1-b83b-44fc-a566-c29a437919eb", "metadata": {}, "mutation_prompt": null}
{"id": "72d421fb-9d28-46f4-baa9-26891c7ab287", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 12  # Increased from 10 to 12 for improved exploration\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with adaptive periodic crossover mechanism and targeted local refinement using improved exploration via increased pop_size in multilayer photonic structures.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "0c92a9f0-9b03-476e-8828-0d42a817ffac", "metadata": {}, "mutation_prompt": null}
{"id": "3058a26b-657b-4e59-90f0-4d59efedd3d3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Increase the chance to apply periodic mutation\n            if np.random.rand() < 0.5 + 0.3 * (self.budget_used / self.budget):  # Dynamically adjust periodic mutation rate\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with dynamic adaptive periodic mutation rate for optimal multilayer photonic structure design.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "8c7fa6f1-b83b-44fc-a566-c29a437919eb", "metadata": {}, "mutation_prompt": null}
{"id": "2659067d-b8b5-4522-97f5-133dcfc7ee2b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            adaptive_periodic_prob = 0.3 * (1 + 0.5 * np.cos(self.budget_used / self.budget * np.pi))  # Line changed\n            if np.random.rand() < adaptive_periodic_prob:\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with improved adaptive periodic mutation probability to optimize multilayer photonic structures.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "0c92a9f0-9b03-476e-8828-0d42a817ffac", "metadata": {}, "mutation_prompt": null}
{"id": "4a1eb4bd-794c-4e0a-aa53-f6b0ed9aaeff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Increase the chance to apply periodic mutation\n            if np.random.rand() < 0.5:  # Increased to 50% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate\n            adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 12  # Increased from 10 to 12\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with increased population size for improved exploration in multilayer photonic structure design.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "8c7fa6f1-b83b-44fc-a566-c29a437919eb", "metadata": {}, "mutation_prompt": null}
{"id": "5c7096af-fdfc-4554-8254-2f888e03bd70", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 12  # Increase initial population size for diversity\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improve convergence by initializing population with slightly increased size to enhance diversity and avoid premature convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "b3f7d14b-ae74-4878-af4e-8df05eb183d2", "metadata": {}, "mutation_prompt": null}
{"id": "515e447a-e1a0-4bfa-99e2-20eeddcafc8a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):  # Add population as argument\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)  # Pass population\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improve local refinement by ensuring population variance is correctly referenced to dynamically adjust bounds, enhancing convergence.", "configspace": "", "generation": 6, "fitness": 0.9876262037639271, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b3f7d14b-ae74-4878-af4e-8df05eb183d2", "metadata": {"aucs": [0.9923946348088752, 0.9742982501284602, 0.9961857263544457], "final_y": [0.16485577353035186, 0.16485577357198322, 0.1648557719053656]}, "mutation_prompt": null}
{"id": "ad0178a9-ce0e-4953-80c2-3bdedd10b871", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Enhanced periodic pattern mutation\n            if np.random.rand() < 0.35:  # 35% chance to apply periodic mutation\n                period = self.dim // 5\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        method = 'L-BFGS-B' if self.budget_used / self.budget < 0.5 else 'TNC'  # Dynamic method selection\n        result = minimize(func, best, method=method, bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune the local search step by adapting the search method dynamically to enhance convergence in complex landscapes.", "configspace": "", "generation": 6, "fitness": 0.9862521437117439, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4169e50b-d950-4f4d-8199-d8fedda67a33", "metadata": {"aucs": [0.9923926025014987, 0.9875520461761981, 0.9788117824575346], "final_y": [0.16485577190592948, 0.16485577390028838, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "f3388daf-e3a0-42e2-a500-f3c30c90401e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.7, CR=0.85):  # Adjusted parameters\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Enhanced periodic pattern mutation\n            if np.random.rand() < 0.35:  # 35% chance to apply periodic mutation\n                period = self.dim // 5\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by modifying mutation strategy and improve crossover effectiveness through dynamic adjustment.", "configspace": "", "generation": 6, "fitness": 0.9881749703810092, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4169e50b-d950-4f4d-8199-d8fedda67a33", "metadata": {"aucs": [0.9941887544411852, 0.9903511421871649, 0.9799850145146777], "final_y": [0.16485577190470102, 0.16485586730891744, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "105461e1-c335-423a-b998-1eb3564f3fe7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):  # Added 'population' parameter\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)  # Passed 'population'\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine local refinement to utilize correct population variable reference.", "configspace": "", "generation": 6, "fitness": 0.9897975055498267, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.990 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b3f7d14b-ae74-4878-af4e-8df05eb183d2", "metadata": {"aucs": [0.9926061426337047, 0.9903511398193027, 0.9864352341964725], "final_y": [0.16485577191390488, 0.16485577190739353, 0.16485577193169287]}, "mutation_prompt": null}
{"id": "bb7eded1-fa60-47c3-b631-e70c93923402", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Enhanced periodic pattern mutation\n            if np.random.rand() < 0.35:  # 35% chance to apply periodic mutation\n                period = self.dim // 5\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(2 * self.budget_used / self.budget * np.pi))  # Modified adaptive_CR\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive crossover probability fluctuation to enhance exploration-exploitation balance over budgeting cycles.", "configspace": "", "generation": 6, "fitness": 0.9794910038011285, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4169e50b-d950-4f4d-8199-d8fedda67a33", "metadata": {"aucs": [0.9923938397124553, 0.9657271079618597, 0.9803520637290704], "final_y": [0.16485577190619805, 0.16485579905965886, 0.16485577190471346]}, "mutation_prompt": null}
{"id": "c8a79734-067c-4927-a1db-cd267bf17b32", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):  # Passed population as an argument\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved local refinement by correcting variable scope error, ensuring population variance calculation works correctly.", "configspace": "", "generation": 6, "fitness": 0.9883581223876426, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b3f7d14b-ae74-4878-af4e-8df05eb183d2", "metadata": {"aucs": [0.9923940548565967, 0.987391866241445, 0.9852884460648861], "final_y": [0.1648557719046978, 0.16485577194608303, 0.16485577192677936]}, "mutation_prompt": null}
{"id": "10602d36-138f-4b84-a827-93fe479f745f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        pop_variance = np.var(best, axis=0)  # Adjusted to capture the context correctly\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine local search step by accurately capturing population context for dynamic bounds adjustment.", "configspace": "", "generation": 6, "fitness": 0.9880622321546381, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b3f7d14b-ae74-4878-af4e-8df05eb183d2", "metadata": {"aucs": [0.992394410266613, 0.9889507437413483, 0.9828415424559531], "final_y": [0.1648557935092314, 0.16485580660632038, 0.1648557719046978]}, "mutation_prompt": null}
{"id": "a0bc7cd9-3bb7-4fdc-8c69-c47d55b07fa6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce local population context in variance calculation to refine local search bounds.", "configspace": "", "generation": 6, "fitness": 0.9908726837631886, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b3f7d14b-ae74-4878-af4e-8df05eb183d2", "metadata": {"aucs": [0.9923948811607107, 0.990350986834268, 0.989872183294587], "final_y": [0.16485577280276442, 0.1648557719046988, 0.1648557730596809]}, "mutation_prompt": null}
{"id": "b83695a2-6d75-408c-af74-5b8974665b8e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        X = np.clip(X, bounds.lb, bounds.ub)\n        X_opposite = np.clip(X_opposite, bounds.lb, bounds.ub)\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            if np.random.rand() < 0.35:\n                period = self.dim // 5\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds):\n        if np.random.rand() < 0.5:  # 50% chance to trigger local search\n            result = minimize(func, best, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n            return result.x if result.success else best\n        return best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer with adaptive quasi-oppositional strategy and selective local search integration to balance exploration and exploitation effectively for multilayer photonic structure optimization.", "configspace": "", "generation": 6, "fitness": 0.9815328939819438, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4169e50b-d950-4f4d-8199-d8fedda67a33", "metadata": {"aucs": [0.9923969331308339, 0.9757985436365508, 0.9764032051784468], "final_y": [0.16485577190826495, 0.16485579502059988, 0.16485577190482947]}, "mutation_prompt": null}
{"id": "49096329-63c3-460e-9aab-36587c5269d6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate based on fitness\n            adaptive_CR = CR * (fitness[i] / np.max(fitness))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive crossover rate based on current solution fitness to enhance exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'fitness' is not defined\").", "error": "NameError(\"name 'fitness' is not defined\")", "parent_id": "a0bc7cd9-3bb7-4fdc-8c69-c47d55b07fa6", "metadata": {}, "mutation_prompt": null}
{"id": "f695b907-0358-45a9-af32-28c3b2d5723e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance population diversity and adaptive mutation strategies for refined local exploration.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "a0bc7cd9-3bb7-4fdc-8c69-c47d55b07fa6", "metadata": {}, "mutation_prompt": null}
{"id": "d82b1003-25c4-4a63-aed7-10f8a76fb9e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):  # Added 'population' parameter\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - 0.5 * pop_variance, bounds.ub + 0.5 * pop_variance)  # Adjusted scaling factor\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)  # Passed 'population'\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance local search by adjusting dynamic bounds scaling based on population variance to improve fine-tuning precision.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "105461e1-c335-423a-b998-1eb3564f3fe7", "metadata": {}, "mutation_prompt": null}
{"id": "b0d97ec7-a448-4ccb-87d8-5c7b8983294c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_std = np.std(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_std, bounds.ub + pop_std)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine local search by dynamically adapting the bounds using standard deviation instead of variance for better convergence.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "a0bc7cd9-3bb7-4fdc-8c69-c47d55b07fa6", "metadata": {}, "mutation_prompt": null}
{"id": "118c736b-07c1-4689-acd4-1a259af8b032", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (0.5 + 0.5 * np.cos(self.budget_used / self.budget * np.pi))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))  # Unchanged line\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the optimization process by introducing adaptive scaling for both mutation and crossover rates in the differential evolution step to improve convergence.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "a0bc7cd9-3bb7-4fdc-8c69-c47d55b07fa6", "metadata": {}, "mutation_prompt": null}
{"id": "2997e44d-8806-4d2f-af02-9b3db5a8cfc1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (0.8 + 0.2 * np.sin(self.budget_used / self.budget * np.pi))  # Self-adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb * 0.9, bounds.ub * 1.1)  # Enhanced dynamic bounds\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a self-adaptive mutation rate and enhance local search with dynamic bounds for improved convergence.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "a0bc7cd9-3bb7-4fdc-8c69-c47d55b07fa6", "metadata": {}, "mutation_prompt": null}
{"id": "9f31a486-fd54-4620-ab9a-538bde0e3f47", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Changed line: Adaptive periodic pattern mutation based on budget progress\n            if np.random.rand() < 0.3 * (1 - (self.budget_used / self.budget)):  # Adjust mutation probability\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive periodic pattern mutation strategy based on the budget progress to enhance exploration.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "105461e1-c335-423a-b998-1eb3564f3fe7", "metadata": {}, "mutation_prompt": null}
{"id": "967a6683-6e90-4698-86c5-d02d68eb2003", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            adaptive_periodic_chance = 0.3 + 0.2 * np.cos(self.budget_used / self.budget * np.pi)  # Adaptive probability\n            if np.random.rand() < adaptive_periodic_chance:\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):  # Added 'population' parameter\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)  # Passed 'population'\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid algorithm with adaptive periodic mutation probability for improved convergence in reflective multilayer design.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "105461e1-c335-423a-b998-1eb3564f3fe7", "metadata": {}, "mutation_prompt": null}
{"id": "3961aeeb-62a3-43ec-adbb-fff4bb15f9be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        diversity = np.mean(np.std(population, axis=0))  # Measure population diversity\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - diversity)  # Use diversity to adjust mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            if np.random.rand() < 0.3 + 0.2 * diversity:  # Adjust periodic mutation probability\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi) * diversity)\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive periodic mutation and crossover rates using dynamic population diversity metrics to improve convergence speed.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "a0bc7cd9-3bb7-4fdc-8c69-c47d55b07fa6", "metadata": {}, "mutation_prompt": null}
{"id": "f354597c-6639-48f1-9986-a2f2dad8be6d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_scaling = 1 + np.sin(self.budget_used / self.budget * np.pi)  # Added dynamic scaling\n        dynamic_bounds = (bounds.lb - dynamic_scaling * pop_variance, bounds.ub + dynamic_scaling * pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance local search by utilizing a dynamically scaled variance in bounds calculation for more precise refinements.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "a0bc7cd9-3bb7-4fdc-8c69-c47d55b07fa6", "metadata": {}, "mutation_prompt": null}
{"id": "31754589-237a-4a45-8e28-542ea6c278a4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improve selection of indices for DE step to avoid dimensionality mismatch errors.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "f695b907-0358-45a9-af32-28c3b2d5723e", "metadata": {}, "mutation_prompt": null}
{"id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            # Ensure `mutant` has the same shape as `a` to avoid broadcasting errors\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)  \n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine mutant generation to ensure shape compatibility and prevent broadcasting errors during DE steps.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "f695b907-0358-45a9-af32-28c3b2d5723e", "metadata": {}, "mutation_prompt": null}
{"id": "481e2646-58d5-44bc-a70a-6cd5798d241b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            if np.random.rand() < 0.3:  # Adjusted to 30% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 5)  # Reduced range for period length\n                modular_size = self.dim // period  # Ensure full modularity\n                mutant = np.tile(mutant[:period], modular_size)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Adjusted periodic mutation and bounds handling for improved modularity in multilayer design optimization.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "f695b907-0358-45a9-af32-28c3b2d5723e", "metadata": {}, "mutation_prompt": null}
{"id": "1cfc64a3-f93e-4f25-8b76-72c90e284cb4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (np.broadcast_to(bounds.lb, pop_variance.shape) - pop_variance, np.broadcast_to(bounds.ub, pop_variance.shape) + pop_variance)  # Corrected indexing\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Implement a correction for indexing in dynamic bounds adjustment to ensure proper shape handling.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "f695b907-0358-45a9-af32-28c3b2d5723e", "metadata": {}, "mutation_prompt": null}
{"id": "611d25ac-a3b6-493c-90b6-4d62e918e011", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        fitness = np.array([func(ind) for ind in population])  # Initialize fitness here\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate based on fitness\n            adaptive_CR = CR * (fitness[i] / np.max(fitness))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Fix the uninitialized `fitness` variable by calculating it prior to being used in the differential evolution step.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "49096329-63c3-460e-9aab-36587c5269d6", "metadata": {}, "mutation_prompt": null}
{"id": "b7ee3449-3ce8-4a57-9b22-d46b15f7ab10", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, fitness, F=0.5, CR=0.9):  # Pass fitness as argument\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            if np.random.rand() < 0.3:  # 30% chance to apply periodic mutation\n                period = self.dim // 4\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive crossover rate based on fitness\n            adaptive_CR = CR * (fitness[i] / np.max(fitness) if np.max(fitness) > 0 else CR)\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds, fitness)  # Add fitness parameter\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce fitness initialization and corrections to adaptive crossover strategy for improved exploration-exploitation balance.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "49096329-63c3-460e-9aab-36587c5269d6", "metadata": {}, "mutation_prompt": null}
{"id": "8f02f8f9-93ee-4402-91bf-a1c907fa5f17", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period + 1)[:self.dim]  # Fix mismatch\n\n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Implement a fix for dimensionality mismatch during array operations in the DE step to ensure solution feasibility.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "f695b907-0358-45a9-af32-28c3b2d5723e", "metadata": {}, "mutation_prompt": null}
{"id": "d811bdf1-2ba1-48c6-b91f-e2b6e6107ee0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            if np.random.rand() < 0.4:  \n                period = np.random.randint(2, self.dim // 2)  \n                mutant = np.tile(mutant[:period], self.dim // period + 1)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (np.maximum(bounds.lb - pop_variance, bounds.lb), \n                          np.minimum(bounds.ub + pop_variance, bounds.ub))\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  \n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance diversity by using adaptive period lengths and dynamic bounds expansion in local search for improved local exploration.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "f695b907-0358-45a9-af32-28c3b2d5723e", "metadata": {}, "mutation_prompt": null}
{"id": "f6cbabcf-71cc-4775-8991-063e7f90cebb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = max(1, self.dim // np.random.randint(3, 6))  # Ensure period is at least 1\n                mutant = np.tile(mutant[:period], -(-self.dim // period))[:self.dim]  # Correct index handling\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive period length in periodic mutation and correct index handling for dimensional consistency.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "f695b907-0358-45a9-af32-28c3b2d5723e", "metadata": {}, "mutation_prompt": null}
{"id": "692728f2-3106-46bc-8a2f-d210e91e0c88", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine the adaptive crossover strategy by ensuring all elements are correctly broadcasted to match dimensions.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "f695b907-0358-45a9-af32-28c3b2d5723e", "metadata": {}, "mutation_prompt": null}
{"id": "0cf718e6-223d-4325-872a-869f69ec45ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n            \n            if np.random.rand() < 0.4:\n                period = self.dim // np.random.randint(3, 6)\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance Differential Evolution indexing to prevent shape mismatches and ensure compatibility.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "31754589-237a-4a45-8e28-542ea6c278a4", "metadata": {}, "mutation_prompt": null}
{"id": "8dfdc404-7f98-4fa7-aa34-55f9cb86752d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False, size=self.dim)]  # Adjusted to resolve mismatch\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Adjusted index selection in DE step to address dimensionality mismatch.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"choice() got multiple values for keyword argument 'size'\").", "error": "TypeError(\"choice() got multiple values for keyword argument 'size'\")", "parent_id": "31754589-237a-4a45-8e28-542ea6c278a4", "metadata": {}, "mutation_prompt": null}
{"id": "d509846a-1d82-4b1b-a8af-772c35b7ad53", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            # Ensure `mutant` has the same shape as `a` to avoid broadcasting errors\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)  \n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Ensure consistent shapes in differential evolution to prevent broadcasting errors.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "metadata": {}, "mutation_prompt": null}
{"id": "b1c0db71-4547-4b91-bad6-121909323b12", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False, size=3)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)  \n\n            if np.random.rand() < 0.4:\n                period = self.dim // np.random.randint(3, 6)\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced differential evolution step to prevent dimensionality mismatch by adjusting indices selection.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"choice() got multiple values for keyword argument 'size'\").", "error": "TypeError(\"choice() got multiple values for keyword argument 'size'\")", "parent_id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "metadata": {}, "mutation_prompt": null}
{"id": "5e87662a-3ae0-495d-b799-9d4ba780d000", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            # Ensure `mutant` has the same shape as `a` to avoid broadcasting errors\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Utilize consistent dimension handling in the DE step to avert broadcasting errors.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "metadata": {}, "mutation_prompt": null}
{"id": "b522aa0b-2fff-460e-82b2-3777d7985c63", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)\n\n            if np.random.rand() < 0.4:\n                period = self.dim // np.random.randint(3, 6)\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            # Ensure crossover operation handles shape compatibility\n            crossover = np.pad(crossover, (0, max(0, len(a) - len(crossover))), 'constant', constant_values=False)\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Implement adaptive dimensional crossover to address broadcasting errors and enhance DE step robustness.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "metadata": {}, "mutation_prompt": null}
{"id": "d1587c38-6af6-42ce-b2f2-ae607a310f7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            # Ensure selection of indices works with the population size\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]  \n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)\n            \n            if np.random.rand() < 0.4:\n                period = self.dim // np.random.randint(3, 6)\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Adaptive mutation scaling and selection strategy refined to avoid dimensionality mismatch errors in DE steps.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (9,) (10,) ')", "parent_id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "metadata": {}, "mutation_prompt": null}
{"id": "14854b65-4cbb-417d-836d-8f1918561a8e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            # Ensure `mutant` has the same shape as `a` to avoid broadcasting errors\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)\n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = self.dim // np.random.randint(3, 6)  # Dynamic period length for diversity\n                mutant = np.tile(mutant[:period], self.dim // period)[:self.dim].reshape(mutant.shape)  # Ensure correct shape\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Apply a consistent shape correction to ensure compatibility during DE steps.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('cannot reshape array of size 9 into shape (10,)').", "error": "ValueError('cannot reshape array of size 9 into shape (10,)')", "parent_id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "metadata": {}, "mutation_prompt": null}
{"id": "9d1713da-0603-43ce-bf3a-e65a07f430a1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)  # Ensure correct shape\n\n            if np.random.rand() < 0.4:\n                period = self.dim // np.random.randint(3, 6)\n                mutant[:period * (self.dim // period)] = np.tile(mutant[:period], self.dim // period)  # Correct tiling\n\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Adaptive DE by refining population initialization and mutation strategy to fix dimensionality issues.", "configspace": "", "generation": 9, "fitness": 0.9846131345247212, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "metadata": {"aucs": [0.9923934669229006, 0.9893744462519004, 0.9720714903993628], "final_y": [0.16485577190470246, 0.16485577537471885, 0.16485578980309756]}, "mutation_prompt": null}
{"id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            # Ensure `mutant` has the same shape as `a` to avoid broadcasting errors\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)  \n\n            # Integrate periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = max(1, self.dim // np.random.randint(3, 6))  # Ensure period is at least 1\n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptability of DE steps to improve compatibility with population shapes and optimize periodicity integration.", "configspace": "", "generation": 9, "fitness": 0.9862462332840961, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "be136dd5-6eb3-4c60-b8ad-85b0f4e28aa1", "metadata": {"aucs": [0.9923943132708667, 0.9893744926518894, 0.9769698939295323], "final_y": [0.1648557719046978, 0.16485577204552448, 0.16485577199082746]}, "mutation_prompt": null}
{"id": "76e14a96-bfd2-438b-9677-2b916a4d5bc5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        fitness_variance = np.var([func(ind) for ind in population])  # Calculate fitness variance\n\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)\n\n            if np.random.rand() < 0.4:  # Increased to 40% chance to apply periodic mutation\n                period = max(1, self.dim // np.random.randint(3, 6))\n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi)) + 0.1 * fitness_variance\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine adaptive DE by incorporating dynamic CR adjustment based on fitness variance for enhanced exploration-exploitation balance.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "metadata": {}, "mutation_prompt": null}
{"id": "9d328007-6aee-4235-bcc6-0829dce8e406", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)  \n\n            # Refined periodic pattern mutation with adaptive probability\n            if np.random.rand() < 0.35:  # Changed to 35% chance to apply periodic mutation\n                period = max(1, self.dim // np.random.randint(3, 6))  # Ensure period is at least 1\n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptability of DE steps to improve compatibility with population shapes and optimize periodicity integration, with refined periodic pattern mutation probability.", "configspace": "", "generation": 10, "fitness": 0.9926644260622944, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "metadata": {"aucs": [0.9923919392563698, 0.9927529640239617, 0.9928483749065519], "final_y": [0.16485614369192136, 0.1648557795531418, 0.16485577389462736]}, "mutation_prompt": null}
{"id": "e2843545-099c-4393-8168-af0cd25705a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))  # Dynamic mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)  \n\n            if np.random.rand() < 0.4:  \n                period = max(1, self.dim // np.random.randint(3, 6))  \n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  \n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic mutation strategy for enhanced adaptability in varying optimization phases.", "configspace": "", "generation": 10, "fitness": 0.9787064739436148, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "metadata": {"aucs": [0.9889575757016249, 0.9679680937820623, 0.9791937523471571], "final_y": [0.16485577234050586, 0.16485694086757452, 0.16485577192754985]}, "mutation_prompt": null}
{"id": "17f1ac61-c865-4c77-9ea7-a5e37ed6db4b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            period = self.dim // np.random.randint(3, 6)\n            mutant[:period * (self.dim // period)] = np.tile(mutant[:period], self.dim // period)  # Updated tiling\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        if not result.success:  # Enhanced backup search\n            random_point = np.random.uniform(bounds.lb, bounds.ub)\n            result = minimize(func, random_point, method='L-BFGS-B', bounds=np.array([bounds.lb, bounds.ub]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Optimized Adaptive Differential Evolution by enhancing local search and periodicity enforcement strategies.", "configspace": "", "generation": 10, "fitness": 0.9880880289096966, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9d1713da-0603-43ce-bf3a-e65a07f430a1", "metadata": {"aucs": [0.9844959325759591, 0.9893741037364617, 0.990394050416669], "final_y": [0.1648557719046978, 0.16485577190500744, 0.16485577190470224]}, "mutation_prompt": null}
{"id": "1597dbf0-ba35-4644-b2ba-d11445a2f5b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            # Adaptive mutation scaling incorporating a learning factor\n            adaptive_F = F * (1 - (self.budget_used / self.budget)**1.2)  \n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)  \n\n            if np.random.rand() < 0.4:\n                period = max(1, self.dim // np.random.randint(3, 6))\n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            # Enhanced adaptive crossover probability with learning factor\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * 2 * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        # Adjust convergence criteria for the local search\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T, options={'ftol': 1e-8})\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive learning for mutation and crossover rates, and improve local search conditions to enhance convergence rate while maintaining diversity.", "configspace": "", "generation": 10, "fitness": 0.9653556286863859, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.022. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "metadata": {"aucs": [0.9924116424201496, 0.9386356349455385, 0.9650196086934696], "final_y": [0.16485578559951775, 0.16485598214521757, 0.16485577592353795]}, "mutation_prompt": null}
{"id": "4e12f29f-7f18-4fe0-947c-874216e52346", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / (2 * self.budget)))  # Adaptive mutation scaling reduced\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)\n\n            if np.random.rand() < 0.5:  # Increased probability for periodic mutation\n                period = max(1, self.dim // np.random.randint(5, 8))  # Adjusted period range\n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            adaptive_CR = CR * (0.3 + 0.7 * np.sin(self.budget_used / self.budget * np.pi))  # Modified crossover\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved adaptive DE with enhanced periodic pattern integration and dynamic parameter adjustment for optimized convergence.", "configspace": "", "generation": 10, "fitness": 0.9784723912629981, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "metadata": {"aucs": [0.9794910532363409, 0.9893744395021675, 0.9665516810504861], "final_y": [0.16485625710026908, 0.1648557782221015, 0.16485577194341483]}, "mutation_prompt": null}
{"id": "c42c96f8-c23c-44bd-9bb1-c9338901ee82", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            # Modified line: Increase adaptive mutation scaling factor\n            adaptive_F = F * (1.2 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)  \n\n            if np.random.rand() < 0.4:  \n                period = max(1, self.dim // np.random.randint(3, 6))  \n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  \n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improve adaptive mutation strategy by increasing mutation scaling factor for better exploration.", "configspace": "", "generation": 10, "fitness": 0.9804469142030786, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "metadata": {"aucs": [0.9935179869633275, 0.9705281358501024, 0.9772946197958057], "final_y": [0.16485577190611445, 0.16485577190671785, 0.1648558803269239]}, "mutation_prompt": null}
{"id": "dc08573e-e1b7-4c0d-af1c-e6d2fefce949", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            # Ensure `mutant` has the same shape as `a` to avoid broadcasting errors\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)  \n\n            # Refine periodic pattern mutation probability\n            if np.random.rand() < 0.3 + 0.1 * np.sin(self.budget_used / self.budget * np.pi):  # Adjust to adaptively balance\n                period = max(1, self.dim // np.random.randint(3, 6))  # Ensure period is at least 1\n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            # Adaptive periodic crossover mechanism\n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]  # Only keep half of the total initialized\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine periodic pattern mutation probability to adaptively balance exploration and exploitation in the search space.", "configspace": "", "generation": 10, "fitness": 0.9884995099245057, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "metadata": {"aucs": [0.983164313559837, 0.9893743764303552, 0.992959839783325], "final_y": [0.1648557981474118, 0.16485577873328472, 0.16485597915161398]}, "mutation_prompt": null}
{"id": "45e223a5-cfa4-49d3-b710-35424422012e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))  # Adaptive mutation scaling\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub).reshape(a.shape)\n\n            if np.random.rand() < 0.4:\n                period = max(1, self.dim // np.random.randint(3, 6))\n                mutant = np.tile(mutant[:period], (self.dim // period) + 1)[:self.dim]\n            \n            adaptive_CR = CR * (0.5 + 0.5 * np.sin(self.budget_used / self.budget * np.pi))\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (np.maximum(bounds.lb, best - pop_variance), np.minimum(bounds.ub, best + pop_variance))\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = max(10, self.budget // 100)  # Dynamic population size based on budget\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Further enhance adaptability of DE by integrating dynamic population size and refined local search constraints.", "configspace": "", "generation": 10, "fitness": 0.9722124822389674, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "106d6525-ff30-4e66-b0c3-bd5fda8b8db8", "metadata": {"aucs": [0.9689960551369134, 0.9897112040672705, 0.9579301875127182], "final_y": [0.1648557763386992, 0.16485577190484468, 0.16485577875595514]}, "mutation_prompt": null}
{"id": "c0563c89-a43b-4eb1-b544-585be1399119", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.budget_used = 0\n    \n    def quasi_oppositional_initialization(self, bounds, pop_size):\n        X = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        X_opposite = bounds.lb + bounds.ub - X\n        return np.concatenate((X, X_opposite), axis=0)\n    \n    def differential_evolution_step(self, population, bounds, F=0.5, CR=0.9):\n        new_population = np.empty_like(population)\n        pop_size = len(population)\n        \n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = F * (1 - (self.budget_used / self.budget))\n            mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n\n            period = self.dim // np.random.randint(3, 6)\n            if np.random.rand() < 0.5:  # Changed probability to encourage periodic structures\n                mutant[:period * (self.dim // period)] = np.tile(mutant[:period], self.dim // period)\n\n            adaptive_CR = CR * (0.6 + 0.4 * np.cos(self.budget_used / self.budget * np.pi))  # Enhanced adaptive CR\n            crossover = np.random.rand(self.dim) < adaptive_CR\n            trial = np.where(crossover, mutant, population[i])\n            new_population[i] = trial\n        \n        return new_population\n    \n    def local_search(self, best, func, bounds, population):\n        pop_variance = np.var(population, axis=0)\n        dynamic_bounds = (bounds.lb - pop_variance, bounds.ub + pop_variance)\n        result = minimize(func, best, method='L-BFGS-B', bounds=np.array([dynamic_bounds[0], dynamic_bounds[1]]).T)\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        population = self.quasi_oppositional_initialization(bounds, pop_size)\n        population = population[:pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.budget_used += len(population)\n        \n        while self.budget_used < self.budget:\n            trial_population = self.differential_evolution_step(population, bounds)\n            trial_fitness = np.array([func(ind) for ind in trial_population])\n            self.budget_used += len(trial_population)\n            \n            for i in range(pop_size):\n                if trial_fitness[i] < fitness[i]:\n                    population[i], fitness[i] = trial_population[i], trial_fitness[i]\n            \n            if self.budget_used + 1 < self.budget:\n                best_idx = np.argmin(fitness)\n                best_solution = population[best_idx]\n                refined_solution = self.local_search(best_solution, func, bounds, population)\n                refined_fitness = func(refined_solution)\n                self.budget_used += 1\n                \n                if refined_fitness < fitness[best_idx]:\n                    population[best_idx], fitness[best_idx] = refined_solution, refined_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved differential mutation strategy by integrating a periodic structure encouragement mechanism and enhanced adaptive crossover probability for better exploration.", "configspace": "", "generation": 10, "fitness": 0.9820901387554438, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9d1713da-0603-43ce-bf3a-e65a07f430a1", "metadata": {"aucs": [0.9895961595119862, 0.9893741217093698, 0.9673001350449754], "final_y": [0.1648557719046978, 0.164855854327558, 0.16485577300990673]}, "mutation_prompt": null}
