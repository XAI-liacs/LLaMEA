{"id": "124dbbb9-448a-4a4e-8fa1-b91e1d3b8969", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom copy import deepcopy\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.best_solution = None\n        self.best_fitness = float('-inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                donor_vector = self.mutation(population, i)\n                trial_vector = self.crossover(population[i], donor_vector)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select based on fitness\n                if trial_fitness > func(population[i]):\n                    population[i] = trial_vector\n                    if trial_fitness > self.best_fitness:\n                        self.best_solution = deepcopy(trial_vector)\n                        self.best_fitness = trial_fitness\n\n            # Local search with periodicity encouragement\n            self.local_search(func, lb, ub)\n\n        return self.best_solution\n\n    def initialize_population(self, lb, ub):\n        return [np.random.uniform(lb, ub, self.dim) for _ in range(self.population_size)]\n\n    def mutation(self, population, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        return population[a] + self.mutation_factor * (population[b] - population[c])\n\n    def crossover(self, target, donor):\n        crossover = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        return np.where(crossover, donor, target)\n\n    def local_search(self, func, lb, ub):\n        # Encourage periodicity by using a periodic perturbation\n        for _ in range(2):  # Two iterations for periodicity injection\n            if self.best_solution is not None:\n                periodic_solution = self.best_solution * (1 + 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim))\n                result = minimize(func, periodic_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.fun > self.best_fitness:\n                    self.best_solution = result.x\n                    self.best_fitness = result.fun", "name": "BraggMirrorOptimizer", "description": "A novel hybrid metaheuristic algorithm that combines Differential Evolution with periodicity-injected local search to efficiently find high-performing solutions for multilayered photonic structures.", "configspace": "", "generation": 0, "fitness": 0.8109632680348331, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.096. And the mean value of best solutions found was 0.214 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7317105194850784, 0.7553959226824436, 0.9457833619369774], "final_y": [0.2578102665286214, 0.22065085422196717, 0.1648564168057629]}, "mutation_prompt": null}
{"id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Hybrid Metaheuristic Algorithm combining Differential Evolution with Symmetry-based Initialization and Local Search Refinement for Global and Local Exploration.", "configspace": "", "generation": 0, "fitness": 0.9831582390608414, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9871453187721476, 0.9835943841920544, 0.9787350142183223], "final_y": [0.16485632498843794, 0.16485607308806227, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "61202440-df1d-4a72-9d64-57c32920a452", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def periodicity_induced_search(self, pop, func, lb, ub):\n        \"\"\"Encourage periodicity in solutions.\"\"\"\n        periodic_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            # Modify mutation strategy to induce periodicity\n            phase_shift = np.random.uniform(-np.pi, np.pi, self.dim)\n            mutant = np.clip(a + self.f * (b - c) + np.sin(phase_shift), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            periodic_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return periodic_pop\n    \n    def adaptive_local_search(self, x, func, lb, ub, iteration):\n        \"\"\"Perform adaptive local search with variable precision.\"\"\"\n        epsilon = 1e-4 * (0.9 ** (iteration // 10))  # Adaptive precision\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options={'ftol': epsilon})\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        iteration = 0\n        while evaluations < self.budget:\n            pop = self.periodicity_induced_search(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.adaptive_local_search(pop[i], func, lb, ub, iteration)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n            \n            iteration += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "EnhancedHybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic Algorithm combining Differential Evolution with Periodicity-Induced Search and Adaptive Local Refinement for improved exploration and exploitation in complex landscapes.", "configspace": "", "generation": 1, "fitness": 0.9530353700461095, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9754797784751646, 0.9498734904258311, 0.9337528412373328], "final_y": [0.16493513857899444, 0.1648656729462964, 0.16492790564728488]}, "mutation_prompt": null}
{"id": "3469f8a4-25bb-4e7c-8df2-ac9942bc510c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom copy import deepcopy\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.best_solution = None\n        self.best_fitness = float('-inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                donor_vector = self.mutation(population, i)\n                trial_vector = self.crossover(population[i], donor_vector)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select based on fitness\n                if trial_fitness > func(population[i]):\n                    population[i] = trial_vector\n                    if trial_fitness > self.best_fitness:\n                        self.best_solution = deepcopy(trial_vector)\n                        self.best_fitness = trial_fitness\n\n            # Local search with periodicity encouragement\n            self.local_search(func, lb, ub)\n\n        return self.best_solution\n\n    def initialize_population(self, lb, ub):\n        return [np.random.uniform(lb, ub, self.dim) for _ in range(self.population_size)]\n\n    def mutation(self, population, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        return population[a] + self.mutation_factor * (population[b] - population[c])\n\n    def crossover(self, target, donor):\n        crossover = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        return np.where(crossover, donor, target)\n\n    def local_search(self, func, lb, ub):\n        # Encourage periodicity by using a periodic perturbation\n        for _ in range(2):  # Two iterations for periodicity injection\n            if self.best_solution is not None:\n                periodic_solution = self.best_solution * (1 + 0.05 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim))  # Adjusted amplitude\n                result = minimize(func, periodic_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.fun > self.best_fitness:\n                    self.best_solution = result.x\n                    self.best_fitness = result.fun", "name": "BraggMirrorOptimizer", "description": "Enhance the local search strategy by adjusting the periodic perturbation amplitude for better fine-tuning in the BraggMirrorOptimizer algorithm.", "configspace": "", "generation": 1, "fitness": 0.75377387766654, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.754 with standard deviation 0.066. And the mean value of best solutions found was 0.239 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "124dbbb9-448a-4a4e-8fa1-b91e1d3b8969", "metadata": {"aucs": [0.8389323857474438, 0.6787154635299985, 0.7436737837221776], "final_y": [0.20044636691927875, 0.2578106989029324, 0.2578121372873008]}, "mutation_prompt": null}
{"id": "3df3f788-3a8d-441b-82e2-6d9cfd16722e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom copy import deepcopy\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.best_solution = None\n        self.best_fitness = float('-inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                donor_vector = self.mutation(population, i)\n                trial_vector = self.crossover(population[i], donor_vector)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select based on fitness\n                if trial_fitness > func(population[i]):\n                    population[i] = trial_vector\n                    if trial_fitness > self.best_fitness:\n                        self.best_solution = deepcopy(trial_vector)\n                        self.best_fitness = trial_fitness\n\n            # Local search with periodicity encouragement\n            self.local_search(func, lb, ub)\n            self.mutation_factor = 0.6 + 0.4 * (self.best_fitness / 1.0)  # Adaptive mutation\n\n        return self.best_solution\n\n    def initialize_population(self, lb, ub):\n        return [np.random.uniform(lb, ub, self.dim) for _ in range(self.population_size)]\n\n    def mutation(self, population, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        return population[a] + self.mutation_factor * (population[b] - population[c])\n\n    def crossover(self, target, donor):\n        crossover = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        return np.where(crossover, donor, target)\n\n    def local_search(self, func, lb, ub):\n        # Encourage periodicity by using a periodic perturbation\n        for _ in range(3):  # More iterations for periodicity injection\n            if self.best_solution is not None:\n                periodic_solution = self.best_solution * (1 + 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim))\n                result = minimize(func, periodic_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.fun > self.best_fitness:\n                    self.best_solution = result.x\n                    self.best_fitness = result.fun", "name": "BraggMirrorOptimizer", "description": "Enhanced BraggMirrorOptimizer with improved periodicity encouragement and adaptive mutation factor adjustment for better convergence. ", "configspace": "", "generation": 1, "fitness": 0.6762717420905481, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.676 with standard deviation 0.129. And the mean value of best solutions found was 0.294 (0. is the best) with standard deviation 0.073.", "error": "", "parent_id": "124dbbb9-448a-4a4e-8fa1-b91e1d3b8969", "metadata": {"aucs": [0.7317105194850784, 0.49792448551906465, 0.7991802212675013], "final_y": [0.2578102665286214, 0.3948768045580757, 0.2280467105816778]}, "mutation_prompt": null}
{"id": "81f9b650-e992-4fe8-985d-50b299f6f57b", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                if np.random.rand() < 0.5:  # Probabilistic trigger for local search\n                    refined = self.local_search(pop[i], func, lb, ub)\n                    if func(refined) < func(pop[i]):\n                        pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "An enhanced Hybrid Metaheuristic, introducing a probabilistic local search trigger to fine-tune exploration near promising regions while respecting a 1.5% code modification constraint.", "configspace": "", "generation": 1, "fitness": 0.9721595443775802, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9541500485285331, 0.9835943813397471, 0.9787342032644604], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485671473855767]}, "mutation_prompt": null}
{"id": "1239f93f-541a-469f-8675-71975dcf2a65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom copy import deepcopy\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.best_solution = None\n        self.best_fitness = float('-inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                donor_vector = self.mutation(population, i)\n                trial_vector = self.crossover(population[i], donor_vector)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select based on fitness\n                if trial_fitness > func(population[i]):\n                    population[i] = trial_vector\n                    if trial_fitness > self.best_fitness:\n                        self.best_solution = deepcopy(trial_vector)\n                        self.best_fitness = trial_fitness\n\n            # Enhanced local search with adaptive periodicity\n            self.local_search(func, lb, ub)\n\n        return self.best_solution\n\n    def initialize_population(self, lb, ub):\n        return [np.random.uniform(lb, ub, self.dim) for _ in range(self.population_size)]\n\n    def mutation(self, population, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        return population[a] + self.mutation_factor * (population[b] - population[c])\n\n    def crossover(self, target, donor):\n        crossover = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        return np.where(crossover, donor, target)\n\n    def local_search(self, func, lb, ub):\n        # Adaptive periodic perturbation and two-stage local search\n        if self.best_solution is not None:\n            periodic_solution = self.best_solution * (1 + 0.05 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim))\n            result = minimize(func, periodic_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun > self.best_fitness:\n                self.best_solution = result.x\n                self.best_fitness = result.fun\n            # Additional local search stage\n            second_stage_solution = self.best_solution + 0.05 * np.random.randn(self.dim)\n            second_result = minimize(func, second_stage_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if second_result.fun > self.best_fitness:\n                self.best_solution = second_result.x\n                self.best_fitness = second_result.fun", "name": "BraggMirrorOptimizer", "description": "Enhanced hybrid metaheuristic algorithm combining Differential Evolution with two-stage local search and adaptive periodicity injection for improved optimization of multilayered photonic structures.", "configspace": "", "generation": 1, "fitness": 0.8027259873637639, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.057. And the mean value of best solutions found was 0.225 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "124dbbb9-448a-4a4e-8fa1-b91e1d3b8969", "metadata": {"aucs": [0.7341452188118774, 0.8748525220119131, 0.7991802212675013], "final_y": [0.2578104038820793, 0.18813101679514865, 0.2280467105816778]}, "mutation_prompt": null}
{"id": "d8c2966e-797f-4ded-841d-7aa39cf8b684", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        # Adaptive DE parameter adjustment\n        self.f = max(0.5, self.f * 0.99)\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced hybrid metaheuristic algorithm with adaptive DE parameters based on convergence speed to improve exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.9831581919686988, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871453187721476, 0.9835943813397471, 0.9787348757942016], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "e9c6307a-ad6a-4db9-a630-8ef681eea667", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic Algorithm introducing adaptive mutation factor and crossover rate for improved exploration and exploitation balance.", "configspace": "", "generation": 1, "fitness": 0.9832141296064741, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871453317724725, 0.9835943813397471, 0.9789026757072027], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485742362657085]}, "mutation_prompt": null}
{"id": "9b212395-2705-41cc-85f6-b9592fa8189f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom copy import deepcopy\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.best_solution = None\n        self.best_fitness = float('-inf')\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                donor_vector = self.mutation(population, i, evaluations)\n                trial_vector = self.crossover(population[i], donor_vector)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select based on fitness\n                if trial_fitness > func(population[i]):\n                    population[i] = trial_vector\n                    if trial_fitness > self.best_fitness:\n                        self.best_solution = deepcopy(trial_vector)\n                        self.best_fitness = trial_fitness\n\n            # Local search with periodicity encouragement\n            self.local_search(func, lb, ub)\n\n        return self.best_solution\n\n    def initialize_population(self, lb, ub):\n        return [np.random.uniform(lb, ub, self.dim) for _ in range(self.population_size)]\n\n    def mutation(self, population, target_idx, evaluations):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_factor = self.mutation_factor * (1 + 0.5 * np.sin(2 * np.pi * evaluations / self.budget))\n        return population[a] + adaptive_factor * (population[b] - population[c])\n\n    def crossover(self, target, donor):\n        crossover = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        return np.where(crossover, donor, target)\n\n    def local_search(self, func, lb, ub):\n        # Encourage periodicity by using a periodic perturbation\n        for _ in range(2):  # Two iterations for periodicity injection\n            if self.best_solution is not None:\n                periodic_solution = self.best_solution * (1 + 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim))\n                result = minimize(func, periodic_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.fun > self.best_fitness:\n                    self.best_solution = result.x\n                    self.best_fitness = result.fun", "name": "BraggMirrorOptimizer", "description": "A refined hybrid metaheuristic algorithm with enhanced local search using adaptive mutation to optimize multilayered photonic structures.", "configspace": "", "generation": 1, "fitness": 0.6997659815883498, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.700 with standard deviation 0.075. And the mean value of best solutions found was 0.277 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": "124dbbb9-448a-4a4e-8fa1-b91e1d3b8969", "metadata": {"aucs": [0.6824737675734213, 0.6176439559241267, 0.7991802212675013], "final_y": [0.2827467808412475, 0.31933168729233796, 0.2280467105816778]}, "mutation_prompt": null}
{"id": "1a240c0a-1e3b-41c4-bed1-056b5732f1ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom copy import deepcopy\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.best_solution = None\n        self.best_fitness = float('-inf')\n        self.adaptive_mutation_step = 0.1  # New adaptive mutation step\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                donor_vector = self.mutation(population, i)\n                trial_vector = self.crossover(population[i], donor_vector)\n\n                # Evaluate trial vector\n                trial_fitness = func(trial_vector)\n                evaluations += 1\n\n                # Select based on fitness\n                if trial_fitness > func(population[i]):\n                    population[i] = trial_vector\n                    if trial_fitness > self.best_fitness:\n                        self.best_solution = deepcopy(trial_vector)\n                        self.best_fitness = trial_fitness\n\n            # Local search with periodicity encouragement\n            self.local_search(func, lb, ub)\n            self.adaptive_mutation_factor()  # New adaptive mutation factor adjustment\n\n        return self.best_solution\n\n    def initialize_population(self, lb, ub):\n        return [np.random.uniform(lb, ub, self.dim) for _ in range(self.population_size)]\n\n    def mutation(self, population, target_idx):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        return population[a] + self.mutation_factor * (population[b] - population[c])\n\n    def crossover(self, target, donor):\n        crossover = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(crossover):\n            crossover[np.random.randint(0, self.dim)] = True\n        return np.where(crossover, donor, target)\n\n    def local_search(self, func, lb, ub):\n        # Encourage periodicity by using a periodic perturbation\n        for _ in range(2):  # Two iterations for periodicity injection\n            if self.best_solution is not None:\n                periodic_solution = self.best_solution * (1 + 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim))\n                result = minimize(func, periodic_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.fun > self.best_fitness:\n                    self.best_solution = result.x\n                    self.best_fitness = result.fun\n\n    def adaptive_mutation_factor(self):  # New method to adaptively adjust mutation factor\n        diversity = np.std([np.linalg.norm(ind) for ind in self.initialize_population(-1, 1)])\n        if diversity < 0.1:\n            self.mutation_factor += self.adaptive_mutation_step\n        else:\n            self.mutation_factor -= self.adaptive_mutation_step", "name": "BraggMirrorOptimizer", "description": "Enhanced BraggMirrorOptimizer with adaptive mutation strategy and diversity preservation for improved solution exploration.", "configspace": "", "generation": 1, "fitness": 0.7219087222569666, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.722 with standard deviation 0.067. And the mean value of best solutions found was 0.261 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "124dbbb9-448a-4a4e-8fa1-b91e1d3b8969", "metadata": {"aucs": [0.7317105194850784, 0.63483542601832, 0.7991802212675013], "final_y": [0.2578102665286214, 0.29814526784167916, 0.2280467105816778]}, "mutation_prompt": null}
{"id": "c18220e3-a121-4e79-b926-e1d75d7a39fe", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = np.clip(self.f * (1 + np.random.uniform(-0.1, 0.1)), 0, 2.0)  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved HybridMetaheuristic with adaptive mutation factor in Differential Evolution for enhanced convergence.", "configspace": "", "generation": 1, "fitness": 0.98315818535206, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871452736207362, 0.9835943813397471, 0.9787349010956969], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced exploration through adaptive mutation factor scaling in Differential Evolution for improved optimization.", "configspace": "", "generation": 2, "fitness": 0.9831648651096607, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871452736207362, 0.9836142685059454, 0.9787350532023007], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "64e2060c-01c0-42ac-a0c6-4a3def90edd9", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.cr * (1 - i/self.population_size))  # Dynamic CR\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Hybrid Metaheuristic Algorithm with Dynamic Crossover Rate Adjustment for Improved Search Efficiency.", "configspace": "", "generation": 2, "fitness": 0.9831582861156037, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871454627887419, 0.9835943813397471, 0.9787350142183223], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "b2fe718c-008c-4e53-a761-f264f79b5b8c", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        periodicity = (ub - lb) / 2  # Introduce periodicity constraint\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb + periodicity, ub - periodicity)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce a periodicity constraint in the DE mutation step to better leverage wave interference principles.", "configspace": "", "generation": 2, "fitness": 0.9831584135753704, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9c6307a-ad6a-4db9-a630-8ef681eea667", "metadata": {"aucs": [0.9871458451680417, 0.9835943813397471, 0.9787350142183223], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "cd649170-d615-4452-9d42-fd1cfc1c30b4", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            \n            # Introduced periodicity constraint in trial solution selection here\n            trial = np.where(cross_points, mutant, pop[i]) * (1 + 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim))\n            \n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic Algorithm introducing periodicity constraint in trial solution selection for optimized Bragg mirror design.", "configspace": "", "generation": 2, "fitness": 0.9831577969461537, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9c6307a-ad6a-4db9-a630-8ef681eea667", "metadata": {"aucs": [0.9871439952803913, 0.9835943813397471, 0.9787350142183223], "final_y": [0.16485672355588643, 0.16485620555962366, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "3e34b37c-e84e-4a16-891b-39cf22c21161", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = 0.5 + 0.5 * np.random.rand()  # Change 1: Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)  # Change 2: Use adaptive_f instead of self.f\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic Algorithm introducing adaptive mutation factor for improved exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.9831574300836315, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.987142894692825, 0.9835943813397471, 0.9787350142183223], "final_y": [0.16485796577169698, 0.16485620555962366, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "4238564e-96ad-4cbb-aa4f-e01c10f0217b", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8\n        self.cr = 0.9\n    \n    def quasi_oppositional_init(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.f = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n            self.cr = 0.7 + 0.2 * np.random.rand()  # Adaptive crossover rate\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n            \n            if evaluations > self.budget * 0.75:\n                self.population_size = max(10, int(self.population_size * 0.8))  # Dynamic pop reduction\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic Algorithm with Adaptive Parameters and Dynamic Population Reduction for Efficient Exploration.", "configspace": "", "generation": 2, "fitness": 0.9831582664985414, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871454039375547, 0.9835943813397471, 0.9787350142183223], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "98e8510d-039c-4a0b-a493-550463c6dcfc", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass ImprovedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.initial_population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def periodicity_promoting_mutation(self, individual, lb, ub):\n        \"\"\"Promote periodicity by adding sinusoidal perturbations.\"\"\"\n        perturbation = np.sin(np.linspace(0, 2 * np.pi, self.dim))\n        mutated = individual + 0.1 * perturbation * (ub - lb)\n        return np.clip(mutated, lb, ub)\n    \n    def adaptive_differential_evolution(self, pop, func, lb, ub, evaluations):\n        \"\"\"Perform DE with adaptive population size reduction.\"\"\"\n        current_pop_size = max(self.initial_population_size // (1 + evaluations // self.budget), 5)\n        next_pop = np.empty((current_pop_size, self.dim))\n        for i in range(current_pop_size):\n            indices = list(range(current_pop_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            mutant = self.periodicity_promoting_mutation(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.initial_population_size]]  # Select top half\n        \n        evaluations = self.initial_population_size * 2\n        while evaluations < self.budget:\n            pop = self.adaptive_differential_evolution(pop, func, lb, ub, evaluations)\n            evaluations += len(pop)\n            \n            for i in range(len(pop)):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "ImprovedHybridMetaheuristic", "description": "Improved Hybrid Metaheuristic Algorithm integrating Periodicity-Promoting Mechanism and Adaptive Population Size for Enhanced Solution Quality and Resource Utilization.", "configspace": "", "generation": 2, "fitness": 0.9798305362187509, "feedback": "The algorithm ImprovedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9771623599561359, 0.9835943813397471, 0.9787348673603699], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "9468aefe-c062-4e16-8ad0-44f57a5f0a98", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n        self.periodicity_weight = 0.05  # Encourage periodicity\n    \n    def quasi_oppositional_init(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.is_periodic(trial):\n                trial -= self.periodicity_weight  # Periodicity encouragement\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n\n    def is_periodic(self, x):\n        mid = len(x) // 2\n        return np.allclose(x[:mid], x[mid:], atol=0.1)\n\n    def local_search(self, x, func, lb, ub):\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  \n\n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n\n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Hybrid Metaheuristic Algorithm with Adaptive Parameter Tuning and Periodicity Encouragement for Enhanced Global and Local Search.", "configspace": "", "generation": 2, "fitness": 0.983157429961422, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871453187721476, 0.9835921374870757, 0.9787348336250427], "final_y": [0.16485632498843794, 0.16485651325243045, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "6d9df337-393f-4401-874e-b09bac71abc1", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.f = 0.5 + 0.3 * np.random.rand()   # Adaptive mutation\n            self.cr = 0.7 + 0.2 * np.random.rand()  # Adaptive crossover\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Refined Hybrid Metaheuristic Algorithm introducing adaptive mutation and crossover rates based on current diversity to enhance exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.9831582716609263, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871454194247097, 0.9835943813397471, 0.9787350142183223], "final_y": [0.16485628792405105, 0.16485620555962366, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "69198d3f-f081-4c04-b290-ee634417e4ad", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n        self.min_f = 0.5  # Minimum mutation factor for adaptation\n        self.max_f = 0.9  # Maximum mutation factor for adaptation\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def adaptive_mutation(self, success_ratio):\n        \"\"\"Adapt the mutation factor based on success rate.\"\"\"\n        self.f = self.min_f + (self.max_f - self.min_f) * success_ratio\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        success_count = 0\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                next_pop[i] = trial\n                success_count += 1\n            else:\n                next_pop[i] = pop[i]\n        self.adaptive_mutation(success_count / self.population_size)\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved Hybrid Metaheuristic with adaptive mutation factor and periodicity constraints to enhance exploration and exploit constructive interference principles.", "configspace": "", "generation": 2, "fitness": 0.9831582381100724, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "cd84fd58-b76a-4e35-a7c4-d9ee88b2fb83", "metadata": {"aucs": [0.9871453187721476, 0.9835943813397471, 0.9787350142183223], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485613929203957]}, "mutation_prompt": null}
{"id": "143969e0-f506-4906-a3f8-7fb87233207a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = min(self.population_size + 1, int(self.budget / 10))  # Dynamic population adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved Hybrid Metaheuristic Algorithm with dynamic adjustment of population size to enhance exploration and convergence balance.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "metadata": {}, "mutation_prompt": null}
{"id": "e6fb72a4-cdc1-4103-97b9-b6d1105c2e27", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        best = min(pop, key=func)  # Elitism: Keep the best solution\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        \n        # Replace the worst individual in the next population with the best one from the current population\n        worst_idx = np.argmax([func(x) for x in next_pop])\n        next_pop[worst_idx] = best\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved convergence by introducing elitism to retain the best solution found so far.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "metadata": {}, "mutation_prompt": null}
{"id": "8b9bdb0c-e580-406f-8c6e-46679204dd3d", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        next_pop[-1] = np.random.uniform(lb, ub, self.dim)  # Diversity enhancement\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced diversity enhancement by replacing the bottom individual in the population with a new random solution in each generation.", "configspace": "", "generation": 3, "fitness": 0.9834422369420786, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9c6307a-ad6a-4db9-a630-8ef681eea667", "metadata": {"aucs": [0.9871452849085891, 0.9839868621714544, 0.9791945637461922], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "4389efd0-a361-4cb1-8604-9d643a1a11f9", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = self.cr * np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced exploration by adjusting the crossover rate in Differential Evolution for improved optimization.", "configspace": "", "generation": 3, "fitness": 0.9835085251330451, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "metadata": {"aucs": [0.987344022974012, 0.9839868621714544, 0.979194690253669], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "e4f8c544-9f33-471c-b350-ad2d9cd9803c", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.6  # DE Mutation Factor (Reduced for better convergence)\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        pop = np.vstack((pop, next_pop))  # Elitism: Combine current and next populations\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select best individuals\n        return pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved convergence by reducing mutation factor and applying elitism in population selection for better solution refinement.", "configspace": "", "generation": 3, "fitness": 0.981924705366727, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "metadata": {"aucs": [0.9825925636750575, 0.9839868621714544, 0.979194690253669], "final_y": [0.164856821890642, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "6d7b01a7-e687-4d6a-b477-8c680d4eb81b", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * (1.0 + 0.1 * np.random.rand())  # Adaptive mutation factor with slight increase\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options={'maxiter': 5})  # Limit local search iterations\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Adaptive mutation factor scaling and enhanced local search integration for improved optimization.", "configspace": "", "generation": 3, "fitness": 0.9577975543766893, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.034. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "metadata": {"aucs": [0.9102111023173987, 0.9839868621714544, 0.9791946986412151], "final_y": [0.18796879414238077, 0.16485620555962366, 0.16485622420621548]}, "mutation_prompt": null}
{"id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover rate scaling in Differential Evolution for enhanced exploration-exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.9835087967916221, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "metadata": {"aucs": [0.9873438178836061, 0.9839870393038609, 0.9791955331873992], "final_y": [0.16485646735423654, 0.16485594542769988, 0.16485604687897548]}, "mutation_prompt": null}
{"id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic algorithm with a dynamic population size adjustment strategy for better convergence.", "configspace": "", "generation": 3, "fitness": 0.9835373011402755, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9c6307a-ad6a-4db9-a630-8ef681eea667", "metadata": {"aucs": [0.9874303509957033, 0.9839868621714544, 0.979194690253669], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "7e87a9c5-28b3-4c5d-8055-1bd649efb19a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass CooperativeCoevolutionaryMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n        self.periodicity = 5  # Inject periodic solutions every 'periodicity' generations\n    \n    def periodic_solution_injection(self, lb, ub):\n        \"\"\"Generate periodic Bragg mirror solutions.\"\"\"\n        half_period = self.dim // 2\n        period = np.concatenate((np.full(half_period, lb), np.full(half_period, ub)))\n        if self.dim % 2 != 0:\n            period = np.append(period, np.random.uniform(lb, ub))\n        return np.tile(period, self.dim // len(period) + 1)[:self.dim]\n    \n    def cooperative_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform adaptive differential evolution with cooperative strategy.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        evaluations = self.population_size\n        \n        while evaluations < self.budget:\n            pop = self.cooperative_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            if evaluations // self.population_size % self.periodicity == 0:\n                periodic_sol = self.periodic_solution_injection(lb, ub)\n                if func(periodic_sol) < max(func(x) for x in pop):\n                    worst_idx = np.argmax([func(x) for x in pop])\n                    pop[worst_idx] = periodic_sol\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "CooperativeCoevolutionaryMetaheuristic", "description": "Cooperative Coevolutionary Approach integrating adaptive differential evolution and periodic solution injection for superior optimization under constraints.", "configspace": "", "generation": 3, "fitness": 0.9590877564071801, "feedback": "The algorithm CooperativeCoevolutionaryMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "metadata": {"aucs": [0.9591153670272632, 0.9632859244038702, 0.954861977790407], "final_y": [0.16485632498843794, 0.16485620555962366, 0.1648574561319326]}, "mutation_prompt": null}
{"id": "ed98ba35-d465-458a-afbc-e679b6908db1", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass EnhancedHybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def modify_cost_function(self, func):\n        \"\"\"Modify the cost function to encourage periodicity.\"\"\"\n        def modified_func(x):\n            # Penalize deviation from periodicity by calculating the variance across layers\n            periodicity_penalty = np.var(x)\n            return func(x) + 0.1 * periodicity_penalty  # 0.1 is a weight for penalty\n        return modified_func\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy, with adaptive intensification.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        modified_func = self.modify_cost_function(func)\n        \n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, modified_func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                # Adaptive intensification for local search\n                if np.random.rand() < 0.5:\n                    refined = self.local_search(pop[i], modified_func, lb, ub)\n                    if func(refined) < func(pop[i]):\n                        pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "EnhancedHybridMetaheuristic", "description": "Improved HybridMetaheuristic with periodicity encouragement via tailored cost function modifications and adaptive local search intensification.", "configspace": "", "generation": 3, "fitness": 0.9348238746212488, "feedback": "The algorithm EnhancedHybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.026. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1b6066c4-36ba-4e74-a2af-60abde124be6", "metadata": {"aucs": [0.9596892757789288, 0.8990833787778665, 0.9456989693069512], "final_y": [0.17268168959310093, 0.1732247381114792, 0.17249384112658894]}, "mutation_prompt": null}
{"id": "33e2089f-0eea-49cb-93d3-f9f5641adbe4", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand(1.0, 1.2)  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce opposition-based differential evolution with adaptive mutation to enhance exploration and exploitation.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'float' object cannot be interpreted as an integer\").", "error": "TypeError(\"'float' object cannot be interpreted as an integer\")", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {}, "mutation_prompt": null}
{"id": "22620e48-17db-4c53-809c-384322a8efe0", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * (np.random.rand() + 0.5)  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive scaling for mutation factor in Differential Evolution to enhance convergence efficiency.", "configspace": "", "generation": 4, "fitness": 0.9443861862328159, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.035. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.9871458928231773, 0.9004725651199246, 0.9455401007553457], "final_y": [0.16485605487390442, 0.16485753918426138, 0.16485603053628262]}, "mutation_prompt": null}
{"id": "e72a960e-fedf-4cc9-be2b-b5cf2b5f3865", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce dynamic adjustment of mutation factor scaling in Differential Evolution for enhanced adaptation.", "configspace": "", "generation": 4, "fitness": 0.9636087784904426, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "metadata": {"aucs": [0.9874303509957033, 0.9578558837202789, 0.9455401007553457], "final_y": [0.16485632498843794, 0.1648564094492705, 0.16485603053628262]}, "mutation_prompt": null}
{"id": "055c963f-53d2-4b54-94f4-75c60e66c5bc", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, improvement_threshold=0.01):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        initial_fitness = func(x)\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        if result.success and (initial_fitness - result.fun) > improvement_threshold:\n            return result.x\n        return x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Integrate selective local search based on fitness improvement threshold to enhance fine-tuning efficiency.", "configspace": "", "generation": 4, "fitness": 0.9444257305558993, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.035. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.9872655273178381, 0.9004715635945142, 0.9455401007553457], "final_y": [0.16485629632606336, 0.16486088410915267, 0.16485603053628262]}, "mutation_prompt": null}
{"id": "7b1e4f2f-f450-4330-bee8-4e02ade9637c", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand() + 0.5  # Adaptive mutation factor with shift\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = 0.5 + 0.5 * np.random.rand()  # Adaptive crossover rate with increased range\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Adjust DE mutation and crossover strategies for improved convergence.", "configspace": "", "generation": 4, "fitness": 0.9443858208043757, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.035. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.9871457980632673, 0.9004715635945142, 0.9455401007553457], "final_y": [0.16485596034911298, 0.16486088410915267, 0.16485603053628262]}, "mutation_prompt": null}
{"id": "3128f46b-2917-4e71-859d-6584d468bf29", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop[:self.population_size]  # Adaptive pop size scaling\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size scaling in Differential Evolution for improved convergence towards global optima.", "configspace": "", "generation": 4, "fitness": 0.9720349680270912, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.9873440282425652, 0.9792648924905072, 0.9494959833482014], "final_y": [0.16485632498843794, 0.16485651325243045, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "90fcf7e3-4b8f-4d20-b189-195bb581d5ab", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = 0.9 * np.cos(np.pi * i / self.population_size)  # Periodicity constraint\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        if np.random.rand() < 0.5:  # Adaptive local search strategy\n            result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n            return result.x if result.success else x\n        else:\n            return x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce a periodicity constraint in Differential Evolution and an adaptive mechanism for local search strategy to improve convergence.", "configspace": "", "generation": 4, "fitness": 0.950800164237009, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.951 with standard deviation 0.028. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.9871462710350102, 0.9197141209206711, 0.9455401007553457], "final_y": [0.164856185152909, 0.16485651325243045, 0.16485603053628262]}, "mutation_prompt": null}
{"id": "9ace3e27-e804-458c-bad2-db76113a6184", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def enforce_periodicity(self, solution, period):\n        \"\"\"Force periodicity in the solution.\"\"\"\n        for i in range(0, self.dim, period):\n            solution[i:i+period] = solution[i:i+period].mean()\n        return solution\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            trial = self.enforce_periodicity(trial, period=2)  # Apply periodicity\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce a periodicity-enforcing mechanism in Differential Evolution to enhance the discovery of structured solutions.", "configspace": "", "generation": 4, "fitness": 0.9709883796275119, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.9881601456366832, 0.9792648924905072, 0.9455401007553457], "final_y": [0.16485604253853292, 0.16485651325243045, 0.16485603053628262]}, "mutation_prompt": null}
{"id": "58633130-5de2-45d4-a44f-1fc554059979", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.5 + (0.4 * np.sin(i))  # Dynamically adjusted Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Integrate a new strategy that dynamically adjusts the crossover rate to further improve the exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.9444800627823399, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.036. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "metadata": {"aucs": [0.987428424826346, 0.9004715635945142, 0.9455401999261596], "final_y": [0.16485751310148167, 0.16486088410915267, 0.16485607174277483]}, "mutation_prompt": null}
{"id": "51b1fe7c-e506-4b9e-8826-c26140bf15d1", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if self.enhanced_fitness(trial, func) < self.enhanced_fitness(pop[i], func) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def enhanced_fitness(self, x, func):\n        \"\"\"Enhance fitness with periodicity encouragement.\"\"\"\n        periodicity_penalty = np.std(x - np.roll(x, self.dim // 2))\n        return func(x) + periodicity_penalty\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([self.enhanced_fitness(x, func) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if self.enhanced_fitness(refined, func) < self.enhanced_fitness(pop[i], func):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([self.enhanced_fitness(x, func) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce periodicity encouragement in the fitness evaluation to enhance solution quality.", "configspace": "", "generation": 4, "fitness": 0.9437474795224162, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.035. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "metadata": {"aucs": [0.9852307742173891, 0.9004715635945142, 0.9455401007553457], "final_y": [0.16485632498843794, 0.16486088410915267, 0.16485603053628262]}, "mutation_prompt": null}
{"id": "c903a9ac-6e39-4c39-9204-2579b757a080", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(5, int(self.budget / (self.population_size + 1)))  # Dynamic adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce a dynamic population size adjustment based on evaluation budget consumption to enhance exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 131 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 131 is out of bounds for axis 0 with size 20')", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {}, "mutation_prompt": null}
{"id": "10200a9f-7db4-4078-9009-20d5cd3a0479", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        best_score = func(pop[0])\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9)) if func(pop[0]) < best_score else min(40, int(self.population_size * 1.1)) # Dynamic Population Adjustment\n            best_score = min(best_score, func(pop[0]))\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size adjustment that increases if the improvement stalls.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "metadata": {}, "mutation_prompt": null}
{"id": "6f24f756-fadb-46b4-bbcb-51cf6bedb866", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        adaptive_population_size = int(self.population_size * (0.5 + 0.5 * np.random.rand()))\n        for i in range(adaptive_population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop[:adaptive_population_size]\n\n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size scaling in Differential Evolution for improved exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 13 is out of bounds for axis 0 with size 13').", "error": "IndexError('index 13 is out of bounds for axis 0 with size 13')", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {}, "mutation_prompt": null}
{"id": "562bde90-acdc-4f8b-b13e-f6eb724037fe", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            if evaluations % (self.budget // 10) == 0:  # Adaptive population size scaling\n                self.population_size = min(self.population_size + 1, int(self.budget / (self.dim * 2)))\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhance convergence by introducing an adaptive population size scaling strategy.", "configspace": "", "generation": 5, "fitness": 0.983479443463013, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.9873440684497196, 0.9835944890428964, 0.9794997728964229], "final_y": [0.16485615033270196, 0.1648559922210714, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "90d3ee4e-ce5a-40f7-b849-c84894945718", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            trial = (trial + np.roll(trial, 1)) / 2  # Encourage periodicity\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Refined Hybrid Metaheuristic with strategic periodic solution encouragement for enhanced optimization performance.", "configspace": "", "generation": 5, "fitness": 0.9832533295620771, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "metadata": {"aucs": [0.987430638780133, 0.9835943813397471, 0.9787349685663512], "final_y": [0.1648561873848805, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "fde705de-9612-4266-a574-c2d32e16b6fe", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                if np.random.rand() < 0.5:  # Stochastic acceptance\n                    next_pop[i] = trial\n                else:\n                    next_pop[i] = pop[i]\n            else:\n                next_pop[i] = pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved Hybrid Metaheuristic with dynamic exploration factors to enhance convergence speed and solution accuracy.", "configspace": "", "generation": 5, "fitness": 0.98350815118433, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "metadata": {"aucs": [0.98743029931682, 0.9835943813397471, 0.9794997728964229], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "8e810aee-5948-400a-ac7f-b363ed14fae0", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Optimized Hybrid Metaheuristic with adaptive local search intensity and periodicity-encouraging initialization for improved reflectivity optimization in Bragg mirrors.", "configspace": "", "generation": 5, "fitness": 0.9838342840028331, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "metadata": {"aucs": [0.9884086977723295, 0.9835943813397471, 0.9794997728964229], "final_y": [0.16485603832335616, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "34c92fd6-7ffd-4441-8dbd-53bfc4301402", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        # Variance Reduction by narrowing the initial range\n        mid_range = (ub - lb) / 10 \n        pop = np.random.uniform(lb + mid_range, ub - mid_range, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Adjust the initialization strategy to include a variance reduction scheme to enhance convergence speed.", "configspace": "", "generation": 5, "fitness": 0.9837338747936241, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.988107470144702, 0.9835943813397471, 0.9794997728964229], "final_y": [0.16485587504227073, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "f697580c-53f1-423c-9119-9c11feacc4f2", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            trial_rounded = np.round(trial * 10) / 10  # Adjust trial to enforce periodicity\n            next_pop[i] = trial_rounded if func(trial_rounded) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with experimental periodicity enforcement in DE for improved exploration.", "configspace": "", "generation": 5, "fitness": 0.9835081684106245, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1300ee90-a073-4070-83b1-ce2e95283f6b", "metadata": {"aucs": [0.9874303509957033, 0.9835943813397471, 0.9794997728964229], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "eb9a33d1-d358-4a42-995c-44ed4b11755e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            if evaluations > self.budget * 0.5:  # New line to adaptively adjust population size\n                self.population_size = int(0.8 * self.population_size)\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive population size adjustment to dynamically balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.9834793941595784, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a13e7e55-2391-4bdc-815a-e94622c06d5f", "metadata": {"aucs": [0.9873440282425652, 0.9835943813397471, 0.9794997728964229], "final_y": [0.16485632498843794, 0.16485620555962366, 0.16485628413675657]}, "mutation_prompt": null}
{"id": "fdb00ec8-ad1e-4bdd-9597-41d79f97aa4a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        # Variance Reduction by narrowing the initial range\n        mid_range = (ub - lb) / 10 \n        pop = np.random.uniform(lb + mid_range, ub - mid_range, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n# Updated line below\n        evaluations, self.population_size = self.population_size * 2, int(min(100, max(10, self.budget / 100)))  # Adaptive population size\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced adaptive population size based on budget utilization to enhance convergence speed.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 32 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 32 is out of bounds for axis 0 with size 20')", "parent_id": "34c92fd6-7ffd-4441-8dbd-53bfc4301402", "metadata": {}, "mutation_prompt": null}
{"id": "dfd2773e-de28-4487-aaba-9de472ade58e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass MultiScaleAdaptiveHybrid:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n        self.scale_factor = 0.1  # Factor to adjust initial population bounds\n        self.periodicity_penalty = 1.0  # Penalty for non-periodic solutions\n    \n    def adaptive_initialization(self, lb, ub):\n        \"\"\"Initialize population using multi-scale strategy.\"\"\"\n        mid_range = (ub - lb) / 2\n        fine_range = mid_range * self.scale_factor\n        large_scale_pop = np.random.uniform(lb, ub, (self.population_size // 2, self.dim))\n        fine_scale_pop = np.random.uniform(lb + fine_range, ub - fine_range, (self.population_size // 2, self.dim))\n        return np.vstack((large_scale_pop, fine_scale_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm with adaptive population size.\"\"\"\n        pop_size = len(pop)\n        next_pop = np.empty((pop_size, self.dim))\n        for i in range(pop_size):\n            indices = list(range(pop_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if self.adjusted_cost(func, trial) < self.adjusted_cost(func, pop[i]) else pop[i]\n        return next_pop\n    \n    def adjusted_cost(self, func, x):\n        \"\"\"Adjust the cost to encourage periodicity.\"\"\"\n        cost = func(x)\n        periodic_penalty = self.periodicity_penalty * np.sum(np.abs(np.diff(x)))\n        return cost + periodic_penalty\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS.\"\"\"\n        result = opt.minimize(self.adjusted_cost, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.adaptive_initialization(lb, ub)\n        evaluations = len(pop)\n        \n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += len(pop)\n            \n            for i in range(len(pop)):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if self.adjusted_cost(func, refined) < self.adjusted_cost(func, pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([self.adjusted_cost(func, x) for x in pop])\n        return pop[best_idx]", "name": "MultiScaleAdaptiveHybrid", "description": "Multi-Scale Adaptive Hybrid Algorithm combining DE with variable population sizes and periodic structure reinforcement for enhanced convergence and reflectivity.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"MultiScaleAdaptiveHybrid.adjusted_cost() missing 1 required positional argument: 'x'\").", "error": "TypeError(\"MultiScaleAdaptiveHybrid.adjusted_cost() missing 1 required positional argument: 'x'\")", "parent_id": "34c92fd6-7ffd-4441-8dbd-53bfc4301402", "metadata": {}, "mutation_prompt": null}
{"id": "012999fa-177a-4eec-80a4-e5dee03628df", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        mid_range = (ub - lb) / 10 \n        pop = np.random.uniform(lb + mid_range, ub - mid_range, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            mutant = np.clip(a + adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options={'maxiter': 5})\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic using adaptive population size and improved local search for better convergence.", "configspace": "", "generation": 6, "fitness": 0.9759092353317739, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.017. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "34c92fd6-7ffd-4441-8dbd-53bfc4301402", "metadata": {"aucs": [0.9522337158139771, 0.9890027915009558, 0.9864911986803887], "final_y": [0.1685098947802276, 0.1648559340912138, 0.16485600945413204]}, "mutation_prompt": null}
{"id": "2d08ae79-d5a2-4810-8201-b482bcbcb46c", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n    \n    def quasi_oppositional_init(self, lb, ub):\n        \"\"\"Initialize population using quasi-oppositional strategy.\"\"\"\n        # Variance Reduction by narrowing the initial range\n        mid_range = (ub - lb) / 10 \n        pop = np.random.uniform(lb + mid_range, ub - mid_range, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        return np.vstack((pop, opp_pop))\n    \n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            adaptive_f = self.f * np.random.rand()  # Adaptive mutation factor\n            # Modification: Introduce directional mutation\n            direction = np.random.choice([-1, 1], 1)\n            mutant = np.clip(a + direction * adaptive_f * (b - c), lb, ub)\n            adaptive_cr = np.random.rand()  # Adaptive crossover rate\n            cross_points = np.random.rand(self.dim) < adaptive_cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform local search using BFGS from scipy.\"\"\"\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B')\n        return result.x if result.success else x\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce directional mutation by modifying the DE mutation strategy for better exploration and convergence.", "configspace": "", "generation": 6, "fitness": 0.9908005112842991, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "34c92fd6-7ffd-4441-8dbd-53bfc4301402", "metadata": {"aucs": [0.9871544802733578, 0.9908672169420644, 0.9943798366374751], "final_y": [0.16485587504227073, 0.16485646452954805, 0.1648560186677217]}, "mutation_prompt": null}
{"id": "9ffa6771-d6df-4205-9fd1-5946ce2c5a53", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(np.mean([pop[i][:period], opp_pop[i][:period]], axis=0), self.dim // period)  # Changed Line\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved the periodic initialization by ensuring an additional layer of periodicity by altering the copying mechanism.", "configspace": "", "generation": 6, "fitness": 0.9900849374797204, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.990 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e810aee-5948-400a-ac7f-b363ed14fae0", "metadata": {"aucs": [0.9863431029785448, 0.9895319969671439, 0.9943797124934723], "final_y": [0.16485592644035096, 0.16485606282398746, 0.1648560138994909]}, "mutation_prompt": null}
{"id": "5a200ee5-148e-43fc-beee-0c23fae5fd27", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.6 * (1 - evaluations / self.budget)  # Adjusted Adaptive intensity range\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced local search by dynamically adjusting the intensity range for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.9908086513702289, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e810aee-5948-400a-ac7f-b363ed14fae0", "metadata": {"aucs": [0.9884086977723295, 0.9896375965020824, 0.9943796598362751], "final_y": [0.16485603832335616, 0.16485650956152276, 0.16485589866994566]}, "mutation_prompt": null}
{"id": "6055a67c-a45e-4902-8a11-4270d9032403", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * (1 - evaluations / self.budget)))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced dynamic scaling of population size based on budget utilization to enhance exploration and convergence efficiency.", "configspace": "", "generation": 6, "fitness": 0.990726244193929, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e810aee-5948-400a-ac7f-b363ed14fae0", "metadata": {"aucs": [0.9882661586990761, 0.9895321829843755, 0.994380390898335], "final_y": [0.16485603832335616, 0.16485612883429346, 0.16485609869112738]}, "mutation_prompt": null}
{"id": "32a2dca8-904c-4298-b864-df102f8e5f40", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.05 + 0.6 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with dynamic mutation factor and intensity for improved convergence in Bragg mirror optimization.", "configspace": "", "generation": 6, "fitness": 0.9907737689779744, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e810aee-5948-400a-ac7f-b363ed14fae0", "metadata": {"aucs": [0.9884086977723295, 0.9895322182632587, 0.994380390898335], "final_y": [0.16485603832335616, 0.16485607012057635, 0.16485609869112738]}, "mutation_prompt": null}
{"id": "3727f3ab-5241-4e68-a398-fbb243d2a3bd", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.85 + np.random.rand() * 0.15  # Intensified Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic by incorporating a refined adaptive mutation strategy and intensified crossover rate adjustments for improved solution exploration.", "configspace": "", "generation": 6, "fitness": 0.9907736952126028, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e810aee-5948-400a-ac7f-b363ed14fae0", "metadata": {"aucs": [0.9884086977723295, 0.9895319969671439, 0.994380390898335], "final_y": [0.16485603832335616, 0.16485606282398746, 0.16485609869112738]}, "mutation_prompt": null}
{"id": "1634651b-6ce9-428f-9b51-89b4ed4e4057", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with focused variance reduction in initialization and improved adaptive mutation strategy for Bragg mirror optimization.", "configspace": "", "generation": 6, "fitness": 0.9916499208920827, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8e810aee-5948-400a-ac7f-b363ed14fae0", "metadata": {"aucs": [0.9910371728705298, 0.9895321989073833, 0.994380390898335], "final_y": [0.16485617090785365, 0.16485608619516057, 0.16485609869112738]}, "mutation_prompt": null}
{"id": "d7a7f516-61c9-4245-8226-b7df21e54a16", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * (0.35 - 0.25 * (self.budget - evaluations) / self.budget)  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.6 * (1 - evaluations / self.budget)  # Adjusted Adaptive intensity range\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced a dynamic strategy for adaptive mutation factor initialization to enhance convergence in DE.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "5a200ee5-148e-43fc-beee-0c23fae5fd27", "metadata": {}, "mutation_prompt": null}
{"id": "c881a295-2b35-4afc-a29d-f35b65289b7a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size), 'gtol': 1e-5}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhancing the local search by adjusting BFGS options for improved convergence efficiency.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "1634651b-6ce9-428f-9b51-89b4ed4e4057", "metadata": {}, "mutation_prompt": null}
{"id": "9f13e2d2-fe1a-4087-ae89-b41832506b8a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            diversity = np.mean(np.std(pop, axis=0))\n            self.cr = max(0.5, min(0.9, 1 - diversity))  # Dynamic Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.6 * (1 - evaluations / self.budget)  # Adjusted Adaptive intensity range\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce dynamic adjustment of the crossover rate based on population diversity for improved exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'mutant' is not defined\").", "error": "NameError(\"name 'mutant' is not defined\")", "parent_id": "5a200ee5-148e-43fc-beee-0c23fae5fd27", "metadata": {}, "mutation_prompt": null}
{"id": "216ce9df-4106-4550-a7b5-421fcfe3934d", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c) + 0.1 * (pop[np.argmin([func(x) for x in pop])] - a), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.6 * (1 - evaluations / self.budget)  # Adjusted Adaptive intensity range\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved exploration by introducing elite preservation and diversity boost in DE mutation.", "configspace": "", "generation": 7, "fitness": 0.9867696269202558, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a200ee5-148e-43fc-beee-0c23fae5fd27", "metadata": {"aucs": [0.9627516645867078, 0.9984552217368776, 0.9991019944371821], "final_y": [0.16485603832335616, 0.16485657424408295, 0.16485643534936456]}, "mutation_prompt": null}
{"id": "93fefeb6-97c7-4634-a840-653f17605dc1", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced dynamic crossover and mutation factor adjustment in DE to enhance diversity and convergence.", "configspace": "", "generation": 7, "fitness": 0.9961992158138943, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1634651b-6ce9-428f-9b51-89b4ed4e4057", "metadata": {"aucs": [0.9910374584682472, 0.9984552217368776, 0.9991049672365581], "final_y": [0.16485607804100733, 0.16485657424408295, 0.16485587117713818]}, "mutation_prompt": null}
{"id": "a7bd193e-26f8-4cfc-bbab-630af0b1700d", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3 * (np.sin(i * np.pi / self.dim) + 1) / 2  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.6 * (1 - evaluations / self.budget)  # Adjusted Adaptive intensity range\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce a multi-layer periodicity adaptive factor to further enhance differential evolution's mutation adaptability.", "configspace": "", "generation": 7, "fitness": 0.9953226951342412, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a200ee5-148e-43fc-beee-0c23fae5fd27", "metadata": {"aucs": [0.9884086977723295, 0.9984559685131532, 0.9991034191172412], "final_y": [0.16485603832335616, 0.16485622697929825, 0.16485589929264977]}, "mutation_prompt": null}
{"id": "b35b4d64-195e-42b6-be32-2716d800cda1", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], (self.dim // period) + 1)[:self.dim]  # Ensure full coverage\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.85 + np.random.rand() * 0.15  # Refined Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.6 * (1 - evaluations / self.budget)  # Adjusted Adaptive intensity range\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced search efficiency by adjusting the adaptive crossover rate range and refining the periodic initialization strategy.", "configspace": "", "generation": 7, "fitness": 0.9953223463631401, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a200ee5-148e-43fc-beee-0c23fae5fd27", "metadata": {"aucs": [0.9884086977723295, 0.9984552217368776, 0.9991031195802131], "final_y": [0.16485603832335616, 0.16485657424408295, 0.16485607181089967]}, "mutation_prompt": null}
{"id": "ba75ef21-cbcb-46be-8203-31e7891feb2e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 22  # Change: Increased population size for better exploration\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced Hybrid Metaheuristic with improved initialization strategy by increasing population size for better exploration.", "configspace": "", "generation": 7, "fitness": 0.9960465821378164, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1634651b-6ce9-428f-9b51-89b4ed4e4057", "metadata": {"aucs": [0.9905816274843178, 0.9984552217368776, 0.9991028971922538], "final_y": [0.16485667412542349, 0.16485657424408295, 0.16485609978139104]}, "mutation_prompt": null}
{"id": "29124324-44f4-44a7-9539-088e95e90084", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.7 + np.random.rand() * 0.3  # Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.6 * (1 - evaluations / self.budget)  # Adjusted Adaptive intensity range\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                self.f *= (1 + 0.1 * (self.budget - evaluations) / self.budget)  # Dynamic scaling adjustment\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced a dynamic scaling factor to modulate the mutation intensity in DE, enhancing exploration near budget limits.", "configspace": "", "generation": 7, "fitness": 0.9953222722338202, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5a200ee5-148e-43fc-beee-0c23fae5fd27", "metadata": {"aucs": [0.9884086977723295, 0.9984552217368776, 0.9991028971922538], "final_y": [0.16485603832335616, 0.16485657424408295, 0.16485609978139104]}, "mutation_prompt": null}
{"id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced periodicity enforcement in initialization guarantees better adherence to optimal solution structures.", "configspace": "", "generation": 7, "fitness": 0.9986627402242257, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1634651b-6ce9-428f-9b51-89b4ed4e4057", "metadata": {"aucs": [0.9984301017435455, 0.9984552217368776, 0.9991028971922538], "final_y": [0.16485604348581984, 0.16485657424408295, 0.16485609978139104]}, "mutation_prompt": null}
{"id": "e73d57d1-ce16-4f72-8b0c-fe2d2e5d5ae7", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and adaptive periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = np.random.randint(1, self.dim // 2)  # Adaptive periodicity\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Incorporate adaptive periodicity in initialization to improve diversity and solution quality.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (9,) into shape (10,)').", "error": "ValueError('could not broadcast input array from shape (9,) into shape (10,)')", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {}, "mutation_prompt": null}
{"id": "0d3413af-3083-435c-9ed4-36d7b537aa2c", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = np.random.choice(range(1, self.dim // 2 + 1))  # Adaptive Period Scaling\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced adaptive scaling of period in initialization to enhance periodicity alignment with optimal solution structures.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (9,) into shape (10,)').", "error": "ValueError('could not broadcast input array from shape (9,) into shape (10,)')", "parent_id": "93fefeb6-97c7-4634-a840-653f17605dc1", "metadata": {}, "mutation_prompt": null}
{"id": "a53497e8-354a-41eb-86c4-bad61619f39e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = lb + ub - pop  # Opposite population\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.5 * (1 - evaluations / self.budget)  # Adjusted adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Integrated adaptive opponent-aware strategy and synchronized local search for enhanced convergence efficiency and diversity.", "configspace": "", "generation": 8, "fitness": 0.9935665004632902, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "93fefeb6-97c7-4634-a840-653f17605dc1", "metadata": {"aucs": [0.9884086977723295, 0.99791039550988, 0.9943804081076608], "final_y": [0.16485603832335616, 0.16485595523655872, 0.16485603275312122]}, "mutation_prompt": null}
{"id": "11171049-72a8-4e82-be1c-d3f6e06f4d07", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            local_search_frequency = 1 + int((self.budget - evaluations) / (10 * self.population_size))  # Dynamic frequency\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                if i % local_search_frequency == 0:\n                    refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                    if func(refined) < func(pop[i]):\n                        pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced dynamic local search frequency adaptation based on remaining budget usage to enhance solution refinement.", "configspace": "", "generation": 8, "fitness": 0.9941996604535682, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984263524715571, 0.9897929253280371, 0.9943797035611105], "final_y": [0.1648559630826708, 0.16485612883429346, 0.16485627996743812]}, "mutation_prompt": null}
{"id": "fb16d9c8-4bdd-4bb5-9748-238bc81af570", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(12, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce a dynamic population size adjustment by slightly decreasing the maximum reduction factor, promoting better diversity.", "configspace": "", "generation": 8, "fitness": 0.9917386326377736, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "93fefeb6-97c7-4634-a840-653f17605dc1", "metadata": {"aucs": [0.9913040539708134, 0.9895321996357707, 0.9943796443067366], "final_y": [0.16485599454915445, 0.16485612883429346, 0.16485627996743812]}, "mutation_prompt": null}
{"id": "a46b0c1b-171e-424e-9181-8868a7a4749e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub, best_solution):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        if func(best_solution) < func(next_pop[0]): next_pop[0] = best_solution  # Elitism\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        best_solution = pop[0]  # Initialize best solution\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub, best_solution)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced elitism by preserving the best solution found so far to potentially enhance convergence and final solution quality.", "configspace": "", "generation": 8, "fitness": 0.9941099688652485, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984175088107692, 0.9895320068866416, 0.994380390898335], "final_y": [0.1648563023483529, 0.16485606282398746, 0.16485609869112738]}, "mutation_prompt": null}
{"id": "0197ca69-668c-4b53-98d4-609835e69678", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * ((b - c) if np.random.rand() < 0.5 else (c - b)), lb, ub)  # Frequency-based mutation\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introducing a frequency-based adaptive mutation strategy to enhance exploration potential in challenging landscapes.", "configspace": "", "generation": 8, "fitness": 0.9941137760118846, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984296147424252, 0.9895320068866416, 0.9943797064065868], "final_y": [0.1648563023483529, 0.16485606282398746, 0.16485627996743812]}, "mutation_prompt": null}
{"id": "fcca3547-e489-4527-88ac-c929e7738062", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else np.tile(trial[:self.dim//2], self.dim // (self.dim//2))  # Promote periodicity\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced periodicity promotion in the differential evolution phase to emphasize constructive interference.", "configspace": "", "generation": 8, "fitness": 0.9830856757164432, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "93fefeb6-97c7-4634-a840-653f17605dc1", "metadata": {"aucs": [0.9653448729075669, 0.9895321829843755, 0.9943799712573872], "final_y": [0.16485683354667413, 0.16485612883429346, 0.16485594480869314]}, "mutation_prompt": null}
{"id": "f5c7504d-498d-4a4a-aee7-2d87b2520bb0", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        variance_factor = 0.2 + 0.3 * np.random.rand()  # Adaptive variance reduction\n        pop = np.clip(pop, lb + (ub - lb) * variance_factor, ub - (ub - lb) * variance_factor)\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Integrate adaptive variance reduction in quasi-oppositional initialization to enhance population diversity and convergence.", "configspace": "", "generation": 8, "fitness": 0.9940529737265078, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9982463931471633, 0.9895321371340249, 0.994380390898335], "final_y": [0.16485611367485642, 0.1648559365532255, 0.16485609869112738]}, "mutation_prompt": null}
{"id": "9625e5ed-7b4b-46e3-a81a-77ce6dd69c87", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced adaptive mutation factor initialization to improve early exploration diversity.", "configspace": "", "generation": 8, "fitness": 0.9916498745735458, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "93fefeb6-97c7-4634-a840-653f17605dc1", "metadata": {"aucs": [0.9910371249334406, 0.9895321078888617, 0.994380390898335], "final_y": [0.164856181135702, 0.16485594378429547, 0.16485609869112738]}, "mutation_prompt": null}
{"id": "3e41ccd9-a4be-4383-82b1-3db0046c4d57", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.5 + 0.5 / (1 + np.exp(-0.2 * (self.budget - evaluations)))  # Sigmoid-based Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced a sigmoid-based adaptive adjustment for mutation factor to enhance convergence strategy in DE. ", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "93fefeb6-97c7-4634-a840-653f17605dc1", "metadata": {}, "mutation_prompt": null}
{"id": "6089a26b-1557-4dfb-afd9-506ff4689392", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with cosine-based strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        cosine_pattern = (np.cos(np.arange(self.dim) * np.pi / (self.dim / 2)) + 1) / 2\n        pop = lb + (ub - lb) * cosine_pattern\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced periodicity and diversified sampling via cosine-based initialization and adaptive local search intensification.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('invalid index to scalar variable.').", "error": "IndexError('invalid index to scalar variable.')", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {}, "mutation_prompt": null}
{"id": "f3b59baa-56aa-4d86-bae7-0da6d17c15ae", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        gradient = opt.approx_fprime(x, func, epsilon=1e-8)  # Gradient-based perturbation\n        x += 0.01 * gradient  # Perturbation applied\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced gradient-based perturbation in local search to improve convergence in fine-tuning phase.", "configspace": "", "generation": 9, "fitness": 0.9959329524598606, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "93fefeb6-97c7-4634-a840-653f17605dc1", "metadata": {"aucs": [0.9902881498771944, 0.9984085712870828, 0.9991021362153045], "final_y": [0.16485586997225066, 0.1648562791770284, 0.16485609978139104]}, "mutation_prompt": null}
{"id": "0fb57c79-a626-4144-899d-f04ba7c4e940", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.5 + np.random.rand() * 0.5  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.7 + np.random.rand() * 0.3  # Enhanced Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // (self.population_size // 2))}  # Change here for refined local search\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced local search strategy by increasing intensity adaptively for better refinement.", "configspace": "", "generation": 9, "fitness": 0.9961822831667481, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "93fefeb6-97c7-4634-a840-653f17605dc1", "metadata": {"aucs": [0.9910370443557355, 0.9984084660233714, 0.9991013391211375], "final_y": [0.16485617090785365, 0.16485629978827343, 0.16485646946811838]}, "mutation_prompt": null}
{"id": "139d072b-57bd-4978-b036-c4841a33ec05", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        best_idx = np.argmin([func(x) for x in pop])\n        best_solution = pop[best_idx].copy()  # Preserve the best solution\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        next_pop[np.argmax([func(x) for x in next_pop])] = best_solution  # Apply elitism\n        return next_pop\n\n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced elitism by preserving the best solution in each generation to prevent loss of optimal solutions.", "configspace": "", "generation": 9, "fitness": 0.9985697499318077, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9982001688833851, 0.9984074168835391, 0.9991016640284989], "final_y": [0.16485588874513257, 0.16485650956152276, 0.16485612978088504]}, "mutation_prompt": null}
{"id": "4ffd19ec-45e6-404f-9d3f-e098bef1fda9", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.45  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Refined mutation strategy by incorporating adaptive scaling for enhanced diversity in Differential Evolution.", "configspace": "", "generation": 9, "fitness": 0.9986468398475262, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984303258586191, 0.9984088450018198, 0.9991013486821396], "final_y": [0.16485592286597484, 0.16485621938061823, 0.16485615952325272]}, "mutation_prompt": null}
{"id": "6b88f513-8821-4f6f-8343-bf883374f555", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        self.cr = 0.8 + np.random.rand() * 0.2  # Dynamic Crossover Rate during local search\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce dynamic crossover probability in local search for enhanced exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.9986520896252286, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984301017435455, 0.9984244167437677, 0.9991017503883725], "final_y": [0.16485604348581984, 0.16485618822212167, 0.16485637338267]}, "mutation_prompt": null}
{"id": "5952e7b5-da8e-4dcd-8560-ffa86ead6b04", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.15, ub - (ub - lb) * 0.15)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.95))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced self-adaptive population size adjustment strategy and refined periodic initialization for enhanced convergence.", "configspace": "", "generation": 9, "fitness": 0.9986323509894716, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9983863119996205, 0.9984084445792082, 0.9991022963895861], "final_y": [0.16485696195182797, 0.16485640651624411, 0.16485609978139104]}, "mutation_prompt": null}
{"id": "ce094a68-0b73-4deb-85dc-268ee8069425", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.2 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity with increased base\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Enhanced periodicity enforcement and adaptive local search for optimized convergence.", "configspace": "", "generation": 9, "fitness": 0.9985821160371128, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984300979116341, 0.9984089947138542, 0.9989072554858505], "final_y": [0.16485604348581984, 0.16485609473870788, 0.16485623421077744]}, "mutation_prompt": null}
{"id": "fbc050a1-f96e-4e25-a9b7-72206972079e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n            pop[i] += np.sin(np.linspace(0, np.pi, self.dim)) * 0.01  # Adaptive periodicity reinforcement\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduced adaptive periodicity reinforcement in initialization to enhance convergence towards optimal structures.", "configspace": "", "generation": 9, "fitness": 0.9986469521238316, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984304720994408, 0.9984082480567495, 0.9991021362153045], "final_y": [0.16485596071961295, 0.16485659020913135, 0.16485609978139104]}, "mutation_prompt": null}
{"id": "1fe8d725-629e-4dee-a5c0-c129ed5df637", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.4 + 0.6 * (1 - evaluations / self.budget)  # Adaptive Mutation Factor Scaling\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        self.cr = 0.8 + np.random.rand() * 0.2  # Dynamic Crossover Rate during local search\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation factor scaling for increased diversity during early iterations.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "6b88f513-8821-4f6f-8343-bf883374f555", "metadata": {}, "mutation_prompt": null}
{"id": "6dbf9f81-68af-430f-946e-144c1ffe26d0", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            shift = np.random.randint(0, period)  # Introduce random periodic segment shift\n            pop[i][:period] = np.mean(pop[i][:period])\n            pop[i] = np.roll(pop[i], shift)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce random periodic segment shifts to enhance diversity in solution periodicity.", "configspace": "", "generation": 10, "fitness": 0.9966793214294519, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.997 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.992479417858849, 0.9984553503584607, 0.9991031960710461], "final_y": [0.16485592889758804, 0.16485648705075007, 0.16485615000289888]}, "mutation_prompt": null}
{"id": "2fe72fa2-b450-430d-a7ba-c54c9f64bfa2", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        adaptive_factor = 0.2 + 0.1 * np.random.rand()  # Adaptive variance reduction factor\n        pop = np.clip(pop, lb + (ub - lb) * adaptive_factor, ub - (ub - lb) * adaptive_factor)\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce an adaptive variance reduction factor in the quasi-oppositional periodic initialization to further enhance periodic structure adherence.", "configspace": "", "generation": 10, "fitness": 0.9986133078981879, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9982732306459905, 0.9984553422369675, 0.9991113508116055], "final_y": [0.1648558722383613, 0.16485657424408295, 0.16485615016690236]}, "mutation_prompt": null}
{"id": "6e099b6c-c123-472c-afd4-63ef3c8092e0", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        success_rate = 0.0\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget + success_rate)  # Adaptive intensity\n            successful_improvements = 0\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                    successful_improvements += 1\n                evaluations += 1\n            success_rate = successful_improvements / self.population_size\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Implemented adaptive local search strategy based on real-time performance metrics to enhance convergence.", "configspace": "", "generation": 10, "fitness": 0.9981164634047713, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.998 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984301017435455, 0.9970514845797095, 0.9988678038910587], "final_y": [0.16485604348581984, 0.16485579528119942, 0.1648558832091056]}, "mutation_prompt": null}
{"id": "2faa77f4-d53a-461b-ba24-ce6df5c34e90", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        variance_reduction_factor = 0.2 + 0.6 * (1 - np.random.rand())  # Adaptive variance reduction\n        pop = np.clip(pop, lb + (ub - lb) * variance_reduction_factor, ub - (ub - lb) * variance_reduction_factor)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive variance reduction in initialization for enhanced exploration-exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.9950341187613043, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9875442005468344, 0.9984551647798185, 0.99910299095726], "final_y": [0.16485609629172382, 0.16485626112878393, 0.16485599590080002]}, "mutation_prompt": null}
{"id": "d8c6b270-e547-401d-9b0b-bee3d7c13f65", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            # Introduce diversity by adding a scaled random vector to the mutant\n            random_vector = np.random.uniform(-0.1, 0.1, self.dim)  # Changed line\n            mutant = np.clip(a + self.f * (b - c) + random_vector, lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce a diversity-preserving mechanism in DE mutation to enhance exploration and avoid premature convergence.", "configspace": "", "generation": 10, "fitness": 0.9986127928843747, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "af21b47f-df50-4420-8bc3-7e82c608da4f", "metadata": {"aucs": [0.9984299773202112, 0.9984553682614746, 0.9989530330714381], "final_y": [0.16485617957271637, 0.16485650956152276, 0.16485591988329917]}, "mutation_prompt": null}
{"id": "b73b1cd1-5bc0-4c19-924c-fe7aed8d739c", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        intensity *= (1 - (self.budget - self.population_size) / self.budget)  # New adaptive intensity scaling\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        self.cr = 0.8 + np.random.rand() * 0.2  # Dynamic Crossover Rate during local search\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Introduce adaptive intensity scaling in local search to dynamically adjust the search depth based on budget utilization.", "configspace": "", "generation": 10, "fitness": 0.9891698117965785, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.014. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "6b88f513-8821-4f6f-8343-bf883374f555", "metadata": {"aucs": [0.9699507459045438, 0.9984566442813579, 0.9991020452038336], "final_y": [0.17279362167521561, 0.1648558907819634, 0.16485655993935655]}, "mutation_prompt": null}
{"id": "ab085ef0-4f47-4b5c-8f05-c2f601ac84c4", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        result.x = x + 0.1 * (result.x - x)  # Adaptive learning rate for refinement\n        self.cr = 0.8 + np.random.rand() * 0.2  # Dynamic Crossover Rate during local search\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Incorporate adaptive learning rate in local search to enhance fine-tuning near convergence.", "configspace": "", "generation": 10, "fitness": 0.9986657320802402, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6b88f513-8821-4f6f-8343-bf883374f555", "metadata": {"aucs": [0.9984301055516416, 0.9984636261501229, 0.9991034645389558], "final_y": [0.1648560331012756, 0.16485650956152276, 0.16485603943891014]}, "mutation_prompt": null}
{"id": "90cca2fc-1b1c-4046-9d6b-6e11455d02dc", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        learning_rate = 0.001 + 0.01 * (1 - intensity)  # New line: Adaptive learning rate\n        result.x = result.x + learning_rate * (np.clip(np.random.randn(self.dim), lb, ub))  # New line: Diversity mechanism\n        self.cr = 0.8 + np.random.rand() * 0.2  # Dynamic Crossover Rate during local search\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.5 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Integrate adaptive learning rate in local search and enhanced diversity mechanism for better convergence.", "configspace": "", "generation": 10, "fitness": 0.998663110218735, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6b88f513-8821-4f6f-8343-bf883374f555", "metadata": {"aucs": [0.9984301761304182, 0.9984571460891378, 0.999102008436649], "final_y": [0.1648560139354579, 0.1648560395563453, 0.16485659597152036]}, "mutation_prompt": null}
{"id": "5a818fd5-f2a4-4f70-ab1c-2d293c7d0607", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.f = 0.8  # DE Mutation Factor\n        self.cr = 0.9  # DE Crossover Rate\n\n    def quasi_oppositional_periodic_init(self, lb, ub):\n        \"\"\"Initialize population with quasi-oppositional strategy and periodicity encouragement.\"\"\"\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        pop = np.clip(pop, lb + (ub - lb) * 0.25, ub - (ub - lb) * 0.25)  # Variance reduction\n        opp_pop = lb + ub - pop\n        for i in range(self.population_size):\n            period = self.dim // 2\n            pop[i][:period] = np.mean(pop[i][:period])  # Enforce stronger periodicity\n            pop[i] = np.tile(pop[i][:period], self.dim // period)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, pop, func, lb, ub):\n        \"\"\"Perform the DE algorithm to evolve the population.\"\"\"\n        next_pop = np.empty((self.population_size, self.dim))\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            self.f = 0.6 + np.random.rand() * 0.4  # Enhanced Adaptive Mutation Factor\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            self.cr = 0.8 + np.random.rand() * 0.2  # Adaptive Crossover Rate\n            cross_points = np.random.rand(self.dim) < self.cr\n            trial = np.where(cross_points, mutant, pop[i])\n            next_pop[i] = trial if func(trial) < func(pop[i]) else pop[i]\n        return next_pop\n    \n    def local_search(self, x, func, lb, ub, intensity=0.1):\n        \"\"\"Perform local search using BFGS with adaptive intensity.\"\"\"\n        options = {'maxfun': int(intensity * self.budget // self.population_size)}\n        result = opt.minimize(func, x, bounds=opt.Bounds(lb, ub), method='L-BFGS-B', options=options)\n        self.cr = 0.8 + np.random.rand() * 0.2  # Dynamic Crossover Rate during local search\n        return result.x if result.success else x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = self.quasi_oppositional_periodic_init(lb, ub)\n        pop = pop[np.argsort([func(x) for x in pop])[:self.population_size]]  # Select top half\n        \n        evaluations = self.population_size * 2\n        while evaluations < self.budget:\n            self.population_size = max(10, int(self.population_size * 0.9))  # Dynamic Population Adjustment\n            pop = self.differential_evolution(pop, func, lb, ub)\n            evaluations += self.population_size\n            \n            intensity = 0.1 + 0.4 * (1 - evaluations / self.budget)  # Adaptive intensity\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n                refined = self.local_search(pop[i], func, lb, ub, intensity=intensity)\n                if func(refined) < func(pop[i]):\n                    pop[i] = refined\n                evaluations += 1\n        \n        best_idx = np.argmin([func(x) for x in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Refined adaptive intensity in local search for smoother convergence and improved final solution quality.", "configspace": "", "generation": 10, "fitness": 0.9986622554935995, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.999 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6b88f513-8821-4f6f-8343-bf883374f555", "metadata": {"aucs": [0.9984275296727931, 0.9984561298719485, 0.999103106936057], "final_y": [0.1648563023483529, 0.16485613106980734, 0.16485591650660614]}, "mutation_prompt": null}
