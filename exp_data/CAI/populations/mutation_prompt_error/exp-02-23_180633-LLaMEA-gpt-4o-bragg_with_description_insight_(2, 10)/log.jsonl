{"id": "a1182f55-fe07-4350-8b48-4041a7caa997", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.dim = dim\n        self.bounds = None\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        return np.concatenate((population, quasi_opposite_population), axis=0)\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n            # Encouraging periodicity in the solution\n            if _ % 10 == 0:\n                population = np.array([\n                    np.mean([population[i - 2], population[i - 1], population[i]], axis=0)\n                    for i in range(2, self.population_size)\n                ])\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution with Quasi-Oppositional learning for global exploration, followed by BFGS local optimization to refine promising solutions while encouraging periodicity in multilayer structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 25, in differential_evolution\nIndexError: index 19 is out of bounds for axis 0 with size 18\n.", "error": "IndexError('index 19 is out of bounds for axis 0 with size 18')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 25, in differential_evolution\nIndexError: index 19 is out of bounds for axis 0 with size 18\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "6f401230-85b8-4c20-9f47-ab0a8c0c7bbe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGSOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Encourage periodicity in initial population\n        for i in range(self.population_size):\n            self.population[i] = self.add_periodicity_bias(self.population[i], lb, ub)\n\n    def add_periodicity_bias(self, individual, lb, ub):\n        period = (ub - lb) / (self.dim // 2)\n        for i in range(0, self.dim, 2):\n            individual[i:i+2] = lb + (i % 2) * period\n        return np.clip(individual, lb, ub)\n\n    def differential_evolution(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.func_evals < self.budget:\n            for i in range(self.population_size):\n                if self.func_evals >= self.budget:\n                    break\n                # Mutation\n                candidates = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(candidates, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == jrand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                trial_fitness = func(trial)\n                self.func_evals += 1\n                if trial_fitness > func(self.population[i]):\n                    self.population[i] = trial\n\n    def local_refinement(self, func):\n        best_idx = np.argmax([func(ind) for ind in self.population])\n        best_solution = self.population[best_idx]\n        \n        result = minimize(lambda x: -func(x), best_solution, bounds=list(zip(func.bounds.lb, func.bounds.ub)),\n                          method='L-BFGS-B', options={'maxfun': self.budget - self.func_evals})\n        self.func_evals += result.nfev\n        return result.x\n\n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        self.differential_evolution(func)\n        best_solution = self.local_refinement(func)\n        return best_solution", "name": "HybridDEBFGSOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) with periodicity-focused bias and local refinement using BFGS for optimizing multilayer photonic structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\n  File \"<string>\", line 19, in initialize_population\n  File \"<string>\", line 24, in add_periodicity_bias\nValueError: could not broadcast input array from shape (10,) into shape (2,)\n.", "error": "ValueError('could not broadcast input array from shape (10,) into shape (2,)')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\n  File \"<string>\", line 19, in initialize_population\n  File \"<string>\", line 24, in add_periodicity_bias\nValueError: could not broadcast input array from shape (10,) into shape (2,)\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "6e0fb098-81cc-400b-b7ea-f6931dd74aed", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass CMAPeriodicSymmetryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = 0.5  # Step size\n        self.lambda_ = 10 + 3 * np.log(dim)  # Population size\n        self.func_evals = 0\n\n    def enforce_periodic_symmetry(self, x, lb, ub):\n        # Ensure symmetry in the solution\n        for i in range(0, self.dim, 2):\n            period_segment = (ub - lb) / (self.dim // 2)\n            x[i:i+2] = lb + (i % 2) * period_segment\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        x0 = np.random.uniform(lb, ub, self.dim)\n        es = CMAEvolutionStrategy(x0, self.sigma, {'popsize': self.lambda_})\n\n        while not es.stop() and self.func_evals < self.budget:\n            solutions = es.ask()\n            fitnesses = []\n\n            for x in solutions:\n                if self.func_evals >= self.budget:\n                    break\n                x = self.enforce_periodic_symmetry(x, lb, ub)\n                fitness = -func(x)\n                fitnesses.append(fitness)\n                self.func_evals += 1\n\n            es.tell(solutions, fitnesses)\n            es.disp()\n\n        result = es.result.xbest\n        best_solution = self.enforce_periodic_symmetry(result, lb, ub)\n        return best_solution", "name": "CMAPeriodicSymmetryOptimizer", "description": "A novel Covariance Matrix Adaptation Evolution Strategy (CMA-ES) combined with periodic symmetry enforcement to optimize multilayer photonic structures, maintaining periodicity through symmetry constraints and leveraging adaptive sampling for convergence. ", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6f401230-85b8-4c20-9f47-ab0a8c0c7bbe", "metadata": {}, "mutation_prompt": null}
{"id": "4fedacab-2bb8-4e15-977d-eb59fe83ca0b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer combining Differential Evolution with Adaptive Quasi-Oppositional Initialization and Periodicity Constraints, followed by BFGS for local refinement.", "configspace": "", "generation": 1, "fitness": 0.9425260090409059, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.943 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a1182f55-fe07-4350-8b48-4041a7caa997", "metadata": {"aucs": [0.9425251373995437, 0.9425251373995437, 0.9425277523236305], "final_y": [0.16486207795704255, 0.16486207795704255, 0.16485698493611578]}, "mutation_prompt": null}
{"id": "b7792772-ae43-4da1-9951-dd4879bb807c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGSOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = None\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        for i in range(self.population_size):\n            self.population[i] = self.add_periodicity_bias(self.population[i], lb, ub)\n\n    def add_periodicity_bias(self, individual, lb, ub):\n        period = (ub - lb) / self.dim\n        for i in range(self.dim):\n            individual[i] = lb + (i % 2) * period\n        return np.clip(individual, lb, ub)\n\n    def differential_evolution(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        adaptive_CR = self.CR * (1 - self.func_evals / self.budget)\n        while self.func_evals < self.budget:\n            for i in range(self.population_size):\n                if self.func_evals >= self.budget:\n                    break\n                candidates = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(candidates, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, lb, ub)\n                trial = np.copy(self.population[i])\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < adaptive_CR or j == jrand:\n                        trial[j] = mutant[j]\n                trial_fitness = func(trial)\n                self.func_evals += 1\n                if trial_fitness > func(self.population[i]):\n                    self.population[i] = trial\n\n    def local_refinement(self, func):\n        best_idx = np.argmax([func(ind) for ind in self.population])\n        best_solution = self.population[best_idx]\n        result = minimize(lambda x: -func(x), best_solution, bounds=list(zip(func.bounds.lb, func.bounds.ub)),\n                          method='L-BFGS-B', options={'maxfun': self.budget - self.func_evals})\n        self.func_evals += result.nfev\n        return result.x\n\n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        self.differential_evolution(func)\n        best_solution = self.local_refinement(func)\n        return best_solution", "name": "HybridDEBFGSOptimizer", "description": "Enhanced Differential Evolution algorithm with corrected periodicity bias and adaptive parameters for optimizing multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('setting an array element with a sequence.').", "error": "ValueError('setting an array element with a sequence.')", "parent_id": "6f401230-85b8-4c20-9f47-ab0a8c0c7bbe", "metadata": {}, "mutation_prompt": null}
{"id": "c70b0316-214e-4d69-a4d3-553dd3dc9f6c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGSOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Encourage periodicity in initial population\n        for i in range(self.population_size):\n            self.population[i] = self.add_periodicity_bias(self.population[i], lb, ub)\n\n    def add_periodicity_bias(self, individual, lb, ub):\n        period = (ub - lb) / self.dim  # Correct periodic calculation\n        for i in range(0, self.dim, 2):\n            individual[i:i+2] = lb + (i % 2) * period\n        return np.clip(individual, lb, ub)\n\n    def differential_evolution(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.func_evals < self.budget:\n            for i in range(self.population_size):\n                if self.func_evals >= self.budget:\n                    break\n                # Mutation\n                candidates = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(candidates, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == jrand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                trial_fitness = func(trial)\n                self.func_evals += 1\n                if trial_fitness > func(self.population[i]):\n                    self.population[i] = trial\n\n    def local_refinement(self, func):\n        best_idx = np.argmax([func(ind) for ind in self.population])\n        best_solution = self.population[best_idx]\n        \n        result = minimize(lambda x: -func(x), best_solution, bounds=list(zip(func.bounds.lb, func.bounds.ub)),\n                          method='L-BFGS-B', options={'maxfun': self.budget - self.func_evals})\n        self.func_evals += result.nfev\n        return result.x\n\n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        self.differential_evolution(func)\n        best_solution = self.local_refinement(func)\n        return best_solution", "name": "HybridDEBFGSOptimizer", "description": "A refined hybrid metaheuristic that improves periodic bias initialization for DE, enhancing the search for optimal multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (10,) into shape (2,)').", "error": "ValueError('could not broadcast input array from shape (10,) into shape (2,)')", "parent_id": "6f401230-85b8-4c20-9f47-ab0a8c0c7bbe", "metadata": {}, "mutation_prompt": null}
{"id": "8bc39888-05e5-4937-8fcb-3d05281ff34f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGSOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Encourage periodicity in initial population\n        for i in range(self.population_size):\n            self.population[i] = self.add_periodicity_bias(self.population[i], lb, ub)\n\n    def add_periodicity_bias(self, individual, lb, ub):\n        period = (ub - lb) / (self.dim // 2)\n        for i in range(0, self.dim, 2):\n            individual[i:i+2] = lb + (i//2 % 2) * period  # Corrected indexing for periodicity\n        return np.clip(individual, lb, ub)\n\n    def differential_evolution(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.func_evals < self.budget:\n            for i in range(self.population_size):\n                if self.func_evals >= self.budget:\n                    break\n                # Mutation\n                candidates = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(candidates, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == jrand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                trial_fitness = func(trial)\n                self.func_evals += 1\n                if trial_fitness > func(self.population[i]):\n                    self.population[i] = trial\n\n    def local_refinement(self, func):\n        best_idx = np.argmax([func(ind) for ind in self.population])\n        best_solution = self.population[best_idx]\n        \n        result = minimize(lambda x: -func(x), best_solution, bounds=list(zip(func.bounds.lb, func.bounds.ub)),\n                          method='L-BFGS-B', options={'maxfun': self.budget - self.func_evals})\n        self.func_evals += result.nfev\n        return result.x\n\n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        self.differential_evolution(func)\n        best_solution = self.local_refinement(func)\n        return best_solution", "name": "HybridDEBFGSOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) with periodicity-focused bias and local refinement using BFGS, optimized for multilayer photonic structures, with corrected periodicity bias initialization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (10,) into shape (2,)').", "error": "ValueError('could not broadcast input array from shape (10,) into shape (2,)')", "parent_id": "6f401230-85b8-4c20-9f47-ab0a8c0c7bbe", "metadata": {}, "mutation_prompt": null}
{"id": "de184a6b-b17c-44ea-9458-2c0697eb5df6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.dim = dim\n        self.bounds = None\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        return np.concatenate((population, quasi_opposite_population), axis=0)\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(len(population)) if idx != i]  # Fix line to use the correct length\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n            # Improved periodicity encouragement strategy\n            if _ % 10 == 0:\n                population = np.array([\n                    0.5 * (population[i - 1] + population[i])\n                    for i in range(1, len(population))  # Use correct length\n                ])\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic optimizer using Differential Evolution with Quasi-Oppositional initialization and a periodicity-encouraging strategy, along with BFGS local optimization.", "configspace": "", "generation": 1, "fitness": 0.9086988632148532, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.909 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a1182f55-fe07-4350-8b48-4041a7caa997", "metadata": {"aucs": [0.93027859111486, 0.9162067171105505, 0.879611281419149], "final_y": [0.16485859045615614, 0.16485655778863406, 0.1648585117884328]}, "mutation_prompt": null}
{"id": "f0db76c6-5b0c-443c-9111-84e1fb3f1af6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n    \n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        return np.concatenate((population, quasi_opposite_population), axis=0)\n    \n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n        \n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size * 2) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                mutant = np.clip(a + 0.8 * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n            \n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n            \n            if _ % 10 == 0:\n                population[:self.population_size] = np.array([\n                    np.mean([population[(i-2) % self.population_size], \n                             population[(i-1) % self.population_size], \n                             population[i]], axis=0)\n                    for i in range(self.population_size)\n                ])\n                \n        return best_individual\n    \n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "An improved hybrid metaheuristic algorithm combining Differential Evolution with Quasi-Oppositional learning and periodicity promotion, ensuring careful boundary handling and population diversity retention strategies for optimizing multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": 0.8740468705860298, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a1182f55-fe07-4350-8b48-4041a7caa997", "metadata": {"aucs": [0.8812883101944914, 0.8682634807787496, 0.8725888207848483], "final_y": [0.16485941252880787, 0.16485750711879932, 0.16485615214150995]}, "mutation_prompt": null}
{"id": "69c3ecd4-8f27-4bfe-b067-2b5aa4979e8f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDEBFGSOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.func_evals = 0\n\n    def initialize_population(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        self.population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Encourage periodicity in initial population\n        for i in range(self.population_size):\n            self.population[i] = self.add_periodicity_bias(self.population[i], lb, ub)\n\n    def add_periodicity_bias(self, individual, lb, ub):\n        period = (ub - lb) / self.dim\n        for i in range(self.dim):\n            individual[i] = lb + (i % 2) * period  # Apply periodic bias\n        return np.clip(individual, lb, ub)\n\n    def differential_evolution(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        while self.func_evals < self.budget:\n            for i in range(self.population_size):\n                if self.func_evals >= self.budget:\n                    break\n                # Mutation\n                candidates = [index for index in range(self.population_size) if index != i]\n                a, b, c = np.random.choice(candidates, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == jrand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                trial_fitness = func(trial)\n                self.func_evals += 1\n                if trial_fitness > func(self.population[i]):\n                    self.population[i] = trial\n\n    def local_refinement(self, func):\n        best_idx = np.argmax([func(ind) for ind in self.population])\n        best_solution = self.population[best_idx]\n        \n        result = minimize(lambda x: -func(x), best_solution, bounds=list(zip(func.bounds.lb, func.bounds.ub)),\n                          method='L-BFGS-B', options={'maxfun': self.budget - self.func_evals})\n        self.func_evals += result.nfev\n        return result.x\n\n    def __call__(self, func):\n        self.initialize_population(func.bounds)\n        self.differential_evolution(func)\n        best_solution = self.local_refinement(func)\n        return best_solution", "name": "EnhancedDEBFGSOptimizer", "description": "Enhanced metaheuristic algorithm combining Differential Evolution (DE) with periodicity-aware initialization and local refinement using BFGS for optimizing multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('setting an array element with a sequence.').", "error": "ValueError('setting an array element with a sequence.')", "parent_id": "6f401230-85b8-4c20-9f47-ab0a8c0c7bbe", "metadata": {}, "mutation_prompt": null}
{"id": "5b3a8065-a979-4b79-b176-3fbcf3fe618e", "solution": "import numpy as np\n\nclass PSOPeriodicityOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.inertia_weight = 0.9\n        self.cognitive_coef = 2.0\n        self.social_coef = 2.0\n        self.bounds = None\n\n    def particle_swarm_optimization(self, func):\n        np.random.seed(42)\n        position = np.random.uniform(self.bounds.lb, self.bounds.ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_position = np.copy(position)\n        personal_best_fitness = np.array([func(ind) for ind in position])\n        global_best_idx = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_position[global_best_idx]\n\n        for _ in range(self.budget // self.population_size):\n            for i in range(self.population_size):\n                inertia = self.inertia_weight * velocity[i]\n                cognitive = self.cognitive_coef * np.random.random() * (personal_best_position[i] - position[i])\n                social = self.social_coef * np.random.random() * (global_best_position - position[i])\n                velocity[i] = inertia + cognitive + social\n                position[i] = np.clip(position[i] + velocity[i], self.bounds.lb, self.bounds.ub)\n\n                fitness = func(position[i])\n                if fitness < personal_best_fitness[i]:\n                    personal_best_position[i] = position[i]\n                    personal_best_fitness[i] = fitness\n\n            global_best_idx = np.argmin(personal_best_fitness)\n            global_best_position = personal_best_position[global_best_idx]\n\n        return global_best_position\n\n    def periodicity_enhancing_local_search(self, func, initial_guess):\n        # Custom objective to enforce periodicity\n        def periodic_objective(x):\n            return func(x) + 0.01 * np.sum(np.abs(np.diff(x, n=2)))\n\n        res = minimize(periodic_objective, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.particle_swarm_optimization(func)\n        best_solution = self.periodicity_enhancing_local_search(func, best_global_solution)\n        return best_solution", "name": "PSOPeriodicityOptimizer", "description": "A novel metaheuristic optimizer combining Particle Swarm Optimization (PSO) for global exploration with adaptive inertia weights and a periodicity-enhancing local search phase using a custom periodicity-enforcing objective function.", "configspace": "", "generation": 1, "fitness": 0.8789275187097362, "feedback": "The algorithm PSOPeriodicityOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.000. And the mean value of best solutions found was 0.180 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "a1182f55-fe07-4350-8b48-4041a7caa997", "metadata": {"aucs": [0.8789422596741867, 0.8789201482275111, 0.8789201482275111], "final_y": [0.1795281911906388, 0.1806593996726208, 0.1806593996726208]}, "mutation_prompt": null}
{"id": "6b0ffad5-fb38-4060-bce3-86c3ec7edeeb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, dim)  # Adaptive population size\n        self.bounds = None\n        self.mutation_factor = 0.9\n        self.crossover_rate = 0.7\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        return np.concatenate((population, quasi_opposite_population), axis=0)\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(2 * self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.mutation_factor * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n            if _ % 10 == 0:  # Encourage periodicity more frequently\n                population = np.array([\n                    np.mean([population[i - 2], population[i - 1], population[i]], axis=0)\n                    for i in range(2, self.population_size)\n                ])\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced hybrid metaheuristic optimizer using Differential Evolution with adaptive population size and periodicity bias for global exploration, and constrained BFGS for local refinement, tailored for optimizing multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 19 is out of bounds for axis 0 with size 8').", "error": "IndexError('index 19 is out of bounds for axis 0 with size 8')", "parent_id": "a1182f55-fe07-4350-8b48-4041a7caa997", "metadata": {}, "mutation_prompt": null}
{"id": "5352ea3e-b21f-4d5d-aff8-4296275ab5f2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.5 * (solution[i] + solution[i + period_length])\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined Enhanced Hybrid Metaheuristic Optimizer with Adaptive Differential Evolution parameters and improved periodicity handling.", "configspace": "", "generation": 2, "fitness": 0.9531137198327588, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4fedacab-2bb8-4e15-977d-eb59fe83ca0b", "metadata": {"aucs": [0.9563515041797812, 0.9622713693231675, 0.9407182859953279], "final_y": [0.1648593606665555, 0.16485954901919542, 0.164856592463422]}, "mutation_prompt": null}
{"id": "2321f69a-7e45-46d7-91af-d3212374a772", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.dim = dim\n        self.bounds = None\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        return np.concatenate((population, quasi_opposite_population), axis=0)\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(len(population)) if idx != i]  # Fix line to use the correct length\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n            # Improved periodicity encouragement strategy\n            if _ % 10 == 0:\n                population = np.array([\n                    0.5 * (population[i] + population[(i+1) % len(population)])  # Changed line to improve periodicity\n                    for i in range(len(population))\n                ])\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced periodicity encouragement by updating the periodicity encouragement strategy to operate on all population members for improved convergence.", "configspace": "", "generation": 2, "fitness": 0.9326184728004664, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.933 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de184a6b-b17c-44ea-9458-2c0697eb5df6", "metadata": {"aucs": [0.9310771632526149, 0.9256781290588576, 0.9411001260899264], "final_y": [0.1648565243926159, 0.16485901465507424, 0.16486783708296926]}, "mutation_prompt": null}
{"id": "217172dc-c678-429d-89ea-563cbe4cdf88", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.dim = dim\n        self.bounds = None\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        return np.concatenate((population, quasi_opposite_population), axis=0)\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(len(population)) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n            # Improved periodicity encouragement strategy\n            if _ % 10 == 0:\n                population = np.array([\n                    0.5 * (population[i] + population[(i + 1) % len(population)])  # Change made here\n                    for i in range(len(population))\n                ])\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Further improve periodicity encouragement strategy by utilizing population averaging across the entire population.", "configspace": "", "generation": 2, "fitness": 0.9241203004592555, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de184a6b-b17c-44ea-9458-2c0697eb5df6", "metadata": {"aucs": [0.9305581117681268, 0.9301641069392067, 0.9116386826704329], "final_y": [0.16485614534509319, 0.16485616498702083, 0.164856511681581]}, "mutation_prompt": null}
{"id": "0ad0d9e2-db49-44c4-b5fb-e43798419b1e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.dim = dim\n        self.bounds = None\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        return np.concatenate((population, quasi_opposite_population), axis=0)\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(len(population)) if idx != i]  # Fix line to use the correct length\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.9 * (b - c), self.bounds.lb, self.bounds.ub)  # Adjusted mutation factor\n                cross_points = np.random.rand(self.dim) < 0.8  # Adjusted crossover probability\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n            # Improved periodicity encouragement strategy\n            if _ % 10 == 0:\n                population = np.array([\n                    0.5 * (population[i - 1] + population[i])\n                    for i in range(1, len(population))  # Use correct length\n                ])\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved differential evolution strategy by adjusting mutation factor and crossover probability for better exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.9307265251901736, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.931 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de184a6b-b17c-44ea-9458-2c0697eb5df6", "metadata": {"aucs": [0.9239906100067367, 0.9334492268309229, 0.9347397387328615], "final_y": [0.16486527957573893, 0.16485674703163944, 0.1648611281946848]}, "mutation_prompt": null}
{"id": "ebfbac6d-771f-4a34-a042-d377a39cc405", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.dim = dim\n        self.bounds = None\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        return np.concatenate((population, quasi_opposite_population), axis=0)\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(len(population)) if idx != i]  # Fix line to use the correct length\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.85 * (b - c), self.bounds.lb, self.bounds.ub)  # Changed mutation factor\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n            # Improved periodicity encouragement strategy\n            if _ % 10 == 0:\n                population = np.array([\n                    0.5 * (population[i - 1] + population[i]) + np.random.normal(scale=0.1, size=self.dim)  # Added noise\n                    for i in range(1, len(population))  # Use correct length\n                ])\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer using an updated mutation factor in Differential Evolution and improved periodicity encouragement.", "configspace": "", "generation": 2, "fitness": 0.9314943520206866, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.931 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de184a6b-b17c-44ea-9458-2c0697eb5df6", "metadata": {"aucs": [0.9351342566429999, 0.9229031926322928, 0.9364456067867672], "final_y": [0.16485831676105767, 0.1648588061616193, 0.16486104753047592]}, "mutation_prompt": null}
{"id": "d314c4e4-3843-46f9-842d-dc87bcf425cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def dynamic_periodicity_enforcement(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def adaptive_differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        F_base, CR_base = 0.8, 0.7\n        F_decay, CR_growth = 0.99, 1.01\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = F_base * (F_decay ** _)\n                CR = min(1.0, CR_base * (CR_growth ** _))\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.dynamic_periodicity_enforcement(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def optimized_local_refinement(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.adaptive_differential_evolution(func)\n        best_solution = self.optimized_local_refinement(func, best_global_solution)\n        return best_solution", "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Enhanced Adaptive Metaheuristic Optimizer using Adaptive Differential Evolution with Quasi-Oppositional Initialization, Dynamic Periodicity Enforcement, and Optimized Local Refinement via L-BFGS-B.", "configspace": "", "generation": 2, "fitness": 0.9507978470550328, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.951 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4fedacab-2bb8-4e15-977d-eb59fe83ca0b", "metadata": {"aucs": [0.9396539422367204, 0.9522853951231499, 0.9604542038052278], "final_y": [0.16485851508894667, 0.1648619087009674, 0.16485732691986532]}, "mutation_prompt": null}
{"id": "7e3b491b-b730-4f86-8972-49b1878c582b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + 0.3 * (generation / (self.budget // (2 * self.population_size)))  # Line changed for adaptive scaling\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)  # Line changed for adaptive scaling\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Further enhancement of the Enhanced Hybrid Metaheuristic Optimizer by incorporating adaptive mutation scaling during evolution for improved convergence.", "configspace": "", "generation": 2, "fitness": 0.9457865451204878, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.946 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4fedacab-2bb8-4e15-977d-eb59fe83ca0b", "metadata": {"aucs": [0.9617211226969332, 0.9353943527550472, 0.9402441599094828], "final_y": [0.16485831609093238, 0.16485903200333862, 0.16485625242109703]}, "mutation_prompt": null}
{"id": "dda6c315-317a-4ee3-b7bc-882a47499128", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def adaptive_periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = (solution[i] + solution[i + period_length]) / 2\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < np.random.uniform(0.6, 0.9)\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.adaptive_periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with a dynamic crossover strategy in Differential Evolution and an additional adaptive periodicity enforcement step for improved convergence.", "configspace": "", "generation": 2, "fitness": 0.9364017599062043, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4fedacab-2bb8-4e15-977d-eb59fe83ca0b", "metadata": {"aucs": [0.944052894796837, 0.9227094658709418, 0.9424429190508345], "final_y": [0.16485705723635047, 0.16485763144878263, 0.16485587538285085]}, "mutation_prompt": null}
{"id": "268fe81f-336b-44b7-82e5-b587d3cefb48", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = np.random.uniform(0.5, 1.0)  # Change: Refined mutation factor\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.7\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer with a refined mutation strategy in Differential Evolution to improve exploration.", "configspace": "", "generation": 2, "fitness": 0.947270534189411, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4fedacab-2bb8-4e15-977d-eb59fe83ca0b", "metadata": {"aucs": [0.9293036493899032, 0.9669049298891446, 0.9456030232891851], "final_y": [0.16485699483396543, 0.1648572013325309, 0.16486013806675015]}, "mutation_prompt": null}
{"id": "592a5c8d-6086-4502-bf62-4547d3e4b502", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicAdaptiveQuantumInspiredDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def adaptive_mutation(self, population, diversity):\n        F_base = 0.5\n        return F_base * (1 + (1 - diversity) / 2)\n\n    def quantum_superposition_crossover(self, a, b, c):\n        alpha = np.random.rand(self.dim)\n        beta = np.sqrt(1 - alpha**2)\n        return alpha * a + beta * b + (1 - alpha - beta) * c\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.initialize_population(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for _ in range(self.budget // self.population_size):\n            diversity = np.std(population, axis=0).mean()\n            F = self.adaptive_mutation(population, diversity)\n\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(self.quantum_superposition_crossover(a, b, c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.9\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='SLSQP', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "DynamicAdaptiveQuantumInspiredDE", "description": "Dynamic Adaptive Quantum-Inspired Differential Evolution (DAQIDE) with a quantum superposition-inspired crossover and adaptive mutation based on population diversity, followed by Sequential Least Squares Programming (SLSQP) for local refinement.", "configspace": "", "generation": 2, "fitness": 0.9310596572528579, "feedback": "The algorithm DynamicAdaptiveQuantumInspiredDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.931 with standard deviation 0.033. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "de184a6b-b17c-44ea-9458-2c0697eb5df6", "metadata": {"aucs": [0.9639972628799431, 0.8857167820966844, 0.9434649267819464], "final_y": [0.16860311456167443, 0.19241684547275995, 0.17515504325163067]}, "mutation_prompt": null}
{"id": "94e84d13-15a2-453b-a589-ef8ef1d0166f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.5 * (solution[i] + solution[i + period_length])\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            self.population_size = int(20 + (self.budget // (2 * self.population_size) - generation) / 2)  # Dynamic population size\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introducing dynamic adjustment of population size based on generation number to improve diversity and exploration.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 53 is out of bounds for axis 0 with size 40').", "error": "IndexError('index 53 is out of bounds for axis 0 with size 40')", "parent_id": "5352ea3e-b21f-4d5d-aff8-4296275ab5f2", "metadata": {}, "mutation_prompt": null}
{"id": "4f61eb85-b4b4-4b1c-abc2-160b81d8445f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CollaborativeCoevolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.subpop_size = 10\n        self.num_subpops = 2\n        self.bounds = None\n\n    def initialize_subpopulations(self, lb, ub):\n        subpopulations = [np.random.uniform(lb, ub, (self.subpop_size, self.dim // self.num_subpops)) for _ in range(self.num_subpops)]\n        return subpopulations\n\n    def periodicity_induced_mutation(self, subpop, dim):\n        period_length = dim // 2\n        for ind in subpop:\n            for i in range(period_length):\n                ind[i + period_length] = np.mean(ind[:period_length])\n        return subpop\n\n    def crossover(self, parent1, parent2, CR):\n        dim = len(parent1)\n        cross_points = np.random.rand(dim) < CR\n        child = np.where(cross_points, parent1, parent2)\n        return child\n\n    def collaborative_evolution(self, func):\n        np.random.seed(42)\n        subpopulations = self.initialize_subpopulations(self.bounds.lb, self.bounds.ub)\n        subpop_fitness = [np.array([func(np.concatenate(subpopulations)) for _ in subpop]) for subpop in subpopulations]\n\n        for generation in range(self.budget // (self.subpop_size * self.num_subpops)):\n            F = 0.8 + 0.2 * np.sin(2 * np.pi * generation / self.budget)\n            CR = 0.8 + 0.2 * np.cos(2 * np.pi * generation / self.budget)\n            for sp_idx, subpop in enumerate(subpopulations):\n                for i in range(self.subpop_size):\n                    indices = [idx for idx in range(self.subpop_size) if idx != i]\n                    a, b, c = subpop[np.random.choice(indices, 3, replace=False)]\n                    mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                    trial = self.crossover(mutant, subpop[i], CR)\n                    trial = self.periodicity_induced_mutation(trial, self.dim // self.num_subpops)\n\n                    trial_fitness = func(np.concatenate(subpopulations))\n                    if trial_fitness < subpop_fitness[sp_idx][i]:\n                        subpop[i] = trial\n                        subpop_fitness[sp_idx][i] = trial_fitness\n\n        best_combined_solution = np.concatenate([subpop[np.argmin(fitness)] for subpop, fitness in zip(subpopulations, subpop_fitness)])\n        return best_combined_solution\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_collaborative_solution = self.collaborative_evolution(func)\n        best_solution = self.local_optimization(func, best_collaborative_solution)\n        return best_solution", "name": "CollaborativeCoevolutionaryOptimizer", "description": "Collaborative Coevolutionary Algorithm with Adaptive Parameter Control and Periodicity-Induced Mutation to optimize multilayer structures using cooperative subpopulations.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (10, 5) and arg 1 with shape (10,).').", "error": "ValueError('shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (10, 5) and arg 1 with shape (10,).')", "parent_id": "5352ea3e-b21f-4d5d-aff8-4296275ab5f2", "metadata": {}, "mutation_prompt": null}
{"id": "b13eef4a-2e5a-41f3-84b6-7c88f7bf5f2e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.5 * (solution[i] + solution[i + period_length])\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb * 0.8, self.bounds.ub * 1.2)))  # Changed bounds\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined Enhanced Hybrid Metaheuristic Optimizer with Adaptive Differential Evolution parameters, improved periodicity handling, and dynamic scaling of L-BFGS-B for effective local search.", "configspace": "", "generation": 3, "fitness": 0.9591402186300425, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5352ea3e-b21f-4d5d-aff8-4296275ab5f2", "metadata": {"aucs": [0.9556142397064658, 0.9593817880604273, 0.9624246281232344], "final_y": [0.1648616176317479, 0.16486097384293175, 0.16485910587160013]}, "mutation_prompt": null}
{"id": "8cffe1c6-59c9-458c-8c71-70f5490fb7d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            # Modified line for enhanced periodicity constraint\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced periodicity constraint by applying a weighted average to improve solution symmetry and performance.", "configspace": "", "generation": 3, "fitness": 0.9619199076000219, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5352ea3e-b21f-4d5d-aff8-4296275ab5f2", "metadata": {"aucs": [0.9555033341594438, 0.9547106761508817, 0.9755457124897404], "final_y": [0.16485652681698393, 0.16486667754345796, 0.16485757760928776]}, "mutation_prompt": null}
{"id": "3c1de55e-0799-48b0-a714-9d25f21c5d5f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def dynamic_periodicity_enforcement(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def adaptive_differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        F_base, CR_base = 0.85, 0.7  # Adjusted F_base for better exploration\n        F_decay, CR_growth = 0.99, 1.01\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = F_base * (F_decay ** _)\n                CR = min(1.0, CR_base * (CR_growth ** _))\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.dynamic_periodicity_enforcement(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def optimized_local_refinement(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.adaptive_differential_evolution(func)\n        best_solution = self.optimized_local_refinement(func, best_global_solution)\n        return best_solution", "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Refined Adaptive Differential Evolution with modified control parameter strategy for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.9421231667687731, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.942 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d314c4e4-3843-46f9-842d-dc87bcf425cd", "metadata": {"aucs": [0.9423291978994344, 0.9307046255364477, 0.953335676870437], "final_y": [0.16486433258319733, 0.16485893062539392, 0.1648646979277243]}, "mutation_prompt": null}
{"id": "bd861bef-d689-47ca-9c2e-dcdce4e11320", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved Enhanced Hybrid Metaheuristic Optimizer with Dynamic Population Size and Enhanced Periodicity Handling for Optimal Reflectivity.", "configspace": "", "generation": 3, "fitness": 0.9658962310123869, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5352ea3e-b21f-4d5d-aff8-4296275ab5f2", "metadata": {"aucs": [0.9619813259642552, 0.9604488546614709, 0.9752585124114342], "final_y": [0.16485805646425544, 0.16485584607698878, 0.16485765011169307]}, "mutation_prompt": null}
{"id": "e9c76243-4308-497f-b967-69a5b2dab2ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def dynamic_periodicity_enforcement(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def adaptive_differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        F_base, CR_base = 0.8, 0.7\n        F_decay, CR_growth = 0.99, 1.01\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(4, int(self.population_size * (1 - 0.01)))  # Dynamic population size adjustment\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = F_base * (F_decay ** _)\n                CR = min(1.0, CR_base * (CR_growth ** _))\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.dynamic_periodicity_enforcement(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def optimized_local_refinement(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.adaptive_differential_evolution(func)\n        best_solution = self.optimized_local_refinement(func, best_global_solution)\n        return best_solution", "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Introduced dynamic population size adjustment based on convergence progress to balance exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.9329998986911944, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.933 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d314c4e4-3843-46f9-842d-dc87bcf425cd", "metadata": {"aucs": [0.9495733039320519, 0.9272560556196605, 0.9221703365218706], "final_y": [0.1648642398928969, 0.1648603303582714, 0.16485865516129528]}, "mutation_prompt": null}
{"id": "fa115ae2-d9bc-49d0-836d-7c3bc2801eb0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def dynamic_periodicity_enforcement(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def adaptive_differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        F_base, CR_base = 0.8, 0.7\n        F_decay, CR_growth = 0.99, 1.01\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = F_base * (F_decay ** _)\n                CR = min(1.0, CR_base * (CR_growth ** _))\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.dynamic_periodicity_enforcement(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def optimized_local_refinement(self, func, initial_guess):\n        iter_count = 0\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        while not res.success and iter_count < 3:  # Retry mechanism for local refinement\n            iter_count += 1\n            res = minimize(func, res.x, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.adaptive_differential_evolution(func)\n        best_solution = self.optimized_local_refinement(func, best_global_solution)\n        return best_solution", "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Advanced Hybrid Metaheuristic Optimizer with enhanced local refinement strategy and adaptive parameter tuning using feedback loops.", "configspace": "", "generation": 3, "fitness": 0.9247087036089955, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.925 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d314c4e4-3843-46f9-842d-dc87bcf425cd", "metadata": {"aucs": [0.9383672000046933, 0.931494106294581, 0.9042648045277126], "final_y": [0.16485632775573889, 0.1648586881876667, 0.16485917275312223]}, "mutation_prompt": null}
{"id": "3e6808a0-4bb3-419c-974e-fcd8c2f7a226", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass OptimizedEnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_enhancement(self, solution):\n        # Enhance periodicity by dynamically adjusting based on the variance of the population\n        period_length = self.dim // 2\n        variance_factor = np.var(solution) / np.max(solution)\n        for i in range(period_length):\n            solution[i + period_length] = (1 - variance_factor) * solution[i] + variance_factor * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n        best_global_score = np.min(population_fitness)\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_enhancement(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_global_score = min(best_global_score, population_fitness[best_idx])\n\n        return population[best_idx]\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "OptimizedEnhancedHybridMetaheuristicOptimizer", "description": "Optimized Enhanced Hybrid Metaheuristic Optimizer utilizing Adaptive Differential Evolution with Dynamic Periodicity Enhancement and Population Variance Control.", "configspace": "", "generation": 3, "fitness": 0.6700748655825711, "feedback": "The algorithm OptimizedEnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.670 with standard deviation 0.087. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "5352ea3e-b21f-4d5d-aff8-4296275ab5f2", "metadata": {"aucs": [0.7450937404191604, 0.7163075478321757, 0.5488233084963776], "final_y": [0.18187856539968794, 0.16485643322360688, 0.25781072879030054]}, "mutation_prompt": null}
{"id": "ecd3adbb-f520-46b8-8628-b46af74e2525", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def dynamic_periodicity_enforcement(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def adaptive_differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        F_base, CR_base = 0.8, 0.7\n        F_decay, CR_growth = 0.99, 1.01\n\n        elite_archive = []  # Line added: Initialize elite archive\n\n        for _ in range(self.budget // (2 * self.population_size)):\n            CR = CR_base * (CR_growth ** _)\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = F_base * (F_decay ** _)\n                CR = min(1.0, CR * (1 - 0.5 * (_ / (self.budget // (2 * self.population_size)))))  # Line modified: Adaptive CR adjustment\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.dynamic_periodicity_enforcement(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n                    elite_archive.append((trial, trial_fitness))  # Line added: Archive elite solutions\n\n            elite_archive.sort(key=lambda x: x[1])\n            if len(elite_archive) > self.population_size:\n                elite_archive = elite_archive[:self.population_size]  # Line added: Maintain elite archive size\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def optimized_local_refinement(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.adaptive_differential_evolution(func)\n        best_solution = self.optimized_local_refinement(func, best_global_solution)\n        return best_solution", "name": "EnhancedAdaptiveMetaheuristicOptimizer", "description": "Enhanced Adaptive Metaheuristic Optimizer with Adaptive Differential Evolution, Quasi-Oppositional Initialization, Dynamic Periodicity Enforcement, and Optimized Local Refinement via L-BFGS-B, improved by Adaptive CR adjustment and Elite Archiving mechanism.", "configspace": "", "generation": 3, "fitness": 0.9032417462085308, "feedback": "The algorithm EnhancedAdaptiveMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.903 with standard deviation 0.031. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d314c4e4-3843-46f9-842d-dc87bcf425cd", "metadata": {"aucs": [0.9425467002400602, 0.8676051734358852, 0.8995733649496471], "final_y": [0.16485996643494127, 0.16485992518047587, 0.16485970687274054]}, "mutation_prompt": null}
{"id": "4b761eb7-be12-4ee3-9977-af50a35afeaa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            # Modified line for enhanced periodicity constraint\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced periodicity constraint by adjusting weight ratios to improve solution symmetry and performance.", "configspace": "", "generation": 4, "fitness": 0.958392734661086, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cffe1c6-59c9-458c-8c71-70f5490fb7d8", "metadata": {"aucs": [0.9560853588325356, 0.9567132184058825, 0.9623796267448395], "final_y": [0.16485641867750733, 0.16485622572463177, 0.16486184805749504]}, "mutation_prompt": null}
{"id": "55ce2def-953e-405a-9816-9cda8c3150ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.65 * solution[i] + 0.35 * solution[i + period_length]  # Adjusted blending ratio\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved the periodicity constraint blending ratio for better solution symmetry and performance.", "configspace": "", "generation": 4, "fitness": 0.9602183225167512, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.960 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bd861bef-d689-47ca-9c2e-dcdce4e11320", "metadata": {"aucs": [0.9703990068681889, 0.9538844215298532, 0.9563715391522113], "final_y": [0.16485681367877503, 0.1648592125906455, 0.16486061993045042]}, "mutation_prompt": null}
{"id": "47eac347-5431-43ce-b38b-5021c43faceb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 8 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.85), 10)\n            F = 0.6 + (0.95 - 0.6) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.8 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        if not res.success:  # Improved local search robustness\n            res = minimize(func, initial_guess, method='Powell', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced Differential Evolution with Adaptive Periodicity and Improved Local Search for Superior Reflectivity Optimization.", "configspace": "", "generation": 4, "fitness": 0.9469280957984475, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bd861bef-d689-47ca-9c2e-dcdce4e11320", "metadata": {"aucs": [0.9598447443140922, 0.9430668633487717, 0.9378726797324783], "final_y": [0.16485913671899444, 0.16485703537005603, 0.16485918891626428]}, "mutation_prompt": null}
{"id": "004ddf66-65df-43bb-9e01-aad59f05610f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, dim * 2)  # Adaptive population size\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            # Enhanced periodicity constraint\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced adaptive population size and a refined periodicity constraint to enhance exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.9409484643971983, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cffe1c6-59c9-458c-8c71-70f5490fb7d8", "metadata": {"aucs": [0.9427573251294968, 0.9381213269544781, 0.9419667411076202], "final_y": [0.1648580220333289, 0.16486133923188873, 0.1648574513658415]}, "mutation_prompt": null}
{"id": "adfadb60-1ae6-4e99-9bbf-d2e11d92bc93", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.2:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced a probabilistic approach to periodically apply local optimization, enhancing solution refinement with minimal computational cost.", "configspace": "", "generation": 4, "fitness": 0.9647031229468688, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "bd861bef-d689-47ca-9c2e-dcdce4e11320", "metadata": {"aucs": [0.9841671629345538, 0.9619055816770059, 0.9480366242290469], "final_y": [0.1648561984329412, 0.16485840710486122, 0.16486092392958962]}, "mutation_prompt": null}
{"id": "e46c642b-551f-4cef-8c79-af85f2886935", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            # Modified line for enhanced periodicity constraint\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined periodicity constraint by adjusting the weighted combination to improve symmetry and performance.", "configspace": "", "generation": 4, "fitness": 0.9578618035085465, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cffe1c6-59c9-458c-8c71-70f5490fb7d8", "metadata": {"aucs": [0.949353900653139, 0.9654597046477669, 0.9587718052247337], "final_y": [0.16486095491413355, 0.16485633113371634, 0.1648565265850226]}, "mutation_prompt": null}
{"id": "f83c2123-39f2-43b9-baa3-5149ca87e927", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation):\n        period_length = self.dim // 2\n        weight_factor = 0.5 + 0.5 * (generation / (self.budget // (2 * self.population_size)))\n        for i in range(period_length):\n            solution[i + period_length] = (1 - weight_factor) * solution[i] + weight_factor * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.1 + (0.9 - 0.1) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introducing adaptive weighting and dynamic CR for improved periodicity and exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.9536699198331805, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cffe1c6-59c9-458c-8c71-70f5490fb7d8", "metadata": {"aucs": [0.9396619970621709, 0.9658849048097482, 0.9554628576276227], "final_y": [0.16485754718371837, 0.16486040481719988, 0.1648583852896387]}, "mutation_prompt": null}
{"id": "2b3c08b1-b772-4f95-a133-6b96252a441b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2\n        influence_factor = 0.3 + 0.4 * (generation / max_generations)  # Adjusted line\n        for i in range(period_length):\n            solution[i + period_length] = influence_factor * solution[i] + (1 - influence_factor) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n        max_generations = self.budget // (2 * self.population_size)\n\n        for generation in range(max_generations):\n            F = 0.5 + (0.9 - 0.5) * generation / max_generations\n            CR = 0.5 + (0.9 - 0.5) * generation / max_generations\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, max_generations)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced a dynamic adjustment of periodicity constraints using a generation-based influence factor for enhanced reflectivity optimization.", "configspace": "", "generation": 4, "fitness": 0.95585339590567, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.956 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cffe1c6-59c9-458c-8c71-70f5490fb7d8", "metadata": {"aucs": [0.9485249908700725, 0.9635608075588684, 0.955474389288069], "final_y": [0.16485766258504864, 0.16485843562629343, 0.16485653287649804]}, "mutation_prompt": null}
{"id": "9adbd516-044d-4167-be53-54c155dfebe8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            # Modified line for dynamic CR\n            CR = 0.3 + 0.7 * np.random.rand() \n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introducing a dynamic crossover rate to enhance exploration and exploitation balance in the differential evolution process.", "configspace": "", "generation": 4, "fitness": 0.9587920246910254, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cffe1c6-59c9-458c-8c71-70f5490fb7d8", "metadata": {"aucs": [0.9622774253120511, 0.9599913350548207, 0.9541073137062044], "final_y": [0.16485743078430048, 0.16485618226000465, 0.16485609038312998]}, "mutation_prompt": null}
{"id": "17fca1d4-92bc-4adf-9053-af3abc8ae137", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MultiPhaseEvolutionaryStrategyOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n        self.phase = 'exploration'\n\n    def adaptive_initialization(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def periodicity_enforcement(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]\n        return solution\n\n    def transition_phase(self, generation):\n        if generation < 0.7 * (self.budget // self.population_size):\n            self.phase = 'exploration'\n        else:\n            self.phase = 'exploitation'\n\n    def evolutionary_strategy(self, func):\n        np.random.seed(42)\n        population = self.adaptive_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(self.periodicity_enforcement(ind)) for ind in population])\n\n        for generation in range(self.budget // self.population_size):\n            self.transition_phase(generation)\n            if self.phase == 'exploration':\n                F = 0.8\n                CR = 0.2\n            else:\n                F = 0.2\n                CR = 0.8\n\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_enforcement(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.evolutionary_strategy(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "MultiPhaseEvolutionaryStrategyOptimizer", "description": "Multi-Phase Evolutionary Strategy Optimizer employs dynamic phase transitions between exploration and exploitation to adaptively refine solutions for multilayered photonic structure optimization.", "configspace": "", "generation": 4, "fitness": 0.8688314778844387, "feedback": "The algorithm MultiPhaseEvolutionaryStrategyOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.869 with standard deviation 0.036. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "8cffe1c6-59c9-458c-8c71-70f5490fb7d8", "metadata": {"aucs": [0.8563056533250459, 0.8322339834085122, 0.9179547969197579], "final_y": [0.17493595649077098, 0.172970386463129, 0.1728583116678123]}, "mutation_prompt": null}
{"id": "8e445220-df0d-452b-bc79-eba045437b87", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                fitness_weights = np.max(population_fitness) - population_fitness  # Inverted weights for selection\n                fitness_weights /= np.sum(fitness_weights)\n                a, b, c = population[np.random.choice(indices, 3, replace=False, p=fitness_weights)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.2:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced weighted random selection for mutation vectors to enhance exploratory diversity.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "adfadb60-1ae6-4e99-9bbf-d2e11d92bc93", "metadata": {}, "mutation_prompt": null}
{"id": "2b518680-cf02-469f-9b3b-58391281685f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending ratio\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced local optimization by modifying the blending ratio of periodicity constraints to improve convergence robustness.", "configspace": "", "generation": 5, "fitness": 0.9742840461447209, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "55ce2def-953e-405a-9816-9cda8c3150ec", "metadata": {"aucs": [0.9572259873702978, 0.9899408217045755, 0.9756853293592891], "final_y": [0.1648599235890491, 0.16485609856636563, 0.16485980047943205]}, "mutation_prompt": null}
{"id": "b1273031-e061-4af9-8661-8aa9bfa54f75", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.6 + (0.9 - 0.6) * generation / (self.budget // (2 * self.population_size))  # Adjusted starting F\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.2:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Slightly adjusted DE mutation strategy to improve exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.9593684820893481, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "adfadb60-1ae6-4e99-9bbf-d2e11d92bc93", "metadata": {"aucs": [0.9494287396867708, 0.9668037815310048, 0.9618729250502689], "final_y": [0.16485674072191547, 0.16486116423599328, 0.16485742996203534]}, "mutation_prompt": null}
{"id": "f0b0eefd-3013-40be-b2f9-7be21a2372a5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.2:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced adaptive mutation factor scaling in DE to enhance exploration capability dynamically.", "configspace": "", "generation": 5, "fitness": 0.9680439594204232, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "adfadb60-1ae6-4e99-9bbf-d2e11d92bc93", "metadata": {"aucs": [0.982137388944608, 0.9702338437742846, 0.9517606455423769], "final_y": [0.1648559344598205, 0.1648587541751334, 0.16486430459203472]}, "mutation_prompt": null}
{"id": "2918ceb8-31ae-4eee-a7b1-c43903bc69dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced local optimization probability to dynamically switch strategies and quickly refine top solutions.", "configspace": "", "generation": 5, "fitness": 0.9695829327997648, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "adfadb60-1ae6-4e99-9bbf-d2e11d92bc93", "metadata": {"aucs": [0.9891769150899558, 0.9586851828777988, 0.96088670043154], "final_y": [0.16485601345728818, 0.16485775808422598, 0.16485621241054094]}, "mutation_prompt": null}
{"id": "37557131-bb20-4da4-acc6-0681a3114b67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.65 * solution[i] + 0.35 * solution[i + period_length]  # Adjusted blending ratio\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n            \n            CR = 0.7  # Adjusted crossover rate for faster convergence\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced adaptive crossover and scaling factors in DE to improve convergence speed and quality.", "configspace": "", "generation": 5, "fitness": 0.9680330652618228, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "55ce2def-953e-405a-9816-9cda8c3150ec", "metadata": {"aucs": [0.9763804124365563, 0.9678835046519626, 0.9598352786969493], "final_y": [0.1648581503016736, 0.16486338172202397, 0.1648612557051755]}, "mutation_prompt": null}
{"id": "70bbfe00-e5a7-4de8-aacf-40869fbad444", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]  # Adjusted blending ratio\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced periodicity constraint to optimize constructive interference by refining solution symmetry.", "configspace": "", "generation": 5, "fitness": 0.9619921146230657, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "55ce2def-953e-405a-9816-9cda8c3150ec", "metadata": {"aucs": [0.96615573334619, 0.9564208528842217, 0.9633997576387854], "final_y": [0.16485607846129624, 0.16485995400223497, 0.16485910224135314]}, "mutation_prompt": null}
{"id": "57b6421f-d69b-4c76-bc7b-271c929dc8d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]  # Adjusted blending ratio\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced periodicity constraint blending ratio for improved symmetry and solution quality.", "configspace": "", "generation": 5, "fitness": 0.9627330750425253, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "55ce2def-953e-405a-9816-9cda8c3150ec", "metadata": {"aucs": [0.9624703706682176, 0.9615014474806242, 0.9642274069787341], "final_y": [0.16486012387283566, 0.16485990864379807, 0.1648655293450607]}, "mutation_prompt": null}
{"id": "2151aaf9-ac96-457e-aff1-b8fa7008ddc9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        adaptive_ratio = 0.65 + np.random.uniform(-0.05, 0.05)  # Introduced adaptive blending ratio\n        for i in range(period_length):\n            solution[i + period_length] = adaptive_ratio * solution[i] + (1 - adaptive_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced an adaptive blending ratio during periodicity constraint application for improved symmetry in solutions.", "configspace": "", "generation": 5, "fitness": 0.9593047832848197, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "55ce2def-953e-405a-9816-9cda8c3150ec", "metadata": {"aucs": [0.9686029109533835, 0.9611592435602474, 0.9481521953408282], "final_y": [0.16485855897143697, 0.16485863805022805, 0.164857248164614]}, "mutation_prompt": null}
{"id": "e013e246-42bc-42a3-8cb0-fa1de2f17cc2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CooperativeCoevolutionOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.subcomponent_size = self.dim // 2\n        self.population_size = 20\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.subcomponent_size))\n\n    def evaluate_full_solution(self, func, subcomponent_a, subcomponent_b):\n        full_solution = np.concatenate((subcomponent_a, subcomponent_b))\n        return func(full_solution)\n\n    def evolutionary_strategy(self, func, subcomponent):\n        np.random.seed(42)\n        population = self.initialize_population(self.bounds.lb[:self.subcomponent_size], self.bounds.ub[:self.subcomponent_size])\n        population_fitness = np.array([self.evaluate_full_solution(func, ind, ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb[:self.subcomponent_size], self.bounds.ub[:self.subcomponent_size])\n                cross_points = np.random.rand(self.subcomponent_size) < CR\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = self.evaluate_full_solution(func, trial, trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n        best_idx = np.argmin(population_fitness)\n        return population[best_idx]\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        subcomponent_a = self.evolutionary_strategy(func, np.zeros(self.subcomponent_size))\n        subcomponent_b = self.evolutionary_strategy(func, subcomponent_a)\n        \n        best_coevolved_solution = np.concatenate((subcomponent_a, subcomponent_b))\n        best_solution = self.local_optimization(func, best_coevolved_solution)\n        return best_solution", "name": "CooperativeCoevolutionOptimizer", "description": "Leveraged cooperative coevolution with a two-phase strategy to optimize subcomponents separately, enhancing overall convergence toward periodic solutions.", "configspace": "", "generation": 5, "fitness": 0.9465592434786027, "feedback": "The algorithm CooperativeCoevolutionOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.005. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "adfadb60-1ae6-4e99-9bbf-d2e11d92bc93", "metadata": {"aucs": [0.9419322551706641, 0.9534412247024079, 0.9443042505627355], "final_y": [0.17285375633132027, 0.1728519330480799, 0.1728529216697955]}, "mutation_prompt": null}
{"id": "6c2eae8c-0645-4f77-9d32-b434e581e8fe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending ratio\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < (CR * np.sin(np.pi * np.arange(self.dim) / self.dim))  # Periodic cross points\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced periodic cross points in differential evolution to enhance the exploration of periodic solutions.", "configspace": "", "generation": 6, "fitness": 0.9754736078744312, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2b518680-cf02-469f-9b3b-58391281685f", "metadata": {"aucs": [0.9504586587879468, 0.9850250077704318, 0.9909371570649149], "final_y": [0.16485632360900626, 0.1648562030432167, 0.16485599771413628]}, "mutation_prompt": null}
{"id": "c92e2b00-efd6-47b6-a423-775129762a22", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            fitness_variance = np.var(population_fitness)\n            F = 0.5 + (0.4 * fitness_variance)  # Adjust mutation factor based on fitness variance\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved exploration by dynamically adjusting the mutation factor F based on the fitness variance of the population.", "configspace": "", "generation": 6, "fitness": 0.9793443162451946, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2918ceb8-31ae-4eee-a7b1-c43903bc69dd", "metadata": {"aucs": [0.9834507862930352, 0.9871960424170123, 0.967386120025536], "final_y": [0.16485594939949544, 0.164856380514256, 0.1648558974964336]}, "mutation_prompt": null}
{"id": "9164e2f1-0a65-4a3c-bc3c-33f0df23eef7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending ratio\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Adjusted CR range\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved dynamic adjustment of crossover probability in DE to enhance exploration and exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.977638369033099, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2b518680-cf02-469f-9b3b-58391281685f", "metadata": {"aucs": [0.9602063406100307, 0.989862780955193, 0.9828459855340734], "final_y": [0.16485680749610043, 0.16485605364078482, 0.1648562394791493]}, "mutation_prompt": null}
{"id": "7acc93d4-8ea5-4f90-93f2-6a0809ebb032", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.55 * solution[i] + 0.45 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 8 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.85), 10)\n            F = 0.6 + (0.8 - 0.6) * generation / (self.budget // (2 * self.population_size))  # Adjusted F\n            CR = 0.6 + (0.8 - 0.6) * generation / (self.budget // (2 * self.population_size))  # Adjusted CR\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.35:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced dynamic strategy switching with refined local optimization and adaptive mutation rates for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.9788643809834966, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2918ceb8-31ae-4eee-a7b1-c43903bc69dd", "metadata": {"aucs": [0.9643498786102975, 0.9835042034086078, 0.9887390609315844], "final_y": [0.1648565032771817, 0.16485599798523498, 0.1648559087694661]}, "mutation_prompt": null}
{"id": "916cdbb9-6a4e-46ba-875c-5b4171ddc055", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # Modified probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced exploration by increasing diversity in local optimization with a new probabilistic check.", "configspace": "", "generation": 6, "fitness": 0.974770207489847, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2918ceb8-31ae-4eee-a7b1-c43903bc69dd", "metadata": {"aucs": [0.9787027963240591, 0.9765314501809015, 0.9690763759645805], "final_y": [0.16485585677660397, 0.1648562590039655, 0.1648561233694985]}, "mutation_prompt": null}
{"id": "81e8357d-9d69-465a-81c9-40c803522e8c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * (np.sin(generation) / 2 + 0.5)  # Adaptive mutation factor\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced adaptive mutation factor adjustment in the differential evolution to enhance solution diversity and convergence speed.", "configspace": "", "generation": 6, "fitness": 0.9777229486362081, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2918ceb8-31ae-4eee-a7b1-c43903bc69dd", "metadata": {"aucs": [0.9622726343136094, 0.9816270672366275, 0.9892691443583872], "final_y": [0.16485603674972515, 0.16485651457011785, 0.1648569204095487]}, "mutation_prompt": null}
{"id": "133e3665-1a42-4974-9b52-cbd5a907959d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            adaptive_blend = 0.5 + 0.1 * np.sin(i)  # Adjusted blending with sinusoidal component\n            solution[i + period_length] = adaptive_blend * solution[i] + (1 - adaptive_blend) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced periodicity constraint with adaptive blending ratio for improved global exploration.", "configspace": "", "generation": 6, "fitness": 0.9736654132841833, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2b518680-cf02-469f-9b3b-58391281685f", "metadata": {"aucs": [0.976192383017725, 0.9879520267083202, 0.9568518301265049], "final_y": [0.1648592009138764, 0.1648560210130262, 0.16485588065706935]}, "mutation_prompt": null}
{"id": "17b6b910-45fc-4941-b921-ca348e7a7ae4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2\n        blend_ratio = 0.6 + 0.3 * (generation / max_generations)  # Dynamically adjusted blending\n        for i in range(period_length):\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, self.budget // (2 * self.population_size))\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced dynamic periodicity blending ratio in the periodicity_constraint function to adaptively enhance interference effects.", "configspace": "", "generation": 6, "fitness": 0.9867502182643421, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2918ceb8-31ae-4eee-a7b1-c43903bc69dd", "metadata": {"aucs": [0.9821445286904656, 0.9895475852579693, 0.9885585408445914], "final_y": [0.16485611685221424, 0.16485592229107404, 0.16485614790096192]}, "mutation_prompt": null}
{"id": "184892cd-36d7-450a-b66d-06ae8f934548", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced dynamic mutation scaling and crossover probability to enhance exploration and exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.9810650345018713, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2918ceb8-31ae-4eee-a7b1-c43903bc69dd", "metadata": {"aucs": [0.9749711780168404, 0.9897165787960159, 0.9785073466927576], "final_y": [0.16485632368368608, 0.1648559518430467, 0.16485597517050343]}, "mutation_prompt": null}
{"id": "4873d78e-d45d-4778-a393-133372d6ef70", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Adjusted blending ratio in periodicity constraint to better encourage periodic solutions.", "configspace": "", "generation": 6, "fitness": 0.9843085599725555, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2918ceb8-31ae-4eee-a7b1-c43903bc69dd", "metadata": {"aucs": [0.9805438764048451, 0.985013165592663, 0.9873686379201587], "final_y": [0.1648559580784691, 0.1648560657984518, 0.16485635277626487]}, "mutation_prompt": null}
{"id": "fa4af273-6a10-45ee-9aaa-a79ea3bde927", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation):\n        adaptive_period_length = max(1, self.dim // (2 + generation // 10))  # Adaptive period length\n        for i in range(adaptive_period_length):\n            solution[i + adaptive_period_length] = 0.7 * solution[i] + 0.3 * solution[i + adaptive_period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced the periodicity constraint by introducing adaptive period length adjustment based on optimization progress to better align with true periodic solutions.", "configspace": "", "generation": 7, "fitness": 0.9792103770272687, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4873d78e-d45d-4778-a393-133372d6ef70", "metadata": {"aucs": [0.9850638091929595, 0.9787338877711123, 0.973833434117734], "final_y": [0.16485596183556062, 0.16485622994773907, 0.1648563403232033]}, "mutation_prompt": null}
{"id": "dbf0367b-038f-40c5-bfe6-f26290799893", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * (1 - generation / (self.budget // (2 * self.population_size)))  # Adjusted adaptation\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Adjusted the dynamic adaptation function in differential evolution to improve blending ratio effectiveness.", "configspace": "", "generation": 7, "fitness": 0.9716905490227865, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4873d78e-d45d-4778-a393-133372d6ef70", "metadata": {"aucs": [0.9786765077222396, 0.9841681208312827, 0.9522270185148374], "final_y": [0.16485608205207858, 0.1648560710639091, 0.16485585600063213]}, "mutation_prompt": null}
{"id": "eb235e27-8738-4956-8c4c-3408aa0f9f1d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            blend_factor = 0.6 + 0.4 * np.cos(i * np.pi / period_length)\n            solution[i + period_length] = blend_factor * solution[i] + (1 - blend_factor) * solution[i + period_length]  # Refined dynamic blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refined dynamic periodicity blending for better adaptation to interference effects.", "configspace": "", "generation": 7, "fitness": 0.9821858573700796, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4873d78e-d45d-4778-a393-133372d6ef70", "metadata": {"aucs": [0.9823868938708645, 0.9753759437506307, 0.9887947344887434], "final_y": [0.16485618965644566, 0.16485587369318755, 0.16485603584387043]}, "mutation_prompt": null}
{"id": "89fe7822-8d27-453f-8787-41cd92435815", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + (F + 0.1 * np.random.rand()) * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced a mutation scaling strategy that adaptively increases exploration diversity for the differential evolution algorithm.", "configspace": "", "generation": 7, "fitness": 0.977508839605732, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4873d78e-d45d-4778-a393-133372d6ef70", "metadata": {"aucs": [0.9623513808391561, 0.9843784202283783, 0.9857967177496616], "final_y": [0.1648565403660568, 0.16485587211418207, 0.16485588944549834]}, "mutation_prompt": null}
{"id": "0382a8c8-dfbd-4799-b657-3c1ea663ee2a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]  # Adjusted blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * (1 - generation / (self.budget // (2 * self.population_size)))  # Dynamic CR adjustment\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced dynamic adjustment in the crossover probability (CR) to better balance exploration and exploitation over generations.", "configspace": "", "generation": 7, "fitness": 0.9815730753404264, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4873d78e-d45d-4778-a393-133372d6ef70", "metadata": {"aucs": [0.984286649611628, 0.9740274255357351, 0.9864051508739159], "final_y": [0.1648562141487283, 0.16485624986555358, 0.16485682339205088]}, "mutation_prompt": null}
{"id": "600b2f98-d60e-43fc-a0af-25ae51765c0a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2\n        blend_ratio = 0.6 + 0.3 * (generation / max_generations)  # Dynamically adjusted blending\n        for i in range(period_length):\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Introduce targeted mutation\n                target = best_individual if np.random.rand() < 0.3 else a\n                mutant = np.clip(target + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, self.budget // (2 * self.population_size))\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced exploration in Differential Evolution by introducing targeted mutation based on best-performing individuals.", "configspace": "", "generation": 7, "fitness": 0.9869562984865063, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "17b6b910-45fc-4941-b921-ca348e7a7ae4", "metadata": {"aucs": [0.9868401762238272, 0.9827036527504662, 0.9913250664852257], "final_y": [0.16485722571459294, 0.16485583463987896, 0.16485593295185097]}, "mutation_prompt": null}
{"id": "6dd70f7c-e060-426c-ab18-eb9c00b86907", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        np.random.seed()  # Change: Ensure diverse initial population\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.6 * solution[i] + 0.4 * solution[i + period_length]  # Change: Fine-tuned blending\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced the local optimization's effectiveness by fine-tuning the reflection coefficients during periodicity constraint application for better convergence and adjusted random seed initialization to ensure diverse population generation.", "configspace": "", "generation": 7, "fitness": 0.9762454053526394, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4873d78e-d45d-4778-a393-133372d6ef70", "metadata": {"aucs": [0.9912909544081694, 0.982116891595166, 0.9553283700545829], "final_y": [0.1648560442946514, 0.1648562099766756, 0.1648558392408822]}, "mutation_prompt": null}
{"id": "f8d36184-e0bb-4050-941a-a6354ff0c523", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.8 * solution[i] + 0.2 * solution[i + period_length]  # Finer periodic blend\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10)  # More gradual reduction\n            F = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved dynamic population control and finer-grained periodic adaptation to enhance solution periodicity and convergence stability.", "configspace": "", "generation": 7, "fitness": 0.9836145839978263, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4873d78e-d45d-4778-a393-133372d6ef70", "metadata": {"aucs": [0.9869432499011306, 0.9883554694157884, 0.9755450326765601], "final_y": [0.16485743676604558, 0.16485649501913557, 0.16485580194103333]}, "mutation_prompt": null}
{"id": "b609373f-c56a-47ba-ae6e-dd1e12d45f6d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        adaptive_blend = 0.6 + 0.4 * np.sin(np.pi * np.arange(period_length) / period_length)\n        for i in range(period_length):\n            solution[i + period_length] = adaptive_blend[i] * solution[i] + (1 - adaptive_blend[i]) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.6 + (0.8 - 0.6) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.6 + (0.8 - 0.6) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant_vector = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant_vector, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced dynamic adaptive blending ratios and enhanced mutation strategy using periodicity to optimize multilayer designs.", "configspace": "", "generation": 7, "fitness": 0.9737534383592849, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4873d78e-d45d-4778-a393-133372d6ef70", "metadata": {"aucs": [0.9744554255984946, 0.9846155050919677, 0.9621893843873922], "final_y": [0.16485612448732445, 0.16485616258104785, 0.16485590674082629]}, "mutation_prompt": null}
{"id": "4dafefb3-a3ff-4993-a1f2-1853eb288680", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2\n        blend_ratio = 0.7 + 0.2 * (generation / max_generations)  # Adjusted blending ratio\n        for i in range(period_length):\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.1 * np.random.randn(self.dim), self.bounds.lb, self.bounds.ub)  # Diversity-promoting mutation\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, self.budget // (2 * self.population_size))\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced exploration by introducing a diversity-promoting mutation strategy and adjusted blending ratio for improved periodicity constraint handling in differential evolution.", "configspace": "", "generation": 7, "fitness": 0.9778367568902406, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "17b6b910-45fc-4941-b921-ca348e7a7ae4", "metadata": {"aucs": [0.9675115363936986, 0.9791627849841419, 0.9868359492928813], "final_y": [0.1648561504650189, 0.1648574576249685, 0.16485592444587682]}, "mutation_prompt": null}
{"id": "beb82c21-5f0a-4509-a727-00411c766e23", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.8 * solution[i] + 0.2 * solution[i + period_length]  # Finer periodic blend\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance convergence stability and solution robustness by increasing mutation diversity and introducing periodicity reinforcement.", "configspace": "", "generation": 8, "fitness": 0.9870908090304936, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f8d36184-e0bb-4050-941a-a6354ff0c523", "metadata": {"aucs": [0.9868225298904668, 0.9868225379790633, 0.9876273592219508], "final_y": [0.16485598820541625, 0.16485598125270595, 0.1648561480902856]}, "mutation_prompt": null}
{"id": "902dd721-8154-4845-9ddc-26407cd9385c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.8 * solution[i] + 0.2 * solution[i + period_length]  # Finer periodic blend\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10)  # More gradual reduction\n            variance = np.var(population, axis=0).mean()  # Calculate population diversity\n            F = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.3 + variance / np.max(population_fitness)  # Dynamic crossover rate adjustment\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced dynamic crossover rate adjustment based on population diversity for better exploration-exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.9648098795877257, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.030. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f8d36184-e0bb-4050-941a-a6354ff0c523", "metadata": {"aucs": [0.9884120515437695, 0.9220069045112552, 0.9840106827081523], "final_y": [0.1648566019803518, 0.164856567672098, 0.16485628625226]}, "mutation_prompt": null}
{"id": "03021c99-e969-43b3-8ce2-1cf415c979cb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2\n        blend_ratio = 0.6 + 0.3 * (generation / max_generations)  # Dynamically adjusted blending\n        for i in range(period_length):\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n        learning_rate = 0.1  # Introduced learning rate for mutation strength adjustment\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Introduce targeted mutation\n                target = best_individual if np.random.rand() < 0.3 else a\n                mutant = np.clip(target + learning_rate * F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, self.budget // (2 * self.population_size))\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduce a learning rate factor to dynamically adjust mutation strength based on fitness improvement trends.", "configspace": "", "generation": 8, "fitness": 0.9828590227103873, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "600b2f98-d60e-43fc-a0af-25ae51765c0a", "metadata": {"aucs": [0.9880375382411867, 0.9786974057946466, 0.9818421240953286], "final_y": [0.16485625138970006, 0.1648560628710385, 0.16485669295185934]}, "mutation_prompt": null}
{"id": "7acaa5d3-81bc-4d9f-97b6-b72b12fff928", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.8 * solution[i] + 0.2 * solution[i + period_length]  # Finer periodic blend\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10)  # More gradual reduction\n            F = 0.3 + (0.4 * np.sin(generation))  # Improved adaptive mutation strategy\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved adaptive mutation strategy in Differential Evolution to enhance convergence and solution quality.", "configspace": "", "generation": 8, "fitness": 0.9835691660415384, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f8d36184-e0bb-4050-941a-a6354ff0c523", "metadata": {"aucs": [0.986822317994272, 0.9784150192621979, 0.9854701608681452], "final_y": [0.16485585687409388, 0.16485639770918048, 0.1648561893338676]}, "mutation_prompt": null}
{"id": "3fdbdc92-983b-409a-81c0-45667efef4be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.85 * solution[i] + 0.15 * solution[i + period_length]  # Adjusted periodic blend\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10)  # More gradual reduction\n            F = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.4 + (0.6 - 0.4) * generation / (self.budget // (2 * self.population_size))  # Adjusted crossover rate\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhanced dynamic adaptation and refined periodicity integration for improved solution convergence and stability.", "configspace": "", "generation": 8, "fitness": 0.9804862726765, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f8d36184-e0bb-4050-941a-a6354ff0c523", "metadata": {"aucs": [0.9893066132572227, 0.964415775092579, 0.9877364296796985], "final_y": [0.16485607705557814, 0.1648560758324663, 0.1648559934432321]}, "mutation_prompt": null}
{"id": "5024d30a-94da-4b30-92f9-e0d71fb67ae4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2 if generation < max_generations // 2 else self.dim // 4  # Adaptive period length\n        blend_ratio = 0.6 + 0.3 * (generation / max_generations)\n        for i in range(period_length):\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                target = best_individual if np.random.rand() < 0.3 else a\n                F_scale = F * np.random.uniform(0.8, 1.2)  # Adaptive mutation scaling\n                mutant = np.clip(target + F_scale * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, self.budget // (2 * self.population_size))\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduced adaptive period length and mutation scaling to enhance exploration and convergence in Differential Evolution.", "configspace": "", "generation": 8, "fitness": 0.9816659688218672, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "600b2f98-d60e-43fc-a0af-25ae51765c0a", "metadata": {"aucs": [0.9757596883922882, 0.9815384286584831, 0.9876997894148304], "final_y": [0.1648565199483193, 0.16485616489986732, 0.16485665598060506]}, "mutation_prompt": null}
{"id": "3ef0d411-f1a5-423c-acb8-9118b3493b81", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2\n        blend_ratio = 0.6 + 0.3 * (generation / max_generations)  # Dynamically adjusted blending\n        for i in range(period_length):\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            diversity = np.std(population, axis=0).mean()  # Calculate diversity\n            F = 0.5 + (0.9 - 0.5) * diversity  # Adjusted mutation factor\n            CR = 0.5 + (0.9 - 0.5) * diversity  # Adjusted crossover rate\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Introduce targeted mutation\n                target = best_individual if np.random.rand() < 0.3 else a\n                mutant = np.clip(target + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, self.budget // (2 * self.population_size))\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved mutation strategy by dynamically adjusting mutation factor and crossover rate based on population diversity.", "configspace": "", "generation": 8, "fitness": 0.9657782922080579, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "600b2f98-d60e-43fc-a0af-25ae51765c0a", "metadata": {"aucs": [0.9270163553368195, 0.9864169295137329, 0.9839015917736212], "final_y": [0.18188018558408736, 0.1648564324081293, 0.1648572631364551]}, "mutation_prompt": null}
{"id": "e251329d-7c32-427e-95cf-40ddefa05e8b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2\n        blend_ratio = 0.6 + 0.3 * (generation / max_generations)  # Dynamically adjusted blending\n        for i in range(period_length):\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * (1 - np.std(population_fitness) / np.mean(population_fitness))\n            CR = 0.5 + (0.9 - 0.5) * (1 - np.std(population_fitness) / np.mean(population_fitness))\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Introduce targeted mutation\n                target = best_individual if np.random.rand() < 0.3 else a\n                mutant = np.clip(target + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, self.budget // (2 * self.population_size))\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.4:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduce enhanced scaling factor and crossover rate adjustment based on fitness variation.", "configspace": "", "generation": 8, "fitness": 0.9844939213150029, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "600b2f98-d60e-43fc-a0af-25ae51765c0a", "metadata": {"aucs": [0.9789822056486762, 0.9862827305732257, 0.9882168277231065], "final_y": [0.1648558563334679, 0.1648573090021097, 0.1648562714850318]}, "mutation_prompt": null}
{"id": "a5e98f25-52fb-4e79-a361-ea60ce87b4e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]  # Enforce exact periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10) \n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))  # Adaptive F\n            CR = 0.3 + (0.8 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  \n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 2  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introducing adaptive mutation strategies and periodicity-aware local optimization to enhance diversity and convergence.", "configspace": "", "generation": 8, "fitness": 0.9847986646635151, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f8d36184-e0bb-4050-941a-a6354ff0c523", "metadata": {"aucs": [0.9891108674005625, 0.9765213884006703, 0.9887637381893123], "final_y": [0.16485689316267582, 0.16485614906433654, 0.16485601482408097]}, "mutation_prompt": null}
{"id": "2c54d0e3-f2b2-4772-96df-1ce5940313c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution, generation, max_generations):\n        period_length = self.dim // 2\n        blend_ratio = 0.6 + 0.3 * (generation / max_generations)  # Dynamically adjusted blending\n        for i in range(period_length):\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            if generation % 10 == 0:  # Dynamic population adjustment\n                self.population_size = max(int(self.population_size * 0.9), 10)\n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))\n            best_idx = np.argmin(population_fitness)\n            best_individual = population[best_idx]\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Introduce targeted mutation\n                target = best_individual if np.random.rand() < 0.3 else a\n                mutant = np.clip(target + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial, generation, self.budget // (2 * self.population_size))\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.6:  # Probabilistic local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improved local search by tweaking the probability of local optimization to enhance fine-tuning around optima.", "configspace": "", "generation": 8, "fitness": 0.97223119319308, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "600b2f98-d60e-43fc-a0af-25ae51765c0a", "metadata": {"aucs": [0.9821824770586057, 0.9470563983347667, 0.9874547041858678], "final_y": [0.1648567899372434, 0.1648560175010274, 0.1648566996409102]}, "mutation_prompt": null}
{"id": "f22cb0bc-816b-4dc7-b920-5b94fb8269bd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]  # Enforce stricter periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance periodicity by refining the periodicity constraint to improve constructive interference.", "configspace": "", "generation": 9, "fitness": 0.987613783671209, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "beb82c21-5f0a-4509-a727-00411c766e23", "metadata": {"aucs": [0.9877021858474232, 0.9876828581289483, 0.9874563070372553], "final_y": [0.1648563307441473, 0.1648564338222106, 0.16485600500415254]}, "mutation_prompt": null}
{"id": "164b4e3d-a16f-443e-b628-6aea8bcc06ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.8 * solution[i] + 0.2 * solution[i + period_length]  # Finer periodic blend\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < np.var(population_fitness):  # Adjust local optimization frequency based on diversity\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance convergence by adjusting local optimization frequency based on population diversity.", "configspace": "", "generation": 9, "fitness": 0.9801757927707192, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "beb82c21-5f0a-4509-a727-00411c766e23", "metadata": {"aucs": [0.9817897508598314, 0.9883514143686566, 0.9703862130836695], "final_y": [0.16485614514094327, 0.16485686353617512, 0.16485620923307942]}, "mutation_prompt": null}
{"id": "b0936ccb-6996-4790-bb74-fe345ed92998", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]  # Enforce exact periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.9), 10)  # Adjusted population decay rate\n            F = 0.5 + (0.8 - 0.5) * generation / (self.budget // (2 * self.population_size))  # Tweak adaptive F range\n            CR = 0.4 + (0.7 - 0.4) * generation / (self.budget // (2 * self.population_size))  # Enhanced crossover adaptation\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.7:  # Increase likelihood of local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 2  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refine adaptive strategies using diversity preservation, periodicity reinforcement, and stochastic local tweaks for robust convergence.", "configspace": "", "generation": 9, "fitness": 0.9836518683978678, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a5e98f25-52fb-4e79-a361-ea60ce87b4e8", "metadata": {"aucs": [0.9743944288489208, 0.9843718941838948, 0.9921892821607877], "final_y": [0.16485603496917822, 0.16485627161611915, 0.1648559250731555]}, "mutation_prompt": null}
{"id": "70451bb9-9b62-4e0f-ac03-636be45d91bf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = solution[i]  # Enforce exact periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10) \n            F = 0.5 + (0.9 - 0.5) * generation / (self.budget // (2 * self.population_size))  # Adaptive F\n            CR = 0.3 + (0.8 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.7:  # Increased probability for local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 2  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Adaptive DE with quasi-oppositional initialization and periodicity-aware mutation enhances convergence and diversity.", "configspace": "", "generation": 9, "fitness": 0.9865969588751463, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a5e98f25-52fb-4e79-a361-ea60ce87b4e8", "metadata": {"aucs": [0.9799223959931829, 0.991283389440056, 0.9885850911922003], "final_y": [0.16485607119539192, 0.1648559593405663, 0.1648562456932]}, "mutation_prompt": null}
{"id": "c9b10264-18ab-4c68-9ea8-a364eb031262", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.8 * solution[i] + 0.2 * solution[i + period_length]  # Finer periodic blend\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.8 - 0.3) * (1 - generation / (self.budget // (2 * self.population_size)))  # Dynamic crossover adjustment\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refine mutation strategy by dynamically adjusting the crossover rate to enhance convergence.", "configspace": "", "generation": 9, "fitness": 0.9645079114824373, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.035. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "beb82c21-5f0a-4509-a727-00411c766e23", "metadata": {"aucs": [0.9878263954732628, 0.9151339604183055, 0.9905633785557434], "final_y": [0.16485616718856044, 0.1648560000486683, 0.16485600270747547]}, "mutation_prompt": null}
{"id": "741b3e33-0a5c-4901-9941-db3fb811102d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = 0.7 * solution[i] + 0.3 * solution[i + period_length]  # Adjusted periodic blend\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance convergence stability and solution robustness by increasing mutation diversity, introducing periodicity reinforcement, and adjusting periodic blend ratio.", "configspace": "", "generation": 9, "fitness": 0.9862864343681176, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "beb82c21-5f0a-4509-a727-00411c766e23", "metadata": {"aucs": [0.9813743014723096, 0.989576557997475, 0.9879084436345678], "final_y": [0.16485600184944016, 0.16485650221952342, 0.16485603337634758]}, "mutation_prompt": null}
{"id": "768f50a7-d82e-44bd-ba63-86992b79d759", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            blend_ratio = 0.7 + 0.3 * (i / period_length)  # Dynamically adjust blend ratio\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Refine the optimization strategy by dynamically adjusting the periodic blend ratio to ensure better alignment with the optimal periodicity pattern. ", "configspace": "", "generation": 9, "fitness": 0.9901647894677987, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.990 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "beb82c21-5f0a-4509-a727-00411c766e23", "metadata": {"aucs": [0.992231646094227, 0.9890801705042311, 0.9891825518049382], "final_y": [0.1648560242839151, 0.1648567187476947, 0.16485613550272027]}, "mutation_prompt": null}
{"id": "47d7d5ec-1469-4abc-a19c-d81b21c333cc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        weight = 0.8 + 0.1 * np.random.rand()  # Change to dynamically adjust weight\n        for i in range(period_length):\n            solution[i + period_length] = weight * solution[i] + (1 - weight) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance solution diversity and convergence by adjusting the periodicity constraint weight dynamically.", "configspace": "", "generation": 9, "fitness": 0.988778078832102, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "beb82c21-5f0a-4509-a727-00411c766e23", "metadata": {"aucs": [0.9896707465700636, 0.9891432279260945, 0.987520262000148], "final_y": [0.16485596321132479, 0.16485604817266175, 0.16485608527751072]}, "mutation_prompt": null}
{"id": "6763346a-45be-4c94-8cad-5a6d6e47d593", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = np.mean(solution[:period_length])  # Enforce averaged periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10) \n            F = 0.6 + (0.9 - 0.6) * np.sin(np.pi * generation / (self.budget // (2 * self.population_size)))  # Adaptive F\n            CR = 0.3 + (0.8 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  \n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 2  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance convergence by modifying the periodicity constraint and mutation factor adaptation.", "configspace": "", "generation": 9, "fitness": 0.9890106573079787, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a5e98f25-52fb-4e79-a361-ea60ce87b4e8", "metadata": {"aucs": [0.9881011195120567, 0.9900394134652252, 0.9888914389466541], "final_y": [0.16485620682123647, 0.1648560143234159, 0.16485616759629973]}, "mutation_prompt": null}
{"id": "bc408c1c-8fdb-4a63-85fe-c0152000569a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        alpha = 0.8  # Static blending coefficient\n        # Dynamic adjustment of blending coefficient based on generation progress\n        dynamic_blend = alpha + 0.1 * np.random.rand()\n        for i in range(period_length):\n            solution[i + period_length] = dynamic_blend * solution[i] + (1 - dynamic_blend) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduce dynamic adjustment of periodicity blending to enhance adaptability and solution quality.  ", "configspace": "", "generation": 9, "fitness": 0.9880976275569023, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "beb82c21-5f0a-4509-a727-00411c766e23", "metadata": {"aucs": [0.9850251203516338, 0.9895464426214803, 0.9897213196975932], "final_y": [0.1648561877364948, 0.16485608946966923, 0.16485594047454888]}, "mutation_prompt": null}
{"id": "1d0f5b70-e39c-44f1-875d-21f8b41976f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = np.mean(solution[:period_length])  # Enforce averaged periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10) \n            F = 0.6 + (0.9 - 0.6) * np.sin(np.pi * generation / (self.budget // (2 * self.population_size)))  # Adaptive F\n            CR = 0.3 + (0.8 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) * np.sin(2 * np.pi * i / self.population_size), self.bounds.lb, self.bounds.ub)  # Introduce periodic strategy\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  \n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 2  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduce a periodic strategy for mutation to improve alignment with optimal periodicity patterns.", "configspace": "", "generation": 10, "fitness": 0.9859563598854718, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6763346a-45be-4c94-8cad-5a6d6e47d593", "metadata": {"aucs": [0.9895207844399952, 0.9826530649740399, 0.9856952302423801], "final_y": [0.1648558706307932, 0.16485626769975004, 0.16485662553384595]}, "mutation_prompt": null}
{"id": "568ea774-51b0-411a-817d-7b5b3fe75b77", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            blend_ratio = 0.7 + 0.3 * (i / period_length)  # Dynamically adjust blend ratio\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            if generation % 10 == 0:  # Reset a portion of the population every 10 generations\n                population[:self.population_size // 2] = np.random.uniform(self.bounds.lb, self.bounds.ub, (self.population_size // 2, self.dim))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance exploration by periodically resetting a portion of the population to improve diversity.", "configspace": "", "generation": 10, "fitness": 0.9863524907325849, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "768f50a7-d82e-44bd-ba63-86992b79d759", "metadata": {"aucs": [0.9870687137756168, 0.9867994519219703, 0.9851893065001674], "final_y": [0.1648560650719847, 0.16485633524380428, 0.1648561412806936]}, "mutation_prompt": null}
{"id": "a27bf892-fc09-4dae-97f1-8c9303afa89e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        periodic_part = np.mean(solution[:period_length])  # Enforce stronger periodicity\n        for i in range(period_length):\n            solution[i + period_length] = periodic_part\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10) \n            F = 0.5 + 0.4 * np.abs(np.sin(np.pi * generation / (self.budget // (2 * self.population_size))))  # Adaptive F\n            CR = 0.3 + (0.8 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  \n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 2  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance periodicity constraint effectiveness and adaptively scale mutation factor for improved convergence.", "configspace": "", "generation": 10, "fitness": 0.986923380956235, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6763346a-45be-4c94-8cad-5a6d6e47d593", "metadata": {"aucs": [0.9883651422660124, 0.9896872015627619, 0.9827177990399306], "final_y": [0.16485601857479193, 0.16485591611550865, 0.1648562870409983]}, "mutation_prompt": null}
{"id": "cdcb3a94-a129-403f-ade6-97d5c5d3c904", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            blend_ratio = 0.7 + 0.3 * (i / period_length)  # Dynamically adjust blend ratio\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.4 + (0.9 - 0.4) * generation / (self.budget // (2 * self.population_size))  # Enhanced mutation factor range\n            CR = 0.3 + (0.7 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Optimize the algorithm by enhancing the mutation factor adjustment for improved exploration.", "configspace": "", "generation": 10, "fitness": 0.9879835518987076, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "768f50a7-d82e-44bd-ba63-86992b79d759", "metadata": {"aucs": [0.9869068741744799, 0.9880506089122005, 0.9889931726094426], "final_y": [0.16485581672314487, 0.1648564944447416, 0.16485626164712186]}, "mutation_prompt": null}
{"id": "ec22f1eb-4d95-4415-8ec8-4ecaf0f44f8e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = np.mean(solution[:period_length])  # Enforce averaged periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10) \n            F = 0.6 + (0.9 - 0.6) * np.sin(np.pi * generation / (self.budget // (2 * self.population_size)))  # Adaptive F\n            CR = 0.3 + (0.8 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n                \n                # Enhance convergence with periodicity-aware mutation\n                trial = trial * np.sin(2 * np.pi * np.arange(self.dim) / self.dim)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  \n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 2  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Enhance convergence by incorporating periodicity-aware mutation within the differential evolution process.", "configspace": "", "generation": 10, "fitness": 0.9453919672079971, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.945 with standard deviation 0.047. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "6763346a-45be-4c94-8cad-5a6d6e47d593", "metadata": {"aucs": [0.8790040171999862, 0.9727985153977539, 0.9843733690262512], "final_y": [0.20044511573331958, 0.16485602677962397, 0.1648561228828075]}, "mutation_prompt": null}
{"id": "78d6af13-8acd-4c70-bddd-01edb5a8629e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = np.mean(solution[:period_length])  # Enforce averaged periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10) \n            F = 0.5 + (0.9 - 0.5) * np.sin(np.pi * generation / (self.budget // (2 * self.population_size)))  # Adaptive F\n            CR = 0.3 + (0.8 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  \n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 3  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improve solution diversity by adapting mutation strategy and refine local optimization periodicity consideration.", "configspace": "", "generation": 10, "fitness": 0.9883670708908427, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6763346a-45be-4c94-8cad-5a6d6e47d593", "metadata": {"aucs": [0.9894409675271565, 0.9867136272285935, 0.9889466179167785], "final_y": [0.1648563616696589, 0.16485593814209532, 0.16485591487879914]}, "mutation_prompt": null}
{"id": "3cdf7c33-a48f-428f-b59a-92a49c888aef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def adaptive_periodicity_constraint(self, solution, generation):\n        period_length = self.dim // 2\n        adaptive_factor = 0.5 + 0.5 * np.cos(np.pi * generation / (self.budget // (2 * self.population_size)))\n        for i in range(period_length):\n            solution[i + period_length] = solution[i] * adaptive_factor + np.mean(solution[:period_length]) * (1 - adaptive_factor)\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10) \n            F = 0.6 + (0.9 - 0.6) * np.sin(np.pi * generation / (self.budget // (2 * self.population_size)))\n            CR = 0.3 + (0.8 - 0.3) * generation / (self.budget // (2 * self.population_size))\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.adaptive_periodicity_constraint(trial, generation)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  \n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Improve convergence by introducing adaptive periodicity and mutation strategies tailored for wave interference and leveraging local refinement.", "configspace": "", "generation": 10, "fitness": 0.9814802827211507, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6763346a-45be-4c94-8cad-5a6d6e47d593", "metadata": {"aucs": [0.9887226743337818, 0.9679958638993029, 0.9877223099303674], "final_y": [0.1648561947774685, 0.16485631270439904, 0.1648559372903905]}, "mutation_prompt": null}
{"id": "e983310c-6df1-49ea-a4a7-3ecc759add5d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            solution[i + period_length] = np.mean(solution[:period_length])  # Enforce averaged periodicity\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 10)\n            F = 0.6 + (0.9 - 0.6) * np.sin(np.pi * generation / (self.budget // (2 * self.population_size)))  # Adaptive F\n            CR = 0.3 + (0.8 - 0.3) * np.cos(np.pi * generation / (self.budget // (2 * self.population_size)))  # Adaptive CR\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  \n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        period_length = self.dim // 2  # Consider periodicity during local optimization\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Introduce adaptive crossover probability to enhance exploration and exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.9864181973829974, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6763346a-45be-4c94-8cad-5a6d6e47d593", "metadata": {"aucs": [0.984897895751185, 0.9868010119584968, 0.9875556844393104], "final_y": [0.16485640617131336, 0.16485588090712966, 0.164856046681772]}, "mutation_prompt": null}
{"id": "a7b41252-7859-4a46-9153-a146cb100dee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            blend_ratio = 0.7 + 0.3 * (i / period_length)  # Dynamically adjust blend ratio\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Increase mutation factor range\n            CR = 0.3 + (0.9 - 0.3) * generation / (self.budget // (2 * self.population_size))  # Modified crossover rate\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Incorporate a dynamic crossover rate adjustment based on the generation to improve exploration and exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.9864495658772042, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "768f50a7-d82e-44bd-ba63-86992b79d759", "metadata": {"aucs": [0.9857213927098738, 0.9835064306269099, 0.9901208742948291], "final_y": [0.16485585256624047, 0.16485609505094312, 0.16485587331931595]}, "mutation_prompt": null}
{"id": "86477de2-e84a-426f-a2d2-fba24ac9ee01", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.bounds = None\n\n    def adaptive_quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_population = lb + ub - population\n        combined_population = np.concatenate((population, quasi_opposite_population), axis=0)\n        return combined_population\n\n    def refined_periodicity_constraint(self, solution):\n        period_length = self.dim // 2\n        for i in range(period_length):\n            blend_ratio = 0.5 + 0.5 * np.sin(np.pi * i / period_length)  # Refined blend ratio using sinusoidal function\n            solution[i + period_length] = blend_ratio * solution[i] + (1 - blend_ratio) * solution[i + period_length]\n        return solution\n\n    def differential_evolution(self, func):\n        np.random.seed(42)\n        population = self.adaptive_quasi_oppositional_initialization(self.bounds.lb, self.bounds.ub)\n        population_fitness = np.array([func(ind) for ind in population])\n\n        for generation in range(self.budget // (2 * self.population_size)):\n            self.population_size = max(int(self.population_size * 0.95), 12)  # Adjust population size control\n            F = 0.5 + 0.4 * np.sin(np.pi * generation / (self.budget // (2 * self.population_size)))  # Adaptive mutation factor\n            CR = 0.4 + 0.6 * np.cos(2 * np.pi * generation / (self.budget // (2 * self.population_size)))  # Adaptive crossover rate\n            for i in range(self.population_size):\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), self.bounds.lb, self.bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.refined_periodicity_constraint(trial)\n\n                trial_fitness = func(trial)\n                if trial_fitness < population_fitness[i]:\n                    population[i] = trial\n                    population_fitness[i] = trial_fitness\n\n            if np.random.rand() < 0.5:  # More frequent local optimization\n                best_idx = np.argmin(population_fitness)\n                best_individual = population[best_idx]\n                best_individual = self.local_optimization(func, best_individual)\n\n        return best_individual\n\n    def local_optimization(self, func, initial_guess):\n        res = minimize(func, initial_guess, method='L-BFGS-B', bounds=list(zip(self.bounds.lb, self.bounds.ub)))\n        return res.x if res.success else initial_guess\n\n    def __call__(self, func):\n        self.bounds = func.bounds\n        best_global_solution = self.differential_evolution(func)\n        best_solution = self.local_optimization(func, best_global_solution)\n        return best_solution", "name": "EnhancedHybridMetaheuristicOptimizer", "description": "Optimize the search strategy by refining the periodicity constraint and incorporating adaptive crossover rates to enhance solution diversity and convergence.", "configspace": "", "generation": 10, "fitness": 0.9878163201829814, "feedback": "The algorithm EnhancedHybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "768f50a7-d82e-44bd-ba63-86992b79d759", "metadata": {"aucs": [0.99016972041716, 0.9864792320027083, 0.9868000081290759], "final_y": [0.16485622462350058, 0.16485634803346993, 0.16485621413772633]}, "mutation_prompt": null}
