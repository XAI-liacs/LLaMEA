{"id": "2e3b649a-7307-412d-8a6e-938014968489", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HPDEOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.bounds = None\n\n    def initialize_population(self, pop_size):\n        lb, ub = self.bounds\n        return np.random.uniform(lb, ub, (pop_size, self.dim))\n\n    def differential_evolution(self, func, pop, CR=0.9, F=0.8):\n        new_pop = np.copy(pop)\n        num_individuals = len(pop)\n        for i in range(num_individuals):\n            indices = [idx for idx in range(num_individuals) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), *self.bounds)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_refinement(self, individual, func):\n        result = minimize(func, individual, bounds=self.bounds, method='L-BFGS-B')\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        self.bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10 * self.dim\n        pop = self.initialize_population(pop_size)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            pop = self.differential_evolution(func, pop)\n            if evaluations + pop_size > self.budget:\n                break\n            evaluations += pop_size\n\n        best = min(pop, key=func)\n        if evaluations < self.budget:  # Use remaining budget for refinement\n            best = self.local_refinement(best, func)\n            evaluations += 1\n\n        return best", "name": "HPDEOptimization", "description": "Hybrid Periodic Differential Evolution (HPDE) with local refinement using BFGS for enhanced exploration and exploitation in multilayer photonic structure optimization.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 34, in __call__\n  File \"<string>\", line 11, in initialize_population\nValueError: too many values to unpack (expected 2)\n.", "error": "ValueError('too many values to unpack (expected 2)')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 34, in __call__\n  File \"<string>\", line 11, in initialize_population\nValueError: too many values to unpack (expected 2)\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "e1ee62af-ece1-4afa-b2a2-8302dcd3eb65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < 0.9, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "A hybrid global-local optimization algorithm combining Quasi-Oppositional Differential Evolution (QODE) for exploration and BFGS for exploitation, tailored to leverage periodicity and modularity in complex optimization landscapes.  ", "configspace": "", "generation": 0, "fitness": 0.9346343374986382, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9201018527107239, 0.9135927393648344, 0.9702084204203562], "final_y": [0.1818851863939165, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "3c62c74c-285d-4ac4-a628-cff728365399", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return res.x if res.success else x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "A hybrid optimization algorithm that combines Differential Evolution (DE) for global exploration and BFGS for local exploitation, enhanced by quasi-oppositional initialization and periodicity constraints to efficiently find optimal multilayer configurations in black-box settings.", "configspace": "", "generation": 0, "fitness": 0.855232884015494, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.019. And the mean value of best solutions found was 0.185 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8394578231899682, 0.8447693560828801, 0.8814714727736336], "final_y": [0.18944551171644175, 0.1914930633341042, 0.1728337930539623]}, "mutation_prompt": null}
{"id": "8406b381-ed2d-47e7-9b4a-bbd69f88dd79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n        self.bounds = None\n\n    def quasi_opposition_based_initialization(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - pop\n        return np.vstack((pop, pop_opposite))\n\n    def enforce_periodicity(self, candidate):\n        period = self.dim // 2\n        periodic_candidate = np.tile(candidate[:period], self.dim // period)\n        return periodic_candidate\n\n    def optimize(self, func, lb, ub):\n        self.bounds = (lb, ub)\n        population = self.quasi_opposition_based_initialization(lb, ub)\n        population = population[:self.pop_size]  # Use only pop_size number of initial solutions\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += len(population)\n        \n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.enforce_periodicity(trial)\n                \n                f = func(trial)\n                self.evaluations += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n            # Local search on the best found solution so far\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb, ub)] * self.dim)\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        return self.optimize(func, lb, ub)", "name": "EnhancedDifferentialEvolution", "description": "An enhanced Differential Evolution algorithm that combines symmetric initialization and periodicity encouragement with local refinement for robust multilayer design optimization.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 64, in __call__\n  File \"<string>\", line 54, in optimize\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 437, in old_bound_to_new\n    lb = np.array([float(_arr_to_scalar(x)) if x is not None else -np.inf\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 437, in <listcomp>\n    lb = np.array([float(_arr_to_scalar(x)) if x is not None else -np.inf\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 14, in _arr_to_scalar\n    return x.item() if isinstance(x, np.ndarray) else x\nValueError: can only convert an array of size 1 to a Python scalar\n.", "error": "ValueError('can only convert an array of size 1 to a Python scalar')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 64, in __call__\n  File \"<string>\", line 54, in optimize\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 437, in old_bound_to_new\n    lb = np.array([float(_arr_to_scalar(x)) if x is not None else -np.inf\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 437, in <listcomp>\n    lb = np.array([float(_arr_to_scalar(x)) if x is not None else -np.inf\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 14, in _arr_to_scalar\n    return x.item() if isinstance(x, np.ndarray) else x\nValueError: can only convert an array of size 1 to a Python scalar\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "2b3fb219-ccbb-4279-844e-fd4e31f18be1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPeriodicDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, dim * 5)\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        scaled_pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        return scaled_pop\n\n    def _mutate(self, target_idx, population):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = population[np.random.choice(indices, 3, replace=False)]\n        mutant = np.clip(a + self.F * (b - c), 0, 1)\n        return mutant\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        offspring = np.where(cross_points, mutant, target)\n        return offspring\n\n    def _periodic_local_search(self, individual, bounds):\n        def objective_wrapper(x):\n            return func(x)\n\n        periodic_start = np.mean(individual[::2]), np.mean(individual[1::2])\n        initial_guess = np.tile(periodic_start, self.dim // 2)\n        result = minimize(objective_wrapper, initial_guess, bounds=bounds, method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant)\n\n                # Scale trial vector to actual bounds\n                trial_scaled = bounds.lb + trial * (bounds.ub - bounds.lb)\n                trial_fitness = func(trial_scaled)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n\n            # Periodic-aware local search\n            if evaluations < self.budget:\n                local_solution, local_fitness = self._periodic_local_search(best_solution, bounds)\n                evaluations += 1\n                if local_fitness < best_fitness:\n                    best_solution = local_solution\n                    best_fitness = local_fitness\n\n        return best_solution * (bounds.ub - bounds.lb) + bounds.lb", "name": "HybridPeriodicDE", "description": "A hybrid metaheuristic algorithm combining Differential Evolution with periodic-aware local search to optimize multilayer photonic structures for maximum reflectivity.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 73, in __call__\n  File \"<string>\", line 37, in _periodic_local_search\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nTypeError: zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds\n.", "error": "TypeError('zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 73, in __call__\n  File \"<string>\", line 37, in _periodic_local_search\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nTypeError: zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "bc1ce4d9-8fde-443b-a3f7-43364ed0bfe8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HPDEOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.bounds = None\n\n    def initialize_population(self, pop_size):\n        lb, ub = np.array(self.bounds).T  # Fixed the bounds unpacking error\n        return np.random.uniform(lb, ub, (pop_size, self.dim))\n\n    def differential_evolution(self, func, pop, CR=0.9, F=0.8):\n        new_pop = np.copy(pop)\n        num_individuals = len(pop)\n        for i in range(num_individuals):\n            indices = [idx for idx in range(num_individuals) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), *self.bounds)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_refinement(self, individual, func):\n        result = minimize(func, individual, bounds=self.bounds, method='L-BFGS-B')\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        self.bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10 * self.dim\n        pop = self.initialize_population(pop_size)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            pop = self.differential_evolution(func, pop)\n            if evaluations + pop_size > self.budget:\n                break\n            evaluations += pop_size\n\n        best = min(pop, key=func)\n        if evaluations < self.budget:  # Use remaining budget for refinement\n            best = self.local_refinement(best, func)\n            evaluations += 1\n\n        return best", "name": "HPDEOptimization", "description": "Minor fix in population initialization to correctly handle bounds unpacking for multilayer photonic optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('clip() takes from 3 to 4 positional arguments but 11 were given').", "error": "TypeError('clip() takes from 3 to 4 positional arguments but 11 were given')", "parent_id": "2e3b649a-7307-412d-8a6e-938014968489", "metadata": {}, "mutation_prompt": null}
{"id": "3a5f3ba6-2a2b-49f6-8438-d81f2461ccd1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPeriodicDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, dim * 5)\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        scaled_pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        return scaled_pop\n\n    def _mutate(self, target_idx, population):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = population[np.random.choice(indices, 3, replace=False)]\n        mutant = np.clip(a + self.F * (b - c), 0, 1)\n        return mutant\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        offspring = np.where(cross_points, mutant, target)\n        return offspring\n\n    def _periodic_local_search(self, individual, bounds):\n        def objective_wrapper(x):\n            return func(x)\n\n        periodic_start = np.mean(individual[::2]), np.mean(individual[1::2])\n        initial_guess = np.tile(periodic_start, self.dim // 2)\n        result = minimize(objective_wrapper, initial_guess, bounds=list(zip(bounds.lb, bounds.ub)), method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant)\n\n                # Scale trial vector to actual bounds\n                trial_scaled = bounds.lb + trial * (bounds.ub - bounds.lb)\n                trial_fitness = func(trial_scaled)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n\n            # Periodic-aware local search\n            if evaluations < self.budget:\n                local_solution, local_fitness = self._periodic_local_search(best_solution, bounds)\n                evaluations += 1\n                if local_fitness < best_fitness:\n                    best_solution = local_solution\n                    best_fitness = local_fitness\n\n        return best_solution * (bounds.ub - bounds.lb) + bounds.lb", "name": "HybridPeriodicDE", "description": "Enhance HybridPeriodicDE by fixing the bounds handling in periodic local search to ensure compatibility with SciPy's minimize function.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "2b3fb219-ccbb-4279-844e-fd4e31f18be1", "metadata": {}, "mutation_prompt": null}
{"id": "06c9e197-4919-4526-9f46-d0811bf4f56a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = 0.4 + 0.1 * np.random.rand()  # Adaptive mutation factor\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        if self.eval_count < self.budget * 0.8:  # Selective local search initiation\n            res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n            return res.x if res.success else x0\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "Enhanced hybrid optimization algorithm with adaptive mutation control and selective local search initiation for improved multilayer photonic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.8443451076475421, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.027. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3c62c74c-285d-4ac4-a628-cff728365399", "metadata": {"aucs": [0.8544329551059304, 0.8069451258559294, 0.8716572419807663], "final_y": [0.16984299844411932, 0.187174692632323, 0.18811327111593978]}, "mutation_prompt": null}
{"id": "a61b9bd8-ecf1-42f6-9a8f-0d897d2177ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPeriodicDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, dim * 5)\n        self.F = 0.8  # Differential weight\n        self.CR = 0.9  # Crossover probability\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        scaled_pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        return scaled_pop\n\n    def _mutate(self, target_idx, population):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = population[np.random.choice(indices, 3, replace=False)]\n        mutant = np.clip(a + self.F * (b - c), 0, 1)\n        return mutant\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        offspring = np.where(cross_points, mutant, target)\n        return offspring\n\n    def _periodic_local_search(self, individual, bounds):\n        def objective_wrapper(x):\n            return func(x)\n\n        periodic_start = np.mean(individual[::2]), np.mean(individual[1::2])\n        initial_guess = np.tile(periodic_start, self.dim // 2)\n        result = minimize(objective_wrapper, initial_guess, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = self.population_size\n\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_fitness = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                if evaluations >= self.budget:\n                    break\n\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant)\n\n                # Scale trial vector to actual bounds\n                trial_scaled = bounds.lb + trial * (bounds.ub - bounds.lb)\n                trial_fitness = func(trial_scaled)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < best_fitness:\n                        best_solution = trial\n                        best_fitness = trial_fitness\n\n            # Periodic-aware local search\n            if evaluations < self.budget:\n                local_solution, local_fitness = self._periodic_local_search(best_solution, bounds)\n                evaluations += 1\n                if local_fitness < best_fitness:\n                    best_solution = local_solution\n                    best_fitness = local_fitness\n\n        return best_solution * (bounds.ub - bounds.lb) + bounds.lb", "name": "HybridPeriodicDE", "description": "A hybrid metaheuristic algorithm combining Differential Evolution with periodic-aware local search to optimize multilayer photonic structures for maximum reflectivity, with a fix for bounds handling in local search.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "2b3fb219-ccbb-4279-844e-fd4e31f18be1", "metadata": {}, "mutation_prompt": null}
{"id": "2d4c1a40-f592-41e9-8e60-ce7b8d87102c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_opposition_based_initialization(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - pop\n        return np.vstack((pop, pop_opposite))\n\n    def enforce_periodicity(self, candidate):\n        period = self.dim // 2\n        periodic_candidate = np.tile(candidate[:period], self.dim // period)\n        return periodic_candidate\n\n    def tune_parameters(self):\n        # Adaptive control of F and CR based on evaluations\n        if self.evaluations < self.budget // 3:\n            self.F = 0.8\n            self.CR = 0.9\n        elif self.evaluations < 2 * self.budget // 3:\n            self.F = 0.7\n            self.CR = 0.8\n        else:\n            self.F = 0.6\n            self.CR = 0.7\n\n    def optimize(self, func, lb, ub):\n        lb = np.array(lb)\n        ub = np.array(ub)\n        population = self.quasi_opposition_based_initialization(lb, ub)\n        population = population[:self.pop_size]  # Use only pop_size number of initial solutions\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += len(population)\n        \n        while self.evaluations < self.budget:\n            self.tune_parameters()\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.enforce_periodicity(trial)\n                \n                f = func(trial)\n                self.evaluations += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n            # Local search on the best found solution so far\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        return self.optimize(func, lb, ub)", "name": "EnhancedDifferentialEvolution", "description": "An improved Differential Evolution algorithm with enhanced quasi-opposition initialization and periodicity enforcement, combined with adaptive parameter tuning and local search refinement to robustly optimize multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": 0.9732624226267731, "feedback": "The algorithm EnhancedDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8406b381-ed2d-47e7-9b4a-bbd69f88dd79", "metadata": {"aucs": [0.974664293187541, 0.9609176455381937, 0.9842053291545846], "final_y": [0.16486049210307074, 0.16485860823842535, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "8541454e-4714-4778-aec5-67afef12b653", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HPDEOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.bounds = None\n\n    def initialize_population(self, pop_size):\n        lb, ub = self.bounds.T  # Corrected to avoid unpacking error by transposing\n        return np.random.uniform(lb, ub, (pop_size, self.dim))\n\n    def differential_evolution(self, func, pop, CR=0.9, F=0.8):\n        new_pop = np.copy(pop)\n        num_individuals = len(pop)\n        for i in range(num_individuals):\n            indices = [idx for idx in range(num_individuals) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), *self.bounds)\n            cross_points = np.random.rand(self.dim) < CR\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_refinement(self, individual, func):\n        result = minimize(func, individual, bounds=self.bounds, method='L-BFGS-B')\n        return result.x if result.success else individual\n\n    def __call__(self, func):\n        self.bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        pop_size = 10 * self.dim\n        pop = self.initialize_population(pop_size)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            pop = self.differential_evolution(func, pop)\n            if evaluations + pop_size > self.budget:\n                break\n            evaluations += pop_size\n\n        best = min(pop, key=func)\n        if evaluations < self.budget:  # Use remaining budget for refinement\n            best = self.local_refinement(best, func)\n            evaluations += 1\n\n        return best", "name": "HPDEOptimization", "description": "Refined HPDE algorithm with corrected initialization to avoid unpacking errors and enhance exploration in multilayer photonic structure optimization.  ", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('clip() takes from 3 to 4 positional arguments but 11 were given').", "error": "TypeError('clip() takes from 3 to 4 positional arguments but 11 were given')", "parent_id": "2e3b649a-7307-412d-8a6e-938014968489", "metadata": {}, "mutation_prompt": null}
{"id": "17168237-4df6-444a-a0b9-742ebac8a5a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = 0.3 + 0.2 * np.random.rand()  # Adaptive mutation factor range enhanced for diversity\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        if self.eval_count < self.budget * 0.8:  # Selective local search initiation\n            res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n            return res.x if res.success else x0\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "Improved exploration by enhancing the adaptive mutation factor range to increase diversity in mutant vectors.", "configspace": "", "generation": 2, "fitness": 0.8636055917530028, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.004. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "06c9e197-4919-4526-9f46-d0811bf4f56a", "metadata": {"aucs": [0.8685816495473709, 0.8584508433921392, 0.8637842823194986], "final_y": [0.16986964062127785, 0.19625195281257102, 0.17074623691076773]}, "mutation_prompt": null}
{"id": "405c4474-dedc-4390-83e4-1e20afeb449e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            CR = 0.5 + 0.5 * np.std(scores) / (np.abs(np.mean(scores)) + 1e-9)  # Adjust CR dynamically\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return res.x if res.success else x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "Improved exploration by adjusting the crossover rate dynamically based on diversity measures in the population.", "configspace": "", "generation": 2, "fitness": 0.8331750628627886, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.050. And the mean value of best solutions found was 0.175 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "3c62c74c-285d-4ac4-a628-cff728365399", "metadata": {"aucs": [0.9025156116598915, 0.7843010039648144, 0.8127085729636604], "final_y": [0.1655979817946036, 0.18340757067922442, 0.1759107069083935]}, "mutation_prompt": null}
{"id": "e92fd56f-4e96-4a90-bdc0-80e4b530616d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.6, CR=0.9, max_iter=1000):  # Changed F from 0.5 to 0.6\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return res.x if res.success else x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "An improved Hybrid Optimization algorithm with enhanced differential mutation strategy to better navigate complex landscapes.", "configspace": "", "generation": 2, "fitness": 0.7960237349248941, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.011. And the mean value of best solutions found was 0.193 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "3c62c74c-285d-4ac4-a628-cff728365399", "metadata": {"aucs": [0.7879673959903604, 0.7879719127601459, 0.8121318960241759], "final_y": [0.17795184006603293, 0.2184722943900732, 0.18183173335878655]}, "mutation_prompt": null}
{"id": "03698f1f-a009-4033-91a4-496ee6071e18", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicMultiStrategyOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n        self.strategy_switch_threshold = 0.1\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def particle_swarm_optimization(self, func, bounds, pop_size, w=0.5, c1=1.5, c2=1.5, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        positions = np.random.uniform(lb, ub, size=(pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(pop_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(pos) for pos in positions])\n        global_best_idx = np.argmin(personal_best_scores)\n        global_best_position = personal_best_positions[global_best_idx]\n        global_best_score = personal_best_scores[global_best_idx]\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            r1, r2 = np.random.rand(2)\n            velocities = (w * velocities +\n                          c1 * r1 * (personal_best_positions - positions) +\n                          c2 * r2 * (global_best_position - positions))\n            positions = np.clip(positions + velocities, lb, ub)\n            scores = np.array([func(pos) for pos in positions])\n            self.eval_count += len(positions)\n            for i in range(pop_size):\n                if scores[i] < personal_best_scores[i]:\n                    personal_best_scores[i] = scores[i]\n                    personal_best_positions[i] = positions[i]\n                    if scores[i] < global_best_score:\n                        global_best_position = positions[i]\n                        global_best_score = scores[i]\n\n        return global_best_position, global_best_score\n\n    def local_optimization(self, func, x0, bounds):\n        res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return res.x if res.success else x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n\n        # Start with DE, and dynamically switch to PSO based on performance\n        best_global, best_score = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter // 2)\n\n        if best_score > self.strategy_switch_threshold:\n            best_global, best_score = self.particle_swarm_optimization(func, bounds, pop_size, max_iter=max_iter // 2)\n\n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        return best_local if self.eval_count < self.budget else best_global", "name": "DynamicMultiStrategyOptimization", "description": "Dynamic Multi-Strategy Optimization (DMSO) that adaptively alternates between diverse exploratory and exploitative strategies based on real-time performance feedback to optimize complex black-box functions efficiently.", "configspace": "", "generation": 2, "fitness": 0.8321385076942857, "feedback": "The algorithm DynamicMultiStrategyOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.036. And the mean value of best solutions found was 0.199 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "3c62c74c-285d-4ac4-a628-cff728365399", "metadata": {"aucs": [0.7861669888172121, 0.8355063641095856, 0.874742170156059], "final_y": [0.21443568002578806, 0.2030418859517995, 0.17934801797683952]}, "mutation_prompt": null}
{"id": "5dfc5a84-16f6-408a-bb70-a7797d30cea4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.5 + 0.3 * (diversity / (ub - lb).mean())  # Adaptive mutation factor\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < 0.9, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced the differential evolution step by adjusting the mutation factor adaptively based on population diversity for improved global search efficiency.", "configspace": "", "generation": 3, "fitness": 0.9346343374986382, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "e1ee62af-ece1-4afa-b2a2-8302dcd3eb65", "metadata": {"aucs": [0.9201018527107239, 0.9135927393648344, 0.9702084204203562], "final_y": [0.1818851863939165, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "8f8fee6e-aa62-4a74-a1f5-108243495d17", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + scaling_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "An enhanced HybridQODEBFGS algorithm incorporating dynamic scaling and crossover rate adjustments based on population diversity for improved exploration and convergence.", "configspace": "", "generation": 3, "fitness": 0.9253271615997694, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.925 with standard deviation 0.028. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "e1ee62af-ece1-4afa-b2a2-8302dcd3eb65", "metadata": {"aucs": [0.9634735254682278, 0.9135927393648344, 0.8989152199662458], "final_y": [0.16485622748550455, 0.18187968905672636, 0.1881335535321328]}, "mutation_prompt": null}
{"id": "665e2007-cd1e-45ba-ba96-40e6e4801a34", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = 0.3 + 0.2 * np.random.rand() * np.sin(np.pi * np.random.rand())  # Incorporate chaotic sequence\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        if self.eval_count < self.budget * 0.8:  # Selective local search initiation\n            res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n            return res.x if res.success else x0\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "Improved mutation strategy by incorporating chaotic sequences to enhance exploration diversity and convergence in the search space.", "configspace": "", "generation": 3, "fitness": 0.831190749010605, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.053. And the mean value of best solutions found was 0.188 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "17168237-4df6-444a-a0b9-742ebac8a5a8", "metadata": {"aucs": [0.8764652103849707, 0.757412863645589, 0.8596941730012555], "final_y": [0.17808504016222726, 0.21957820219699764, 0.16649878715517274]}, "mutation_prompt": null}
{"id": "6a20d5b9-02b9-479e-8cf1-fe47bfef1ad0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = 0.3 + 0.2 * np.random.rand()  # Adaptive mutation factor range enhanced for diversity\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                \n                # Calculate crowding distance and use it in trial selection\n                distances = np.linalg.norm(population - mutant, axis=1)\n                diversity_factor = np.exp(-distances[j] / np.std(distances))\n                trial = np.where(np.random.rand(self.dim) * diversity_factor < CR, mutant, population[j])\n                \n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        if self.eval_count < self.budget * 0.8:  # Selective local search initiation\n            res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n            return res.x if res.success else x0\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "Enhanced HybridOptimization algorithm with improved crowding distance-based selection to maintain diversity and maintain periodic structures.", "configspace": "", "generation": 3, "fitness": 0.8421710681216595, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.058. And the mean value of best solutions found was 0.200 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "17168237-4df6-444a-a0b9-742ebac8a5a8", "metadata": {"aucs": [0.8291528597347727, 0.7790134840726648, 0.9183468605575414], "final_y": [0.21465407126474656, 0.21231494016118502, 0.17205557054303133]}, "mutation_prompt": null}
{"id": "782bd533-b5bd-4c6a-9c3f-8d2db6c6395e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = 0.4 + 0.2 * np.random.rand()  # Adaptive mutation factor range enhanced for diversity\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        if self.eval_count < self.budget * 0.9:  # Slightly earlier local search initiation\n            res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n            return res.x if res.success else x0\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "Enhanced exploration by incorporating diversity-driven mutation and strategic local search timing for improved solution quality in multilayer photonic structure optimization.", "configspace": "", "generation": 3, "fitness": 0.8625792590861532, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.017. And the mean value of best solutions found was 0.181 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "17168237-4df6-444a-a0b9-742ebac8a5a8", "metadata": {"aucs": [0.8862271541846731, 0.8451230285829797, 0.8563875944908065], "final_y": [0.16928798950741297, 0.18311134543088892, 0.18929728391829725]}, "mutation_prompt": null}
{"id": "d3a87de8-3e57-42d2-9fa7-88fc22dd4f99", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + scaling_factor * (b - c) + np.random.normal(0, 0.1, self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced population diversity by using Gaussian mutation to improve exploration capabilities.", "configspace": "", "generation": 4, "fitness": 0.9346343374986382, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "8f8fee6e-aa62-4a74-a1f5-108243495d17", "metadata": {"aucs": [0.9201018527107239, 0.9135927393648344, 0.9702084204203562], "final_y": [0.1818851863939165, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "13b6a14a-5968-4217-928a-12d9245d2b0d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = 0.3 + 0.2 * np.random.rand()  # Adaptive mutation factor range enhanced for diversity\n                CR_adaptive = 0.8 + 0.1 * np.random.rand()  # Adaptive crossover rate for diversity\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR_adaptive, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        if self.eval_count < self.budget * 0.8:  # Selective local search initiation\n            res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n            return res.x if res.success else x0\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "Enhanced exploration by adjusting the crossover rate adaptively based on the population diversity, improving solution convergence.", "configspace": "", "generation": 4, "fitness": 0.8888565185537223, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.020. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "17168237-4df6-444a-a0b9-742ebac8a5a8", "metadata": {"aucs": [0.9159469274376635, 0.8812223842864653, 0.8694002439370379], "final_y": [0.16686330319654308, 0.1654843917789226, 0.1882062178292382]}, "mutation_prompt": null}
{"id": "a7c687d2-2392-43d5-a133-422c96f98d7c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_init(self, lb, ub, population_size):\n        pop = np.random.uniform(lb, ub, size=(population_size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def differential_evolution(self, func, bounds, pop_size, F=0.5, CR=0.9, max_iter=1000):\n        lb, ub = bounds.lb, bounds.ub\n        population = self.quasi_oppositional_init(lb, ub, pop_size)\n        scores = np.array([func(ind) for ind in population])\n        best_idx = np.argmin(scores)\n        best = population[best_idx]\n        best_score = scores[best_idx]\n        \n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            for j in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                idxs = [idx for idx in range(len(population)) if idx != j]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = 0.3 + 0.2 * np.random.rand()  # Adaptive mutation factor range enhanced for diversity\n                CR_adaptive = 0.5 + 0.4 * np.random.rand()  # Adaptive crossover rate for improved convergence\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                trial = np.where(np.random.rand(self.dim) < CR_adaptive, mutant, population[j])\n                trial_score = func(trial)\n                self.eval_count += 1\n                if trial_score < scores[j]:\n                    population[j] = trial\n                    scores[j] = trial_score\n                    if trial_score < best_score:\n                        best = trial\n                        best_score = trial_score\n\n        return best, best_score\n\n    def local_optimization(self, func, x0, bounds):\n        if self.eval_count < self.budget * 0.8:  # Selective local search initiation\n            res = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n            return res.x if res.success else x0\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop_size = 10\n        max_iter = self.budget // pop_size\n        best_global, _ = self.differential_evolution(func, bounds, pop_size, max_iter=max_iter)\n        \n        # Local optimization\n        best_local = self.local_optimization(func, best_global, bounds)\n        if self.eval_count < self.budget:\n            return best_local\n        else:\n            return best_global", "name": "HybridOptimization", "description": "Improved convergence speed by adjusting the crossover rate adaptively based on population diversity.", "configspace": "", "generation": 4, "fitness": 0.8744465310210617, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.027. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "17168237-4df6-444a-a0b9-742ebac8a5a8", "metadata": {"aucs": [0.8746369932947207, 0.8414068495361048, 0.9072957502323598], "final_y": [0.16681446541887746, 0.17749934075533902, 0.16634966478763835]}, "mutation_prompt": null}
{"id": "2283cb33-3d51-4454-93fd-bd25023a45f7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.5 + 0.3 * (diversity / (ub - lb).mean())  \n        periodicity = np.sin(np.linspace(0, 2 * np.pi, self.dim))\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c) + 0.1 * periodicity, lb, ub)\n            cr = 0.9 - (0.5 * (diversity / (ub - lb).mean()))  \n            trial = np.where(np.random.rand(self.dim) < cr, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced HybridQODEBFGS with periodicity-guided mutation and dynamic crossover rate to leverage known optimal structures and further improve convergence efficiency and solution quality.", "configspace": "", "generation": 4, "fitness": 0.9346343374986382, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "5dfc5a84-16f6-408a-bb70-a7797d30cea4", "metadata": {"aucs": [0.9201018527107239, 0.9135927393648344, 0.9702084204203562], "final_y": [0.1818851863939165, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "225abb6a-7222-4aab-8eb7-dc4a74748d57", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_opposition_based_initialization(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - pop\n        return np.vstack((pop, pop_opposite))\n\n    def enforce_periodicity(self, candidate):\n        period = self.dim // 2\n        periodic_candidate = np.tile(candidate[:period], self.dim // period)\n        return periodic_candidate\n\n    def adaptive_tune_parameters(self):\n        diversity = np.std(self.population, axis=0).mean()\n        self.F = 0.5 + 0.3 * (diversity / (np.max([diversity, 1e-5])))\n        self.CR = 0.5 + 0.4 * (1 - (diversity / (np.max([diversity, 1e-5]))))\n\n    def optimize(self, func, lb, ub):\n        lb = np.array(lb)\n        ub = np.array(ub)\n        self.population = self.quasi_opposition_based_initialization(lb, ub)\n        population = self.population[:self.pop_size]\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += len(population)\n        \n        while self.evaluations < self.budget:\n            self.adaptive_tune_parameters()\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.enforce_periodicity(trial)\n                \n                f = func(trial)\n                self.evaluations += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            res = minimize(func, best_solution, method='BFGS', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        return self.optimize(func, lb, ub)", "name": "EnhancedDifferentialEvolution", "description": "An enhanced Differential Evolution algorithm with self-adaptive mutation and crossover rates, periodicity preservation, and Quasi-Oppositional initialization, augmented with targeted local refinement utilizing BFGS for optimizing multilayer photonic structures.", "configspace": "", "generation": 4, "fitness": 0.9155643482520633, "feedback": "The algorithm EnhancedDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.044. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2d4c1a40-f592-41e9-8e60-ce7b8d87102c", "metadata": {"aucs": [0.8634017063844541, 0.9122437259974978, 0.9710476123742379], "final_y": [0.16486398937194824, 0.18090170406000405, 0.16485782544055982]}, "mutation_prompt": null}
{"id": "49c2c2cb-6fa9-4298-b57d-a2ecda7640a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.5 + 0.3 * (diversity / (ub - lb).mean())  # Adaptive mutation factor\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c) + np.random.normal(0, 0.1, size=self.dim)  # Gaussian mutation\n            mutant = np.clip(mutant, lb, ub)\n            trial = np.where(np.random.rand(self.dim) < 0.9, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        options = {'maxiter': self.budget//10, 'disp': False}\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='trust-constr', options=options)  # Dynamic trust-region\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Introduced Gaussian-based mutation in differential evolution and enhanced local optimization with dynamic trust-region approach for improved exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.9336677614777275, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.934 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "5dfc5a84-16f6-408a-bb70-a7797d30cea4", "metadata": {"aucs": [0.9192225871541209, 0.9124733031485532, 0.9693073941305083], "final_y": [0.18187810393465142, 0.18187810393212667, 0.16485577190558331]}, "mutation_prompt": null}
{"id": "6ea44c24-e4a8-414b-83ec-8e49a4a48af6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.5 + 0.3 * (diversity / (ub - lb).mean())  # Adaptive mutation factor\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < 0.9, mutant + np.random.normal(0, 0.01, self.dim), pop[i])  # Modified line\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced mutation strategy by periodically introducing small Gaussian perturbations to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.9190582888028164, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.919 with standard deviation 0.073. And the mean value of best solutions found was 0.183 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "5dfc5a84-16f6-408a-bb70-a7797d30cea4", "metadata": {"aucs": [0.9713915583744234, 0.8155748876136695, 0.9702084204203562], "final_y": [0.16485652884757696, 0.22064956958340554, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "1226bd89-e581-46ab-b776-4fd40f993448", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            # Mutate with periodic influence and refined Gaussian noise\n            periodic_component = np.sin(2 * np.pi * pop[i] / (ub-lb))\n            mutant = np.clip(a + scaling_factor * (b - c) + periodic_component + np.random.normal(0, 0.05, self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Improved exploration and exploitation balance by refining Gaussian mutation and integrating periodic influence in mutation.", "configspace": "", "generation": 5, "fitness": 0.9190627806815589, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.919 with standard deviation 0.042. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "d3a87de8-3e57-42d2-9fa7-88fc22dd4f99", "metadata": {"aucs": [0.9201018527107239, 0.8668780689135966, 0.9702084204203562], "final_y": [0.1818851863939165, 0.20044682983098205, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "eac76fe9-dacf-4e3c-b9b5-7f0d65572447", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutation_strength = 0.1 + 0.1 * np.tanh(np.std(fitness))  # Adjusted mutation strength\n            mutant = np.clip(a + scaling_factor * (b - c) + np.random.normal(0, mutation_strength, self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Introduced a dynamic Gaussian mutation scaling factor to enhance diversity and adaptability in exploration.", "configspace": "", "generation": 5, "fitness": 0.9189751074950133, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.919 with standard deviation 0.042. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "d3a87de8-3e57-42d2-9fa7-88fc22dd4f99", "metadata": {"aucs": [0.9201018527107239, 0.8666150493539596, 0.9702084204203562], "final_y": [0.1818851863939165, 0.20044692386472607, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "fd43eddc-e03c-47d9-84a7-37a5c92208a1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\nclass AdaptivePeriodicOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def periodic_guided_mutation(self, individual, lb, ub, t):\n        periodicity = np.sin(2 * np.pi * t / self.budget)\n        scale_factor = np.random.uniform(0.4, 0.9)  \n        perturbation = scale_factor * periodicity * (ub - lb)\n        return np.clip(individual + perturbation, lb, ub)\n\n    def evaluate_population(self, pop):\n        return np.array([self.func(ind) for ind in pop])\n\n    def select_best(self, pop, fitness):\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]\n\n    def bayesian_local_refinement(self, candidate, lb, ub):\n        def acquisition(x):\n            x = np.atleast_2d(x)\n            mu, sigma = self.predict(x)\n            return mu - 1.96 * sigma\n\n        bounds = list(zip(lb, ub))\n        res = minimize(acquisition, candidate, bounds=bounds, method='L-BFGS-B', options={'maxiter': self.budget // 20})\n        return res.x, self.func(res.x)\n\n    def predict(self, x):\n        distances = np.linalg.norm(x - self.pop, axis=1)\n        weights = norm.pdf(distances)\n        weights /= np.sum(weights)\n        mu = np.dot(weights, self.fitness)\n        sigma = np.sqrt(np.dot(weights, (self.fitness - mu) ** 2))\n        return mu, sigma\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        self.pop = self.initialize_population(lb, ub)\n        self.fitness = self.evaluate_population(self.pop)\n        evaluations = len(self.pop)\n\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                candidate = self.periodic_guided_mutation(self.pop[i], lb, ub, evaluations)\n                candidate_fitness = self.func(candidate)\n                if candidate_fitness < self.fitness[i]:\n                    self.pop[i], self.fitness[i] = candidate, candidate_fitness\n                evaluations += 1\n\n            if evaluations < self.budget:\n                best_solution, best_fitness = self.select_best(self.pop, self.fitness)\n                refined_solution, refined_fitness = self.bayesian_local_refinement(best_solution, lb, ub)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n                evaluations += self.budget // 20\n\n        return best_solution", "name": "AdaptivePeriodicOptimization", "description": "AdaptivePeriodicOptimization (APO) algorithm utilizes an adaptive periodicity-guided swarm-based global search coupled with a Bayesian optimization local refinement to efficiently explore and exploit the optimization landscape for high-quality solutions.", "configspace": "", "generation": 5, "fitness": 0.7548580615800325, "feedback": "The algorithm AdaptivePeriodicOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.755 with standard deviation 0.049. And the mean value of best solutions found was 0.234 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "2283cb33-3d51-4454-93fd-bd25023a45f7", "metadata": {"aucs": [0.685930060076859, 0.7831450983308683, 0.7954990263323701], "final_y": [0.2708528050669192, 0.2165699484672794, 0.21574531948424258]}, "mutation_prompt": null}
{"id": "ea4c171e-eaa8-404b-9e21-988a0687b1fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.5 + 0.3 * (diversity / (ub - lb).mean())\n        periodicity = np.sin(np.linspace(0, 2 * np.pi, self.dim))\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            if np.random.rand() < 0.5:  # Selective mutation strategy\n                mutant = np.clip(a + F * (b - c) + 0.1 * periodicity, lb, ub)\n            else:\n                mutant = np.clip(a + F * (b - c), lb, ub)\n            cr = 0.7 + 0.2 * np.cos((evaluations / self.budget) * np.pi)  \n            trial = np.where(np.random.rand(self.dim) < cr, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced HybridQODEBFGS with periodicity-driven adaptive crossover rate and selective mutation strategy to further leverage known optimal structures and improve global search efficiency. ", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "2283cb33-3d51-4454-93fd-bd25023a45f7", "metadata": {}, "mutation_prompt": null}
{"id": "0d0aeaf7-6395-4ed6-a656-039322db0e8f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def periodicity_based_initialization(self, lb, ub, period=2):\n        pop = np.tile(np.linspace(lb, ub, period), (self.population_size//period, 1))\n        return pop + np.random.uniform(lb, ub, pop.shape) * 0.1\n\n    def initialize_population(self, lb, ub):\n        pop = self.periodicity_based_initialization(lb, ub)\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.5 + 0.3 * (diversity / (ub - lb).mean())  # Adaptive mutation factor\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            crossover_rate = 0.7 + 0.3 * (1 - fitness[i] / np.max(fitness))\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced HybridQODEBFGS by incorporating periodicity-based initialization and adaptive crossover rate for improved convergence and solution quality.", "configspace": "", "generation": 6, "fitness": 0.9023086045237162, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.902 with standard deviation 0.061. And the mean value of best solutions found was 0.185 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "5dfc5a84-16f6-408a-bb70-a7797d30cea4", "metadata": {"aucs": [0.8231246537859582, 0.9135927393648344, 0.9702084204203562], "final_y": [0.2072545776011402, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "ea757c3b-3d44-4ce3-8653-5583f03ac381", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + scaling_factor * (b - c) + np.random.normal(0, 0.1 * np.random.rand(), self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Improved exploration by enhancing population diversity with a Gaussian mutation that includes random scaling to further diversify the solutions.", "configspace": "", "generation": 6, "fitness": 0.9185716349102776, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.919 with standard deviation 0.043. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "d3a87de8-3e57-42d2-9fa7-88fc22dd4f99", "metadata": {"aucs": [0.9201018527107239, 0.8654046315997527, 0.9702084204203562], "final_y": [0.1818851863939165, 0.20044491352025207, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "89e7bf97-cf4c-44f3-b34c-202f254e3734", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            variance = np.std(pop)  # Adaptive variance based on population convergence\n            mutant = np.clip(a + scaling_factor * (b - c) + np.random.normal(0, variance, self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced solution diversity by incorporating a Gaussian mutation with adaptive variance based on population convergence.", "configspace": "", "generation": 6, "fitness": 0.9346343374986382, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d3a87de8-3e57-42d2-9fa7-88fc22dd4f99", "metadata": {"aucs": [0.9201018527107239, 0.9135927393648344, 0.9702084204203562], "final_y": [0.1818851863939165, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "85a80198-c6b5-417e-a59b-f6e8e0011629", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + scaling_factor * (b - c) + np.random.normal(0, 0.1, self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='COBYLA', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Improved local optimization by switching from L-BFGS-B to COBYLA to better handle box constraints and nonlinearities for enhanced convergence.", "configspace": "", "generation": 6, "fitness": 0.950056226076236, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.950 with standard deviation 0.021. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d3a87de8-3e57-42d2-9fa7-88fc22dd4f99", "metadata": {"aucs": [0.9204154836790902, 0.9595447741292616, 0.9702084204203562], "final_y": [0.18187809682027578, 0.16485651620159458, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "55e93fab-1b96-4855-afbb-059baf7ea82a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity) * np.cos(diversity * np.pi)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + scaling_factor * (b - c) + np.random.normal(0, 0.1, self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='COBYLA', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Improved adaptive parameter calculation by incorporating cosine modulation to better balance exploration and exploitation phases.", "configspace": "", "generation": 7, "fitness": 0.9182070970482942, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.918 with standard deviation 0.042. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "85a80198-c6b5-417e-a59b-f6e8e0011629", "metadata": {"aucs": [0.9204154836790902, 0.8658168343643399, 0.9683889731014528], "final_y": [0.18187809682027578, 0.20044491748626858, 0.16485577190537515]}, "mutation_prompt": null}
{"id": "c2086ab8-80e0-4c64-b9c6-e02e5302d1f6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        periodic_seed = np.tile(np.linspace(lb[0], ub[0], self.dim), (self.population_size, 1))\n        return np.concatenate((pop, opp_pop, periodic_seed), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.5 + 0.3 * (diversity / (ub - lb).mean())  # Adaptive mutation factor\n        CR = 0.9 - 0.4 * (diversity / (ub - lb).mean())  # Adaptive crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(3 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced global search using adaptive crossover rates and periodicity-based seeding for improved convergence.", "configspace": "", "generation": 7, "fitness": 0.9272995755051775, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.927 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "5dfc5a84-16f6-408a-bb70-a7797d30cea4", "metadata": {"aucs": [0.9140653240113618, 0.9048707970161313, 0.9629626054880396], "final_y": [0.1818851863939165, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "ed315e91-e770-4e28-87e3-ca7a2cbc419f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < 0.8, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "A novel hybrid algorithm combining dynamic scaling of mutation factors based on convergence speed with symmetry preservation to enhance the search for periodic solutions in multilayer photonic structures.", "configspace": "", "generation": 7, "fitness": 0.9798955424631383, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5dfc5a84-16f6-408a-bb70-a7797d30cea4", "metadata": {"aucs": [0.9791286440051933, 0.9819246223337733, 0.9786333610504482], "final_y": [0.16486153438580498, 0.16486529844920406, 0.16485729695690932]}, "mutation_prompt": null}
{"id": "41a38a61-eb59-4261-82aa-1f5dbd5fac05", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.5 * np.tanh(diversity)\n        crossover_rate = 0.7 + 0.2 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def levy_flight(self, size):\n        return np.random.standard_cauchy(size) * 0.05\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + scaling_factor * (b - c) + self.levy_flight(self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "A refined HybridQODEBFGS algorithm introducing Lévy flights for enhanced exploration and efficient convergence in complex optimization landscapes.", "configspace": "", "generation": 7, "fitness": 0.9346343374986382, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d3a87de8-3e57-42d2-9fa7-88fc22dd4f99", "metadata": {"aucs": [0.9201018527107239, 0.9135927393648344, 0.9702084204203562], "final_y": [0.1818851863939165, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "06926e88-7e41-4d2a-880d-d81d66a547b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def adaptive_parameters(self, pop):\n        diversity = np.std(pop, axis=0).mean()\n        scaling_factor = 0.5 + 0.3 * np.tanh(diversity)\n        crossover_rate = 0.6 + 0.3 * np.tanh(diversity)\n        return scaling_factor, crossover_rate\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / (np.abs(v) ** (1 / beta))\n        return 0.01 * step\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        scaling_factor, crossover_rate = self.adaptive_parameters(pop)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + scaling_factor * (b - c) + self.levy_flight(self.dim), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < crossover_rate, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//8})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "Enhanced diversity and convergence by introducing Lévy flight mutation and adaptive local search intervals, while maintaining exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.9346343374986382, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d3a87de8-3e57-42d2-9fa7-88fc22dd4f99", "metadata": {"aucs": [0.9201018527107235, 0.9135927393648346, 0.9702084204203562], "final_y": [0.1818851863939165, 0.18187968905672636, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "0135c39b-475c-4814-81d7-d715dbef9058", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_opposition_based_initialization(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - pop\n        return np.vstack((pop, pop_opposite))\n\n    def enforce_periodicity(self, candidate):\n        period = self.dim // 2\n        periodic_candidate = np.tile(candidate[:period], self.dim // period)\n        return periodic_candidate\n\n    def tune_parameters(self):\n        # Adaptive control of F based on population diversity\n        fitness_std = np.std([ind for ind in self.pop_size])\n        self.F = max(0.5, min(1.0, 0.8 * fitness_std))  # Adjust mutation factor dynamically\n        if self.evaluations < self.budget // 3:\n            self.CR = 0.9\n        elif self.evaluations < 2 * self.budget // 3:\n            self.CR = 0.8\n        else:\n            self.CR = 0.7\n\n    def optimize(self, func, lb, ub):\n        lb = np.array(lb)\n        ub = np.array(ub)\n        population = self.quasi_opposition_based_initialization(lb, ub)\n        population = population[:self.pop_size]  # Use only pop_size number of initial solutions\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += len(population)\n        \n        while self.evaluations < self.budget:\n            self.tune_parameters()\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.enforce_periodicity(trial)\n                \n                f = func(trial)\n                self.evaluations += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n            # Local search on the best found solution so far\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        return self.optimize(func, lb, ub)", "name": "EnhancedDifferentialEvolution", "description": "Enhanced Differential Evolution refined by dynamically adjusting mutation factors based on diversity, improving convergence while preserving periodicity.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'int' object is not iterable\").", "error": "TypeError(\"'int' object is not iterable\")", "parent_id": "2d4c1a40-f592-41e9-8e60-ce7b8d87102c", "metadata": {}, "mutation_prompt": null}
{"id": "3baa6e04-4644-4bcd-9c2c-ddae141eceaa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Optimized variant of the DynamicScalingSymmetryDE by incorporating crossover rate adaptation based on population diversity to enhance exploration-exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.9781736293875505, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ed315e91-e770-4e28-87e3-ca7a2cbc419f", "metadata": {"aucs": [0.9804239070004594, 0.9768991819627734, 0.9771977991994185], "final_y": [0.16485840152580555, 0.16485736611058632, 0.16486049848929507]}, "mutation_prompt": null}
{"id": "53c08bfd-6300-46ee-b143-6a80c8ff8b1c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridQODEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def quasi_opposition(self, x, lb, ub):\n        return lb + ub - x\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opp_pop = self.quasi_opposition(pop, lb, ub)\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        for i in range(self.population_size):\n            indices = [idx for idx in range(2 * self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            diversity = np.std(pop, axis=0).mean()  # Calculate diversity\n            mutation_factor = 0.5 + (0.3 * diversity)  # Adaptive mutation factor\n            mutant = np.clip(a + mutation_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < 0.9, mutant, pop[i])\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)), \n                       method='L-BFGS-B', options={'maxiter': self.budget//10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        \n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "HybridQODEBFGS", "description": "A refined HybridQODEBFGS algorithm with adaptive mutation factor based on population diversity for enhanced exploration and exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.9512396074283457, "feedback": "The algorithm HybridQODEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.951 with standard deviation 0.022. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "e1ee62af-ece1-4afa-b2a2-8302dcd3eb65", "metadata": {"aucs": [0.9201018527107239, 0.9634085491539569, 0.9702084204203562], "final_y": [0.1818851863939165, 0.16485686086390283, 0.1648568487921228]}, "mutation_prompt": null}
{"id": "18a0ca61-13d1-4a2a-8491-b44201993948", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_opposition_based_initialization(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - pop\n        return np.vstack((pop, pop_opposite))\n\n    def enforce_periodicity(self, candidate):\n        period = self.dim // 2\n        periodic_candidate = np.tile(candidate[:period], self.dim // period)\n        return periodic_candidate\n\n    def tune_parameters(self):\n        # Adaptive control of F and CR based on evaluations\n        if self.evaluations < self.budget // 3:\n            self.F = 0.8\n            self.CR = 0.9\n        elif self.evaluations < 2 * self.budget // 3:\n            self.F = 0.7\n            self.CR = 0.85  # Slight adjustment to CR\n        else:\n            self.F = 0.6\n            self.CR = 0.7\n\n    def optimize(self, func, lb, ub):\n        lb = np.array(lb)\n        ub = np.array(ub)\n        population = self.quasi_opposition_based_initialization(lb, ub)\n        population = population[:self.pop_size]  # Use only pop_size number of initial solutions\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += len(population)\n        \n        while self.evaluations < self.budget:\n            self.tune_parameters()\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.enforce_periodicity(trial)\n                \n                f = func(trial)\n                self.evaluations += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n            # Local search on the best found solution so far\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        return self.optimize(func, lb, ub)", "name": "EnhancedDifferentialEvolution", "description": "Slightly refined the adaptive tuning parameters to potentially improve performance by adjusting the crossover rate (CR) in the middle budget phase.", "configspace": "", "generation": 8, "fitness": 0.9323241843449388, "feedback": "The algorithm EnhancedDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.932 with standard deviation 0.049. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2d4c1a40-f592-41e9-8e60-ce7b8d87102c", "metadata": {"aucs": [0.8625580797547897, 0.9642014419947363, 0.9702130312852901], "final_y": [0.16485891165590572, 0.1648589280919983, 0.16486068035287538]}, "mutation_prompt": null}
{"id": "dc6f0772-b0df-4c02-a3d5-58a2e9ff4ff1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_opposition_based_initialization(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - pop\n        return np.vstack((pop, pop_opposite))\n\n    def enforce_periodicity(self, candidate):\n        period = self.dim // 2\n        periodic_candidate = np.tile(candidate[:period], self.dim // period)\n        return periodic_candidate[:self.dim]  # Fix potential overrun\n\n    def tune_parameters(self):\n        # Adaptive control of F and CR based on evaluations\n        if self.evaluations < self.budget // 3:\n            self.F = 0.9  # Adjusted F\n            self.CR = 0.85  # Adjusted CR\n        elif self.evaluations < 2 * self.budget // 3:\n            self.F = 0.7\n            self.CR = 0.8\n        else:\n            self.F = 0.6\n            self.CR = 0.75  # Adjusted CR\n\n    def tailored_crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def optimize(self, func, lb, ub):\n        lb = np.array(lb)\n        ub = np.array(ub)\n        population = self.quasi_opposition_based_initialization(lb, ub)\n        population = population[:self.pop_size]  # Use only pop_size number of initial solutions\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += len(population)\n        \n        while self.evaluations < self.budget:\n            self.tune_parameters()\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                trial = self.tailored_crossover(mutant, population[i])  # Use new crossover\n                trial = self.enforce_periodicity(trial)\n                \n                f = func(trial)\n                self.evaluations += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        return self.optimize(func, lb, ub)", "name": "EnhancedDifferentialEvolution", "description": "Enhanced Differential Evolution with tailored crossover and new periodicity enforcement for improved photonic structure optimization.", "configspace": "", "generation": 8, "fitness": 0.969536934136951, "feedback": "The algorithm EnhancedDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.022. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2d4c1a40-f592-41e9-8e60-ce7b8d87102c", "metadata": {"aucs": [0.9865992050248611, 0.9378062682314069, 0.9842053291545846], "final_y": [0.16485677998433346, 0.16486102505210942, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "3790f7ac-ed02-4436-a864-9f5d027e38f7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.6 + 0.4 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved DynamicScalingSymmetryDE by fine-tuning the dynamic scaling factor to better balance exploration and exploitation.", "configspace": "", "generation": 9, "fitness": 0.9798227155548721, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3baa6e04-4644-4bcd-9c2c-ddae141eceaa", "metadata": {"aucs": [0.9788319724552366, 0.9810382167420238, 0.9795979574673556], "final_y": [0.16486034855451925, 0.1648558829992387, 0.16485755054184148]}, "mutation_prompt": null}
{"id": "2f43e95f-1838-4595-9333-66edfb2373a1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_opposition_based_initialization(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - pop\n        return np.vstack((pop, pop_opposite))\n\n    def enforce_periodicity(self, candidate):\n        period = self.dim // 2\n        periodic_candidate = np.tile(candidate[:period], self.dim // period)\n        return periodic_candidate[:self.dim]  # Fix potential overrun\n\n    def tune_parameters(self):\n        diversity = np.std([np.linalg.norm(ind) for ind in population])\n        self.F = 0.5 + (0.5 * diversity / (np.max(fitness) - np.min(fitness)))\n        if self.evaluations < self.budget // 3:\n            self.CR = 0.85  # Adjusted CR\n        elif self.evaluations < 2 * self.budget // 3:\n            self.CR = 0.8\n        else:\n            self.CR = 0.75  # Adjusted CR\n\n    def tailored_crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def optimize(self, func, lb, ub):\n        lb = np.array(lb)\n        ub = np.array(ub)\n        population = self.quasi_opposition_based_initialization(lb, ub)\n        population = population[:self.pop_size]  # Use only pop_size number of initial solutions\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += len(population)\n        \n        while self.evaluations < self.budget:\n            self.tune_parameters()\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                trial = self.tailored_crossover(mutant, population[i])  # Use new crossover\n                trial = self.enforce_periodicity(trial)\n                \n                f = func(trial)\n                self.evaluations += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n            best_idx = np.argmin(fitness)\n            best_solution = population[best_idx]\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if res.fun < fitness[best_idx]:\n                population[best_idx] = res.x\n                fitness[best_idx] = res.fun\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        return self.optimize(func, lb, ub)", "name": "EnhancedDifferentialEvolution", "description": "Enhanced Differential Evolution with adaptive mutation factor based on population diversity to improve exploration and exploitation by dynamically adjusting F.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "dc6f0772-b0df-4c02-a3d5-58a2e9ff4ff1", "metadata": {}, "mutation_prompt": null}
{"id": "21675727-a57a-43c1-80f4-76b0124ca13a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = 10 * dim\n        self.population_size = self.initial_population_size\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  \n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  \n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 20})\n        return res.x, res.fun\n\n    def adapt_population_size(self, evaluations):\n        self.population_size = max(self.initial_population_size // 2, 5)\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.adapt_population_size(evaluations)\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 20\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced DynamicScalingSymmetryDE with adaptive population size and improved local search for better exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.9647917518500356, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.026. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3baa6e04-4644-4bcd-9c2c-ddae141eceaa", "metadata": {"aucs": [0.9283451472693257, 0.9849262261558616, 0.98110388212492], "final_y": [0.18187923754875202, 0.1648573159330271, 0.16486185185630187]}, "mutation_prompt": null}
{"id": "0e0d6625-0ca5-4a7c-a33c-7ad141ba5d4f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)  # New periodicity enforcement\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        # Reinforce periodic patterns based on diversity\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.1:\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        # Dynamic adjustment of local search effort\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 10, 100)})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 10, 100)\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved version of DynamicScalingSymmetryDE that incorporates a novel diversity-based periodicity reinforcement technique and a dynamic local search strategy to enhance solution quality in photonic structure optimization.", "configspace": "", "generation": 9, "fitness": 0.9764208692601545, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3baa6e04-4644-4bcd-9c2c-ddae141eceaa", "metadata": {"aucs": [0.9633089371130872, 0.9820006420565062, 0.9839530286108699], "final_y": [0.16485974868993747, 0.16485909885271088, 0.16485778825365438]}, "mutation_prompt": null}
{"id": "69da6271-5261-45c2-941b-24293703d1fa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_opposition_based_initialization(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - pop\n        return np.vstack((pop, pop_opposite))\n\n    def enforce_periodicity(self, candidate):\n        period = self.dim // 2\n        phase_shift = np.random.uniform(0, 2 * np.pi / period, 1)\n        periodic_candidate = np.cos(2 * np.pi * np.arange(self.dim) / period + phase_shift) * (candidate.max() - candidate.min()) + candidate.min()\n        return np.clip(periodic_candidate, candidate.min(), candidate.max())\n\n    def tune_parameters(self):\n        # Adaptive control of F and CR based on evaluations\n        if self.evaluations < self.budget // 3:\n            self.F = 0.8\n            self.CR = 0.9\n        elif self.evaluations < 2 * self.budget // 3:\n            self.F = 0.7\n            self.CR = 0.8\n        else:\n            self.F = 0.6\n            self.CR = 0.7\n\n    def optimize(self, func, lb, ub):\n        lb = np.array(lb)\n        ub = np.array(ub)\n        population = self.quasi_opposition_based_initialization(lb, ub)\n        population = population[:self.pop_size]  # Use only pop_size number of initial solutions\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += len(population)\n        \n        while self.evaluations < self.budget:\n            self.tune_parameters()\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial = self.enforce_periodicity(trial)\n                \n                f = func(trial)\n                self.evaluations += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n            # Enhanced local search induction using dynamic strategy\n            for best_idx in np.argsort(fitness)[:self.pop_size // 5]:  # Top 20% of population\n                best_solution = population[best_idx]\n                res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n                if res.fun < fitness[best_idx]:\n                    population[best_idx] = res.x\n                    fitness[best_idx] = res.fun\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        return self.optimize(func, lb, ub)", "name": "EnhancedDifferentialEvolution", "description": "A refined Enhanced Differential Evolution algorithm integrating adaptive quasi-oppositional strategies, periodicity enforcement via adaptive phase shifting, and dynamic local search induction to optimize multilayer photonic structures more effectively.", "configspace": "", "generation": 9, "fitness": 0.9767975177556375, "feedback": "The algorithm EnhancedDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2d4c1a40-f592-41e9-8e60-ce7b8d87102c", "metadata": {"aucs": [0.9683358117565251, 0.9778514123558028, 0.9842053291545846], "final_y": [0.16485938207980466, 0.16485620555962366, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "fcbded00-c3da-43a7-b8c9-ab24fb6922d7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.45 * np.exp(-convergence_speed)  # Modified scaling factor for convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced adaptation by modifying scaling factor in DE for improved convergence speed optimization.", "configspace": "", "generation": 10, "fitness": 0.9554873536965435, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.024. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3baa6e04-4644-4bcd-9c2c-ddae141eceaa", "metadata": {"aucs": [0.9236131458226777, 0.9820674519754665, 0.9607814632914862], "final_y": [0.18187838410502621, 0.1648565064265275, 0.16485626915478924]}, "mutation_prompt": null}
{"id": "0ec24b40-b121-46cf-a42b-2110d7064c66", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * (np.std(fitness) / np.mean(fitness)) * (1 - convergence_speed)  # Enhanced crossover rate adaptation\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced crossover rate adaptation in DynamicScalingSymmetryDE by incorporating both population diversity and convergence trend for better exploration-exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.9826827349575266, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3baa6e04-4644-4bcd-9c2c-ddae141eceaa", "metadata": {"aucs": [0.9821401769145585, 0.9820349075794325, 0.9838731203785888], "final_y": [0.16485615089967387, 0.164856791965879, 0.1648625397649799]}, "mutation_prompt": null}
{"id": "b0163d7c-cb43-4aa0-abc8-48387096add7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        best_fitness_index = np.argmin(fitness)  # New line to select best solution\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced mutation strategy with dynamic scaling adaptation in Differential Evolution to improve convergence speed.", "configspace": "", "generation": 10, "fitness": 0.978646747085107, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3baa6e04-4644-4bcd-9c2c-ddae141eceaa", "metadata": {"aucs": [0.9817808373417601, 0.9745289097328765, 0.9796304941806843], "final_y": [0.16485845629523177, 0.16485665656155457, 0.1648567861675374]}, "mutation_prompt": null}
{"id": "b1b86c3d-1b12-4552-94b2-3502c0b70e81", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        diversity = np.std(pop, axis=0).mean()  # Calculate population diversity\n        CR = 0.9 * np.exp(-diversity)  # Adaptive crossover rate based on diversity\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introducing adaptive crossover rate adjustment based on population diversity to improve exploration-exploitation balance in DynamicScalingSymmetryDE.", "configspace": "", "generation": 10, "fitness": 0.9800203718975485, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ed315e91-e770-4e28-87e3-ca7a2cbc419f", "metadata": {"aucs": [0.981792525884315, 0.9770582708317733, 0.9812103189765576], "final_y": [0.16485694477719282, 0.1648566118622956, 0.16485618912116828]}, "mutation_prompt": null}
{"id": "ee7bd012-b661-4a7b-9c74-2e910ee4bea2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.1:\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:  # Introduce adaptive phase shifting when diversity is sufficient\n            phase_shift = np.mean(solution[:self.dim // 2]) - np.mean(solution[self.dim // 2:])\n            solution[:self.dim // 2] -= phase_shift / 2\n            solution[self.dim // 2:] += phase_shift / 2\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 10, 100)})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 10, 100)\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Refined differential evolution with adaptive periodicity enforcement and enhanced local search to optimize multilayer photonic structures.", "configspace": "", "generation": 10, "fitness": 0.9808881526279755, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0e0d6625-0ca5-4a7c-a33c-7ad141ba5d4f", "metadata": {"aucs": [0.9791936616110879, 0.9824776461518109, 0.9809931501210277], "final_y": [0.16485584691735233, 0.16486002313687165, 0.16485771688850526]}, "mutation_prompt": null}
{"id": "e7067c93-7d26-4e40-836b-24e6b1471e02", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < 0.8, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 5})  # Increased iterations\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 5  # Adjusted local search evaluations\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced adaptive local search phase by increasing iterations to improve convergence precision.", "configspace": "", "generation": 11, "fitness": 0.961444498430509, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ed315e91-e770-4e28-87e3-ca7a2cbc419f", "metadata": {"aucs": [0.9237755119073063, 0.9819246223337729, 0.9786333610504482], "final_y": [0.18187923367532843, 0.16486529844920406, 0.16485729695690932]}, "mutation_prompt": null}
{"id": "f6347b9b-023a-42f6-a504-b766c1ed3953", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.5 + 0.3 * np.exp(-convergence_speed)  # Adaptive recombination probability\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])  # Use adaptive Cr\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introduce an adaptive recombination probability to enhance the balance between exploration and exploitation in the trial vector generation step.", "configspace": "", "generation": 11, "fitness": 0.9791898832351839, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ed315e91-e770-4e28-87e3-ca7a2cbc419f", "metadata": {"aucs": [0.9782197839144144, 0.9784333599221398, 0.980916505868997], "final_y": [0.16485699175858048, 0.1648560059907883, 0.16485648788713037]}, "mutation_prompt": null}
{"id": "e9653921-1a1f-4afb-beb4-fec5223c7d2a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        diversity = np.std(pop, axis=0).mean()  # Calculate population diversity\n        F = 0.5 + 0.5 * np.exp(-convergence_speed - diversity)  # Dynamic scaling factor incorporating both convergence speed and diversity\n        CR = 0.9 * np.exp(-diversity)  # Adaptive crossover rate based on diversity\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced mutation factor adaptation based on convergence speed and diversity to improve the balance between exploration and exploitation in DynamicScalingSymmetryDE.", "configspace": "", "generation": 11, "fitness": 0.9825956491858955, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1b86c3d-1b12-4552-94b2-3502c0b70e81", "metadata": {"aucs": [0.981792525884315, 0.9818504351516141, 0.984143986521757], "final_y": [0.16485694477719282, 0.16486129753812107, 0.1648633163615919]}, "mutation_prompt": null}
{"id": "ffa36433-0841-4250-a180-449115446776", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.6 + 0.3 * np.exp(-convergence_speed)  # Adjust mutation scaling factor for exploration\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved convergence by modifying the mutation scaling factor in DynamicScalingSymmetryDE to better adjust exploration.", "configspace": "", "generation": 11, "fitness": 0.9795059927784356, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3790f7ac-ed02-4436-a864-9f5d027e38f7", "metadata": {"aucs": [0.9794600563318829, 0.9804676524429585, 0.9785902695604654], "final_y": [0.1648591293098205, 0.164857354106267, 0.1648564569177955]}, "mutation_prompt": null}
{"id": "d93a2ea9-d42d-4b55-978f-ae39f216b1b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Enhanced symmetric initialization strategy\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.6 + 0.4 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        # More adaptive local optimization iteration count\n        iter_count = max(1, self.budget // 15)\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Fine-tuned DynamicScalingSymmetryDE with enhanced initialization and adaptive local search to boost exploration-exploitation efficiency.", "configspace": "", "generation": 11, "fitness": 0.9883060350523923, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3790f7ac-ed02-4436-a864-9f5d027e38f7", "metadata": {"aucs": [0.9887088750637605, 0.9882895467052497, 0.9879196833881668], "final_y": [0.16485694477719282, 0.16485596040902606, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "afe2fc09-7900-4135-9901-19fed1157194", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.9 * np.exp(-diversity)\n        weight_factor = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic weight adjustment\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introduced dynamic weight adjustment based on population convergence to maximize balance between exploration and exploitation.", "configspace": "", "generation": 12, "fitness": 0.981679731303822, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1b86c3d-1b12-4552-94b2-3502c0b70e81", "metadata": {"aucs": [0.981792525884315, 0.9818504351516141, 0.9813962328755369], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "638ed281-bc11-404a-bade-f835290fc5ae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * (np.std(fitness) / np.mean(fitness)) * (1 - convergence_speed)  # Enhanced crossover rate adaptation\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, i)  # New line for periodicity\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n    \n    def enforce_periodicity(self, solution, index):  # New function for adaptive periodicity\n        factor = (index % 3) * 0.1\n        for i in range(len(solution)):\n            solution[i] *= 1 + factor  # Adjust periodicity factor\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhance exploration by integrating adaptive periodic constraints and further refining local optimization initiation to improve convergence.", "configspace": "", "generation": 12, "fitness": 0.9763830200071771, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0ec24b40-b121-46cf-a42b-2110d7064c66", "metadata": {"aucs": [0.9809758803802822, 0.9743201903153362, 0.9738529893259134], "final_y": [0.16486411209491103, 0.1648559039554881, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "4d3a877a-0b1c-40e4-9cfc-c8af5eb4a17e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.5 + 0.5 * np.exp(-convergence_speed - diversity)\n        CR = 0.9 * np.exp(-diversity)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub, convergence_speed, diversity)  # Modified line\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub, convergence_speed, diversity):  # Modified line\n        sym_factor = 0.5 + 0.5 * np.exp(-convergence_speed - diversity)  # Modified line\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg * sym_factor, lb[i], ub[i])  # Modified line\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introduced adaptive symmetry enforcement based on convergence speed and diversity to refine solution quality.", "configspace": "", "generation": 12, "fitness": 0.9574342613491839, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "e9653921-1a1f-4afb-beb4-fec5223c7d2a", "metadata": {"aucs": [0.9759539296100899, 0.9224958651115484, 0.9738529893259134], "final_y": [0.16485618976864924, 0.1818804950148526, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "fd2dd102-8902-4202-8458-52a351674ab1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.1:\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:  # Introduce adaptive phase shifting when diversity is sufficient\n            phase_shift = np.mean(solution[:self.dim // 2]) - np.mean(solution[self.dim // 2:])\n            solution[:self.dim // 2] -= (phase_shift + np.random.normal(0, 0.1)) / 2\n            solution[self.dim // 2:] += (phase_shift + np.random.normal(0, 0.1)) / 2\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 10, 100)})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 10, 100)\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introduced adaptive phase adjustment in enforce_periodicity to enhance solution diversity and convergence.", "configspace": "", "generation": 12, "fitness": 0.9635724759747459, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "ee7bd012-b661-4a7b-9c74-2e910ee4bea2", "metadata": {"aucs": [0.9846604968859635, 0.9805386128179562, 0.9255183182203182], "final_y": [0.1648624080386426, 0.16485785654282026, 0.18187895965843148]}, "mutation_prompt": null}
{"id": "a33650e1-073b-4141-a607-9daa4c823800", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        F *= 1.0 + 0.1 * np.std(fitness) / np.mean(fitness)  # Adaptive mutation factor scaling\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.1:\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:  # Introduce adaptive phase shifting when diversity is sufficient\n            phase_shift = np.mean(solution[:self.dim // 2]) - np.mean(solution[self.dim // 2:])\n            solution[:self.dim // 2] -= phase_shift / 2\n            solution[self.dim // 2:] += phase_shift / 2\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 10, 100)})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 10, 100)\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introduced adaptive mutation factor scaling based on fitness variance to enhance exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.9797223842218078, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee7bd012-b661-4a7b-9c74-2e910ee4bea2", "metadata": {"aucs": [0.9784762761650918, 0.9814058701625396, 0.9792850063377919], "final_y": [0.16485682805213753, 0.1648567010156219, 0.16485593888291494]}, "mutation_prompt": null}
{"id": "ab194ec6-727f-4d15-a3f5-360f66db6012", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.9 * np.exp(-diversity)\n        weight_factor = 0.5 + 0.5 * np.exp(-convergence_speed) * np.random.rand()  # Dynamic weight adjustment with randomness\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Adjusted the mutation strategy within the differential evolution step to enhance the exploration capabilities by varying the weight factor more dynamically.", "configspace": "", "generation": 13, "fitness": 0.981679731303822, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "afe2fc09-7900-4135-9901-19fed1157194", "metadata": {"aucs": [0.981792525884315, 0.9818504351516141, 0.9813962328755369], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "ea460284-0fc8-4e00-a79f-d9f9b8f77ceb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        diversity = np.std(pop, axis=0).mean()\n        F = 0.4 + 0.6 * np.exp(-convergence_speed - diversity)  # Adjusted dynamic scaling factor\n        CR = 0.9 * np.exp(-diversity)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Fine-tuned mutation factor in DynamicScalingSymmetryDE by adjusting the formula for improved balance between exploration and exploitation.", "configspace": "", "generation": 13, "fitness": 0.981679731303822, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9653921-1a1f-4afb-beb4-fec5223c7d2a", "metadata": {"aucs": [0.981792525884315, 0.9818504351516141, 0.9813962328755369], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "8dad5f6e-9b70-4b3d-9c71-76b7666e46fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.1:\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:  # Introduce adaptive phase shifting when diversity is sufficient\n            phase_shift = np.mean(solution[:self.dim // 2]) - np.mean(solution[self.dim // 2:])\n            phase_intensity = 0.8  # Reduced phase shift intensity for finer adjustments\n            solution[:self.dim // 2] -= (phase_shift / 2) * phase_intensity\n            solution[self.dim // 2:] += (phase_shift / 2) * phase_intensity\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 10, 100)})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 10, 100)\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved periodicity enforcement by refining phase shifting with adaptive intensity to enhance solution quality.", "configspace": "", "generation": 13, "fitness": 0.9716248948898835, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee7bd012-b661-4a7b-9c74-2e910ee4bea2", "metadata": {"aucs": [0.9509980520688519, 0.9801677136303426, 0.983708918970456], "final_y": [0.16486657445792208, 0.1648567342648224, 0.16486208394275814]}, "mutation_prompt": null}
{"id": "72679677-1be5-4cd8-9dd6-cee5a254cac7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.15:  # Modified threshold for diversity_factor\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:  # Introduce adaptive phase shifting when diversity is sufficient\n            phase_shift = np.median(solution[:self.dim // 2]) - np.median(solution[self.dim // 2:])  # Used median instead of mean\n            solution[:self.dim // 2] -= phase_shift / 3  # Modified phase shift adjustment\n            solution[self.dim // 2:] += phase_shift / 3  # Modified phase shift adjustment\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 10, 100)})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 10, 100)\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Refined adaptive phase shifting and periodicity enforcement to enhance Differential Evolution's capability for optimizing multilayer photonic structures.", "configspace": "", "generation": 13, "fitness": 0.9806932493331719, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee7bd012-b661-4a7b-9c74-2e910ee4bea2", "metadata": {"aucs": [0.9823453408834656, 0.9791333563951657, 0.9806010507208845], "final_y": [0.16485666113629804, 0.16485718791019288, 0.16486014176068686]}, "mutation_prompt": null}
{"id": "7a327cf8-64ee-4483-9e3b-e88d2539baf1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        F = 0.6  # Adjusted scaling factor\n        Cr = 0.9 - 0.5 * np.std(fitness) / np.mean(fitness)  # Refine crossover rate adaptation\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.15:  # Adjusted threshold for periodicity enforcement\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:\n            phase_shift = np.mean(solution[:self.dim // 2]) - np.mean(solution[self.dim // 2:])\n            solution[:self.dim // 2] -= phase_shift / 2\n            solution[self.dim // 2:] += phase_shift / 2\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 8, 100)})  # Adjusted maxiter for local search\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 8, 100)  # Adjusted evaluations increment\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Refined differential evolution with enhanced adaptive local search incorporation and improved periodicity enforcement for multi-layer optimization.", "configspace": "", "generation": 13, "fitness": 0.9825490787110972, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee7bd012-b661-4a7b-9c74-2e910ee4bea2", "metadata": {"aucs": [0.9799008550773576, 0.985435172152605, 0.9823112089033288], "final_y": [0.16485604607667248, 0.16485954530077074, 0.16485969754441754]}, "mutation_prompt": null}
{"id": "b30a1b74-54f8-4cfa-a0cb-146048134026", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        diversity = np.std(pop, axis=0).mean()  # Calculate population diversity\n        F = 0.5 + 0.5 * np.exp(-0.5 * convergence_speed - diversity)  # Adjusted dynamic scaling factor for improved balance\n        CR = 0.9 * np.exp(-diversity)  # Adaptive crossover rate based on diversity\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved the balance between exploration and exploitation by adjusting the mutation factor with a new adaptive strategy based on diversity and convergence speed.", "configspace": "", "generation": 14, "fitness": 0.981679731303822, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9653921-1a1f-4afb-beb4-fec5223c7d2a", "metadata": {"aucs": [0.981792525884315, 0.9818504351516141, 0.9813962328755369], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "620ecfb0-e5ce-4e2d-b295-fdc8a0783926", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.9 * np.exp(-diversity)\n        weight_factor = 0.5 + 0.5 * np.exp(-diversity)  # Dynamic weight adjustment\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Refined the mutation strategy by incorporating a diversity-based adaptive mutation factor for improved exploration-exploitation balance.", "configspace": "", "generation": 14, "fitness": 0.981679731303822, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "afe2fc09-7900-4135-9901-19fed1157194", "metadata": {"aucs": [0.981792525884315, 0.9818504351516141, 0.9813962328755369], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "2c89140f-3831-43a7-b611-fa1701025743", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.9 * np.exp(-diversity)\n        weight_factor = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic weight adjustment\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = len(solution) // 4\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period], lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 8})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved DynamicScalingSymmetryDE by incorporating adaptive periodicity enforcement and enhanced local optimization strategy for better convergence.", "configspace": "", "generation": 14, "fitness": 0.9868729538371692, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "afe2fc09-7900-4135-9901-19fed1157194", "metadata": {"aucs": [0.9869664681052056, 0.9867477259251567, 0.9869046674811454], "final_y": [0.16486323183404827, 0.1648584717517827, 0.16485608733386847]}, "mutation_prompt": null}
{"id": "e00ec687-b2c0-4a2f-a4f4-1b714e2fc69f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.9 * np.exp(-diversity) * (1 - convergence_speed)  # Enhanced adaptation of CR\n        weight_factor = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic weight adjustment\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced diversity preservation by adapting CR based on population convergence and diversity.", "configspace": "", "generation": 14, "fitness": 0.981679731303822, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "afe2fc09-7900-4135-9901-19fed1157194", "metadata": {"aucs": [0.981792525884315, 0.9818504351516141, 0.9813962328755369], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "a02b7a6d-a9d3-4cb8-ac80-c8df023b270c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        F = 0.6\n        Cr = 0.9 - 0.5 * np.std(fitness) / np.mean(fitness)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.1:  # Adjusted threshold for periodicity enforcement\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:\n            phase_shift = np.var(solution[:self.dim // 2]) - np.var(solution[self.dim // 2:])  # Use variance for phase shift\n            solution[:self.dim // 2] -= phase_shift / 4  # Adjusted phase shift correction\n            solution[self.dim // 2:] += phase_shift / 4  # Adjusted phase shift correction\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 8, 100)})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 8, 100)\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "DynamicScalingSymmetryDE with adaptive phase shift correction and enhanced periodicity enforcement to improve convergence and solution quality.", "configspace": "", "generation": 14, "fitness": 0.980190123014446, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7a327cf8-64ee-4483-9e3b-e88d2539baf1", "metadata": {"aucs": [0.9754407517800419, 0.9831148107758029, 0.9820148064874931], "final_y": [0.16485648182622303, 0.1648587300478217, 0.16485668963967048]}, "mutation_prompt": null}
{"id": "0bd6519a-703e-48b5-b119-513a29c99f76", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n        self.fitness_improvement_window = []\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        diversity = np.std(pop, axis=0).mean()  # Calculate population diversity\n        recent_improvement = np.mean(self.fitness_improvement_window[-5:]) if self.fitness_improvement_window else 0\n        F = 0.5 + 0.5 * np.exp(-convergence_speed - diversity - recent_improvement)  # Adjusted dynamic scaling factor\n        CR = 0.9 * np.exp(-diversity + recent_improvement)  # Adjusted adaptive crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n            self.fitness_improvement_window.append(np.min(self.fitness))\n            \n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved the exploration-exploitation balance by adjusting the dynamic scaling and crossover rates based on both population diversity and a moving average of fitness improvements.", "configspace": "", "generation": 15, "fitness": 0.981679731303822, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9653921-1a1f-4afb-beb4-fec5223c7d2a", "metadata": {"aucs": [0.981792525884315, 0.9818504351516141, 0.9813962328755369], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "a37d4e93-650a-4f90-8b6a-47b3442e2a06", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        F = 0.6  # Adjusted scaling factor\n        Cr = 0.9 - 0.5 * np.std(fitness) / np.mean(fitness)  # Refine crossover rate adaptation\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.15:  # Adjusted threshold for periodicity enforcement\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:\n            phase_shift = np.mean(solution[:self.dim // 2]) - np.mean(solution[self.dim // 2:])\n            solution[:self.dim // 2] -= phase_shift / 2\n            solution[self.dim // 2:] += phase_shift / 2\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 6, 150)})  # Adjusted maxiter for local search\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 8, 100)  # Adjusted evaluations increment\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced local search efficiency by increasing `maxiter` during local optimization for improved convergence.", "configspace": "", "generation": 15, "fitness": 0.9793433438705733, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7a327cf8-64ee-4483-9e3b-e88d2539baf1", "metadata": {"aucs": [0.9784614434955722, 0.9826431682350355, 0.9769254198811123], "final_y": [0.164858782388571, 0.16485721794474606, 0.1648570954781179]}, "mutation_prompt": null}
{"id": "c0bc734d-86ed-4979-884d-32e255d3e11e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic scaling based on convergence speed\n        Cr = 0.9 - 0.4 * (np.std(fitness) / np.mean(fitness)) * (1 - convergence_speed)  # Enhanced crossover rate adaptation\n        elite_index = np.argmin(fitness)  # Identify elite solution\n        for i in range(self.population_size):\n            if i == elite_index:  # Preserve elite solution\n                continue\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 5})  # Refined local search\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved convergence by incorporating elite preservation and refined local search in DynamicScalingSymmetryDE.", "configspace": "", "generation": 15, "fitness": 0.9801806965365198, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0ec24b40-b121-46cf-a42b-2110d7064c66", "metadata": {"aucs": [0.980143091669371, 0.9828203548529366, 0.9775786430872517], "final_y": [0.16485841337743945, 0.16486149878694334, 0.1648588451180465]}, "mutation_prompt": null}
{"id": "835cb06c-3200-41a4-98b0-22ff745de299", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        F = 0.5 + 0.1 * np.std(pop) / np.mean(pop)  # Fine-tuned dynamic scaling factor\n        Cr = 0.9 - 0.5 * np.std(fitness) / np.mean(fitness)  # Refine crossover rate adaptation\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.15:  # Adjusted threshold for periodicity enforcement\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:\n            phase_shift = np.mean(solution[:self.dim // 2]) - np.mean(solution[self.dim // 2:])\n            solution[:self.dim // 2] -= phase_shift / 2\n            solution[self.dim // 2:] += phase_shift / 2\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 8, 100)})  # Adjusted maxiter for local search\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 8, 100)  # Adjusted evaluations increment\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Fine-tuned dynamic scaling factor based on population diversity to boost exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.9811814905748918, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7a327cf8-64ee-4483-9e3b-e88d2539baf1", "metadata": {"aucs": [0.9818489952244615, 0.9796148928232297, 0.9820805836769844], "final_y": [0.16485844854587872, 0.1648560664301043, 0.16485713605144592]}, "mutation_prompt": null}
{"id": "96d5047c-06c2-4de7-a373-53aeda809268", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        diversity = np.std(pop) / np.mean(pop)\n        F = 0.6 + 0.2 * diversity  # Adaptive adjustment of scaling factor F\n        Cr = 0.9 - 0.5 * np.std(fitness) / np.mean(fitness)  # Refine crossover rate adaptation\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial = self.enforce_periodicity(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        diversity_factor = np.std(solution) / np.mean(solution)\n        if diversity_factor < 0.15:  # Adjusted threshold for periodicity enforcement\n            solution = np.clip(np.mean(solution) * np.ones(self.dim), lb, ub)\n        else:\n            phase_shift = np.mean(solution[:self.dim // 2]) - np.mean(solution[self.dim // 2:])\n            solution[:self.dim // 2] -= phase_shift / 2\n            solution[self.dim // 2:] += phase_shift / 2\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': min(self.budget // 8, 100)})  # Adjusted maxiter for local search\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += min(self.budget // 8, 100)  # Adjusted evaluations increment\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introduced adaptive adjustment of the scaling factor F based on population diversity to enhance exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.9785713703378592, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7a327cf8-64ee-4483-9e3b-e88d2539baf1", "metadata": {"aucs": [0.9747814460532042, 0.9792229085772489, 0.9817097563831246], "final_y": [0.1648599527813548, 0.16485622348076567, 0.1648569657549842]}, "mutation_prompt": null}
{"id": "605c0616-7261-45aa-95b9-01ec4f0574a0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        Cr = 0.9 - 0.4 * np.std(fitness) / (np.mean(fitness) + 1e-9) * (1 - convergence_speed)  # Prevent division by zero\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n            # Encourage periodicity\n            if i > 0 and i % (self.dim // 4) == 0:\n                solution[i:i+2] = solution[i-2:i]\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        diversity_factor = np.std(self.fitness) / (np.mean(self.fitness) + 1e-9)  # Diversity-based adjustment\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': int(self.budget // 10 * (1 + diversity_factor))})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Fine-tuned DynamicScalingSymmetryDE with adaptive periodicity promotion and diversity-based local search enhancement.", "configspace": "", "generation": 16, "fitness": 0.9870214948905827, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0ec24b40-b121-46cf-a42b-2110d7064c66", "metadata": {"aucs": [0.9861060562653163, 0.9879690035195684, 0.9869894248868635], "final_y": [0.16485625396049475, 0.16486009702669324, 0.16485608733386847]}, "mutation_prompt": null}
{"id": "6dd56872-8da7-4595-8d5d-02eb891b4ac7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Adjusted dynamic scaling\n        Cr = 0.8 - 0.3 * np.std(fitness) / np.mean(fitness)  # Adjusted crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        iter_count = max(1, self.budget // 20)  # Adjusted iteration count\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced adaptive convergence strategies in DynamicScalingSymmetryDE with improved local optimization and symmetric exploration for better performance.", "configspace": "", "generation": 16, "fitness": 0.9854055415204855, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d93a2ea9-d42d-4b55-978f-ae39f216b1b6", "metadata": {"aucs": [0.9909239374524933, 0.9818436897478513, 0.9834489973611121], "final_y": [0.16486553190758935, 0.164856324031451, 0.16485668172635504]}, "mutation_prompt": null}
{"id": "648be86f-6a7d-409f-9f57-5b663adb613d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        diversity = np.std(pop, axis=0).mean()  # Calculate population diversity\n        F = 0.5 + 0.5 * np.exp(-convergence_speed - diversity)  # Dynamic scaling factor incorporating both convergence speed and diversity\n        CR = 0.9 * np.exp(-diversity)  # Adaptive crossover rate based on diversity\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        solution += 0.01 * np.sin(2 * np.pi * np.arange(len(solution)) / len(solution))  # Adaptive periodicity enforcement\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introduced adaptive periodicity enforcement using a sinusoidal function to enhance solution periodicity in DynamicScalingSymmetryDE.", "configspace": "", "generation": 16, "fitness": 0.9842632503221972, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9653921-1a1f-4afb-beb4-fec5223c7d2a", "metadata": {"aucs": [0.9817850463600385, 0.9844920446766651, 0.986512659929888], "final_y": [0.1648568333682452, 0.16485675384281806, 0.16485619622475]}, "mutation_prompt": null}
{"id": "d46b1cb1-80ee-4b06-ad14-d2f749fdc39a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.sqrt(np.var(pop, axis=0).mean())  # Adjusted diversity measure\n        CR = 0.9 * np.exp(-diversity)\n        weight_factor = 0.5 + 0.5 * np.exp(-convergence_speed)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = max(1, len(solution) // np.random.randint(3, 5))  # Adaptive periodicity length\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period], lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 8})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Introduced adaptive periodicity length and enhanced the diversity measure to improve convergence and exploration capabilities.", "configspace": "", "generation": 16, "fitness": 0.9857334041628208, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c89140f-3831-43a7-b611-fa1701025743", "metadata": {"aucs": [0.9840204105850983, 0.9868785853803883, 0.9863012165229763], "final_y": [0.1648581333380872, 0.1648584717517827, 0.16485619622475]}, "mutation_prompt": null}
{"id": "c2961933-2a0b-4790-9e55-698b99e8bd31", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.9 * np.exp(-diversity)\n        weight_factor = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic weight adjustment\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = len(solution) // 5  # Adjusted periodicity enforcement\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period], lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})  # Updated max iterations\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced exploration and convergence by optimizing periodicity enforcement and the update mechanism for local optimization.", "configspace": "", "generation": 16, "fitness": 0.9865046202912078, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c89140f-3831-43a7-b611-fa1701025743", "metadata": {"aucs": [0.9869664681052056, 0.9858396040396052, 0.9867077887288126], "final_y": [0.16486323183404827, 0.1648570872326237, 0.16485765754303394]}, "mutation_prompt": null}
{"id": "580dce4a-702b-4b2c-b1ba-9cf66d492436", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.9 * np.exp(-diversity)\n        weight_factor = 0.5 + 0.5 * np.exp(-convergence_speed)  # Dynamic weight adjustment\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub, fitness), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub, fitness):\n        midpoint = len(solution) // 2\n        adjustment_factor = 0.5 + 0.5 * np.exp(-np.std(fitness) / np.mean(fitness))\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg * adjustment_factor, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = len(solution) // 4\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period], lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 8})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved DynamicScalingSymmetryDE by incorporating dynamic symmetry and periodicity adjustment based on fitness, enhancing convergence and exploration.", "configspace": "", "generation": 17, "fitness": 0.9871398117796671, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c89140f-3831-43a7-b611-fa1701025743", "metadata": {"aucs": [0.9862174290394837, 0.987495788371592, 0.9877062179279259], "final_y": [0.1648642815273561, 0.16485669000801728, 0.16485774566980183]}, "mutation_prompt": null}
{"id": "47273664-b986-4daf-b198-644b5660b35d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.9 * np.exp(-diversity)\n        weight_factor = 0.3 + 0.7 * np.exp(-convergence_speed)  # Adjusted dynamic weight\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = len(solution) // 4  # Adjusted periodicity enforcement\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period], lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 12})  # Updated max iterations\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced periodicity adjustment and symmetry strategies to improve exploration and exploitation balance in DE.", "configspace": "", "generation": 17, "fitness": 0.9868895988101293, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c2961933-2a0b-4790-9e55-698b99e8bd31", "metadata": {"aucs": [0.9869664681052056, 0.9867477259251567, 0.9869546024000259], "final_y": [0.16486323183404827, 0.1648584717517827, 0.16486058825196048]}, "mutation_prompt": null}
{"id": "edec8503-f1f6-4915-a18d-90a473cb8e62", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.3 * np.exp(-convergence_speed)  # Adjusted scaling factor\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.95 * np.exp(-diversity)  # Adjusted crossover rate\n        weight_factor = 0.4 + 0.6 * np.exp(-convergence_speed)  # Modified weight adjustment\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = len(solution) // 4  # Refined periodicity enforcement\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period] * 0.9 + solution[i] * 0.1, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 9})  # Updated max iterations\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced DynamicScalingSymmetryDE by refining periodicity enforcement and convergence strategy for improved optimization performance.", "configspace": "", "generation": 17, "fitness": 0.9872202044919461, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c2961933-2a0b-4790-9e55-698b99e8bd31", "metadata": {"aucs": [0.9862808060234227, 0.9874601240642487, 0.9879196833881668], "final_y": [0.1648597712180947, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "d6fde807-6e29-41f1-86c0-2d1bf7f3b475", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Enhanced symmetric initialization strategy\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.7 + 0.3 * np.exp(-convergence_speed * 2)  # Modified dynamic scaling\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)  # Adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        # More adaptive local optimization iteration count\n        iter_count = max(1, self.budget // 10)  # Adjusted to provide more iterations\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Fine-tuned DynamicScalingSymmetryDE by further enhancing local optimization and modifying the scaling factor formula for better convergence.", "configspace": "", "generation": 17, "fitness": 0.9886123351793988, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d93a2ea9-d42d-4b55-978f-ae39f216b1b6", "metadata": {"aucs": [0.9904571980857811, 0.9874601240642487, 0.9879196833881668], "final_y": [0.16486155133715963, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "2f09593e-8ff5-4640-b9a7-7c74afdbe593", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Enhanced symmetric initialization strategy\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.3 * np.exp(-convergence_speed)  # Adjusted dynamic scaling for enhanced exploration\n        Cr = 0.8 - 0.3 * np.std(fitness) / np.mean(fitness)  # Adjusted crossover rate for better exploitation\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        # More adaptive local optimization iteration count\n        iter_count = max(1, self.budget // 15)\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced DynamicScalingSymmetryDE by refining crossover and dynamic scaling strategy for improved convergence.", "configspace": "", "generation": 17, "fitness": 0.9886572739826237, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d93a2ea9-d42d-4b55-978f-ae39f216b1b6", "metadata": {"aucs": [0.9902353416606863, 0.9878167968990181, 0.9879196833881668], "final_y": [0.16485921812296978, 0.16485743765140248, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "8cbe40cc-04b6-43bc-b485-bc65d10a6b84", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.3 * np.exp(-convergence_speed)  # Adjusted scaling factor\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.95 * np.exp(-diversity)  # Adjusted crossover rate\n        weight_factor = 0.4 + 0.6 * np.exp(-convergence_speed)  # Modified weight adjustment\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = len(solution) // 3  # Refined periodicity enforcement\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period] * 0.9 + solution[i] * 0.1, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 9})  # Updated max iterations\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Optimized DynamicScalingSymmetryDE by fine-tuning periodicity enforcement to enhance solution accuracy and convergence. ", "configspace": "", "generation": 18, "fitness": 0.9841618544881023, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "edec8503-f1f6-4915-a18d-90a473cb8e62", "metadata": {"aucs": [0.9805320730721266, 0.9858623281482181, 0.9860911622439624], "final_y": [0.16485588629321302, 0.16486431125127166, 0.16485600481728335]}, "mutation_prompt": null}
{"id": "06f15558-7711-4a5d-87db-0919028ed1c9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Enhanced symmetric initialization strategy\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.7 + 0.3 * np.exp(-convergence_speed)  # Adjusted dynamic scaling formula\n        Cr = 0.9 - 0.3 * np.std(fitness) / np.mean(fitness)  # Modified adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] * 0.6 + solution[-(i + 1)] * 0.4)  # Improved symmetry enforcement\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        iter_count = max(1, self.budget // 15)  # Adjusted local optimization iteration count\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 15\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced exploration-exploitation balance by integrating adaptive convergence criteria and improved symmetry enforcement mechanism.", "configspace": "", "generation": 18, "fitness": 0.9882165696112586, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6fde807-6e29-41f1-86c0-2d1bf7f3b475", "metadata": {"aucs": [0.9887088750637607, 0.9880211503818482, 0.9879196833881668], "final_y": [0.16485694477719282, 0.16485889120377706, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "a48163a7-7219-4fad-91f0-bc49ac2dd5d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Adjusted dynamic scaling for better adaptability\n        Cr = 0.8 - 0.3 * np.std(fitness) / np.mean(fitness)  # Adjusted crossover rate for enhanced diversity\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_periodicity(trial, lb, ub)  # Improved periodicity enforcement\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = self.dim // 4  # Encourage periodicity with a quarter-length cycle\n        for i in range(period):\n            avg = np.mean([solution[j] for j in range(i, self.dim, period)])\n            for j in range(i, self.dim, period):\n                solution[j] = np.clip(avg, lb[j], ub[j])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        iter_count = max(1, self.budget // 15)\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced DynamicScalingSymmetryDE by improving periodicity enforcement and dynamic scaling adaptability for better performance.", "configspace": "", "generation": 18, "fitness": 0.9928958848002228, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d93a2ea9-d42d-4b55-978f-ae39f216b1b6", "metadata": {"aucs": [0.9925366851509545, 0.993208551469737, 0.9929424177799769], "final_y": [0.1648608543629977, 0.1648567287513465, 0.16485631249866628]}, "mutation_prompt": null}
{"id": "375c0bb2-de64-4b19-8008-2dab508bd5b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.4 * np.exp(-convergence_speed)  # Tweaked scaling factor\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.85 * np.exp(-diversity)  # Adjusted crossover rate\n        weight_factor = 0.6 + 0.4 * np.exp(-convergence_speed)  # Modified dynamic weight\n        beta = 0.2 * (1 - np.exp(-diversity))  # New diversity-based factor\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + weight_factor * (b - c) + beta * (np.mean(pop, axis=0) - pop[i]), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub, fitness), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub, fitness):\n        midpoint = len(solution) // 2\n        adjustment_factor = 0.5 + 0.5 * np.exp(-np.std(fitness) / np.mean(fitness))\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg * adjustment_factor, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = len(solution) // 4\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period], lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 8})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Optimized DynamicScalingSymmetryDE by introducing adaptive parameters and improved diversity management to enhance convergence and solution quality.", "configspace": "", "generation": 18, "fitness": 0.9871398117796671, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "580dce4a-702b-4b2c-b1ba-9cf66d492436", "metadata": {"aucs": [0.9862174290394837, 0.987495788371592, 0.9877062179279259], "final_y": [0.1648642815273561, 0.16485669000801728, 0.16485774566980183]}, "mutation_prompt": null}
{"id": "c1fa6c27-15a6-4e03-b52b-e9f81abce114", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.3 * np.exp(-convergence_speed)\n        diversity = np.std(pop, axis=0).mean()\n        CR = 0.95 * np.exp(-diversity)\n        weight_factor = 0.4 + 0.6 * np.exp(-convergence_speed)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c, d = pop[np.random.choice(indices, 4, replace=False)]  # Added an extra parent for mutation\n            mutant = np.clip(a + F * (b - c + d - a), lb, ub)  # Updated mutation formula\n            trial = np.where(np.random.rand(self.dim) < CR, mutant, pop[i])\n            trial = self.enforce_periodicity(self.enforce_symmetry(trial, lb, ub), lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = len(solution) // 4\n        for i in range(len(solution)):\n            solution[i] = np.clip(solution[i % period] * 0.9 + solution[i] * 0.1, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': self.budget // 10})  # Reduced max iterations\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 8\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Further enhanced DynamicScalingSymmetryDE by optimizing the mutation strategy to promote more diverse exploration and refined local optimization for efficiency.", "configspace": "", "generation": 18, "fitness": 0.9869502338259615, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "edec8503-f1f6-4915-a18d-90a473cb8e62", "metadata": {"aucs": [0.9862808060234227, 0.9876152930544361, 0.9869546024000259], "final_y": [0.1648597712180947, 0.164861529754194, 0.16486058825196048]}, "mutation_prompt": null}
{"id": "83d937ac-6b56-4d0c-9659-a99d9082d3d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Enhanced symmetric initialization strategy\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.8 + 0.2 * np.exp(-convergence_speed)  # Adjusted dynamic scaling formula\n        Cr = 0.8 - 0.2 * np.std(fitness) / np.mean(fitness)  # Modified adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] * 0.5 + solution[-(i + 1)] * 0.5)  # Improved symmetry enforcement\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        iter_count = max(1, self.budget // 10)  # Adjusted local optimization iteration count\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count, 'ftol': 1e-9})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced DynamicScalingSymmetryDE by refining local search strategy and improving symmetry enforcement for better convergence.", "configspace": "", "generation": 19, "fitness": 0.9880295608387253, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06f15558-7711-4a5d-87db-0919028ed1c9", "metadata": {"aucs": [0.9887088750637605, 0.9874601240642487, 0.9879196833881668], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "c2cc548b-47ad-40e4-a92d-4e30619c63ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Enhanced symmetric initialization strategy\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.8 + 0.2 * np.exp(-convergence_speed)  # Adjusted dynamic scaling formula\n        Cr = 0.9 - 0.3 * np.std(fitness) / np.mean(fitness)  # Modified adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] * 0.5 + solution[-(i + 1)] * 0.5)  # Improved symmetry enforcement\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        iter_count = max(1, self.budget // 15)  # Adjusted local optimization iteration count\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 15\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced DynamicScalingSymmetryDE by refining symmetry enforcement and dynamic scaling to improve convergence speed.", "configspace": "", "generation": 19, "fitness": 0.987987497842933, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06f15558-7711-4a5d-87db-0919028ed1c9", "metadata": {"aucs": [0.9887088750637607, 0.9873339350768717, 0.9879196833881668], "final_y": [0.16485694477719282, 0.1648559373715458, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "2364b62e-7355-4e09-bdff-29f894808a4b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Enhanced symmetric initialization strategy\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.8 + 0.2 * np.exp(-convergence_speed)  # Adjusted dynamic scaling formula\n        Cr = 0.85 - 0.35 * np.std(fitness) / np.mean(fitness)  # Modified adaptation of crossover rate\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] * 0.5 + solution[-(i + 1)] * 0.5)  # Improved symmetry enforcement\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        iter_count = max(1, self.budget // 15)  # Adjusted local optimization iteration count\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 15\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved DynamicScalingSymmetryDE by refining symmetry enforcement and dynamic scaling to boost performance within the given modification constraints.", "configspace": "", "generation": 19, "fitness": 0.9880295608387253, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "06f15558-7711-4a5d-87db-0919028ed1c9", "metadata": {"aucs": [0.9887088750637607, 0.9874601240642485, 0.9879196833881668], "final_y": [0.16485694477719282, 0.16486129753812107, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "b8370521-f71e-4d09-be33-8e3ae9eb03bc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        return pop\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.7 + 0.3 * np.exp(-convergence_speed * 2)\n        Cr = 0.9 - 0.4 * np.std(fitness) / np.mean(fitness)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_symmetry(trial, lb, ub)\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_symmetry(self, solution, lb, ub):\n        midpoint = len(solution) // 2\n        for i in range(midpoint):\n            avg = (solution[i] + solution[-(i + 1)]) / 2\n            solution[i] = solution[-(i + 1)] = np.clip(avg, lb[i], ub[i])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        iter_count = max(1, self.budget // 5)  # Adjusted to allow more iterations if budget permits\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Improved the local optimization by allowing more adaptive L-BFGS-B iterations based on remaining budget.", "configspace": "", "generation": 19, "fitness": 0.9883468198968486, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6fde807-6e29-41f1-86c0-2d1bf7f3b475", "metadata": {"aucs": [0.9886588830789543, 0.9884618932234246, 0.9879196833881668], "final_y": [0.1648561463606013, 0.16485644375105935, 0.16485653636573427]}, "mutation_prompt": null}
{"id": "df0495f5-f15d-4fcc-81d0-6a07e21e884c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicScalingSymmetryDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.fitness = np.inf * np.ones(self.population_size)\n        self.bounds = None\n\n    def initialize_population(self, lb, ub):\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        for i in range(self.population_size):\n            midpoint = len(pop[i]) // 2\n            for j in range(midpoint):\n                avg = (pop[i][j] + pop[i][-j-1]) / 2\n                pop[i][j] = pop[i][-j-1] = np.clip(avg, lb[j], ub[j])\n        quasi_oppositional_pop = lb + ub - pop  # Quasi-Oppositional Initialization\n        return np.concatenate((pop, quasi_oppositional_pop))\n\n    def differential_evolution_step(self, pop, fitness, lb, ub):\n        convergence_speed = np.mean(np.abs(fitness - np.min(fitness)))\n        F = 0.5 + 0.5 * np.exp(-convergence_speed)  # Adjusted dynamic scaling for better adaptability\n        Cr = 0.8 - 0.3 * np.std(fitness) / np.mean(fitness)  # Adjusted crossover rate for enhanced diversity\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), lb, ub)\n            trial = np.where(np.random.rand(self.dim) < Cr, mutant, pop[i])\n            trial = self.enforce_periodicity(trial, lb, ub)  # Improved periodicity enforcement\n            trial_fitness = self.func(trial)\n            if trial_fitness < fitness[i]:\n                pop[i], fitness[i] = trial, trial_fitness\n\n    def enforce_periodicity(self, solution, lb, ub):\n        period = self.dim // 4  # Encourage periodicity with a quarter-length cycle\n        for i in range(period):\n            avg = np.mean([solution[j] for j in range(i, self.dim, period)])\n            for j in range(i, self.dim, period):\n                solution[j] = np.clip(avg, lb[j], ub[j])\n        return solution\n\n    def local_optimization(self, best_solution, lb, ub):\n        iter_count = max(1, self.budget // 15)\n        res = minimize(self.func, best_solution, bounds=list(zip(lb, ub)),\n                       method='L-BFGS-B', options={'maxiter': iter_count})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n\n        pop = self.initialize_population(lb, ub)\n        self.fitness = np.array([self.func(ind) for ind in pop])\n        evaluations = len(pop)\n\n        while evaluations < self.budget:\n            self.differential_evolution_step(pop, self.fitness, lb, ub)\n            evaluations += self.population_size\n\n            if evaluations < self.budget:\n                best_index = np.argmin(self.fitness)\n                best_solution, best_fitness = self.local_optimization(pop[best_index], lb, ub)\n                if best_fitness < self.fitness[best_index]:\n                    pop[best_index], self.fitness[best_index] = best_solution, best_fitness\n                evaluations += self.budget // 10\n\n        return pop[np.argmin(self.fitness)]", "name": "DynamicScalingSymmetryDE", "description": "Enhanced initialization with quasi-oppositional strategy to improve exploration efficiency.", "configspace": "", "generation": 19, "fitness": 0.988571359466167, "feedback": "The algorithm DynamicScalingSymmetryDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a48163a7-7219-4fad-91f0-bc49ac2dd5d8", "metadata": {"aucs": [0.9880344787846734, 0.9879541408969353, 0.9897254587168925], "final_y": [0.16485909426524714, 0.16485802062826482, 0.1648571676543481]}, "mutation_prompt": null}
