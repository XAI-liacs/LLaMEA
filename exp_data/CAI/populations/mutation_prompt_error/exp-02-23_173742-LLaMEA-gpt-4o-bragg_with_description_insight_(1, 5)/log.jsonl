{"id": "93e30dcd-f996-4021-a7a6-fdaf7e268e69", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n        \n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += diff ** 2\n            return penalty\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        scores = np.empty(pop_size)\n        \n        for i in range(pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores)\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "The algorithm employs a dual-phase global-local search with a periodicity-enforcing mechanism, utilizing Differential Evolution for broad exploration followed by local refinement via BFGS to optimize reflective multilayer structures.", "configspace": "", "generation": 0, "fitness": 0.592222502200668, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.592 with standard deviation 0.068. And the mean value of best solutions found was 0.342 (0. is the best) with standard deviation 0.043.", "error": "", "parent_id": null, "metadata": {"aucs": [0.5296889052598271, 0.6862776261182197, 0.5607009752239571], "final_y": [0.38167549823931923, 0.28245825211074405, 0.363215335056561]}, "mutation_prompt": null}
{"id": "03f442fd-9a8d-481a-aea6-dd6da17795f3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ImprovedBraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n        \n        def periodic_penalty(x):\n            # Encourage periodicity by penalizing irregular patterns\n            penalty = 0.0\n            period = (bounds.ub[0] - bounds.lb[0]) / 2  # Assume half-periodicity\n            for i in range(1, len(x)):\n                diff = np.abs((x[i] - x[i-1]) % period)\n                penalty += (diff - period / 2) ** 2  # Penalize deviation from half-period\n            return penalty\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        scores = np.array([func(ind) + periodic_penalty(ind) for ind in population])\n        self.eval_count += pop_size\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Modify crossover for periodicity influence\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores)\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count, 'disp': False})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "ImprovedBraggMirrorOptimizer", "description": "The algorithm integrates a probabilistic periodicity-driven mutation strategy in Differential Evolution, coupled with adaptive local search using L-BFGS-B, to enhance convergence towards optimal periodic multilayer configurations.", "configspace": "", "generation": 1, "fitness": 0.5819271052157428, "feedback": "The algorithm ImprovedBraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.582 with standard deviation 0.050. And the mean value of best solutions found was 0.348 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "93e30dcd-f996-4021-a7a6-fdaf7e268e69", "metadata": {"aucs": [0.5333635141431061, 0.650017610412825, 0.5624001910912972], "final_y": [0.37776203886314785, 0.3050739333348772, 0.36011929273383436]}, "mutation_prompt": null}
{"id": "c420999e-db53-480c-85ca-c4e84ccf73df", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPSOBraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def particle_swarm_optimization(self, func, bounds, swarm_size=20, inertia=0.5, cognitive=1.5, social=1.5, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // swarm_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i - 1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += diff ** 2\n            return penalty\n\n        swarm = np.random.uniform(bounds.lb, bounds.ub, (swarm_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (swarm_size, self.dim))\n        personal_best_positions = np.copy(swarm)\n        personal_best_scores = np.array([func(x) + periodic_penalty(x) for x in swarm])\n        global_best_idx = np.argmin(personal_best_scores)\n        global_best_position = personal_best_positions[global_best_idx]\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            for i in range(swarm_size):\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                velocities[i] = (\n                    inertia * velocities[i]\n                    + cognitive * r1 * (personal_best_positions[i] - swarm[i])\n                    + social * r2 * (global_best_position - swarm[i])\n                )\n                swarm[i] = np.clip(swarm[i] + velocities[i], bounds.lb, bounds.ub)\n\n                score = func(swarm[i]) + periodic_penalty(swarm[i])\n                self.eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = swarm[i]\n\n                if score < personal_best_scores[global_best_idx]:\n                    global_best_idx = i\n                    global_best_position = swarm[i]\n\n        return global_best_position\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.particle_swarm_optimization(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "HybridPSOBraggMirrorOptimizer", "description": "The algorithm leverages a hybrid Particle Swarm Optimization (PSO) with periodicity constraints and a local Nelder-Mead method for refining promising regions to optimize reflective multilayer structures efficiently.", "configspace": "", "generation": 1, "fitness": 0.5483704756272173, "feedback": "The algorithm HybridPSOBraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.548 with standard deviation 0.032. And the mean value of best solutions found was 0.370 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "93e30dcd-f996-4021-a7a6-fdaf7e268e69", "metadata": {"aucs": [0.5675071466447557, 0.5743782116757234, 0.5032260685611728], "final_y": [0.35568458020783567, 0.35078615523890533, 0.40308643474021744]}, "mutation_prompt": null}
{"id": "237ad1bf-cebf-4523-8a87-6c47a00aea8e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n        \n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2  # Adjusted penalty mechanism\n            return penalty\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        scores = np.empty(pop_size)\n        \n        for i in range(pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores)\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "The algorithm optimizes multilayer structures using Differential Evolution with enhanced periodicity via an improved penalty mechanism, followed by BFGS for refinement.", "configspace": "", "generation": 1, "fitness": 0.6027523748518221, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.603 with standard deviation 0.014. And the mean value of best solutions found was 0.331 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "93e30dcd-f996-4021-a7a6-fdaf7e268e69", "metadata": {"aucs": [0.6217145015108665, 0.5903562923399476, 0.5961863307046522], "final_y": [0.32017241650267414, 0.3407855582767847, 0.3330172565926557]}, "mutation_prompt": null}
{"id": "89aaa567-55ff-477d-86b6-31498f00bc3f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedBraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n        \n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += diff ** 2\n            return penalty\n        \n        # Quasi-oppositional initialization\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opposite_population = (bounds.lb + bounds.ub) - population\n        population = np.vstack((population, opposite_population))\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n        \n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "EnhancedBraggMirrorOptimizer", "description": "The algorithm integrates quasi-oppositional differential evolution with adaptive periodicity penalties and hybridizes with local search using BFGS, enhancing exploration and exploitation for optimizing reflective multilayer structures.", "configspace": "", "generation": 1, "fitness": 0.5788724589375653, "feedback": "The algorithm EnhancedBraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.579 with standard deviation 0.053. And the mean value of best solutions found was 0.350 (0. is the best) with standard deviation 0.033.", "error": "", "parent_id": "93e30dcd-f996-4021-a7a6-fdaf7e268e69", "metadata": {"aucs": [0.6418235370186985, 0.5127561630044392, 0.5820376767895582], "final_y": [0.3134298421743149, 0.3924669604330884, 0.34427890995149746]}, "mutation_prompt": null}
{"id": "ffdd0fd2-6e60-43f7-967b-ad18119f4f9b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HierarchicalCooperativeOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def island_differential_evolution(self, func, bounds, islands=3, pop_size=10, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // (islands * pop_size)\n\n        populations = [np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim)) for _ in range(islands)]\n        scores = [np.empty(pop_size) for _ in range(islands)]\n\n        def periodic_enforcement(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += diff ** 2\n            return penalty\n\n        for island in range(islands):\n            for i in range(pop_size):\n                scores[island][i] = func(populations[island][i]) + periodic_enforcement(populations[island][i])\n                self.eval_count += 1\n\n        for _ in range(max_iter):\n            for island in range(islands):\n                for i in range(pop_size):\n                    if self.eval_count >= self.budget:\n                        break\n\n                    indices = [idx for idx in range(pop_size) if idx != i]\n                    a, b, c = populations[island][np.random.choice(indices, 3, replace=False)]\n                    mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                    \n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    \n                    trial = np.where(cross_points, mutant, populations[island][i])\n                    trial_score = func(trial) + periodic_enforcement(trial)\n                    self.eval_count += 1\n\n                    if trial_score < scores[island][i]:\n                        populations[island][i] = trial\n                        scores[island][i] = trial_score\n\n            # Cooperative phase\n            best_individuals = [pop[np.argmin(sc)] for pop, sc in zip(populations, scores)]\n            cooperative_trial = np.mean(best_individuals, axis=0)\n            cooperative_score = func(cooperative_trial) + periodic_enforcement(cooperative_trial)\n            self.eval_count += 1\n\n            for island in range(islands):\n                if cooperative_score < np.min(scores[island]):\n                    worst_idx = np.argmax(scores[island])\n                    populations[island][worst_idx] = cooperative_trial\n                    scores[island][worst_idx] = cooperative_score\n\n        best_island = np.argmin([np.min(sc) for sc in scores])\n        best_idx = np.argmin(scores[best_island])\n        return populations[best_island][best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.island_differential_evolution(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B',\n                           bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "HierarchicalCooperativeOptimizer", "description": "A hierarchical exploration-exploitation algorithm integrating island-based differential evolution and cooperative coevolutionary strategy to enhance diversity and modular design capability in multilayer photonic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.5361193037456137, "feedback": "The algorithm HierarchicalCooperativeOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.536 with standard deviation 0.043. And the mean value of best solutions found was 0.380 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "93e30dcd-f996-4021-a7a6-fdaf7e268e69", "metadata": {"aucs": [0.4787518092881241, 0.5462160944773728, 0.5833900074713443], "final_y": [0.42034254569894247, 0.371909391777694, 0.34818347535177785]}, "mutation_prompt": null}
{"id": "d33fb585-e911-4dd3-b551-c53a76c2600f", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass PSOBraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def pso(self, func, bounds, swarm_size=30, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // swarm_size\n        \n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        lb, ub = bounds.lb, bounds.ub\n        positions = np.random.uniform(lb, ub, (swarm_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (swarm_size, self.dim))\n        personal_best_positions = np.copy(positions)\n        personal_best_scores = np.array([func(p) + periodic_penalty(p) for p in positions])\n        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n\n        inertia_weight = 0.5\n        cognitive_constant = 1.5\n        social_constant = 2.0\n\n        for _ in range(max_iter):\n            for i in range(swarm_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                velocities[i] = (inertia_weight * velocities[i]\n                                 + cognitive_constant * r1 * (personal_best_positions[i] - positions[i])\n                                 + social_constant * r2 * (global_best_position - positions[i]))\n                positions[i] = np.clip(positions[i] + velocities[i], lb, ub)\n\n                score = func(positions[i]) + periodic_penalty(positions[i])\n                self.eval_count += 1\n\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = positions[i]\n\n            global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n\n        return global_best_position\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.pso(func, bounds)\n        \n        if self.eval_count < self.budget:\n            cma_options = {'bounds': [bounds.lb, bounds.ub], 'verbose': -9, 'maxfevals': self.budget - self.eval_count}\n            es = CMAEvolutionStrategy(best_solution, 0.5, cma_options)\n            while not es.stop():\n                solutions = es.ask()\n                es.tell(solutions, [func(s) for s in solutions])\n                self.eval_count += len(solutions)\n            best_solution = es.result.xbest\n\n        return best_solution", "name": "PSOBraggMirrorOptimizer", "description": "This algorithm combines a Particle Swarm Optimization (PSO) approach with a periodicity-enforcing mechanism and a local search refinement using CMA-ES for optimizing multilayer structures.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "237ad1bf-cebf-4523-8a87-6c47a00aea8e", "metadata": {}, "mutation_prompt": null}
{"id": "45aa9bfa-ef29-419c-a9b0-ef0aabfb8540", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.3) ** 2  # Adjusted penalty mechanism\n            return penalty\n\n        # Quasi-oppositional initialization\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_opposite_population = bounds.lb + bounds.ub - population\n        population = np.vstack((population, quasi_opposite_population))\n        population = population[:pop_size]  # Keep population size constant\n        scores = np.empty(pop_size)\n        \n        for i in range(pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores)\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "The algorithm optimizes multilayer structures using Differential Evolution with quasi-oppositional population initialization, enhanced periodicity via a penalty mechanism, and BFGS for final refinement.", "configspace": "", "generation": 2, "fitness": 0.6308388817589157, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.631 with standard deviation 0.072. And the mean value of best solutions found was 0.316 (0. is the best) with standard deviation 0.047.", "error": "", "parent_id": "237ad1bf-cebf-4523-8a87-6c47a00aea8e", "metadata": {"aucs": [0.5296889052598271, 0.6862776261182197, 0.6765501138987002], "final_y": [0.38167549823931923, 0.28245825211074405, 0.28271076776377646]}, "mutation_prompt": null}
{"id": "05bfa362-aebe-44b9-b2ad-56785a45d4b4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CoEvolutionaryBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // (2 * pop_size)\n\n        def repair_periodicity(x):\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                x[i] = x[i-1] + np.round(diff / 0.2) * 0.2\n            return np.clip(x, bounds.lb, bounds.ub)\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        opposite_population = bounds.ub + bounds.lb - population\n        scores = np.empty(pop_size)\n\n        for i in range(pop_size):\n            scores[i] = func(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial = repair_periodicity(trial)\n                trial_score = func(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            # Opposite population update\n            opposite_population = bounds.ub + bounds.lb - population\n            for i in range(pop_size):\n                opposite_score = func(opposite_population[i])\n                self.eval_count += 1\n                if opposite_score < scores[i]:\n                    population[i] = opposite_population[i]\n                    scores[i] = opposite_score\n\n        best_idx = np.argmin(scores)\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "CoEvolutionaryBraggOptimizer", "description": "The algorithm employs a co-evolutionary strategy with dual populations - one focusing on exploration via Quasi-Oppositional DE and the other on exploitation using a customized periodic repair mechanism, followed by local search with BFGS for refinement.", "configspace": "", "generation": 2, "fitness": 0.4763851874209093, "feedback": "The algorithm CoEvolutionaryBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.476 with standard deviation 0.119. And the mean value of best solutions found was 0.432 (0. is the best) with standard deviation 0.087.", "error": "", "parent_id": "237ad1bf-cebf-4523-8a87-6c47a00aea8e", "metadata": {"aucs": [0.6408544415010556, 0.36359203406564744, 0.4247090866960248], "final_y": [0.3134298421743149, 0.5192011663143732, 0.46469205749927356]}, "mutation_prompt": null}
{"id": "4476cd03-a30f-4039-af8c-41076b307c5e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "The algorithm integrates Quasi-Oppositional Differential Evolution with a dynamic penalty mechanism for periodicity, complemented by Nelder-Mead for local search refinement.", "configspace": "", "generation": 2, "fitness": 0.6732611169813119, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.673 with standard deviation 0.009. And the mean value of best solutions found was 0.291 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "237ad1bf-cebf-4523-8a87-6c47a00aea8e", "metadata": {"aucs": [0.6850313764497342, 0.6717180534525735, 0.6630339210416278], "final_y": [0.28117237983741783, 0.2928186348033651, 0.2983586840689728]}, "mutation_prompt": null}
{"id": "98d0d016-5881-4082-8de4-38890b92e648", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n        \n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2  # Adjusted penalty mechanism\n            return penalty\n\n        # Change: Symmetric initialization for improved exploration\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size // 2, self.dim))\n        population = np.vstack([population, 2 * (bounds.lb + bounds.ub) / 2 - population])\n        \n        scores = np.empty(pop_size)\n        \n        for i in range(pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores)\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.differential_evolution(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "The algorithm optimizes multilayer structures using Differential Evolution with enhanced periodicity via an improved penalty mechanism, refined by a minor tweak to population initialization symmetry, followed by BFGS for refinement.", "configspace": "", "generation": 2, "fitness": 0.4603186762953164, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.460 with standard deviation 0.026. And the mean value of best solutions found was 0.435 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "237ad1bf-cebf-4523-8a87-6c47a00aea8e", "metadata": {"aucs": [0.48372155530307936, 0.4725253868868451, 0.4247090866960248], "final_y": [0.4153822420030788, 0.4250121777016216, 0.46469205749927356]}, "mutation_prompt": null}
{"id": "765cacd4-9c3d-4d77-9b58-048798c842c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.8, CR=0.9, max_iter=None):  # Changed F from 0.5 to 0.8\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "The algorithm improves performance by adjusting the mutation strategy of Differential Evolution to increase exploration potential.", "configspace": "", "generation": 3, "fitness": 0.6487120359850201, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.649 with standard deviation 0.086. And the mean value of best solutions found was 0.294 (0. is the best) with standard deviation 0.065.", "error": "", "parent_id": "4476cd03-a30f-4039-af8c-41076b307c5e", "metadata": {"aucs": [0.7367356308275633, 0.531533778587429, 0.6778666985400683], "final_y": [0.2226479492573965, 0.37959327743466864, 0.27853650872574187]}, "mutation_prompt": null}
{"id": "1e20be2c-68c0-4800-a00b-7c93127d3784", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.95, max_iter=None):  # Changed CR from 0.9 to 0.95\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced the exploration by increasing the crossover probability in the Quasi-Oppositional Differential Evolution phase.", "configspace": "", "generation": 3, "fitness": 0.6528257353236576, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.653 with standard deviation 0.071. And the mean value of best solutions found was 0.306 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "4476cd03-a30f-4039-af8c-41076b307c5e", "metadata": {"aucs": [0.7326961386268335, 0.5606191266264484, 0.6651619407176912], "final_y": [0.26205350761196755, 0.3629234453609149, 0.2944942128688006]}, "mutation_prompt": null}
{"id": "25a7e89b-6868-4d55-ba61-67556189206a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty * (self.eval_count / self.budget + 1)  # Adaptive penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        diversity_threshold = np.std(population, axis=0).mean()  # Diversity metric\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i] or np.std(trial) < diversity_threshold:  # Diversity-driven selection\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "The algorithm refines BraggMirrorOptimizer by leveraging a diversity-driven selection mechanism and adaptive penalty for enhancing convergence towards periodic solutions.", "configspace": "", "generation": 3, "fitness": 0.6185041939085897, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.619 with standard deviation 0.035. And the mean value of best solutions found was 0.319 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "4476cd03-a30f-4039-af8c-41076b307c5e", "metadata": {"aucs": [0.6686991323832225, 0.5932608336881497, 0.5935526156543967], "final_y": [0.2973392312727732, 0.33697428363070936, 0.3240053500332427]}, "mutation_prompt": null}
{"id": "5066c020-1da2-43b2-b32f-92b50628add2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty * 0.5  # Adjusted scaling for penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            CR = 0.9 - (0.5 * (_ / max_iter))  # Adaptive crossover rate\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhancing exploration by introducing adaptive crossover rates and periodic penalty scaling in Quasi-Oppositional DE.", "configspace": "", "generation": 3, "fitness": 0.5977202823401256, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.598 with standard deviation 0.027. And the mean value of best solutions found was 0.326 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "4476cd03-a30f-4039-af8c-41076b307c5e", "metadata": {"aucs": [0.6146620087404376, 0.5595552408997605, 0.6189435973801789], "final_y": [0.32398831355673996, 0.3471138504475064, 0.3076911066424868]}, "mutation_prompt": null}
{"id": "6b9b721e-07c0-4fde-928b-6c2cdd483479", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for iter in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Dynamic scaling factor\n                F_dynamic = 0.5 + 0.5 * (iter / max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='BFGS', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxiter': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "The algorithm improves global exploration by implementing dynamic scaling factors in Differential Evolution and enhances local refinement using a hybrid Nelder-Mead and BFGS strategy.", "configspace": "", "generation": 3, "fitness": 0.6193920562949896, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.619 with standard deviation 0.041. And the mean value of best solutions found was 0.322 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "4476cd03-a30f-4039-af8c-41076b307c5e", "metadata": {"aucs": [0.6013201590144853, 0.5813275451187878, 0.6755284647516955], "final_y": [0.3360041453617595, 0.34414161639998486, 0.286968517980964]}, "mutation_prompt": null}
{"id": "ec826823-599b-4257-83dd-fdf14afd4153", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveBraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def adaptive_de(self, func, bounds, pop_size=20, F_min=0.4, F_max=0.9, CR_min=0.5, CR_max=1.0, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        scores = np.array([func(ind) + periodic_penalty(ind) for ind in population])\n        self.eval_count += pop_size\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                F = F_min + (F_max - F_min) * (gen / max_iter)\n                CR = CR_min + (CR_max - CR_min) * (gen / max_iter)\n\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores)\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "AdaptiveBraggMirrorOptimizer", "description": "Introduced adaptive differential evolution with a self-adjusting crossover probability and mutation factor, incorporating periodicity constraints, to enhance exploration and convergence.", "configspace": "", "generation": 4, "fitness": 0.6311676748197036, "feedback": "The algorithm AdaptiveBraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.631 with standard deviation 0.055. And the mean value of best solutions found was 0.310 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "1e20be2c-68c0-4800-a00b-7c93127d3784", "metadata": {"aucs": [0.5540815276508465, 0.6602516624402788, 0.6791698343679855], "final_y": [0.344756281345575, 0.29340142211415954, 0.29271227223017615]}, "mutation_prompt": null}
{"id": "b87062ab-321a-45bf-9eea-305fe19c22d9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.4, CR=0.95, max_iter=None):  # Changed F from 0.5 to 0.4\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced selection pressure in Quasi-Oppositional Differential Evolution by reducing the mutation factor from 0.5 to 0.4.", "configspace": "", "generation": 4, "fitness": 0.6268496489160168, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.627 with standard deviation 0.034. And the mean value of best solutions found was 0.297 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "1e20be2c-68c0-4800-a00b-7c93127d3784", "metadata": {"aucs": [0.6744616947865032, 0.5993297068735917, 0.6067575450879555], "final_y": [0.27466748928714746, 0.3188705173837958, 0.29667455602447124]}, "mutation_prompt": null}
{"id": "3f739e4b-e54e-468d-a40a-a038fc08f566", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.95, max_iter=None):  # Changed CR from 0.9 to 0.95\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])  # Changed calculation of diff to abs\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced exploitation by combining quasi-oppositional DE with a simple periodicity penalty function to encourage periodic solutions.", "configspace": "", "generation": 4, "fitness": 0.6919817837560212, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.692 with standard deviation 0.081. And the mean value of best solutions found was 0.271 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "1e20be2c-68c0-4800-a00b-7c93127d3784", "metadata": {"aucs": [0.8020370857496423, 0.6107504310849994, 0.6631578344334217], "final_y": [0.21984335684257927, 0.30041619080127413, 0.2941635055048353]}, "mutation_prompt": null}
{"id": "c2382cfb-73fb-4ea9-bbc1-b78a6d035425", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.95, max_iter=None):  # Changed CR from 0.9 to 0.95\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        # Replace uniform initialization with chaotic map initialization\n        def logistic_map(i, r=3.99):\n            return r * i * (1 - i)\n        \n        population = np.random.rand(pop_size, self.dim)\n        for i in range(1, pop_size):\n            population[i] = logistic_map(population[i-1])\n        population = bounds.lb + (population * (bounds.ub - bounds.lb))\n        \n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Introduce dynamic scaling factor F\n                F_dynamic = np.random.uniform(0.4, 0.9)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Integrate a diversity enhancement strategy using chaotic initialization and dynamic scaling in Differential Evolution. ", "configspace": "", "generation": 4, "fitness": 0.613810085063707, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.614 with standard deviation 0.032. And the mean value of best solutions found was 0.291 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "1e20be2c-68c0-4800-a00b-7c93127d3784", "metadata": {"aucs": [0.6467643196466812, 0.5697087883826233, 0.6249571471618165], "final_y": [0.2971570253331569, 0.25808539638834627, 0.31872243842358416]}, "mutation_prompt": null}
{"id": "e70382d3-346d-46d0-9bd0-136b89881f22", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.95, max_iter=None):  # Changed CR from 0.9 to 0.95\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='BFGS',  # Switched from Nelder-Mead to BFGS\n                           options={'maxiter': self.budget - self.eval_count})  # Changed maxfev to maxiter\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved local search by switching from Nelder-Mead to BFGS for more efficient fine-tuning.", "configspace": "", "generation": 4, "fitness": 0.6677507983510867, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.668 with standard deviation 0.051. And the mean value of best solutions found was 0.282 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "1e20be2c-68c0-4800-a00b-7c93127d3784", "metadata": {"aucs": [0.704645996726992, 0.5961421053536847, 0.7024642929725833], "final_y": [0.2643849939769869, 0.3250672837234808, 0.2557356282551324]}, "mutation_prompt": null}
{"id": "9cd26cd7-6f07-49c3-a5f9-58e4bb7d7389", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.95, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = (x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])  # Removed abs to refine penalty\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                CR = max(0.1, CR - 0.001)  # Added adaptive crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved exploitation by refining penalty computation and utilizing adaptive crossover rate in quasi-oppositional DE.", "configspace": "", "generation": 5, "fitness": 0.7719474724045199, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.115. And the mean value of best solutions found was 0.230 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "3f739e4b-e54e-468d-a40a-a038fc08f566", "metadata": {"aucs": [0.6418235370186985, 0.7519942253306684, 0.9220246548641927], "final_y": [0.3134298421743149, 0.20388495969388287, 0.17291564439518226]}, "mutation_prompt": null}
{"id": "70d1a385-c900-4514-8033-14fd888d372e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=30, F=0.5, CR=0.95, max_iter=None):  # Increased pop_size from 20 to 30\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved exploration by increasing the population size in the quasi-oppositional DE to enhance the search space coverage.", "configspace": "", "generation": 5, "fitness": 0.7253596938703352, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.725 with standard deviation 0.092. And the mean value of best solutions found was 0.232 (0. is the best) with standard deviation 0.041.", "error": "", "parent_id": "3f739e4b-e54e-468d-a40a-a038fc08f566", "metadata": {"aucs": [0.8358626628382471, 0.7284824685046971, 0.6117339502680614], "final_y": [0.17343178477532595, 0.25746638224622875, 0.2646824455424439]}, "mutation_prompt": null}
{"id": "fa769456-4dac-4f90-ae98-428cd34cb4d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.95, max_iter=None):  # Changed CR from 0.9 to 0.95\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            weight = 0.5  # Added a weight factor to the penalty\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])  # Changed calculation of diff to abs\n                penalty += weight * (diff - 0.2) ** 2  # Updated penalty calculation with the weight factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduced an additional periodicity penalty weight and refined the periodic penalty calculation for improved performance. ", "configspace": "", "generation": 5, "fitness": 0.6935431302923383, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.694 with standard deviation 0.066. And the mean value of best solutions found was 0.272 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "3f739e4b-e54e-468d-a40a-a038fc08f566", "metadata": {"aucs": [0.7831304981307072, 0.624628526089458, 0.6728703666568495], "final_y": [0.2226479492573965, 0.3072619798813978, 0.28692113147403264]}, "mutation_prompt": null}
{"id": "e020b911-31f4-4d2c-b395-c2b10f8b129e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.95, max_iter=None):  # Changed CR from 0.9 to 0.95\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])  # Changed calculation of diff to abs\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Changed loop variable from '_' to 'gen'\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_adaptive = 0.3 + 0.7 * np.exp(-gen / max_iter)  # Added adaptive factor for F based on generation\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration by integrating Adaptive Differential Evolution with periodicity penalties in the BraggMirrorOptimizer.", "configspace": "", "generation": 5, "fitness": 0.7408024319392291, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.741 with standard deviation 0.048. And the mean value of best solutions found was 0.214 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "3f739e4b-e54e-468d-a40a-a038fc08f566", "metadata": {"aucs": [0.6734884744492355, 0.7747345404568127, 0.774184280911639], "final_y": [0.2852278220117572, 0.1837521943198649, 0.17330217769681444]}, "mutation_prompt": null}
{"id": "8aa6a014-caff-41a2-8b5d-ad71385e2bc5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.98, max_iter=None):  # Changed CR from 0.95 to 0.98\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved global exploration by increasing the crossover rate (CR) to enhance solution diversity and convergence speed.", "configspace": "", "generation": 5, "fitness": 0.7832557644915646, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.046. And the mean value of best solutions found was 0.211 (0. is the best) with standard deviation 0.032.", "error": "", "parent_id": "3f739e4b-e54e-468d-a40a-a038fc08f566", "metadata": {"aucs": [0.8404867017509399, 0.7817243572032395, 0.7275562345205145], "final_y": [0.2010745070424763, 0.1779439162891554, 0.2538651913466494]}, "mutation_prompt": null}
{"id": "a04a2268-edea-40e5-ac42-8705581c8a45", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.6, CR=0.98, max_iter=None):  # Changed F from 0.5 to 0.6\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration by adjusting Differential Evolution scaling factor (F) for improved diversity.", "configspace": "", "generation": 6, "fitness": 0.7153894241590905, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.715 with standard deviation 0.075. And the mean value of best solutions found was 0.264 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "8aa6a014-caff-41a2-8b5d-ad71385e2bc5", "metadata": {"aucs": [0.6443215111016553, 0.8183021306006255, 0.6835446307749904], "final_y": [0.3118786615075325, 0.19448101184397582, 0.28544052678819976]}, "mutation_prompt": null}
{"id": "eaa25d24-7110-4491-b43b-43d5703a9f0f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  # Changed F from 0.5 to 0.45\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced local exploration by reducing mutation factor (F) to improve fine-tuning of solutions.", "configspace": "", "generation": 6, "fitness": 0.7882985973673738, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.068. And the mean value of best solutions found was 0.212 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": "8aa6a014-caff-41a2-8b5d-ad71385e2bc5", "metadata": {"aucs": [0.8747299921903531, 0.7089754590464526, 0.7811903408653157], "final_y": [0.1728310998535748, 0.26366577595269036, 0.1980977297124269]}, "mutation_prompt": null}
{"id": "d139627e-705d-4649-b542-b58c74adaec7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.98, max_iter=None):  # Changed CR from 0.95 to 0.98\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.5 + 0.3 * np.random.rand()  # Adjust F dynamically\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Optimized population diversity by modifying mutation strategy to enhance convergence reliability.", "configspace": "", "generation": 6, "fitness": 0.7828319578279332, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.091. And the mean value of best solutions found was 0.199 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "8aa6a014-caff-41a2-8b5d-ad71385e2bc5", "metadata": {"aucs": [0.9093864259615096, 0.7017418645245761, 0.737367582997714], "final_y": [0.17334307224748313, 0.22512508789719998, 0.19859409425063546]}, "mutation_prompt": null}
{"id": "a2b42460-4804-4e32-b2d1-8306593ed35a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.98, max_iter=None):  # Changed CR from 0.95 to 0.98\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            diversity = np.std(population, axis=0).mean()  # Added line for diversity calculation\n            CR = 0.9 + 0.1 * diversity  # Adjusted CR based on population diversity\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.4 + 0.2 * diversity  # Adjusted F based on population diversity\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved exploration by introducing dynamic crossover rate (CR) and adaptive mutation factor (F) based on diversity.", "configspace": "", "generation": 6, "fitness": 0.7483782214963061, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.748 with standard deviation 0.096. And the mean value of best solutions found was 0.250 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "8aa6a014-caff-41a2-8b5d-ad71385e2bc5", "metadata": {"aucs": [0.6418235370186985, 0.7278958533207793, 0.8754152741494405], "final_y": [0.3134298421743149, 0.26034073542842195, 0.17685836255272747]}, "mutation_prompt": null}
{"id": "02b3fb81-26ac-4c27-9982-1f5badf9ddee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.5, CR=0.98, max_iter=None):  # Changed CR from 0.95 to 0.98\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def periodic_cost(x):  # New function for periodicity-based cost\n            return np.sum(np.sin(2 * np.pi * x / (bounds.ub[0] - bounds.lb[0])) ** 2)\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial) + periodic_cost(trial)  # Included periodic_cost\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(lambda x: func(x) + periodic_penalty(x), best_solution,  # Included periodic_penalty in minimize\n                           method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced local exploitation by integrating a periodicity-based cost function to refine solutions near local optima.", "configspace": "", "generation": 6, "fitness": 0.7514184256789228, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.751 with standard deviation 0.101. And the mean value of best solutions found was 0.246 (0. is the best) with standard deviation 0.048.", "error": "", "parent_id": "8aa6a014-caff-41a2-8b5d-ad71385e2bc5", "metadata": {"aucs": [0.8080091889873474, 0.6100086063701802, 0.8362374816792408], "final_y": [0.2066181277717113, 0.313059178354551, 0.2169717402745891]}, "mutation_prompt": null}
{"id": "a61a2120-ab84-44e4-ba77-b1be69754451", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for iteration in range(max_iter):\n            dynamic_F = 0.45 + 0.1 * np.sin(np.pi * iteration / max_iter)  # Add dynamic adaptation to F\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + dynamic_F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration by adding a dynamic adaptive mutation factor in the differential evolution process.", "configspace": "", "generation": 7, "fitness": 0.7785153030692108, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.779 with standard deviation 0.062. And the mean value of best solutions found was 0.217 (0. is the best) with standard deviation 0.035.", "error": "", "parent_id": "eaa25d24-7110-4491-b43b-43d5703a9f0f", "metadata": {"aucs": [0.8014001582067795, 0.6936826171824548, 0.8404631338183981], "final_y": [0.1813293412660415, 0.26385325148973804, 0.2059123096143476]}, "mutation_prompt": null}
{"id": "0d84849b-6afb-4d71-ab76-c31616884043", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Adaptive CR adjustment\n                cross_points = np.random.rand(self.dim) < (CR + 0.02 * np.random.randn())\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Incorporate adaptive crossover rate based on feedback mechanism to enhance exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.8579218126199741, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.063. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "eaa25d24-7110-4491-b43b-43d5703a9f0f", "metadata": {"aucs": [0.8201762463081792, 0.94680190993789, 0.806787281613853], "final_y": [0.18204461832711327, 0.17490106335555167, 0.18813515588837193]}, "mutation_prompt": null}
{"id": "2c9605cf-67f8-46ae-9665-37aa34ac3e71", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.48, CR=0.98, max_iter=None):  # Changed F from 0.45 to 0.48\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += np.sin(diff * np.pi) ** 2  # Modified penalty to use sine function\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Slightly increased mutation factor (F) and added a sinusoidal periodic penalty to enhance exploration near local optima.", "configspace": "", "generation": 7, "fitness": 0.7691671795290684, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.067. And the mean value of best solutions found was 0.225 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "eaa25d24-7110-4491-b43b-43d5703a9f0f", "metadata": {"aucs": [0.6955385793245713, 0.8575625822304795, 0.7544003770321541], "final_y": [0.2601488588191776, 0.17392977327237724, 0.2414746880885963]}, "mutation_prompt": null}
{"id": "b6dc1a89-9b87-45af-875a-8660a677db80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            F = 0.4 + 0.3 * np.sin(np.pi * gen / max_iter)  # Adaptive mutation\n            CR = 0.9 - 0.4 * np.sin(np.pi * gen / max_iter)  # Adaptive crossover\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduced adaptive mutation and crossover rates to enhance the exploration-exploitation balance in the optimization process.", "configspace": "", "generation": 7, "fitness": 0.6952982099193812, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.079. And the mean value of best solutions found was 0.271 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "eaa25d24-7110-4491-b43b-43d5703a9f0f", "metadata": {"aucs": [0.8053379742592872, 0.656346403376624, 0.6242102521222324], "final_y": [0.21957638298948, 0.3046374661166267, 0.287756838788531]}, "mutation_prompt": null}
{"id": "fb13ff91-1825-47bf-bb7c-ebc3039fa28b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F_init=0.5, CR=0.98, max_iter=None):  # Adaptive F\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        F = F_init\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    F = min(F + 0.01, 1)  # Increase F if successful\n                else:\n                    F = max(F - 0.01, 0.1)  # Decrease F if not successful\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],  # Changed method\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive mutation factor and enhance local search using gradient-based refinement techniques.", "configspace": "", "generation": 7, "fitness": 0.7862947083513744, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.116. And the mean value of best solutions found was 0.239 (0. is the best) with standard deviation 0.057.", "error": "", "parent_id": "eaa25d24-7110-4491-b43b-43d5703a9f0f", "metadata": {"aucs": [0.8756988608398357, 0.8605157520547626, 0.6226695121595247], "final_y": [0.2005407674454336, 0.19697665702440714, 0.3201798991607383]}, "mutation_prompt": null}
{"id": "6635ce1a-03c5-4c3e-98b6-f1d45aa5cd7f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Dynamic scaling factor\n                F_dynamic = 0.2 + 0.3 * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < (CR + 0.02 * np.random.randn())\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Utilize a dynamic scaling factor for differential evolution to enhance solution diversity and convergence speed.", "configspace": "", "generation": 8, "fitness": 0.8116750759827357, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.081. And the mean value of best solutions found was 0.212 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "0d84849b-6afb-4d71-ab76-c31616884043", "metadata": {"aucs": [0.8458167816975498, 0.7001805187559806, 0.8890279274946766], "final_y": [0.19569582854948808, 0.26325288127698765, 0.1778991646295377]}, "mutation_prompt": null}
{"id": "cbf9c1b4-4d11-4bc7-a198-6965e38a0a91", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Adaptive CR adjustment\n                cross_points = np.random.rand(self.dim) < (CR + 0.02 * np.random.randn())\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                # Change: Encourage periodicity\n                trial_score = func(trial) + periodic_penalty(trial) + 0.1 * np.sin(np.pi * np.sum(trial))\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce periodicity encouragement through sinusoidal modulation in scoring to enhance constructive interference in design.", "configspace": "", "generation": 8, "fitness": 0.6954284735391697, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.093. And the mean value of best solutions found was 0.265 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "0d84849b-6afb-4d71-ab76-c31616884043", "metadata": {"aucs": [0.8122941618971267, 0.6889763209966395, 0.585014937723743], "final_y": [0.19892726654966886, 0.27557222080618227, 0.3214056734989359]}, "mutation_prompt": null}
{"id": "a2f9c679-8be4-4093-93db-1a8be0405109", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        # Change: Adjust population initialization strategy\n        population = np.random.normal(loc=(bounds.lb + bounds.ub) / 2, scale=(bounds.ub - bounds.lb) / 5, size=(pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Adaptive CR adjustment\n                cross_points = np.random.rand(self.dim) < (CR + 0.02 * np.random.randn())\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance exploration by adjusting the population initialization strategy using a Gaussian distribution centered around known optimal solutions.", "configspace": "", "generation": 8, "fitness": 0.8000864102027951, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.091. And the mean value of best solutions found was 0.217 (0. is the best) with standard deviation 0.048.", "error": "", "parent_id": "0d84849b-6afb-4d71-ab76-c31616884043", "metadata": {"aucs": [0.8706373412192872, 0.6719686281335759, 0.8576532612555222], "final_y": [0.1940420048084036, 0.2837147379660603, 0.17426645026916787]}, "mutation_prompt": null}
{"id": "5463da28-0e7d-4c8b-9f86-28c0f66f5569", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for iteration in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Change: Adaptive scaling factor F adjustment\n                F_iter = 0.45 * (1 - iteration / max_iter) + 0.1 * (iteration / max_iter)\n                \n                mutant = np.clip(a + F_iter * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < (CR + 0.02 * np.random.randn())\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive scaling factor F based on iteration progression to improve convergence rate.", "configspace": "", "generation": 8, "fitness": 0.7412275012693273, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.741 with standard deviation 0.115. And the mean value of best solutions found was 0.254 (0. is the best) with standard deviation 0.067.", "error": "", "parent_id": "0d84849b-6afb-4d71-ab76-c31616884043", "metadata": {"aucs": [0.842499894635625, 0.8008893587681529, 0.5802932504042042], "final_y": [0.1921776544854994, 0.22213657546935295, 0.34690222195661236]}, "mutation_prompt": null}
{"id": "b7fa9740-153d-4954-9f73-d255e29829c9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def surrogate_assisted_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        scores = np.empty(pop_size)\n        for i in range(pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            gp.fit(population, scores)\n            surrogate_scores = gp.predict(population)\n\n            for i in range(pop_size):\n                indices = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                \n                if surrogate_scores[i] > scores[i]:\n                    trial = mutant\n                else:\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, population[i])\n\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores)\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.surrogate_assisted_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce surrogate model-assisted DE with dynamic population size to enhance convergence and exploit promising areas.", "configspace": "", "generation": 8, "fitness": 0.7895594962751696, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.136. And the mean value of best solutions found was 0.240 (0. is the best) with standard deviation 0.057.", "error": "", "parent_id": "0d84849b-6afb-4d71-ab76-c31616884043", "metadata": {"aucs": [0.9602133558399384, 0.7807168664478549, 0.6277482665377156], "final_y": [0.17116635758114573, 0.23801291296720317, 0.3102166711655826]}, "mutation_prompt": null}
{"id": "f9ec9683-41b0-46d9-839e-f541c4c5b0ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.2 + 0.3 * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Adaptive crossover probability\n                CR_adaptive = 0.9 - (0.7 * np.random.rand())\n                cross_points = np.random.rand(self.dim) < (CR_adaptive + 0.02 * np.random.randn())\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive crossover probability in Differential Evolution to enhance the balance between exploration and exploitation.", "configspace": "", "generation": 9, "fitness": 0.8209786008328291, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.068. And the mean value of best solutions found was 0.190 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "6635ce1a-03c5-4c3e-98b6-f1d45aa5cd7f", "metadata": {"aucs": [0.854723340110598, 0.7258879821485831, 0.8823244802393064], "final_y": [0.18368397681620763, 0.21095090386110316, 0.17523047865374652]}, "mutation_prompt": null}
{"id": "60778214-de87-4390-989b-21eceb1d6a00", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Enhanced mutation strategy\n                F_dynamic = 0.3 + 0.4 * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Dynamic crossover probability\n                CR_dynamic = CR + 0.05 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Use enhanced mutation strategy with dynamic crossover probability to bolster diversity and convergence in differential evolution.", "configspace": "", "generation": 9, "fitness": 0.8392166325607345, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.020. And the mean value of best solutions found was 0.187 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "6635ce1a-03c5-4c3e-98b6-f1d45aa5cd7f", "metadata": {"aucs": [0.8661875292717156, 0.8166087588473312, 0.8348536095631571], "final_y": [0.1929803445801308, 0.18102431786100448, 0.18696439932061149]}, "mutation_prompt": null}
{"id": "8c52d235-5dd0-41da-8bcc-bcfa60dae0cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Changed to use `gen` variable\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Dynamic scaling factor\n                F_dynamic = 0.2 + 0.3 * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Adaptive crossover rate\n                CR_dynamic = CR * (1 - gen / max_iter)\n                cross_points = np.random.rand(self.dim) < (CR_dynamic + 0.02 * np.random.randn())\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce a small adaptive shift in the crossover rate (CR) based on generation count to balance exploration and exploitation.", "configspace": "", "generation": 9, "fitness": 0.8187645926849738, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.819 with standard deviation 0.029. And the mean value of best solutions found was 0.204 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "6635ce1a-03c5-4c3e-98b6-f1d45aa5cd7f", "metadata": {"aucs": [0.8590812834146081, 0.7923490215344722, 0.804863473105841], "final_y": [0.20195652940290432, 0.20588770713211124, 0.2046118773537472]}, "mutation_prompt": null}
{"id": "a9df2668-2d21-47bc-a7d6-d60c6ca2d22a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.2 + 0.3 * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Adaptive crossover rate\n                CR_adaptive = CR * (1 - (self.eval_count / self.budget))\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    # Change: Enhance local search by re-evaluating\n                    trial_score = func(trial) + periodic_penalty(trial)\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive crossover rates and local search enhancements to improve the convergence of differential evolution.", "configspace": "", "generation": 9, "fitness": 0.786574361328693, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.019. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "6635ce1a-03c5-4c3e-98b6-f1d45aa5cd7f", "metadata": {"aucs": [0.7601608173191319, 0.7993076726131811, 0.8002545940537656], "final_y": [0.24041936291177224, 0.21745659178544108, 0.20059806673686043]}, "mutation_prompt": null}
{"id": "993b9657-aec8-4e3f-9ec2-a540533e44ed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.2 + 0.3 * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Adaptive crossover probability\n                CR_dynamic = 0.5 + 0.3 * np.random.rand()\n                cross_points = np.random.rand(self.dim) < (CR_dynamic + 0.02 * np.random.randn())\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive crossover probability in DE to enhance exploration and exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.8129031873036777, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.023. And the mean value of best solutions found was 0.197 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6635ce1a-03c5-4c3e-98b6-f1d45aa5cd7f", "metadata": {"aucs": [0.841866641803287, 0.7862591594499354, 0.8105837606578108], "final_y": [0.19384785528787496, 0.20700057452028275, 0.18922894148428437]}, "mutation_prompt": null}
{"id": "6549447f-ecec-4c62-b9bb-97fa4a90c691", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def hybrid_pso(self, func, bounds, pop_size=20, w=0.5, c1=1.5, c2=1.5, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        # Particle Swarm Initialization\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (pop_size, self.dim))\n        personal_best = np.copy(population)\n        personal_best_scores = np.array([func(ind) for ind in population])\n        global_best_idx = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_idx]\n\n        self.eval_count += pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        for _ in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            \n            for i in range(pop_size):\n                # Update velocity and position\n                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n                velocities[i] = (\n                    w * velocities[i] +\n                    c1 * r1 * (personal_best[i] - population[i]) +\n                    c2 * r2 * (global_best - population[i])\n                )\n                population[i] = np.clip(population[i] + velocities[i], bounds.lb, bounds.ub)\n                \n                # Evaluate and apply penalty\n                score = func(population[i]) + periodic_penalty(population[i])\n                self.eval_count += 1\n\n                # Update personal best\n                if score < personal_best_scores[i]:\n                    personal_best[i] = population[i]\n                    personal_best_scores[i] = score\n\n                    # Update global best\n                    if score < personal_best_scores[global_best_idx]:\n                        global_best_idx = i\n                        global_best = personal_best[i]\n\n        return global_best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.hybrid_pso(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "HybridBraggOptimizer", "description": "Hybrid optimization approach combining Particle Swarm Optimization (PSO) with local search refinement to enhance convergence towards global optima while maintaining exploration capabilities.", "configspace": "", "generation": 10, "fitness": 0.7745126848194169, "feedback": "The algorithm HybridBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.029. And the mean value of best solutions found was 0.239 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "60778214-de87-4390-989b-21eceb1d6a00", "metadata": {"aucs": [0.8097297300087294, 0.7762093312176772, 0.7375989932318442], "final_y": [0.22667311859616213, 0.23315850233207047, 0.2585173037120664]}, "mutation_prompt": null}
{"id": "1cd0cf76-996d-4e48-ad0e-9a1fad218f41", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        # Change: Adaptive population size\n        pop_size = min(max(10, self.budget // (self.dim * 100)), pop_size)\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Enhanced mutation strategy\n                F_dynamic = 0.3 + 0.4 * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Dynamic crossover probability\n                CR_dynamic = CR + 0.05 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive population size for enhanced exploration-exploitation balance in quasi-oppositional differential evolution.", "configspace": "", "generation": 10, "fitness": 0.7805747047284984, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.034. And the mean value of best solutions found was 0.209 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "60778214-de87-4390-989b-21eceb1d6a00", "metadata": {"aucs": [0.8281497205151077, 0.7550399470416873, 0.7585344466287002], "final_y": [0.18187817677903595, 0.21180664320612108, 0.2323346768883695]}, "mutation_prompt": null}
{"id": "65704fb6-9a89-4b8a-a195-83dac2e09533", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR + 0.05 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce an adaptive scaling factor dynamically adjusted based on iteration progress to enhance diversity and convergence in Differential Evolution.", "configspace": "", "generation": 10, "fitness": 0.783680590784621, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.075. And the mean value of best solutions found was 0.223 (0. is the best) with standard deviation 0.045.", "error": "", "parent_id": "60778214-de87-4390-989b-21eceb1d6a00", "metadata": {"aucs": [0.8405060536184136, 0.8335088979535956, 0.6770268207818539], "final_y": [0.21353537909412945, 0.17344760703045048, 0.2826929229835633]}, "mutation_prompt": null}
{"id": "d0d78a2a-c5c6-45cd-93f5-ff453b10d66d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty * (self.eval_count / self.budget)  # Change: Adaptive penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Chaos theory-inspired mutation\n                F_dynamic = 0.3 + 0.4 * np.sin(2 * np.pi * np.random.rand())\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR + 0.05 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive penalty and chaos theory-inspired mutation to enhance exploration and exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.7761927034745871, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.014. And the mean value of best solutions found was 0.221 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "60778214-de87-4390-989b-21eceb1d6a00", "metadata": {"aucs": [0.7888732690663179, 0.7572582694647537, 0.7824465718926898], "final_y": [0.229646511602826, 0.21339985528571626, 0.22012820721713133]}, "mutation_prompt": null}
{"id": "56d6b5f9-0e4f-4361-9624-70aa5eff8f7c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty_weight = 0.1 + 0.4 * np.random.rand()  # Change: Adaptive penalty weight\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += penalty_weight * (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for _ in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Enhanced mutation strategy\n                F_dynamic = 0.3 + 0.4 * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Dynamic crossover probability\n                CR_dynamic = CR + 0.05 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive scaling factor and penalty weight to enhance convergence in differential evolution.", "configspace": "", "generation": 10, "fitness": 0.7641380300580051, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.029. And the mean value of best solutions found was 0.225 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "60778214-de87-4390-989b-21eceb1d6a00", "metadata": {"aucs": [0.7834978369192974, 0.7234853961869688, 0.7854308570677492], "final_y": [0.2226479492573965, 0.22824691969394706, 0.22457648550805276]}, "mutation_prompt": null}
{"id": "7c33ed2c-59be-4cef-a1c3-02ba197ce3d7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        # Dynamic opposition\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        if np.random.rand() < 0.5:  # Added dynamic opposition\n            quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n            population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Adaptive and increased crossover variability\n                CR_dynamic = CR + 0.1 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance the algorithm by introducing dynamic opposition in the initialization phase and adaptive crossover rates, further improving diversity and convergence.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 20 is out of bounds for axis 0 with size 20').", "error": "IndexError('index 20 is out of bounds for axis 0 with size 20')", "parent_id": "65704fb6-9a89-4b8a-a195-83dac2e09533", "metadata": {}, "mutation_prompt": null}
{"id": "915e3aad-ba3c-4de0-a156-cc71c636de3e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR + 0.05 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improve mutation diversity by introducing non-uniform mutation step size in Differential Evolution.", "configspace": "", "generation": 11, "fitness": 0.7756689292045286, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.069. And the mean value of best solutions found was 0.231 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "65704fb6-9a89-4b8a-a195-83dac2e09533", "metadata": {"aucs": [0.8416002540402241, 0.6801822138976938, 0.805224319675668], "final_y": [0.202683187392525, 0.26731534051111383, 0.22201401458524328]}, "mutation_prompt": null}
{"id": "15b6e856-d4aa-4596-b6f3-0d0b13507a37", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR + 0.05 * (1 - gen / max_iter)  # Change: Dynamic CR \n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Adjust the crossover rate dynamically based on iteration progress to enhance solution diversity in Differential Evolution.", "configspace": "", "generation": 11, "fitness": 0.773946750942105, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.060. And the mean value of best solutions found was 0.236 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "65704fb6-9a89-4b8a-a195-83dac2e09533", "metadata": {"aucs": [0.809887687472044, 0.6890133226065556, 0.8229392427477153], "final_y": [0.21402992059332426, 0.2684397569723823, 0.22422812165466643]}, "mutation_prompt": null}
{"id": "003ba21d-cb63-424b-a245-bacd10999ee1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = 0.7 + 0.3 * np.cos(np.pi * gen / max_iter)  # Changed line\n                cross_points = np.random.rand(self.dim) < CR_dynamic  # Changed line\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance exploration and exploitation by introducing a dynamic crossover rate and refining penalty functions.", "configspace": "", "generation": 11, "fitness": 0.7725796697311904, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.018. And the mean value of best solutions found was 0.216 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "65704fb6-9a89-4b8a-a195-83dac2e09533", "metadata": {"aucs": [0.7978743487289273, 0.7574459204182181, 0.7624187400464257], "final_y": [0.23121644865659285, 0.19452376384658876, 0.22297079280570775]}, "mutation_prompt": null}
{"id": "5cf5b91d-c03f-4138-972c-912122e6e8ef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        # Change: Introduce diversity with low-discrepancy sequence\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        sobol_seq = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim)) * 0.5\n        population = np.concatenate((population, sobol_seq), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR + 0.05 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance exploration by introducing a diversity-preserving component in the initialization phase for better global search.", "configspace": "", "generation": 11, "fitness": 0.7568359304260618, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.757 with standard deviation 0.057. And the mean value of best solutions found was 0.245 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "65704fb6-9a89-4b8a-a195-83dac2e09533", "metadata": {"aucs": [0.7844881634334693, 0.8086402856655055, 0.6773793421792107], "final_y": [0.236738599928212, 0.22507454363875556, 0.2726807746391874]}, "mutation_prompt": null}
{"id": "85a91529-c21b-4178-b5a0-b1820e45c1f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))  # Changed line\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Incorporate adaptive crossover strategy based on population diversity to enhance exploration in DE.", "configspace": "", "generation": 12, "fitness": 0.8146988606566743, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.023. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "915e3aad-ba3c-4de0-a156-cc71c636de3e", "metadata": {"aucs": [0.8471381620485698, 0.7946999388665025, 0.8022584810549506], "final_y": [0.19311175843661543, 0.22015786595111708, 0.17500364986556738]}, "mutation_prompt": null}
{"id": "49add74b-d21f-4b3e-bf29-2f7819f89ca7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                # Change: Adaptive crossover rate (CR) scaling\n                CR_dynamic = CR * (1 - gen / max_iter) + 0.1 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive crossover rate scaling based on generation progress to improve exploration and exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.7498387108841698, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.750 with standard deviation 0.063. And the mean value of best solutions found was 0.244 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "915e3aad-ba3c-4de0-a156-cc71c636de3e", "metadata": {"aucs": [0.8104743787252472, 0.6626966182844598, 0.7763451356428022], "final_y": [0.19726495814930167, 0.2995811440659143, 0.23569816677890532]}, "mutation_prompt": null}
{"id": "d5a5bd6f-4974-4abd-b41b-0923016bb54c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen):\n            scaling_factor = 1 - (gen / max_iter)\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty * scaling_factor\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0)\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR + 0.05 * np.random.randn()\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance trial solution evaluation by incorporating a dynamic periodic penalty scaling factor based on the generation count.", "configspace": "", "generation": 12, "fitness": 0.7609112929023997, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.761 with standard deviation 0.103. And the mean value of best solutions found was 0.246 (0. is the best) with standard deviation 0.048.", "error": "", "parent_id": "915e3aad-ba3c-4de0-a156-cc71c636de3e", "metadata": {"aucs": [0.8945752703409631, 0.7431193089888206, 0.6450392993774154], "final_y": [0.1851120243951857, 0.2518566943363214, 0.3020258859036328]}, "mutation_prompt": null}
{"id": "53be58e5-3887-44a3-a336-dfcb74aa4786", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (1 - gen / max_iter) + 0.05 * np.random.randn()  # Adjusted crossover rate\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance adaptability by introducing a dynamic crossover rate adjustment based on generation progress.", "configspace": "", "generation": 12, "fitness": 0.7844013814780704, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.014. And the mean value of best solutions found was 0.203 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "915e3aad-ba3c-4de0-a156-cc71c636de3e", "metadata": {"aucs": [0.7761378923037716, 0.8041532142884875, 0.7729130378419518], "final_y": [0.19154175049191213, 0.19871271455403228, 0.21784165301525849]}, "mutation_prompt": null}
{"id": "7afabccc-e92c-4eaa-aa9b-be36e98c0b96", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (1 - gen / max_iter) + 0.05 * np.random.randn()  # Updated line\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improve exploration by dynamically adjusting the crossover rate based on generation count.", "configspace": "", "generation": 12, "fitness": 0.7005917401820102, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.701 with standard deviation 0.093. And the mean value of best solutions found was 0.253 (0. is the best) with standard deviation 0.058.", "error": "", "parent_id": "915e3aad-ba3c-4de0-a156-cc71c636de3e", "metadata": {"aucs": [0.8287996359123915, 0.6117087524304996, 0.6612668322031396], "final_y": [0.17998803531596574, 0.3226838493415527, 0.25500050000847907]}, "mutation_prompt": null}
{"id": "ef0d6db6-39e3-42ef-bde8-faebfff196fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change: Adaptive mutation strategy\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))  # Changed line\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Incorporate an adaptive scaling factor in mutation to improve solution diversity in DE.", "configspace": "", "generation": 13, "fitness": 0.8003819662152769, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.052. And the mean value of best solutions found was 0.195 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "85a91529-c21b-4178-b5a0-b1820e45c1f4", "metadata": {"aucs": [0.8613062495626949, 0.8053206543679706, 0.7345189947151654], "final_y": [0.18292136324675723, 0.1890079705753941, 0.21254250095770277]}, "mutation_prompt": null}
{"id": "71afd02d-c76f-4c4e-a6f4-32ca714d1026", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPSOBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def dynamic_topology_pso(self, func, bounds, pop_size=30, omega=0.5, phip=1.5, phig=1.5, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return penalty\n        \n        # Initialize particle positions and velocities\n        x = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        v = np.random.uniform(-0.1, 0.1, (pop_size, self.dim))\n        pbest = x.copy()\n        pbest_scores = np.array([func(xi) + periodic_penalty(xi) for xi in x])\n        gbest_idx = np.argmin(pbest_scores)\n        gbest = pbest[gbest_idx].copy()\n        gbest_score = pbest_scores[gbest_idx]\n        self.eval_count = pop_size\n\n        for gen in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n            \n            # Dynamic topology adjustment\n            neighborhood_size = max(1, int(pop_size * (1 - gen / max_iter)))\n            for i in range(pop_size):\n                neighbors = np.random.choice(pop_size, neighborhood_size, replace=False)\n                local_best_idx = np.argmin(pbest_scores[neighbors])\n                local_best = pbest[neighbors[local_best_idx]]\n\n                # Update velocities and positions\n                r_p = np.random.rand(self.dim)\n                r_g = np.random.rand(self.dim)\n                v[i] = omega * v[i] + phip * r_p * (pbest[i] - x[i]) + phig * r_g * (local_best - x[i])\n                x[i] = np.clip(x[i] + v[i], bounds.lb, bounds.ub)\n\n                # Evaluate the new positions\n                score = func(x[i]) + periodic_penalty(x[i])\n                self.eval_count += 1\n\n                # Update personal best\n                if score < pbest_scores[i]:\n                    pbest[i] = x[i]\n                    pbest_scores[i] = score\n\n                    # Update global best\n                    if score < gbest_score:\n                        gbest = x[i]\n                        gbest_score = score\n\n        return gbest\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.dynamic_topology_pso(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='L-BFGS-B', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfun': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "HybridPSOBraggOptimizer", "description": "Hybrid Particle Swarm Optimization with Dynamic Topology Adjustment and Local Search for Enhanced Solution Refinement.", "configspace": "", "generation": 13, "fitness": 0.6984878228054953, "feedback": "The algorithm HybridPSOBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.698 with standard deviation 0.097. And the mean value of best solutions found was 0.283 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "85a91529-c21b-4178-b5a0-b1820e45c1f4", "metadata": {"aucs": [0.7659721195432351, 0.7684819896182684, 0.5610093592549819], "final_y": [0.2489147901895492, 0.23764821036251482, 0.36148719470313984]}, "mutation_prompt": null}
{"id": "3ac1024f-09fb-44b1-bc2f-ce40152119b4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.1) ** 2  # Changed line\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i])\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.5 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)  # Changed line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9)) \n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance diversity and adaptability in DE by incorporating quasi-oppositional learning and periodic penalty refinements.", "configspace": "", "generation": 13, "fitness": 0.7944229333430051, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.069. And the mean value of best solutions found was 0.220 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "85a91529-c21b-4178-b5a0-b1820e45c1f4", "metadata": {"aucs": [0.8918794352231362, 0.749162825275172, 0.7422265395307069], "final_y": [0.1776843279420811, 0.24963842364545752, 0.23286437402478766]}, "mutation_prompt": null}
{"id": "784715d2-51e6-4182-8a8f-7bcc440b24e1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.5 * (gen / max_iter)  # Added dynamic penalty factor\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor  # Adjust penalty with dynamic factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)  # Updated penalty call\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance DE by introducing a dynamic penalty factor for periodicity to better guide exploration.", "configspace": "", "generation": 13, "fitness": 0.8287928880371874, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.041. And the mean value of best solutions found was 0.195 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "85a91529-c21b-4178-b5a0-b1820e45c1f4", "metadata": {"aucs": [0.8579991967877557, 0.7707357613850001, 0.8576437059388067], "final_y": [0.19618718488052467, 0.21009376330605745, 0.17979944235369594]}, "mutation_prompt": null}
{"id": "4efd10d6-fcba-4c78-858b-e36ad4d2298e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, scale=1.0):\n            penalty = 0.0\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2\n            return scale * penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], scale=0.9)  # Change\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, scale=0.9)  # Change\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Utilize adaptive penalty scaling to enhance the role of periodic constraints in quasi-oppositional DE for Bragg mirror optimization.", "configspace": "", "generation": 13, "fitness": 0.7653592716621871, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.765 with standard deviation 0.014. And the mean value of best solutions found was 0.212 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "85a91529-c21b-4178-b5a0-b1820e45c1f4", "metadata": {"aucs": [0.775611602656878, 0.7451364336189961, 0.7753297787106874], "final_y": [0.2441030511493657, 0.2031175858140687, 0.19008804337757956]}, "mutation_prompt": null}
{"id": "7d7ec4e0-1549-4b09-adcd-fd5ae8c5bfdf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.5 * (gen / max_iter)\n            threshold = 0.1 * gen / max_iter  # New: Adaptive threshold for penalties\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                if diff < threshold:  # Only apply penalty if below threshold\n                    penalty += (diff - 0.2) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            # New: Adaptive population scaling\n            if gen % 10 == 0:\n                pop_size = max(10, int(pop_size * 0.9))\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive population scaling and a penalty thresholding mechanism to enhance convergence and solution quality.", "configspace": "", "generation": 14, "fitness": 0.7738102260285431, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.096. And the mean value of best solutions found was 0.231 (0. is the best) with standard deviation 0.047.", "error": "", "parent_id": "784715d2-51e6-4182-8a8f-7bcc440b24e1", "metadata": {"aucs": [0.9092537122187976, 0.6921281760490474, 0.7200487898177843], "final_y": [0.1648557719047855, 0.2674656529238414, 0.259659738762156]}, "mutation_prompt": null}
{"id": "9cbc6bb9-35dc-415f-bd51-4f6a7583f728", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.75 * (gen / max_iter)  # Changed dynamic factor to 0.75\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive periodic penalty scaling to improve solution diversity and convergence in DE.", "configspace": "", "generation": 14, "fitness": 0.7691413346140116, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.041. And the mean value of best solutions found was 0.227 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "784715d2-51e6-4182-8a8f-7bcc440b24e1", "metadata": {"aucs": [0.8209214283397304, 0.7213945081606847, 0.7651080673416198], "final_y": [0.19113252328787578, 0.2583794886525508, 0.23160245413782077]}, "mutation_prompt": null}
{"id": "05cfa932-8e3b-45aa-9ccc-80043a95c4fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.5 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.1) ** 2 * dynamic_factor  # Adjusted penalty threshold\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance DE with stochastic ranking and adaptive penalty dynamics for improved exploration and exploitation.", "configspace": "", "generation": 14, "fitness": 0.7132684451055985, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.713 with standard deviation 0.094. And the mean value of best solutions found was 0.237 (0. is the best) with standard deviation 0.071.", "error": "", "parent_id": "784715d2-51e6-4182-8a8f-7bcc440b24e1", "metadata": {"aucs": [0.7781215429811305, 0.5807935973022368, 0.7808901950334286], "final_y": [0.1821063315155672, 0.3369445474986107, 0.1921154808278187]}, "mutation_prompt": null}
{"id": "147f1e42-002e-4dcd-a28d-55d6ea017348", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)  # Adjusted penalty factor\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor  # Adjust penalty with dynamic factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)  # Updated penalty call\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:  # Local search every 5 generations\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',  # Added local search\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Integrate a local search phase with adaptive penalty adjustment to enhance convergence in the BraggMirrorOptimizer.", "configspace": "", "generation": 14, "fitness": 0.775782688922891, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.085. And the mean value of best solutions found was 0.227 (0. is the best) with standard deviation 0.042.", "error": "", "parent_id": "784715d2-51e6-4182-8a8f-7bcc440b24e1", "metadata": {"aucs": [0.8656080879984973, 0.6616219526318867, 0.8001180261382889], "final_y": [0.2033019545524003, 0.28652819070803703, 0.1919480489494959]}, "mutation_prompt": null}
{"id": "4f637fcf-c175-488e-b567-348c564664bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.5 * (gen / max_iter)  # Added dynamic penalty factor\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor  # Adjust penalty with dynamic factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)**2)  # Changed to introduce adaptive scaling\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)  # Updated penalty call\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive scaling factor in DE to enhance exploration-exploitation balance.", "configspace": "", "generation": 14, "fitness": 0.7272441132873707, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.727 with standard deviation 0.099. And the mean value of best solutions found was 0.233 (0. is the best) with standard deviation 0.043.", "error": "", "parent_id": "784715d2-51e6-4182-8a8f-7bcc440b24e1", "metadata": {"aucs": [0.8318733591482869, 0.594148541023162, 0.7557104396906629], "final_y": [0.1990709380891187, 0.2944365156744405, 0.20627155578212453]}, "mutation_prompt": null}
{"id": "8555c885-ec07-42b5-b8fd-82a99892b83a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.6 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced quasi-oppositional differential evolution with adaptive local search and dynamic mutation factor.", "configspace": "", "generation": 15, "fitness": 0.8953182524526514, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.895 with standard deviation 0.062. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "147f1e42-002e-4dcd-a28d-55d6ea017348", "metadata": {"aucs": [0.8656080879984973, 0.9818401143520217, 0.8385065550074353], "final_y": [0.2033019545524003, 0.16493934821225975, 0.20767163488945106]}, "mutation_prompt": null}
{"id": "f8606a47-df5d-4c3e-bdf2-cf59fef74891", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)  # Adjusted penalty factor\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor  # Adjust penalty with dynamic factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)  # Updated penalty call\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            np.random.shuffle(population[:pop_size])  # Change 1: Shuffle population for diversity\n\n            if gen % 5 == 0:  # Local search every 5 generations\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',  # Added local search\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance solution diversity and adaptive parameter control for improved exploration-exploitation balance in BraggMirrorOptimizer.", "configspace": "", "generation": 15, "fitness": 0.8821401566433344, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.077. And the mean value of best solutions found was 0.186 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "147f1e42-002e-4dcd-a28d-55d6ea017348", "metadata": {"aucs": [0.8485164167855874, 0.8087471333518663, 0.9891569197925498], "final_y": [0.18384924034977856, 0.20880023052405683, 0.1654589049697246]}, "mutation_prompt": null}
{"id": "4eb1367b-4423-4a06-8b3b-a2420f25ce5b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)  # Adjusted penalty factor\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor  # Adjust penalty with dynamic factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.5 * (1 - gen / max_iter) * np.std(population) / (np.mean(population) + 1e-9))  # Line 1 changed\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)  # Updated penalty call\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:  # Local search every 5 generations\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',  # Added local search\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance exploration by dynamically scaling Differential Evolution parameters based on diversity metrics.", "configspace": "", "generation": 15, "fitness": 0.857213715305759, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.097. And the mean value of best solutions found was 0.204 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "147f1e42-002e-4dcd-a28d-55d6ea017348", "metadata": {"aucs": [0.8656080879984973, 0.73439296624552, 0.9716400916732595], "final_y": [0.2033019545524003, 0.24076947157730821, 0.1686029966889908]}, "mutation_prompt": null}
{"id": "705d4b10-0298-41e5-afd3-a4a5e6f1bfb9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):  \n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)  # Adjusted penalty factor\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor  # Adjust penalty with dynamic factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):  # Change: Added iteration progress\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)  # Modified mutation factor\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (1 - (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9)))  # Modified crossover probability\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)  # Updated penalty call\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:  # Local search every 5 generations\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',  # Added local search\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance the mutation factor and crossover probability dynamics for improved exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.8486685610189397, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.075. And the mean value of best solutions found was 0.212 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "147f1e42-002e-4dcd-a28d-55d6ea017348", "metadata": {"aucs": [0.8729295164572669, 0.9258895429362155, 0.7471866236633367], "final_y": [0.1957685102920943, 0.18479546817257797, 0.2545555846790124]}, "mutation_prompt": null}
{"id": "4afc68fc-08b1-4f2d-8df7-a2335b9c608d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)  # Adjusted penalty factor\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n        \n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n        \n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.4 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                \n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9)) * (0.5 + 0.5 * np.sin(gen / max_iter * np.pi))  # Adjusted CR\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                \n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 3 == 0:  # More frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n        \n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced BraggMirrorOptimizer by incorporating a dynamic crossover rate and adjusted local search strategy to improve convergence.", "configspace": "", "generation": 15, "fitness": 0.777636266365786, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.067. And the mean value of best solutions found was 0.210 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "147f1e42-002e-4dcd-a28d-55d6ea017348", "metadata": {"aucs": [0.867528788034656, 0.7056905021253237, 0.7596895089373779], "final_y": [0.2033019545524003, 0.2198205175749507, 0.20681172936921233]}, "mutation_prompt": null}
{"id": "c473c680-5639-417f-8144-e7a635f29cbf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            # Adaptive population size\n            pop_size_dynamic = min(2 * pop_size, int(1.5 * pop_size * (1 - gen / max_iter)))  # Changed line\n\n            for i in range(pop_size_dynamic):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.6 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduced an adaptive population size adjustment to enhance exploration capabilities in later generations.  ", "configspace": "", "generation": 16, "fitness": 0.9025028066090762, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.903 with standard deviation 0.055. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "8555c885-ec07-42b5-b8fd-82a99892b83a", "metadata": {"aucs": [0.8636952546980783, 0.980620852138108, 0.8631923129910424], "final_y": [0.2033019545524003, 0.16493934821225975, 0.16607699427666178]}, "mutation_prompt": null}
{"id": "0a355bbb-9e37-4e45-bb7f-96b030bc201a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor  # Modified line\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.4 + (0.5 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved quasi-oppositional DE with adaptive periodicity penalty and enhanced mutation factor to boost search efficiency.", "configspace": "", "generation": 16, "fitness": 0.9361506179110107, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.051. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "8555c885-ec07-42b5-b8fd-82a99892b83a", "metadata": {"aucs": [0.8656080879984973, 0.9827937078303528, 0.960050057904182], "final_y": [0.2033019545524003, 0.1648608145308772, 0.1648617040778959]}, "mutation_prompt": null}
{"id": "a77033a7-4d02-4b28-a12a-d21a7239fbd0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        elite_size = 2  # New addition for elite preservation\n        for gen in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            elite_indices = np.argsort(scores)[:elite_size]\n            for i in range(pop_size):\n                if i in elite_indices:\n                    continue\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                if gen < max_iter // 2:  # Dual-phase mutation strategy\n                    F_dynamic = 0.5 + (0.2 * (1 - gen / max_iter)) * np.random.rand()\n                else:\n                    F_dynamic = 0.3 + (0.6 * (1 - gen / max_iter)) * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved BraggMirrorOptimizer using elite preservation and dual-phase mutation for enhanced exploration and exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.9290728605254518, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.929 with standard deviation 0.046. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "8555c885-ec07-42b5-b8fd-82a99892b83a", "metadata": {"aucs": [0.8658961930039211, 0.9484286431931434, 0.9728937453792909], "final_y": [0.2033019545524003, 0.1648579972309422, 0.1648803016899124]}, "mutation_prompt": null}
{"id": "2b063bba-c5f1-4e97-9334-fbe38e1025bf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.6 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)  \n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})  # Modified line\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Adaptive differential evolution with enhanced local search and periodicity enforcement.", "configspace": "", "generation": 16, "fitness": 0.8047309994230322, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.168. And the mean value of best solutions found was 0.224 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "8555c885-ec07-42b5-b8fd-82a99892b83a", "metadata": {"aucs": [0.9238794923765605, 0.5664649687323959, 0.9238485371601404], "final_y": [0.18191503432310152, 0.30957359366813786, 0.18188016442217736]}, "mutation_prompt": null}
{"id": "bf561a3e-7657-4258-9268-7e4d69915168", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.2) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        def diversity_penalty(population):\n            return np.var(population, axis=0).mean()  # Added line\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            if self.eval_count >= self.budget:\n                break\n\n            diversified_scores = scores + diversity_penalty(population)  # Modified line\n\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.6 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < diversified_scores[i]:  # Modified line\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved the algorithm by integrating a diversity preservation mechanism and adaptive selection pressure to enhance exploration-exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.7915030954848857, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.116. And the mean value of best solutions found was 0.214 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "8555c885-ec07-42b5-b8fd-82a99892b83a", "metadata": {"aucs": [0.8656080879984973, 0.6276407117811942, 0.8812604866749658], "final_y": [0.2033019545524003, 0.25684774926913545, 0.1823299257835207]}, "mutation_prompt": null}
{"id": "5f8d8764-8959-49ed-9b5b-740fc37e9f0a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def adaptive_de(self, func, bounds, pop_size=20, F=0.4, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def symmetric_initialization(x, lb, ub):\n            return lb + ub - x + np.random.uniform(-0.1, 0.1, size=x.shape)\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        sym_population = symmetric_initialization(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, sym_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = F * (0.5 + 0.5 * np.random.rand())\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 7 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced differential evolution with adaptive symmetry, local refinement, and periodicity preservation to optimize multi-layer structures.", "configspace": "", "generation": 17, "fitness": 0.8397830196153562, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.145. And the mean value of best solutions found was 0.212 (0. is the best) with standard deviation 0.055.", "error": "", "parent_id": "0a355bbb-9e37-4e45-bb7f-96b030bc201a", "metadata": {"aucs": [0.9089766077892993, 0.6374929251267949, 0.9728795259299745], "final_y": [0.18245634124355659, 0.2901232468656555, 0.1648585871345557]}, "mutation_prompt": null}
{"id": "ce2a82da-9e0d-478e-9168-0ff1b348e855", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.sin(gen / max_iter * np.pi)  # Modified line\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 3 == 0:  # Modified line for more frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})  # Increased iterations for local search\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Hybrid DE with adaptive local search and multi-scale periodicity to improve solution refinement and exploration.", "configspace": "", "generation": 17, "fitness": 0.8779375065301066, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.089. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "0a355bbb-9e37-4e45-bb7f-96b030bc201a", "metadata": {"aucs": [0.8762302987956277, 0.7701759784166643, 0.987406242378028], "final_y": [0.2014936191562502, 0.2098885908203687, 0.1648591287120047]}, "mutation_prompt": null}
{"id": "4606630a-f0f9-45ed-9c07-5f9194e0637e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.9 * (gen / max_iter)  # Changed 0.7 to 0.9\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.3 + (0.5 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)  # Changed 0.4 to 0.3\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced quasi-oppositional DE with dynamically adjusted periodicity penalty and refined exploration strategy.", "configspace": "", "generation": 17, "fitness": 0.8044477981446289, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.051. And the mean value of best solutions found was 0.216 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "0a355bbb-9e37-4e45-bb7f-96b030bc201a", "metadata": {"aucs": [0.8685234660820083, 0.7441104763637203, 0.8007094519881583], "final_y": [0.19657521660907218, 0.24357306094275755, 0.20733013485613838]}, "mutation_prompt": null}
{"id": "da332708-c7f2-429b-ab9f-691164b8d1a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.4 + (0.6 * (max_iter - gen) / max_iter) * np.random.rand()  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.var(population.ravel()) / (np.mean(population.ravel()) + 1e-9))  # Modified line\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced quasi-oppositional DE with adaptive boundary scaling and refined crossover to boost convergence efficiency.", "configspace": "", "generation": 17, "fitness": 0.8293266927408132, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.148. And the mean value of best solutions found was 0.189 (0. is the best) with standard deviation 0.028.", "error": "", "parent_id": "0a355bbb-9e37-4e45-bb7f-96b030bc201a", "metadata": {"aucs": [0.9173526028040061, 0.6212514827999593, 0.9493759926184742], "final_y": [0.17360209694475015, 0.22838511856434218, 0.16486189736821066]}, "mutation_prompt": null}
{"id": "11ab6af2-6533-45df-a21f-00534aece2d3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.4 + (0.5 * (1 - gen / max_iter)) * np.random.rand() * (1 - gen/max_iter)\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (0.5 + 0.5 * np.random.rand())  # Modified line to vary CR more dynamically\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 5})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced quasi-oppositional DE with dynamic crossover rate and diversity maintenance to improve exploration and exploitation balance.", "configspace": "", "generation": 17, "fitness": 0.7660881216952387, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.766 with standard deviation 0.095. And the mean value of best solutions found was 0.242 (0. is the best) with standard deviation 0.039.", "error": "", "parent_id": "0a355bbb-9e37-4e45-bb7f-96b030bc201a", "metadata": {"aucs": [0.8659579480225188, 0.6389942525417137, 0.7933121645214833], "final_y": [0.1967311112547233, 0.2908966782665434, 0.23702060804921254]}, "mutation_prompt": null}
{"id": "b6a00efa-dbd2-49c4-86c1-57c6cfeb16bd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)  # Modified line\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 3 == 0:  # Modified line for more frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})  # Increased iterations for local search\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced periodic penalty function dynamic adjustment to better guide convergence towards periodic solutions.", "configspace": "", "generation": 18, "fitness": 0.7939024858164675, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.191. And the mean value of best solutions found was 0.242 (0. is the best) with standard deviation 0.098.", "error": "", "parent_id": "ce2a82da-9e0d-478e-9168-0ff1b348e855", "metadata": {"aucs": [0.9536690214426667, 0.5256437277917775, 0.902394708214958], "final_y": [0.1648599979242561, 0.3805127825650805, 0.18188258115571654]}, "mutation_prompt": null}
{"id": "0a4f347e-8cad-4cb7-9af5-6e18b419ba7b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, init_pop_size=20, F=0.5, CR=0.9, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // init_pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.sin(gen / max_iter * np.pi)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        pop_size = init_pop_size\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.5 + 0.5 * ((max_iter - gen) / max_iter) * np.random.rand()  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 2 == 0:  # More frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 15})  # Increased iterations for local search\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n            if gen % 5 == 0:  # Dynamic population resizing\n                pop_size = int(init_pop_size * (1 + 0.1 * np.sin(2 * np.pi * gen / max_iter)))\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced hybrid DE with dynamic population size and adaptive mutation strategies for improved exploration and exploitation balance.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 40 is out of bounds for axis 0 with size 40').", "error": "IndexError('index 40 is out of bounds for axis 0 with size 40')", "parent_id": "ce2a82da-9e0d-478e-9168-0ff1b348e855", "metadata": {}, "mutation_prompt": null}
{"id": "276cd3b5-7e1d-4f01-9497-9118085cc80d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.sin(gen / max_iter * np.pi)  # Modified line\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 4 * dynamic_factor  # Changed from squared to quartic\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 3 == 0:  # Modified line for more frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})  # Increased iterations for local search\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced periodic penalty function to improve periodicity emphasis and solution refinement.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 41 is out of bounds for axis 0 with size 40').", "error": "IndexError('index 41 is out of bounds for axis 0 with size 40')", "parent_id": "ce2a82da-9e0d-478e-9168-0ff1b348e855", "metadata": {}, "mutation_prompt": null}
{"id": "b7fab0cf-a8e4-4a18-966e-94e373068fae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.sin(gen / max_iter * np.pi)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 2 == 0:  # Modified line for more frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced adaptive local search frequency to improve convergence speed and solution refinement.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 41 is out of bounds for axis 0 with size 40').", "error": "IndexError('index 41 is out of bounds for axis 0 with size 40')", "parent_id": "ce2a82da-9e0d-478e-9168-0ff1b348e855", "metadata": {}, "mutation_prompt": null}
{"id": "af5582ed-10c2-4ab7-a11d-8db8e96799f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.sin(gen / max_iter * np.pi)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * ((np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9)) + 0.5)  # Modified line\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 3 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduced dynamic adjustment to the crossover rate CR during the Differential Evolution process to improve exploration and exploitation balance.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 40 is out of bounds for axis 0 with size 40').", "error": "IndexError('index 40 is out of bounds for axis 0 with size 40')", "parent_id": "ce2a82da-9e0d-478e-9168-0ff1b348e855", "metadata": {}, "mutation_prompt": null}
{"id": "cc1b5a70-f0b2-414a-84bc-f65ad0dd8554", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.3 + 0.1 * np.sin(2 * np.pi * gen / max_iter)) * np.random.rand()  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 3 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:  # Modified line\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhanced adaptive differential weighting to promote diversity and escape local optima in the Bragg mirror optimization.", "configspace": "", "generation": 19, "fitness": 0.88881664446959, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.026. And the mean value of best solutions found was 0.195 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "b6a00efa-dbd2-49c4-86c1-57c6cfeb16bd", "metadata": {"aucs": [0.8762302987956277, 0.9248907515890507, 0.8653288830240913], "final_y": [0.2014936191562502, 0.18189304228001635, 0.20045127952088027]}, "mutation_prompt": null}
{"id": "b5c293d0-29f0-41e0-a9da-de4a893955f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)  # Modified line\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        history_best_score = np.inf\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()  # Modified line\n                F_adaptive = 0.5 + 0.5 * (history_best_score / np.max(scores[:pop_size]))  # New adaptive line\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)  # Changed line\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < history_best_score:  # New line for adaptive F\n                        history_best_score = trial_score  # New line to update history\n\n            if gen % 3 == 0:  # Modified line for more frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})  # Increased iterations for local search\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Incorporate adaptive differential weights based on historical progress to enhance solution convergence in complex landscapes.", "configspace": "", "generation": 19, "fitness": 0.892233671420818, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.084. And the mean value of best solutions found was 0.185 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "b6a00efa-dbd2-49c4-86c1-57c6cfeb16bd", "metadata": {"aucs": [0.9732174728179149, 0.9265427209339305, 0.7769408205106085], "final_y": [0.16485916397808542, 0.1818808705189464, 0.20739939075516112]}, "mutation_prompt": null}
{"id": "0ea7883a-45fe-47e9-b202-72d2fe066b8b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + gen / max_iter ** 0.5 + np.cos(gen / max_iter * np.pi)  # Modified line\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 3 == 0:  # Modified line for more frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})  # Increased iterations for local search\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Improved adaptive dynamic factor in penalty function for better convergence to periodic solutions.", "configspace": "", "generation": 19, "fitness": 0.8446292350308117, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.075. And the mean value of best solutions found was 0.213 (0. is the best) with standard deviation 0.032.", "error": "", "parent_id": "b6a00efa-dbd2-49c4-86c1-57c6cfeb16bd", "metadata": {"aucs": [0.9151830652016498, 0.8773808787194252, 0.7413237611713603], "final_y": [0.18190350672681788, 0.20044990038799715, 0.2578153188182656]}, "mutation_prompt": null}
{"id": "d77dc388-e19c-4ae9-b831-ef5b41c17424", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.8 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)  # Modified line\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.5 * (1 - gen / max_iter)) * np.random.rand()  # Modified line\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 3 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Adoption of enhanced population diversity control and dynamic penalty adjustment to improve convergence towards periodic solutions.", "configspace": "", "generation": 19, "fitness": 0.8227463044295034, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.161. And the mean value of best solutions found was 0.233 (0. is the best) with standard deviation 0.072.", "error": "", "parent_id": "b6a00efa-dbd2-49c4-86c1-57c6cfeb16bd", "metadata": {"aucs": [0.8762302987956277, 0.6039129271318606, 0.9880956873610218], "final_y": [0.2014936191562502, 0.33228225987504634, 0.16485615784604124]}, "mutation_prompt": null}
{"id": "3ef1c7eb-7e4f-4d59-94db-103032a8fcd9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.5 * np.sin(gen / max_iter * np.pi))  # Adaptive mutation rate\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                CR_dynamic = 0.95 if gen < max_iter / 2 else 0.9  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            if gen % 5 == 0:  # Less frequent but deeper local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 15})  # Enhanced iterations for local search\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive mutation and crossover rates along with more effective local search strategy to balance exploration and exploitation in optimization.", "configspace": "", "generation": 19, "fitness": 0.8099021530581091, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.180. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.054.", "error": "", "parent_id": "b6a00efa-dbd2-49c4-86c1-57c6cfeb16bd", "metadata": {"aucs": [0.8789474604371482, 0.5625982999374439, 0.9881606987997353], "final_y": [0.20045225354776652, 0.2927332122646029, 0.16485991028804492]}, "mutation_prompt": null}
{"id": "a8e744cb-9c63-46e9-98a9-8259bd7ea849", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)  # Modified line\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        history_best_score = np.inf\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()  # Modified line\n                F_adaptive = 0.5 + 0.5 * (history_best_score / np.max(scores[:pop_size]))  # New adaptive line\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)  # Changed line\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < history_best_score:  # New line for adaptive F\n                        history_best_score = trial_score  # New line to update history\n\n            if gen % 3 == 0:  # Modified line for more frequent local search\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})  # Increased iterations for local search\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Incorporate adaptive differential weights based on historical progress to enhance solution convergence in complex landscapes.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "No code was extracted. The code should be encapsulated with ``` in your response.", "error": "The code should be encapsulated with ``` in your response.", "parent_id": "b5c293d0-29f0-41e0-a9da-de4a893955f8", "metadata": {"aucs": [0.9732174728179149, 0.9265427209339305, 0.7769408205106085], "final_y": [0.16485916397808542, 0.1818808705189464, 0.20739939075516112]}, "mutation_prompt": null}
{"id": "53f16c6b-6ad8-4dc4-a333-eeb506c77c5c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)  # Modified line\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        history_best_score = np.inf\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                F_adaptive = 0.5 + 0.5 * (history_best_score / (np.max(scores[:pop_size]) + np.std(scores)))  # Modified line\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < history_best_score:\n                        history_best_score = trial_score\n\n            if gen % 3 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance mutation diversity by adjusting the adaptive factor based on population diversity.", "configspace": "", "generation": 20, "fitness": 0.689950478733062, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.690 with standard deviation 0.040. And the mean value of best solutions found was 0.253 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "b5c293d0-29f0-41e0-a9da-de4a893955f8", "metadata": {"aucs": [0.7085115248704565, 0.7273069904748983, 0.6340329208538309], "final_y": [0.276307121774266, 0.2580641427745496, 0.2256467095241692]}, "mutation_prompt": null}
{"id": "d0d60124-6ff3-41d2-96bd-b4ec163b7f54", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        history_best_score = np.inf\n        for gen in range(max_iter):\n            elite_idx = np.argmin(scores[:pop_size])  # Line added for elitism\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                F_adaptive = 0.5 + 0.5 * (history_best_score / np.max(scores[:pop_size]))\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < history_best_score:\n                        history_best_score = trial_score\n\n            if gen % 3 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n            population[elite_idx] = population[np.argmin(scores[:pop_size])]  # Line to maintain elite solution\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce elitism and adaptive exploration-exploitation balance in the DE algorithm to improve convergence and solution quality.", "configspace": "", "generation": 20, "fitness": 0.8330765017621659, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.148. And the mean value of best solutions found was 0.224 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "b5c293d0-29f0-41e0-a9da-de4a893955f8", "metadata": {"aucs": [0.8762302987956277, 0.6339613974162883, 0.9890378090745816], "final_y": [0.2014936191562502, 0.30671190142535665, 0.1648622974145928]}, "mutation_prompt": null}
{"id": "ff4ad693-2282-4dde-be56-e4b73e61bc6c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        history_best_score = np.inf\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                F_adaptive = 0.5 + 0.5 * (history_best_score / np.max(scores[:pop_size]))\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n\n                CR_dynamic = CR * (np.std(population.ravel()) / (np.mean(population.ravel()) + 1e-9))\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < history_best_score:\n                        history_best_score = trial_score\n\n            if gen % 3 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n            # Adaptive population resizing and elitism\n            elite_idx = np.argsort(scores)[:pop_size // 2]\n            new_population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n            population[elite_idx] = new_population[:len(elite_idx)]\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Introduce adaptive population size and elitism to enhance convergence and solution quality in complex landscapes.", "configspace": "", "generation": 20, "fitness": 0.826487261382486, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.124. And the mean value of best solutions found was 0.209 (0. is the best) with standard deviation 0.033.", "error": "", "parent_id": "b5c293d0-29f0-41e0-a9da-de4a893955f8", "metadata": {"aucs": [0.9776632727393777, 0.828091119403747, 0.673707392004333], "final_y": [0.16503841874961567, 0.2213013556941954, 0.24209134713065772]}, "mutation_prompt": null}
{"id": "128ab3d2-5261-4e36-be2a-d15ba91b4228", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_de(self, func, bounds, pop_size=20, F=0.45, CR=0.98, max_iter=None):\n        if max_iter is None:\n            max_iter = self.budget // pop_size\n\n        def periodic_penalty(x, gen, max_iter):\n            penalty = 0.0\n            dynamic_factor = 1 + 0.7 * (gen / max_iter) + np.cos(gen / max_iter * np.pi)\n            for i in range(1, len(x)):\n                diff = abs(x[i] - x[i-1]) % (bounds.ub[0] - bounds.lb[0])\n                penalty += (diff - 0.25) ** 2 * dynamic_factor\n            return penalty\n\n        def quasi_opposite(x, lb, ub):\n            return lb + ub - x\n\n        population = np.random.uniform(bounds.lb, bounds.ub, (pop_size, self.dim))\n        quasi_population = quasi_opposite(population, bounds.lb, bounds.ub)\n        population = np.concatenate((population, quasi_population), axis=0)\n\n        scores = np.empty(2 * pop_size)\n        for i in range(2 * pop_size):\n            scores[i] = func(population[i]) + periodic_penalty(population[i], 0, max_iter)\n            self.eval_count += 1\n\n        history_best_score = np.inf\n        for gen in range(max_iter):\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                indices = [idx for idx in range(2 * pop_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = 0.6 + (0.4 * (1 - gen / max_iter)) * np.random.rand()\n                F_adaptive = 0.5 + 0.5 * (history_best_score / np.max(scores[:pop_size]))\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n\n                # Changed CR line for dynamic variation\n                CR_dynamic = CR * (np.random.rand() * 0.5 + 0.5)  # New dynamic adjustment\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n                trial_score = func(trial) + periodic_penalty(trial, gen, max_iter)\n                self.eval_count += 1\n\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < history_best_score:\n                        history_best_score = trial_score\n\n            if gen % 3 == 0:\n                loc_idx = np.argmin(scores[:pop_size])\n                local_res = minimize(func, population[loc_idx], method='L-BFGS-B',\n                                     bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                                     options={'maxiter': 10})\n                if local_res.fun + periodic_penalty(local_res.x, gen, max_iter) < scores[loc_idx]:\n                    population[loc_idx] = local_res.x\n                    scores[loc_idx] = local_res.fun\n\n        best_idx = np.argmin(scores[:pop_size])\n        return population[best_idx]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.quasi_oppositional_de(func, bounds)\n\n        if self.eval_count < self.budget:\n            res = minimize(func, best_solution, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)],\n                           options={'maxfev': self.budget - self.eval_count})\n            best_solution = res.x\n            self.eval_count += res.nfev\n\n        return best_solution", "name": "BraggMirrorOptimizer", "description": "Enhance diversity by varying crossover rates dynamically to improve exploration and convergence in complex optimization landscapes.", "configspace": "", "generation": 20, "fitness": 0.6460516073450733, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.646 with standard deviation 0.041. And the mean value of best solutions found was 0.263 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "b5c293d0-29f0-41e0-a9da-de4a893955f8", "metadata": {"aucs": [0.7024269356212709, 0.6303213147145126, 0.6054065716994362], "final_y": [0.2286021998158917, 0.28866522200118205, 0.2730013404984801]}, "mutation_prompt": null}
