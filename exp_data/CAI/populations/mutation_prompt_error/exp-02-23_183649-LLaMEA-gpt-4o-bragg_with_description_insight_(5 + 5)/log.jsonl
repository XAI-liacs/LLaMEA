{"id": "5b2245e1-a31f-4061-a229-ba29db7914cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=15, F=0.8, Cr=0.9):\n        population = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.eval_count += pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < Cr, mutant, population[i])\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_optimization(self, func, x0):\n        result = minimize(func, x0, method='BFGS', options={'maxiter': self.budget - self.eval_count})\n        self.eval_count += result.nfev\n        return result.x, result.fun\n\n    def enforce_periodicity(self, x, period):\n        return np.tile(x[:period], self.dim // period)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        period = self.dim // 2  # Aim for a periodic solution with half the dimensions\n        x_global, f_global = self.differential_evolution(func, bounds)\n\n        x_periodic = self.enforce_periodicity(x_global, period)\n        x_local, f_local = self.local_optimization(func, x_periodic)\n\n        return x_local if f_local < f_global else x_global", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution for global exploration and a tailored periodicity constraint with BFGS for local optimization to efficiently navigate the complex optimization landscape of multilayer photonic structures.", "configspace": "", "generation": 0, "fitness": 0.8781172596932857, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.046. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8748457174288696, 0.8238980701483568, 0.935607991502631], "final_y": [0.18188023201023884, 0.18187981399664055, 0.1648559817943026]}, "mutation_prompt": null}
{"id": "90c02209-f513-4c97-91d8-fe37ad3dc37c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def periodicity_constraint(self, x):\n        \"\"\"Encourages periodicity by adding a penalty for non-repeating patterns.\"\"\"\n        period_penalty = np.sum((x[:-1] - x[1:])**2)\n        return period_penalty\n\n    def __call__(self, func):\n        # Initialize parameters for Differential Evolution\n        population_size = max(10, self.dim * 5)\n        scale_factor = 0.8\n        crossover_rate = 0.7\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (population_size, self.dim))\n        best_solution = None\n        best_fitness = np.inf\n        evals_used = 0\n\n        while evals_used < self.budget:\n            new_population = np.zeros_like(population)\n            for i in range(population_size):\n                if evals_used >= self.budget:\n                    break\n                # Mutation and crossover\n                indices = np.random.choice(population_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + scale_factor * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate trial solution with periodicity penalty\n                trial_fitness = func(trial) + self.periodicity_constraint(trial)\n                evals_used += 1\n\n                # Replacement\n                if trial_fitness < func(population[i]):\n                    new_population[i] = trial\n                else:\n                    new_population[i] = population[i]\n\n                # Track the best solution\n                if trial_fitness < best_fitness:\n                    best_fitness = trial_fitness\n                    best_solution = trial\n\n            population = new_population\n\n        # Local refinement with BFGS\n        if evals_used < self.budget:\n            local_result = minimize(lambda x: func(x) + self.periodicity_constraint(x),\n                                    best_solution, method='L-BFGS-B',\n                                    bounds=[(func.bounds.lb, func.bounds.ub)] * self.dim,\n                                    options={'maxfun': self.budget - evals_used})\n            if local_result.fun < best_fitness:\n                best_solution = local_result.x\n\n        return best_solution", "name": "HybridDEBFGS", "description": "The algorithm employs a hybrid approach combining Differential Evolution with periodicity-enhancing constraints and local refinement using BFGS to efficiently explore complex search spaces with multiple local minima.", "configspace": "", "generation": 0, "fitness": 0.580708704757242, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.581 with standard deviation 0.037. And the mean value of best solutions found was 0.335 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": null, "metadata": {"aucs": [0.6265304065341105, 0.5350609539677926, 0.5805347537698231], "final_y": [0.28941272664350226, 0.37808218960117124, 0.33752764084796016]}, "mutation_prompt": null}
{"id": "72c76dc7-f301-4dc2-bd66-315230e185bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.periodicity_factor = 0.9\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def mutate(self, population):\n        indices = np.random.choice(range(self.population_size), 3, replace=False)\n        a, b, c = population[indices]\n        return a + self.mutation_factor * (b - c)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def periodic_correction(self, candidate, lb, ub):\n        midpoint = (ub + lb) / 2\n        half_range = (ub - lb) / 2\n        return midpoint + self.periodicity_factor * (candidate - midpoint) % half_range\n\n    def local_optimize(self, candidate, func):\n        result = minimize(func, candidate, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)))\n        return result.x if result.success else candidate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_candidate = None\n        best_value = np.inf\n\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                target = population[i]\n                mutant = self.mutate(population)\n                trial = self.crossover(target, mutant)\n                trial = self.periodic_correction(trial, lb, ub)\n\n                f_target = func(target)\n                f_trial = func(trial)\n                self.evaluations += 2\n\n                if f_trial < f_target:\n                    population[i] = trial\n                    if f_trial < best_value:\n                        best_candidate, best_value = trial, f_trial\n\n            if best_candidate is not None:\n                best_candidate = self.local_optimize(best_candidate, func)\n                if func(best_candidate) < best_value:\n                    best_value = func(best_candidate)\n                self.evaluations += 1\n\n        return best_candidate", "name": "PeriodicDifferentialEvolution", "description": "This algorithm combines Differential Evolution with periodicity encouragement and local search to efficiently navigate the optimization landscape and exploit constructive interference principles.", "configspace": "", "generation": 0, "fitness": 0.7221472876990669, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.722 with standard deviation 0.095. And the mean value of best solutions found was 0.270 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": null, "metadata": {"aucs": [0.592930749305203, 0.7530802169846353, 0.8204308968073625], "final_y": [0.33835473129502946, 0.25058879671857637, 0.2206495310817398]}, "mutation_prompt": null}
{"id": "c80125ee-e5f0-42ac-ab6b-a03c9f01413f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Can be adjusted\n        self.F = 0.8  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        # Symmetric initialization can help explore more effectively\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "A hybrid Differential Evolution and Local Search strategy that leverages symmetry and periodicity for efficient exploration and fine-tuning in multilayer photonic structure optimization.", "configspace": "", "generation": 0, "fitness": 0.7540843907853478, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.754 with standard deviation 0.081. And the mean value of best solutions found was 0.222 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7081227408804045, 0.6857705352283868, 0.8683598962472524], "final_y": [0.18188084819022143, 0.2827469502029847, 0.20044647890928624]}, "mutation_prompt": null}
{"id": "478a0c74-07f3-4692-818e-e7e0fd1d8e13", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('-inf')\n        self.func_evals = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.best_solution = self.population[0]\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        new_population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        quasi_opposite_population = lb + ub - new_population\n        combined_population = np.vstack((new_population, quasi_opposite_population))\n        fitness_values = [self.evaluate(ind) for ind in combined_population]\n        indices = np.argsort(fitness_values)[-self.population_size:]\n        self.population = combined_population[indices]\n\n    def mutation(self, target_idx):\n        indices = [i for i in range(self.population_size) if i != target_idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant_vector = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return np.clip(mutant_vector, 0, 1)\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover_mask, mutant, target)\n        return trial_vector\n\n    def enforce_periodicity(self, vector, period):\n        for i in range(0, self.dim, period):\n            vector[i:i+period] = np.mean(vector[i:i+period])\n        return vector\n\n    def evaluate(self, individual):\n        if self.func_evals >= self.budget:\n            return float('-inf')\n        self.func_evals += 1\n        return self.func(individual)\n\n    def optimize(self, func):\n        self.func = func\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.quasi_oppositional_initialization(lb, ub)\n\n        while self.func_evals < self.budget:\n            for i in range(self.population_size):\n                target = self.population[i]\n                mutant = self.mutation(i)\n                trial = self.crossover(target, mutant)\n                trial = self.enforce_periodicity(trial, period=2)\n                trial_fitness = self.evaluate(trial)\n                if trial_fitness > self.evaluate(target):\n                    self.population[i] = trial\n                    if trial_fitness > self.best_fitness:\n                        self.best_solution = trial\n                        self.best_fitness = trial_fitness\n\n        # Local optimization using BFGS for fine-tuning\n        result = minimize(lambda x: -self.evaluate(x), self.best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        if result.success:\n            self.best_solution = result.x\n            self.best_fitness = -result.fun\n\n    def __call__(self, func):\n        self.optimize(func)\n        return self.best_solution", "name": "PhotonicOptimizer", "description": "A novel hybrid metaheuristic algorithm that leverages Quasi-Oppositional Differential Evolution with periodicity constraints and local tuning using BFGS to optimize multilayered photonic structures for maximum reflectivity.", "configspace": "", "generation": 0, "fitness": 0.48776903934121085, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.488 with standard deviation 0.043. And the mean value of best solutions found was 0.415 (0. is the best) with standard deviation 0.032.", "error": "", "parent_id": null, "metadata": {"aucs": [0.5273329837524533, 0.5081586275780816, 0.4278155066930974], "final_y": [0.3855862044582483, 0.3992702095910784, 0.45911886491463516]}, "mutation_prompt": null}
{"id": "3ad0d57f-ddaa-49e0-bceb-10b17a2f6104", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.periodicity_factor = 0.9\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def mutate(self, population):\n        indices = np.random.choice(range(self.population_size), 3, replace=False)\n        a, b, c = population[indices]\n        self.mutation_factor = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n        return a + self.mutation_factor * (b - c)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def periodic_correction(self, candidate, lb, ub):\n        midpoint = (ub + lb) / 2\n        half_range = (ub - lb) / 2\n        return midpoint + self.periodicity_factor * (candidate - midpoint) % half_range\n\n    def local_optimize(self, candidate, func):\n        result = minimize(func, candidate, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)))\n        return result.x if result.success else candidate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_candidate = None\n        best_value = np.inf\n\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                target = population[i]\n                mutant = self.mutate(population)\n                trial = self.crossover(target, mutant)\n                trial = self.periodic_correction(trial, lb, ub)\n\n                f_target = func(target)\n                f_trial = func(trial)\n                self.evaluations += 2\n\n                if f_trial < f_target:\n                    population[i] = trial\n                    if f_trial < best_value:\n                        best_candidate, best_value = trial, f_trial\n\n            if best_candidate is not None:\n                best_candidate = self.local_optimize(best_candidate, func)\n                if func(best_candidate) < best_value:\n                    best_value = func(best_candidate)\n                self.evaluations += 1\n\n        return best_candidate", "name": "PeriodicDifferentialEvolution", "description": "An enhanced Periodic Differential Evolution algorithm integrating adaptive mutation factor for better convergence in multilayer photonic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.7802505361431238, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.029. And the mean value of best solutions found was 0.238 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "72c76dc7-f301-4dc2-bd66-315230e185bb", "metadata": {"aucs": [0.7516320183411783, 0.7699682595187848, 0.8191513305694081], "final_y": [0.2505898309975204, 0.24257668399757004, 0.22064913140632592]}, "mutation_prompt": null}
{"id": "49b5996d-7e2b-40bb-9c3f-823ce3149f6f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20 * dim  # Increased population size for better exploration\n        self.F = 0.8  # Mutation factor\n        self.CR = 0.9  # Crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        # Symmetric initialization can help explore more effectively\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        elitism_count = 1  # Preserve the best solution in each generation\n        for i in range(self.population_size - elitism_count):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        # Preserve the best solution found so far\n        best_idx = np.argmin(scores)\n        self.pop[-elitism_count:] = self.pop[best_idx]\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploration via increased population size and added elitism in the hybrid Differential Evolution and Local Search strategy.", "configspace": "", "generation": 1, "fitness": 0.7784686532708881, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.117. And the mean value of best solutions found was 0.238 (0. is the best) with standard deviation 0.054.", "error": "", "parent_id": "c80125ee-e5f0-42ac-ab6b-a03c9f01413f", "metadata": {"aucs": [0.939703197385688, 0.6666316333871578, 0.7290711290398184], "final_y": [0.16486299573460128, 0.29182857307098753, 0.25781344451454136]}, "mutation_prompt": null}
{"id": "dca80431-8321-43f9-9a5b-5dab12b8fb53", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "A refined hybrid Differential Evolution strategy with adaptive crossover and dynamic population size for enhanced exploration and exploitation in multilayer photonic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.7859249996856198, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.135. And the mean value of best solutions found was 0.235 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "c80125ee-e5f0-42ac-ab6b-a03c9f01413f", "metadata": {"aucs": [0.976649808838963, 0.6947927319826883, 0.686332458235208], "final_y": [0.16485811630036507, 0.2578102922334896, 0.28274701549715675]}, "mutation_prompt": null}
{"id": "202aa93c-9b70-4c9a-aca3-b47b1afd5c6d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.population = None\n        self.best_solution = None\n        self.best_fitness = float('-inf')\n        self.func_evals = 0\n\n    def initialize_population(self, lb, ub):\n        self.population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        self.best_solution = self.population[0]\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        new_population = lb + (ub - lb) * np.random.rand(self.population_size, self.dim)\n        quasi_opposite_population = lb + ub - new_population\n        combined_population = np.vstack((new_population, quasi_opposite_population))\n        fitness_values = [self.evaluate(ind) for ind in combined_population]\n        indices = np.argsort(fitness_values)[-self.population_size:]\n        self.population = combined_population[indices]\n\n    def mutation(self, target_idx):\n        indices = [i for i in range(self.population_size) if i != target_idx]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant_vector = self.population[a] + self.F * (self.population[b] - self.population[c]) + np.random.normal(0, 0.1, size=self.dim)  # Perturbation added\n        return np.clip(mutant_vector, 0, 1)\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.CR\n        trial_vector = np.where(crossover_mask, mutant, target)\n        return trial_vector\n\n    def enforce_periodicity(self, vector, period):\n        for i in range(0, self.dim, period):\n            vector[i:i+period] = np.mean(vector[i:i+period])\n        return vector\n\n    def evaluate(self, individual):\n        if self.func_evals >= self.budget:\n            return float('-inf')\n        self.func_evals += 1\n        return self.func(individual)\n\n    def optimize(self, func):\n        self.func = func\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.quasi_oppositional_initialization(lb, ub)\n\n        while self.func_evals < self.budget:\n            for i in range(self.population_size):\n                target = self.population[i]\n                mutant = self.mutation(i)\n                trial = self.crossover(target, mutant)\n                trial = self.enforce_periodicity(trial, period=2)\n                trial_fitness = self.evaluate(trial)\n                if trial_fitness > self.evaluate(target):\n                    self.population[i] = trial\n                    if trial_fitness > self.best_fitness:\n                        self.best_solution = trial\n                        self.best_fitness = trial_fitness\n\n        # Local optimization using BFGS for fine-tuning\n        result = minimize(lambda x: -self.evaluate(x), self.best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        if result.success:\n            self.best_solution = result.x\n            self.best_fitness = -result.fun\n\n    def __call__(self, func):\n        self.optimize(func)\n        return self.best_solution", "name": "PhotonicOptimizer", "description": "The PhotonicOptimizer utilizes Quasi-Oppositional Differential Evolution enhanced with periodicity constraints and integrates a perturbation in mutation for diverse exploration, followed by local optimization with BFGS for fine-tuning.", "configspace": "", "generation": 1, "fitness": 0.5249923284557608, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.525 with standard deviation 0.090. And the mean value of best solutions found was 0.392 (0. is the best) with standard deviation 0.062.", "error": "", "parent_id": "478a0c74-07f3-4692-818e-e7e0fd1d8e13", "metadata": {"aucs": [0.6421092710931762, 0.5081586275780816, 0.4247090866960248], "final_y": [0.3134298421743149, 0.3992702095910784, 0.46469205749927356]}, "mutation_prompt": null}
{"id": "05f4aeff-2be1-4577-872f-e445a46a8fd6", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass AdaptiveQuantumInspiredEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.evaluations = 0\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.pop = None\n    \n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def quantum_gate_operation(self, individual, lb, ub):\n        # Apply a simple quantum-inspired rotation using a random angle\n        theta = np.random.uniform(-np.pi / 2, np.pi / 2, self.dim)\n        rotation_matrix = np.cos(theta) + np.sin(theta) * 1j\n        shifted_individual = individual * rotation_matrix.real\n        return np.clip(shifted_individual, lb, ub)\n\n    def mutation(self, individual, lb, ub):\n        # Introduce small random changes\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        mutated_individual = individual + perturbation\n        return np.clip(mutated_individual, lb, ub)\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def optimize(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            individual = self.pop[i]\n            # Apply quantum gate operation\n            new_individual = self.quantum_gate_operation(individual, lb, ub)\n            # Apply mutation\n            trial = self.mutation(new_individual, lb, ub)\n\n            trial_score = func(trial)\n            self.evaluations += 1\n            if trial_score < scores[i]:\n                scores[i] = trial_score\n                self.pop[i] = trial\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.optimize(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "AdaptiveQuantumInspiredEA", "description": "A novel Adaptive Quantum-Inspired Evolutionary Algorithm (AQEA) that incorporates quantum gates for maintaining diversity, combined with adaptive local search to efficiently navigate the optimization landscape of multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": 0.876461425671415, "feedback": "The algorithm AdaptiveQuantumInspiredEA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.099. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "c80125ee-e5f0-42ac-ab6b-a03c9f01413f", "metadata": {"aucs": [0.922150248468032, 0.7383622065943547, 0.9688718219518587], "final_y": [0.18188138392684028, 0.2578103028708183, 0.16486073196010476]}, "mutation_prompt": null}
{"id": "2ba431f0-fc2d-47b5-aa87-8d81070635aa", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass AdaptiveQuantumInspiredEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Increased population size\n        self.evaluations = 0\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.pop = None\n    \n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def quantum_gate_operation(self, individual, lb, ub):\n        theta = np.random.uniform(-np.pi, np.pi, self.dim)  # Broadened angle range\n        rotation_matrix = np.exp(1j * theta)\n        shifted_individual = individual * rotation_matrix.real\n        return np.clip(shifted_individual, lb, ub)\n\n    def mutation(self, individual, lb, ub, iteration):\n        # Adaptive mutation based on the iteration\n        perturbation_scale = 0.1 * (1 - iteration / self.budget)\n        perturbation = np.random.normal(0, perturbation_scale, self.dim)\n        mutated_individual = individual + perturbation\n        return np.clip(mutated_individual, lb, ub)\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def optimize(self, func, scores, lb, ub):\n        iteration = 0\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                individual = self.pop[i]\n                new_individual = self.quantum_gate_operation(individual, lb, ub)\n                trial = self.mutation(new_individual, lb, ub, iteration)\n                \n                trial_score = func(trial)\n                self.evaluations += 1\n                \n                if trial_score < scores[i]:\n                    scores[i] = trial_score\n                    self.pop[i] = trial\n                    if trial_score < self.best_score:\n                        self.best_score = trial_score\n                        self.best_solution = trial\n            iteration += 1\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        self.optimize(func, scores, lb, ub)\n        return self.best_solution", "name": "AdaptiveQuantumInspiredEA", "description": "An enhanced Adaptive Quantum-Inspired Evolutionary Algorithm with improved quantum gate operations and adaptive mutation strategies to efficiently explore and exploit the complex landscape of multilayer photonic structure optimization.", "configspace": "", "generation": 2, "fitness": 0.6325919467276799, "feedback": "The algorithm AdaptiveQuantumInspiredEA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.633 with standard deviation 0.102. And the mean value of best solutions found was 0.310 (0. is the best) with standard deviation 0.057.", "error": "", "parent_id": "05f4aeff-2be1-4577-872f-e445a46a8fd6", "metadata": {"aucs": [0.6876674730021135, 0.4896964058146299, 0.7204119613662963], "final_y": [0.282747892772544, 0.39009606157905696, 0.2578112059195682]}, "mutation_prompt": null}
{"id": "c6c20475-6eeb-4048-b9bc-d5d798022d56", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 1.0)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced Hybrid Differential Evolution with Adaptive Mutation and Quasi-Oppositional Initialization for Improved Exploration in Multilayer Photonic Structure Optimization.", "configspace": "", "generation": 2, "fitness": 0.8273500659873871, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.079. And the mean value of best solutions found was 0.220 (0. is the best) with standard deviation 0.031.", "error": "", "parent_id": "dca80431-8321-43f9-9a5b-5dab12b8fb53", "metadata": {"aucs": [0.9290571979236114, 0.7355403554841702, 0.81745264455438], "final_y": [0.18188299065547997, 0.25781185564577425, 0.2206489084719162]}, "mutation_prompt": null}
{"id": "1b8d034a-7957-4a80-84a7-b47b77bdf8b5", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass CoevolutionaryStrategy:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.sub_population_size = 10 * dim  # Sub-population size for diversity\n        self.num_sub_populations = 2  # Number of sub-populations\n        self.F = 0.7  # Mutation factor\n        self.CR = 0.8  # Crossover probability\n        self.populations = [None] * self.num_sub_populations\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n        self.elitist_archive = []\n\n    def initialize_populations(self, lb, ub):\n        for i in range(self.num_sub_populations):\n            self.populations[i] = np.random.uniform(lb, ub, (self.sub_population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        scores = np.apply_along_axis(func, 1, population)\n        self.evaluations += len(scores)\n        return scores\n\n    def cooperative_step(self, func, scores, population, lb, ub):\n        for i in range(self.sub_population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.sub_population_size) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                population[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n                    self.elitist_archive.append(trial)\n                    if len(self.elitist_archive) > 10:\n                        self.elitist_archive.pop(0)\n\n    def periodicity_bias(self, population, lb, ub):\n        period = np.mean(self.elitist_archive, axis=0) if self.elitist_archive else np.ones(self.dim) * ((lb + ub) / 2)\n        biased_population = population + np.sin(2 * np.pi * (population - period))\n        return np.clip(biased_population, lb, ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_populations(lb, ub)\n        while self.evaluations < self.budget:\n            for i in range(self.num_sub_populations):\n                scores = self.evaluate_population(func, self.populations[i])\n                self.cooperative_step(func, scores, self.populations[i], lb, ub)\n                self.populations[i] = self.periodicity_bias(self.populations[i], lb, ub)\n        return self.best_solution", "name": "CoevolutionaryStrategy", "description": "A novel Coevolutionary Strategy that employs cooperative sub-populations to explore diverse regions of the search space, incorporating a periodicity bias and elitist archive to enhance convergence in multilayer photonic structure optimization.", "configspace": "", "generation": 2, "fitness": 0.6240491734475497, "feedback": "The algorithm CoevolutionaryStrategy got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.624 with standard deviation 0.065. And the mean value of best solutions found was 0.301 (0. is the best) with standard deviation 0.045.", "error": "", "parent_id": "49b5996d-7e2b-40bb-9c3f-823ce3149f6f", "metadata": {"aucs": [0.6807129392583628, 0.6589612718536155, 0.5324733092306708], "final_y": [0.2505161437237243, 0.29344697061566727, 0.35924688121428405]}, "mutation_prompt": null}
{"id": "fbbdf925-0cbb-46d8-82c6-ba959ccc519e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.periodicity_factor = 0.9\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def mutate(self, population):\n        indices = np.random.choice(range(self.population_size), 3, replace=False)\n        a, b, c = population[indices]\n        self.mutation_factor = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n        return a + self.mutation_factor * (b - c)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def periodic_correction(self, candidate, lb, ub):\n        midpoint = (ub + lb) / 2\n        half_range = (ub - lb) / 2\n        return midpoint + self.periodicity_factor * (candidate - midpoint) % half_range\n\n    def local_optimize(self, candidate, func):\n        result = minimize(func, candidate, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)))\n        return result.x if result.success else candidate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_candidate = None\n        best_value = np.inf\n\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                target = population[i]\n                mutant = self.mutate(population)\n                trial = self.crossover(target, mutant)\n                trial = self.periodic_correction(trial, lb, ub)\n\n                f_target = func(target)\n                f_trial = func(trial)\n                self.evaluations += 2\n\n                if f_trial < f_target:\n                    population[i] = trial\n                    if f_trial < best_value:\n                        best_candidate, best_value = trial, f_trial\n\n            if best_candidate is not None:\n                best_candidate = self.local_optimize(best_candidate, func)\n                if func(best_candidate) < best_value:\n                    best_value = func(best_candidate)\n                self.evaluations += 1\n\n            population[np.argmax([func(ind) for ind in population])] = best_candidate  # Elitism integration\n\n        return best_candidate", "name": "PeriodicDifferentialEvolution", "description": "Improved convergence by integrating elitism to retain and propagate the best solution in the population.", "configspace": "", "generation": 2, "fitness": 0.6979467064590995, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.698 with standard deviation 0.045. And the mean value of best solutions found was 0.278 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "3ad0d57f-ddaa-49e0-bceb-10b17a2f6104", "metadata": {"aucs": [0.6984903271435727, 0.6428581767165205, 0.7524916155172052], "final_y": [0.2749572048646777, 0.30958664126578894, 0.250588903016824]}, "mutation_prompt": null}
{"id": "bc827547-df03-43f0-b5a6-25598fb2f569", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  \n        self.F = 0.9  # Increased mutation factor for broader exploration\n        self.CR = np.random.uniform(0.6, 0.95)  # Slightly shifted adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        enhanced_quasi_opposite_pop = (self.pop + quasi_opposite_pop) / 2  # Added dynamic adjustment\n        self.pop = np.vstack((self.pop, enhanced_quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            periodic_adjustment = 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim)  # Encourage periodicity\n            mutant = np.clip(mutant + periodic_adjustment, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced Hybrid Differential Evolution with Dynamic Quasi-Oppositional Strategy and Adaptive Periodicity Encouragement for Improved Reflectivity Optimization.", "configspace": "", "generation": 2, "fitness": 0.7765995151523369, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.100. And the mean value of best solutions found was 0.244 (0. is the best) with standard deviation 0.047.", "error": "", "parent_id": "dca80431-8321-43f9-9a5b-5dab12b8fb53", "metadata": {"aucs": [0.8177937310616905, 0.8728850802194069, 0.6391197341759135], "final_y": [0.2206501047825714, 0.2004466013162134, 0.30958349424063436]}, "mutation_prompt": null}
{"id": "e403014e-f131-4933-92c9-c27a3c8d3582", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.periodicity_factor = 0.9\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def mutate(self, population):\n        indices = np.random.choice(range(self.population_size), 3, replace=False)\n        a, b, c = population[indices]\n        self.mutation_factor = 0.6 + 0.2 * np.random.rand()  # Improved adaptive mutation factor\n        return a + self.mutation_factor * (b - c)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def periodic_correction(self, candidate, lb, ub):\n        midpoint = (ub + lb) / 2\n        half_range = (ub - lb) / 2\n        return midpoint + self.periodicity_factor * (candidate - midpoint) % half_range\n\n    def local_optimize(self, candidate, func):\n        result = minimize(func, candidate, method='L-BFGS-B', options={'maxiter': 10}, bounds=list(zip(func.bounds.lb, func.bounds.ub)))  # Refined local optimization\n        return result.x if result.success else candidate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_candidate = None\n        best_value = np.inf\n\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                target = population[i]\n                mutant = self.mutate(population)\n                trial = self.crossover(target, mutant)\n                trial = self.periodic_correction(trial, lb, ub)\n\n                f_target = func(target)\n                f_trial = func(trial)\n                self.evaluations += 2\n\n                if f_trial < f_target:\n                    population[i] = trial\n                    if f_trial < best_value:\n                        best_candidate, best_value = trial, f_trial\n\n            if best_candidate is not None:\n                best_candidate = self.local_optimize(best_candidate, func)\n                if func(best_candidate) < best_value:\n                    best_value = func(best_candidate)\n                self.evaluations += 1\n\n        return best_candidate", "name": "PeriodicDifferentialEvolution", "description": "Improved adaptive mutation factor and refined local optimization step within the Periodic Differential Evolution to enhance solution precision.", "configspace": "", "generation": 3, "fitness": 0.7189756586454065, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.719 with standard deviation 0.056. And the mean value of best solutions found was 0.268 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "3ad0d57f-ddaa-49e0-bceb-10b17a2f6104", "metadata": {"aucs": [0.7668585661231555, 0.7501152418876461, 0.6399531679254176], "final_y": [0.2425796420388726, 0.2505890597542909, 0.30958418662088516]}, "mutation_prompt": null}
{"id": "e6ae9597-a8de-45ba-9dbf-26a123ffd4e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicDifferentialEvolution:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.periodicity_factor = 0.9\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opposite_population = lb + ub - population\n        return np.concatenate((population, opposite_population))[:self.population_size]\n\n    def mutate(self, population):\n        indices = np.random.choice(range(self.population_size), 3, replace=False)\n        a, b, c = population[indices]\n        self.mutation_factor = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n        return a + self.mutation_factor * (b - c)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.crossover_probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def periodic_correction(self, candidate, lb, ub):\n        midpoint = (ub + lb) / 2\n        half_range = (ub - lb) / 2\n        return midpoint + self.periodicity_factor * (candidate - midpoint) % half_range\n\n    def local_optimize(self, candidate, func):\n        result = minimize(func, candidate, method='L-BFGS-B', bounds=list(zip(func.bounds.lb, func.bounds.ub)))\n        return result.x if result.success else candidate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_candidate = None\n        best_value = np.inf\n\n        while self.evaluations < self.budget:\n            for i in range(self.population_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                target = population[i]\n                mutant = self.mutate(population)\n                trial = self.crossover(target, mutant)\n                trial = self.periodic_correction(trial, lb, ub)\n\n                f_target = func(target)\n                f_trial = func(trial)\n                self.evaluations += 2\n\n                if f_trial < f_target:\n                    population[i] = trial\n                    if f_trial < best_value:\n                        best_candidate, best_value = trial, f_trial\n\n            if best_candidate is not None:\n                best_candidate = self.local_optimize(best_candidate, func)\n                if func(best_candidate) < best_value:\n                    best_value = func(best_candidate)\n                self.evaluations += 1\n\n        return best_candidate", "name": "PeriodicDifferentialEvolution", "description": "Improved population initialization using a quasi-oppositional strategy to enhance exploration in Periodic Differential Evolution.", "configspace": "", "generation": 3, "fitness": 0.793382881700036, "feedback": "The algorithm PeriodicDifferentialEvolution got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.058. And the mean value of best solutions found was 0.234 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "3ad0d57f-ddaa-49e0-bceb-10b17a2f6104", "metadata": {"aucs": [0.7519433979809073, 0.875163431743564, 0.7530418153756369], "final_y": [0.2505893378343478, 0.2004466161629701, 0.2505884274605156]}, "mutation_prompt": null}
{"id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.CR + self.evaluations / self.budget) % 1\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved adaptive crossover strategy by updating the CR parameter dynamically based on search progress for better convergence.", "configspace": "", "generation": 3, "fitness": 0.9068173568612107, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.907 with standard deviation 0.069. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "dca80431-8321-43f9-9a5b-5dab12b8fb53", "metadata": {"aucs": [0.8131616314973309, 0.9293841524720701, 0.977906286614231], "final_y": [0.20044604720194636, 0.18187855173186307, 0.16485818467135183]}, "mutation_prompt": null}
{"id": "2d50f60f-1c28-435d-8fa1-c12177a0ed45", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=15, F=0.8, Cr=0.9):\n        population = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.eval_count += pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Self-adaptive mutation factor\n                F_adaptive = F + 0.1 * np.random.rand() - 0.05\n                F_adaptive = np.clip(F_adaptive, 0.5, 1.0)\n                \n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < Cr, mutant, population[i])\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_optimization(self, func, x0):\n        result = minimize(func, x0, method='BFGS', options={'maxiter': self.budget - self.eval_count})\n        self.eval_count += result.nfev\n        return result.x, result.fun\n\n    def enforce_periodicity(self, x, period):\n        # Constrain periodicity by averaging\n        x = np.array(x)\n        sections = np.split(x, self.dim // period)\n        averaged_sections = np.mean(sections, axis=0)\n        return np.tile(averaged_sections, self.dim // period)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        period = self.dim // 2  # Aim for a periodic solution with half the dimensions\n        x_global, f_global = self.differential_evolution(func, bounds)\n\n        x_periodic = self.enforce_periodicity(x_global, period)\n        x_local, f_local = self.local_optimization(func, x_periodic)\n\n        return x_local if f_local < f_global else x_global", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic incorporating self-adaptive Differential Evolution and constrained periodicity enforcement for improved optimization of multilayer photonic structures.", "configspace": "", "generation": 3, "fitness": 0.8933610514219666, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.893 with standard deviation 0.035. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "5b2245e1-a31f-4061-a229-ba29db7914cf", "metadata": {"aucs": [0.9037405322850232, 0.8468800267703409, 0.9294625952105358], "final_y": [0.164857461039135, 0.18188530242919454, 0.16485586598536894]}, "mutation_prompt": null}
{"id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduction of elitism to maintain the best solutions in the population throughout iterations and enhance exploitation.", "configspace": "", "generation": 3, "fitness": 0.9081843333084038, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.908 with standard deviation 0.050. And the mean value of best solutions found was 0.189 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "dca80431-8321-43f9-9a5b-5dab12b8fb53", "metadata": {"aucs": [0.9786233383744103, 0.8742343895498447, 0.8716952720009562], "final_y": [0.1648559765392702, 0.20044864065577161, 0.20045147805278918]}, "mutation_prompt": null}
{"id": "a5f7fc7d-d8f3-4493-ae29-e77d3fbb91b1", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        self.population_size = min(10 * self.dim, self.population_size + 1) # Increase population size slightly\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced the exploration capability by adjusting the population size dynamically during optimization.", "configspace": "", "generation": 4, "fitness": 0.8695619733372668, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.075. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "metadata": {"aucs": [0.763611637490481, 0.9221466203741167, 0.9229276621472031], "final_y": [0.16485746672130452, 0.18187975049999383, 0.18187898716495743]}, "mutation_prompt": null}
{"id": "2c04b09f-2e5d-4530-9826-b3fb70399d00", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced adaptive mutation factor update to enhance convergence and solution quality.", "configspace": "", "generation": 4, "fitness": 0.9016787931184792, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.902 with standard deviation 0.035. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "metadata": {"aucs": [0.9257740198480565, 0.852084185831372, 0.9271781736760092], "final_y": [0.16485913564984278, 0.16485948172815512, 0.18188459435262627]}, "mutation_prompt": null}
{"id": "c863caaa-a0f7-4f48-8648-8f44941cf141", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced HybridDeLocalSearch with adaptive mutation factor for improved convergence and exploration balance.", "configspace": "", "generation": 4, "fitness": 0.8877140014521414, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.888 with standard deviation 0.070. And the mean value of best solutions found was 0.198 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "metadata": {"aucs": [0.8732946262162584, 0.8097275772037893, 0.9801198009363767], "final_y": [0.20044750615211548, 0.22804618042389635, 0.16485621656494798]}, "mutation_prompt": null}
{"id": "83f09ad7-596a-42c7-a1b3-91f492a1030f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=15, F=0.8, Cr=0.9):\n        population = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.eval_count += pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < Cr, mutant, population[i])\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_optimization(self, func, x0):\n        x0 = x0 + np.random.normal(0, 0.01, size=self.dim)  # Slightly perturb initial point\n        result = minimize(func, x0, method='BFGS', options={'maxiter': self.budget - self.eval_count})\n        self.eval_count += result.nfev\n        return result.x, result.fun\n\n    def enforce_periodicity(self, x, period):\n        return np.tile(x[:period], self.dim // period)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        period = self.dim // 2  # Aim for a periodic solution with half the dimensions\n        x_global, f_global = self.differential_evolution(func, bounds)\n\n        x_periodic = self.enforce_periodicity(x_global, period)\n        x_local, f_local = self.local_optimization(func, x_periodic)\n\n        return x_local if f_local < f_global else x_global", "name": "HybridMetaheuristic", "description": "This refined hybrid metaheuristic algorithm employs Differential Evolution for global exploration and improves local exploitation through a dynamic adjustment of the BFGS local optimization method's initial point for better convergence.", "configspace": "", "generation": 4, "fitness": 0.8585443736390069, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.030. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5b2245e1-a31f-4061-a229-ba29db7914cf", "metadata": {"aucs": [0.8451132360050863, 0.9007464822131418, 0.8297734026987926], "final_y": [0.16486776776706225, 0.16560031836215905, 0.1660448465774611]}, "mutation_prompt": null}
{"id": "4dfb7435-1660-418c-ac4d-a0f8591cf657", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=15, F=0.8, Cr=0.9):\n        population = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.eval_count += pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Self-adaptive mutation factor\n                F_adaptive = F + 0.1 * np.random.rand() - 0.05\n                F_adaptive = np.clip(F_adaptive, 0.5, 1.0)\n                \n                # Dynamically adjust the crossover rate\n                Cr_adaptive = Cr * (1 - self.eval_count / self.budget)\n                \n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < Cr_adaptive, mutant, population[i])\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_optimization(self, func, x0):\n        result = minimize(func, x0, method='BFGS', options={'maxiter': self.budget - self.eval_count})\n        self.eval_count += result.nfev\n        return result.x, result.fun\n\n    def enforce_periodicity(self, x, period):\n        # Constrain periodicity by averaging\n        x = np.array(x)\n        sections = np.split(x, self.dim // period)\n        averaged_sections = np.mean(sections, axis=0)\n        return np.tile(averaged_sections, self.dim // period)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        period = self.dim // 2  # Aim for a periodic solution with half the dimensions\n        x_global, f_global = self.differential_evolution(func, bounds)\n\n        x_periodic = self.enforce_periodicity(x_global, period)\n        x_local, f_local = self.local_optimization(func, x_periodic)\n\n        return x_local if f_local < f_global else x_global", "name": "HybridMetaheuristic", "description": "Enhanced exploration by dynamically adjusting the crossover rate based on iteration progress to improve search space exploration.", "configspace": "", "generation": 4, "fitness": 0.8535054324617849, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.031. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2d50f60f-1c28-435d-8fa1-c12177a0ed45", "metadata": {"aucs": [0.8610214416149178, 0.8119799550184013, 0.8875149007520359], "final_y": [0.16563477323014786, 0.18259024595307938, 0.1662120449180201]}, "mutation_prompt": null}
{"id": "8deca35a-ec67-48a6-855d-f3140e42f06f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial * np.sin(np.pi * np.arange(self.dim))  # Encourage periodicity\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='TNC')  # Changed method\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced adaptive strategies with periodicity encouragement and improved local search parameters.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('`x0` violates bound constraints.').", "error": "ValueError('`x0` violates bound constraints.')", "parent_id": "c863caaa-a0f7-4f48-8648-8f44941cf141", "metadata": {}, "mutation_prompt": null}
{"id": "da3ccee7-7e70-4697-8f82-036bd4765222", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.5  # Initial mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.5 * np.sin(2 * np.pi * self.evaluations / self.budget)  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.CR + self.evaluations / self.budget) % 1\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Integration of learning-based strategy to dynamically adjust the mutation factor F, enhancing exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('`x0` violates bound constraints.').", "error": "ValueError('`x0` violates bound constraints.')", "parent_id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "metadata": {}, "mutation_prompt": null}
{"id": "e532f628-135b-482a-8630-3bf6af83512d", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            # Update crossover probability based on progress\n            self.CR = 0.9 * (1 - self.evaluations / self.budget) + 0.5 * (self.evaluations / self.budget)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced an adaptive crossover probability update based on the iteration progress to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('`x0` violates bound constraints.').", "error": "ValueError('`x0` violates bound constraints.')", "parent_id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "metadata": {}, "mutation_prompt": null}
{"id": "4fa376c7-c644-416f-a95a-2430cb8a9b9a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(5, min(10 * dim, budget // dim))  # Adaptive population size\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size * 2) if idx != i]  # Adjusted for larger population\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n            if self.evaluations % 50 == 0:  # Periodicity enforcement\n                self.pop += 0.05 * (2 * np.pi * np.random.rand(*self.pop.shape))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introducing an adaptive population size and periodicity enforcement to improve exploration and convergence in multilayer photonic structure optimization.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('`x0` violates bound constraints.').", "error": "ValueError('`x0` violates bound constraints.')", "parent_id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "metadata": {}, "mutation_prompt": null}
{"id": "47212eda-1019-406b-aac7-a64a5c5cefc4", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass ImprovedHybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def levy_flight(self, L):\n        u = np.random.normal(0, 1, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (np.abs(v) ** (1/L))\n        return 0.01 * step  # Scale factor to control step size\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            perturbation = self.levy_flight(1.5)  # Lvy flight perturbation\n            candidate_solution = np.clip(self.best_solution + perturbation, lb, ub)\n            candidate_score = func(candidate_solution)\n            self.evaluations += 1\n            if candidate_score < self.best_score:\n                self.best_score = candidate_score\n                self.best_solution = candidate_solution\n            else:\n                result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n                if result.fun < self.best_score:\n                    self.best_score = result.fun\n                    self.best_solution = result.x\n                    self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "ImprovedHybridDeLocalSearch", "description": "Improved HybridDeLocalSearch by integrating Lvy flight mechanism for better exploration and escape from local minima, while maintaining adaptive mutation and crossover strategies.", "configspace": "", "generation": 5, "fitness": 0.8829431343193986, "feedback": "The algorithm ImprovedHybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.883 with standard deviation 0.027. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "2c04b09f-2e5d-4530-9826-b3fb70399d00", "metadata": {"aucs": [0.9203010859340615, 0.8668957493140739, 0.8616325677100604], "final_y": [0.18188082154704976, 0.20044500181462965, 0.16485891941404784]}, "mutation_prompt": null}
{"id": "29286eb3-15c1-425e-b975-a448f8dba48a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced evaluation efficiency by dynamically adjusting population size based on remaining budget to optimize resource allocation.", "configspace": "", "generation": 6, "fitness": 0.940634282504564, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.941 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "c863caaa-a0f7-4f48-8648-8f44941cf141", "metadata": {"aucs": [0.9784826930111978, 0.915143134232083, 0.9282770202704114], "final_y": [0.16485794781773, 0.16485663034306386, 0.1818855153269492]}, "mutation_prompt": null}
{"id": "cd02f9c1-b135-4d29-bde9-dca26cf7d684", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            # Introduce dynamic weighting in local search\n            weight = 1 + (0.1 * np.random.rand())\n            modified_func = lambda x: func(x) * weight\n            result = opt.minimize(modified_func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced local search integration with dynamic weighting to improve exploitation precision near promising areas.", "configspace": "", "generation": 6, "fitness": 0.8986687253191556, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.899 with standard deviation 0.060. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2c04b09f-2e5d-4530-9826-b3fb70399d00", "metadata": {"aucs": [0.9780071923543119, 0.8851808987203678, 0.8328180848827873], "final_y": [0.16485804473191212, 0.16485810235413934, 0.16485779705676462]}, "mutation_prompt": null}
{"id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        # Update the crossover rate based on diversity\n        self.CR = 0.6 + 0.3 * (np.std(scores) / np.mean(scores))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Utilize adaptive crossover rate update strategy based on population diversity to enhance exploration and convergence.", "configspace": "", "generation": 6, "fitness": 0.9160166850071398, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.057. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "2c04b09f-2e5d-4530-9826-b3fb70399d00", "metadata": {"aucs": [0.9250842874141698, 0.8423516992587561, 0.9806140683484935], "final_y": [0.18189268904890188, 0.2004455343346947, 0.16486266041254716]}, "mutation_prompt": null}
{"id": "fe7309d3-cb2b-49fa-b6a6-87cd6de9bff7", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n            self.CR = 0.5 + 0.3 * np.random.rand()  # Dynamic CR adaptation\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced dynamic adaptation of both mutation factor and crossover probability for improved convergence and exploration balance.", "configspace": "", "generation": 6, "fitness": 0.8732448248162091, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.003. And the mean value of best solutions found was 0.188 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "2c04b09f-2e5d-4530-9826-b3fb70399d00", "metadata": {"aucs": [0.8696046128919197, 0.8722234960229025, 0.8779063655338051], "final_y": [0.20044578001309843, 0.1818784541030053, 0.18188167126582333]}, "mutation_prompt": null}
{"id": "71cc5e4e-85c2-436e-b283-bd675819878c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def differential_evolution(self, func, bounds, pop_size=15, F=0.8, Cr=0.9):\n        population = np.random.rand(pop_size, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.eval_count += pop_size\n        best_idx = np.argmin(fitness)\n        best_ind = population[best_idx]\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                \n                # Self-adaptive mutation factor\n                F_adaptive = F + 0.1 * np.random.rand() - 0.05\n                F_adaptive = np.clip(F_adaptive, 0.5, 1.0)\n                \n                # Self-adaptive crossover rate\n                Cr_adaptive = Cr + 0.1 * np.random.rand() - 0.05\n                Cr_adaptive = np.clip(Cr_adaptive, 0.5, 1.0)\n\n                mutant = np.clip(a + F_adaptive * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < Cr_adaptive, mutant, population[i])\n                trial_fitness = func(trial)\n                self.eval_count += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    if trial_fitness < fitness[best_idx]:\n                        best_idx = i\n                        best_ind = trial\n\n        return best_ind, fitness[best_idx]\n\n    def local_optimization(self, func, x0):\n        result = minimize(func, x0, method='BFGS', options={'maxiter': self.budget - self.eval_count})\n        self.eval_count += result.nfev\n        return result.x, result.fun\n\n    def enforce_periodicity(self, x, period):\n        # Constrain periodicity by averaging\n        x = np.array(x)\n        sections = np.split(x, self.dim // period)\n        averaged_sections = np.mean(sections, axis=0)\n        return np.tile(averaged_sections, self.dim // period)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        period = self.dim // 2  # Aim for a periodic solution with half the dimensions\n        x_global, f_global = self.differential_evolution(func, bounds)\n\n        # Introduce elitism by using the best solution found globally\n        x_periodic = self.enforce_periodicity(x_global, period)\n        x_local, f_local = self.local_optimization(func, x_periodic)\n\n        return x_local if f_local < f_global else x_global", "name": "HybridMetaheuristic", "description": "Incorporate self-adaptive crossover rates and elitism strategies into the hybrid method for improved exploration and exploitation in the optimization of multilayer photonic structures.", "configspace": "", "generation": 6, "fitness": 0.852325426700982, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.005. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2d50f60f-1c28-435d-8fa1-c12177a0ed45", "metadata": {"aucs": [0.8475758983760712, 0.850755875911418, 0.8586445058154568], "final_y": [0.18187821070470178, 0.16485675975620528, 0.18187826122164463]}, "mutation_prompt": null}
{"id": "aea3a266-e00d-4f45-b992-402bc5c20752", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + (0.9 - 0.5) * (1 - self.evaluations / self.budget)  # Update F based on progress\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced adaptive mutation strategy by dynamically updating the mutation factor (F) based on convergence progress for improved exploration.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 61 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 61 is out of bounds for axis 0 with size 50')", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {}, "mutation_prompt": null}
{"id": "10bf5e22-6892-4693-8a4f-c382284b9e88", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = list(range(self.population_size))\n            idxs.remove(i)  # More efficient removal\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            F_mod = self.F * np.random.uniform(0.5, 1.5)  # Added adaptive mutation strategy\n            mutant = a + F_mod * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            unique_pop = np.unique(self.pop, axis=0)  # Preserve diversity\n            self.pop = unique_pop[np.random.choice(len(unique_pop), self.population_size, replace=True)]\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploration by integrating a diversity-preserving mechanism and adaptive mutation strategy to improve convergence speed and solution quality.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 67 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 67 is out of bounds for axis 0 with size 50')", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {}, "mutation_prompt": null}
{"id": "949efe81-253d-4d01-8baa-188ea51d700a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.CR + self.evaluations / self.budget) % 1\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            # Change: Promote symmetry by setting half-layer adjustments\n            trial = np.where(cross_points, (mutant + np.flip(mutant)) / 2, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved periodicity by incorporating a constraint to promote symmetric layer thickness adjustments in the differential evolution step.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 61 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 61 is out of bounds for axis 0 with size 50')", "parent_id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "metadata": {}, "mutation_prompt": null}
{"id": "dee78a31-bd0a-4657-9b1f-bede17cb1dd1", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            # Encourage periodicity by adjusting the trial evaluation\n            f_trial = func(trial) + 0.1 * np.sin(np.pi * np.mean(trial))**2\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved the solution quality by introducing periodicity encouragement via a tailored cost function adjustment.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 61 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 61 is out of bounds for axis 0 with size 50')", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {}, "mutation_prompt": null}
{"id": "a4322e76-f153-4da9-8f40-48b999f7b94e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass SinusoidalDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.F = 0.8\n        self.CR = np.random.uniform(0.5, 0.9)\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.random.rand()\n            mutant = a + self.F * (b - c)\n            sinusoidal_perturbation = np.sin(2 * np.pi * np.random.rand(self.dim)) * 0.1\n            mutant = np.clip(mutant + sinusoidal_perturbation, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        self.CR = 0.6 + 0.3 * (np.std(scores) / np.mean(scores))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "SinusoidalDeLocalSearch", "description": "Leverage periodicity by introducing sinusoidal perturbations in Differential Evolution to enhance convergence on wave-like landscapes.", "configspace": "", "generation": 7, "fitness": 0.8900731071065118, "feedback": "The algorithm SinusoidalDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.024. And the mean value of best solutions found was 0.186 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "metadata": {"aucs": [0.9245774019237799, 0.8726692955933665, 0.8729726238023889], "final_y": [0.18187915026870383, 0.18187982403960612, 0.19561783783027897]}, "mutation_prompt": null}
{"id": "99a9867d-b4c9-4349-bf82-6bbf1a1c1e62", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.CR + self.evaluations / self.budget) % 1\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n            self.F = 0.5 + 0.3 * (np.var(self.pop) / self.dim)  # Dynamic mutation factor\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced convergence by dynamically updating mutation factor based on population diversity.", "configspace": "", "generation": 8, "fitness": 0.8397104581985646, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.127. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.053.", "error": "", "parent_id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "metadata": {"aucs": [0.9759246864634705, 0.6698451637671858, 0.8733615243650377], "final_y": [0.16485734670653374, 0.29182766960286366, 0.20044644594968564]}, "mutation_prompt": null}
{"id": "ce36a8b4-7c6e-4e9e-88e5-923978b56e05", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * (1 - self.evaluations / self.budget)  # Dynamic mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.CR + self.evaluations / self.budget) % 1\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced a dynamic mutation factor based on evaluations to balance exploration and exploitation for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.7883639214843569, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.101. And the mean value of best solutions found was 0.185 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "metadata": {"aucs": [0.9261049202790073, 0.6876238702223922, 0.7513629739516713], "final_y": [0.18187837728127598, 0.20725562044481738, 0.16486071988034823]}, "mutation_prompt": null}
{"id": "c1e23ce0-9c6e-42da-bac4-c457c183736e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.CR - self.evaluations / (2 * self.budget))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced dynamic adjustment of crossover rate by incorporating an exploration-exploitation balance mechanism for better convergence.", "configspace": "", "generation": 8, "fitness": 0.8043478182601072, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.106. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "metadata": {"aucs": [0.9516048733902944, 0.7081279301716907, 0.7533106512183362], "final_y": [0.16485615623870942, 0.18187892925511562, 0.18187852969631113]}, "mutation_prompt": null}
{"id": "b2c2b42c-e03f-4881-8850-db33df57416f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0).mean()\n            self.F = 0.5 + 0.3 * np.random.rand() + 0.1 * diversity  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved adaptive mutation factor strategy by incorporating population diversity to better guide search dynamics.", "configspace": "", "generation": 8, "fitness": 0.651915521080778, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.652 with standard deviation 0.110. And the mean value of best solutions found was 0.257 (0. is the best) with standard deviation 0.107.", "error": "", "parent_id": "2c04b09f-2e5d-4530-9826-b3fb70399d00", "metadata": {"aucs": [0.49635990552734544, 0.7375719870630907, 0.7218146706518977], "final_y": [0.406856141256859, 0.1648559574325581, 0.20044713452873897]}, "mutation_prompt": null}
{"id": "8c0b0f61-e34b-4c21-9db0-5b7292c00dfe", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations >= 0.9 * self.budget:  # Prioritize local search near the budget limit\n                self.local_search(func, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved search efficiency by prioritizing local search when close to the budget limit to better exploit high-potential regions.", "configspace": "", "generation": 8, "fitness": 0.7643708761974851, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.021. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.7808323330700827, 0.7352927210408577, 0.7769875744815151], "final_y": [0.16485686162422952, 0.2004464773053274, 0.16485611975794778]}, "mutation_prompt": null}
{"id": "50562ddc-8f6a-42a2-8bc1-4f556e779a12", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.9  # Mutation factor improved from 0.8 to enhance exploration\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved mutation strategy by enhancing the differential weight for better exploration in the search space.", "configspace": "", "generation": 9, "fitness": 0.8216860797747904, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.097. And the mean value of best solutions found was 0.224 (0. is the best) with standard deviation 0.043.", "error": "", "parent_id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "metadata": {"aucs": [0.6900534292468334, 0.922071964967282, 0.8529328451102554], "final_y": [0.28274886143527855, 0.18187954609727786, 0.20725443182752734]}, "mutation_prompt": null}
{"id": "81a2cdb4-d56a-4b11-9d7d-4a41317a89e8", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        progress_ratio = self.evaluations / self.budget\n        self.F = np.random.uniform(0.5, 0.9 - 0.4 * progress_ratio)  # Change 1: Adaptive mutation factor\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced adaptive differential mutation factor based on search progress for improved exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.8407727093396985, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.079. And the mean value of best solutions found was 0.214 (0. is the best) with standard deviation 0.033.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.9287827197841975, 0.8573553190262104, 0.7361800892086876], "final_y": [0.17781378242168866, 0.2072557681978635, 0.2578101890505321]}, "mutation_prompt": null}
{"id": "4c5a01d5-9f25-4eb0-8fd6-612c815a82bf", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.random.rand() * (np.std(scores) / np.mean(scores))  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        # Update the crossover rate based on diversity\n        self.CR = 0.6 + 0.3 * (np.std(scores) / np.mean(scores))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced adaptive mutation factor update strategy based on population diversity to enhance exploration and convergence capability.", "configspace": "", "generation": 9, "fitness": 0.8676378407915747, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.078. And the mean value of best solutions found was 0.194 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "metadata": {"aucs": [0.8246883557191549, 0.9767637599999119, 0.8014614066556571], "final_y": [0.18813399258774532, 0.16485873022832687, 0.2280481258834275]}, "mutation_prompt": null}
{"id": "0cb8de94-2d69-4bed-8132-1776e5fdfab0", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        # Change: Introduce periodicity bias for initialization\n        period = (ub - lb) / self.dim\n        self.pop = lb + np.tile(period, (self.population_size // 2, 1))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduce a periodicity bias in the initialization for better constructive interference in photonic optimization.", "configspace": "", "generation": 9, "fitness": 0.7705874355751846, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.211. And the mean value of best solutions found was 0.232 (0. is the best) with standard deviation 0.083.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.8578793119322627, 0.9746385199393159, 0.4792444748539749], "final_y": [0.18187905713387253, 0.16485774171814116, 0.34877635932255047]}, "mutation_prompt": null}
{"id": "ed51737a-22ac-44b7-94b9-b527c90f70bc", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity_factor = np.std(self.pop)\n            self.F = 0.5 + 0.4 * np.random.rand() * (1 + diversity_factor)  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        # Update the crossover rate based on diversity\n        self.CR = 0.6 + 0.3 * (np.std(scores) / np.mean(scores))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improve convergence by incorporating adaptive and diversity-driven mutation strategy in DE steps.", "configspace": "", "generation": 9, "fitness": 0.5056868615152008, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.506 with standard deviation 0.047. And the mean value of best solutions found was 0.399 (0. is the best) with standard deviation 0.035.", "error": "", "parent_id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "metadata": {"aucs": [0.5727875771444273, 0.4727618942417279, 0.47151111315944716], "final_y": [0.34877809223545464, 0.4239521570081969, 0.4239522707934321]}, "mutation_prompt": null}
{"id": "28a419a5-eac1-400a-bef6-4017ff048a28", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.random.rand()  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial + np.sin(trial * 2 * np.pi))  # Bias towards periodic solutions\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced HybridDeLocalSearch by introducing periodicity bias and dynamic population resizing to improve solution convergence.", "configspace": "", "generation": 10, "fitness": 0.8822665214893263, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.056. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "2c04b09f-2e5d-4530-9826-b3fb70399d00", "metadata": {"aucs": [0.8036556953155083, 0.923303201103475, 0.9198406680489952], "final_y": [0.22804717550147513, 0.18187901869606615, 0.1648565501321685]}, "mutation_prompt": null}
{"id": "aaf62610-acd2-4c83-9c64-6a014f492fa0", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Initial mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0).mean()  # Calculate diversity\n            self.F = 0.5 + 0.4 * (1 - diversity / (ub - lb).mean())  # Dynamic mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduce a dynamic mutation factor based on population diversity to enhance exploration and convergence.", "configspace": "", "generation": 10, "fitness": 0.8842897342193276, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.884 with standard deviation 0.101. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.9783193293888004, 0.7440549327110072, 0.930494940558175], "final_y": [0.16485999187964673, 0.25781030337081956, 0.18188050961284552]}, "mutation_prompt": null}
{"id": "c1f323d8-8005-4074-99ee-107cbb71b3cd", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.F = np.random.uniform(0.5, 0.9)\n        self.CR = np.random.uniform(0.5, 0.9)\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def update_rates(self):\n        progress_ratio = self.evaluations / self.budget\n        self.F = 0.5 + progress_ratio * 0.4  # Gradually increase F\n        self.CR = 0.9 - progress_ratio * 0.4  # Gradually decrease CR\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        self.update_rates()  # Update rates based on progress\n        for i in range(min(self.population_size, self.budget - self.evaluations)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced dynamic mutation and crossover rate updates based on evaluation progress to improve exploration and convergence.", "configspace": "", "generation": 10, "fitness": 0.8896008558193227, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.042. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.8858289522458616, 0.8400507331929945, 0.9429228820191118], "final_y": [0.18188029461120536, 0.16485612078648593, 0.1648655131181882]}, "mutation_prompt": null}
{"id": "5895e485-f87c-40fd-a5a9-7c65cb19f09c", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Initial mutation factor\n        self.CR = np.random.uniform(0.3, 0.9)  # Adjusted adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.5 * np.random.rand()  # Enhanced adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.time_varying_cr()\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def time_varying_cr(self):\n        return 0.4 + 0.5 * (1 - (self.evaluations / self.budget))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploration by incorporating time-varying crossover strategy and self-adaptive mutation to improve solution quality and convergence.", "configspace": "", "generation": 10, "fitness": 0.8868998775565388, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.029. And the mean value of best solutions found was 0.197 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "metadata": {"aucs": [0.8744886390685617, 0.8592222993701353, 0.9269886942309198], "final_y": [0.20044698065181643, 0.20725475817932104, 0.1818787260088892]}, "mutation_prompt": null}
{"id": "1d7858ec-7e69-4ef4-a78f-4e465d73b1e2", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0)  # Calculate population diversity\n            self.F = 0.5 + 0.4 * diversity.mean()  # Update mutation factor based on diversity\n            self.CR = 0.9 - 0.4 * diversity.mean()  # Update crossover probability based on diversity\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved exploitation by dynamically updating mutation factor based on diversity, and tweaking crossover probability for better convergence.", "configspace": "", "generation": 10, "fitness": 0.9278071163595186, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.928 with standard deviation 0.042. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.9788334493006752, 0.9294690841012185, 0.875118815676662], "final_y": [0.16486138194267108, 0.18188212846554275, 0.20044758911692573]}, "mutation_prompt": null}
{"id": "84cbdace-1627-4806-8d7b-5989d1192244", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adjust mutation factor based on layer index to encourage periodic solutions\n            self.F = 0.5 + 0.3 * np.abs(np.sin((np.pi * np.arange(self.dim)) / self.dim)) \n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        # Update the crossover rate based on diversity\n        self.CR = 0.6 + 0.3 * (np.std(scores) / np.mean(scores))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced a periodicity encouraging mechanism by adjusting the mutation factor based on layer index, favoring known optimal periodic solutions.", "configspace": "", "generation": 11, "fitness": 0.8823531439320477, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.045. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "metadata": {"aucs": [0.8187726913961293, 0.922398986000152, 0.9058877543998616], "final_y": [0.181879028508672, 0.18187903574341746, 0.18188126892020662]}, "mutation_prompt": null}
{"id": "7ebe0b0c-0c03-4118-8bb5-839258d88def", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        additional_diversity = np.random.uniform(lb, ub, (self.population_size, self.dim))  # Added line for diversity\n        self.pop = np.vstack((self.pop, quasi_opposite_pop, additional_diversity))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved exploration by adjusting the initialization strategy to enhance diversity and coverage of the search space.", "configspace": "", "generation": 11, "fitness": 0.8891833522444518, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.093. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.9258388054716207, 0.7620210978157216, 0.9796901534460131], "final_y": [0.18188155162661646, 0.22804692244805203, 0.1648564557858757]}, "mutation_prompt": null}
{"id": "d9286c35-0519-432f-b8bb-efc6371c2385", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.5 * np.random.rand()  # Stochastic update for mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        # Update the crossover rate based on diversity\n        self.CR = 0.6 + 0.3 * (np.std(scores) / np.mean(scores))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduce stochastic update to mutation factor to promote exploration and avoid premature convergence.", "configspace": "", "generation": 11, "fitness": 0.8566922325319214, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.083. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "metadata": {"aucs": [0.7396025946374002, 0.9033761793397793, 0.9270979236185847], "final_y": [0.2578102643067587, 0.16485941599434362, 0.18188255228526562]}, "mutation_prompt": null}
{"id": "2542cb3a-277f-4a27-a4a5-4aee9f09c3e5", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(lambda x: func(x) + self.periodicity_penalty(x), 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial) + self.periodicity_penalty(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def periodicity_penalty(self, x):\n        return 0.01 * np.var(np.diff(x))  # Penalize non-periodicity\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(lambda x: func(x) + self.periodicity_penalty(x), self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploitation by incorporating periodicity enforcement in the cost function to guide towards periodic solutions for better performance.", "configspace": "", "generation": 11, "fitness": 0.8512822170660268, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.048. And the mean value of best solutions found was 0.192 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "metadata": {"aucs": [0.7840748863309916, 0.8778054726402431, 0.8919662922268456], "final_y": [0.21133736538259584, 0.18188435668187108, 0.181881404827278]}, "mutation_prompt": null}
{"id": "b579c0cf-b445-4ab2-aa8c-62764197877f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            if self.evaluations // 100 % 2 == 0:  # Alternate strategy based on evaluation count\n                self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduce a dual-population strategy to alternate between exploration and exploitation phases for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.8525147202724012, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.067. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.9422210681093836, 0.7802814929639591, 0.835041599743861], "final_y": [0.16485960008780653, 0.16485895289223818, 0.181878894227828]}, "mutation_prompt": null}
{"id": "36027e3a-fca1-4ec9-9bf3-688d2c510611", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive reduction of mutation factor F based on diversity\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced differential evolution by adaptively reducing mutation factor (F) based on population diversity to maintain solution diversity.", "configspace": "", "generation": 12, "fitness": 0.9289688271210409, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.929 with standard deviation 0.034. And the mean value of best solutions found was 0.180 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "metadata": {"aucs": [0.9766074334460175, 0.9044750264424346, 0.9058240214746708], "final_y": [0.16485691759212806, 0.18813762538707135, 0.1881321717105804]}, "mutation_prompt": null}
{"id": "e79fd9a1-00de-4c7f-858d-7e37e6269e60", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + 0.3 * (self.evaluations / self.budget)  # Dynamic mutation factor\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploration by dynamically adjusting the mutation factor based on the iteration count to maintain diversity.", "configspace": "", "generation": 12, "fitness": 0.8283139899217696, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.093. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "cfcc9644-dd19-4c68-9abf-83dd7acccbff", "metadata": {"aucs": [0.8655233441032085, 0.7006510944035145, 0.9187675312585857], "final_y": [0.1818791874348551, 0.24272342621617293, 0.18188152762238807]}, "mutation_prompt": null}
{"id": "19438fb9-493c-4b60-a485-0ea56913002b", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * (self.evaluations / self.budget)  # Adaptive mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.CR + self.evaluations / self.budget) % 1\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced an adaptive mutation factor update based on search progress for improved exploration and convergence.", "configspace": "", "generation": 12, "fitness": 0.7526500094759978, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.753 with standard deviation 0.058. And the mean value of best solutions found was 0.237 (0. is the best) with standard deviation 0.035.", "error": "", "parent_id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "metadata": {"aucs": [0.8349933671408649, 0.7173450131222671, 0.7056116481648613], "final_y": [0.18813184307607633, 0.25854542089417176, 0.26529346391460695]}, "mutation_prompt": null}
{"id": "4bd9e884-07b5-407e-92c1-91fbdbc53a0f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Initial mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 + 0.2 * (self.evaluations / self.budget)  # Dynamic mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < (self.CR + self.evaluations / self.budget) % 1\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduce a dynamic mutation factor that increases with the search progress to enhance exploration while maintaining convergence.", "configspace": "", "generation": 12, "fitness": 0.8780634479949173, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.075. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "b998a68c-720f-4e4a-a6d7-62ac6286d75d", "metadata": {"aucs": [0.9782052576884847, 0.7983939719334854, 0.857591114362782], "final_y": [0.16485719417022782, 0.16486017813678533, 0.2072550762438521]}, "mutation_prompt": null}
{"id": "0d507a12-bc34-40eb-a5e4-af6e84187c11", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop) + self.periodicity_penalty(self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial) + self.periodicity_penalty(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def periodicity_penalty(self, solution):\n        # Encourage periodicity by penalizing deviations\n        penalty = np.sum(np.abs(np.diff(solution, n=2)))\n        return penalty\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Incorporate periodicity promotion by introducing a penalty for non-periodic solutions, enhancing constructive interference alignment.", "configspace": "", "generation": 12, "fitness": 0.7677160088510909, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.768 with standard deviation 0.084. And the mean value of best solutions found was 0.226 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.6694554122112948, 0.8748564739132817, 0.7588361404286963], "final_y": [0.2513339641347715, 0.20044575409003074, 0.22564787587032376]}, "mutation_prompt": null}
{"id": "ddfeeb2b-f475-476c-b0b9-c2dac69d0036", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.6 + 0.4 * np.random.rand()  # Adjusted mutation factor range\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        # Update the crossover rate based on diversity\n        self.CR = 0.6 + 0.3 * (np.std(scores) / np.mean(scores))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploration by dynamically adjusting the mutation factor range for better diversity management.", "configspace": "", "generation": 13, "fitness": 0.8955932242306704, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.896 with standard deviation 0.117. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "metadata": {"aucs": [0.7298114247252188, 0.9826556788651272, 0.9743125691016657], "final_y": [0.24257738734271572, 0.16485621648447757, 0.16485742658943459]}, "mutation_prompt": null}
{"id": "f0ae3e3f-c849-45a2-b727-c381203e5623", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = 0.5  # Initial crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive reduction of mutation factor F based on diversity\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            # Dynamic adjustment of crossover probability CR based on diversity\n            self.CR = 0.5 + 0.4 * (np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced dynamic adjustment of crossover probability (CR) based on population diversity to enhance exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.9583517781337537, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "36027e3a-fca1-4ec9-9bf3-688d2c510611", "metadata": {"aucs": [0.9821208694155689, 0.9382820990411626, 0.9546523659445297], "final_y": [0.1648606962460717, 0.16485762253656988, 0.16485914719673023]}, "mutation_prompt": null}
{"id": "7db90b89-26d9-466e-a599-d108c842ce31", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive reduction of mutation factor F based on diversity\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            self.CR = 0.9 * (1 - np.std(scores) / np.mean(scores))  # Adjusting crossover probability\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced diversity by dynamically adjusting both mutation factor and crossover probability based on population diversity.", "configspace": "", "generation": 13, "fitness": 0.8858766344850543, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.050. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "36027e3a-fca1-4ec9-9bf3-688d2c510611", "metadata": {"aucs": [0.8466406987407846, 0.9565454809918232, 0.8544437237225548], "final_y": [0.16486209666351814, 0.1648589456673637, 0.16486793754211382]}, "mutation_prompt": null}
{"id": "8a73d3c6-9a93-4cbd-848c-97f43e07faf9", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * (1 - scores[i] / np.max(scores))  # Fitness-based scaling for F\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n        # Update the crossover rate based on diversity\n        self.CR = 0.6 + 0.3 * (np.std(scores) / np.mean(scores))\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved exploration by using fitness-based scaling for adaptive mutation factor in differential evolution.", "configspace": "", "generation": 13, "fitness": 0.8515752794318209, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.061. And the mean value of best solutions found was 0.184 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e14f0d19-9a5f-4e27-985c-dd84e165c966", "metadata": {"aucs": [0.7807257363317144, 0.9301872478283768, 0.8438128541353713], "final_y": [0.18813269814636135, 0.18187935970113644, 0.18187885052071429]}, "mutation_prompt": null}
{"id": "8f0d8f5b-4004-4ad8-8ab7-2ad0b408382e", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            confidence_factor = np.random.uniform(0.4, 0.6)  # Introduced confidence-based mutation\n            mutant = a + confidence_factor * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploration by incorporating confidence-based mutation and adaptive population resizing for efficient resource utilization.", "configspace": "", "generation": 13, "fitness": 0.9605865819483995, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.9801623039767523, 0.9465731893540132, 0.955024252514433], "final_y": [0.16485806149517412, 0.1648590272379451, 0.16485613355927486]}, "mutation_prompt": null}
{"id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive reduction of mutation factor F based on diversity\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))  # Dynamic population size\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced performance by introducing a dynamic population size based on the remaining budget to adaptively balance exploration and exploitation.", "configspace": "", "generation": 14, "fitness": 0.9651153678759531, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.023. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "36027e3a-fca1-4ec9-9bf3-688d2c510611", "metadata": {"aucs": [0.9819234230242382, 0.9810346526161249, 0.9323880279874959], "final_y": [0.16485577482666391, 0.16485696098889624, 0.18187864834555256]}, "mutation_prompt": null}
{"id": "bdfcfed9-9141-469e-80d1-eb62d74e34df", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.F = np.random.uniform(0.5, 0.9)\n        self.CR = np.random.uniform(0.5, 0.9)\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def enforce_periodicity(self):\n        for i in range(self.population_size):  # Enforcing periodic patterns\n            period = self.dim // 2\n            self.pop[i][:period] = self.pop[i][period:2*period]  # Reflect periodicity\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0)\n            self.F = 0.6 + 0.3 * diversity.mean()  # Adjusted mutation factor\n            self.CR = 0.8 - 0.3 * diversity.mean()  # Adjusted crossover probability\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        self.enforce_periodicity()  # New call for periodic enforcement\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved solution diversity and convergence by introducing a multi-phase approach with periodic pattern enforcement and adaptive mutation strategies.", "configspace": "", "generation": 14, "fitness": 0.6830315170137228, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.683 with standard deviation 0.225. And the mean value of best solutions found was 0.313 (0. is the best) with standard deviation 0.117.", "error": "", "parent_id": "1d7858ec-7e69-4ef4-a78f-4e465d73b1e2", "metadata": {"aucs": [0.983485449156291, 0.6250679701962614, 0.4405411316886162], "final_y": [0.16485688572286505, 0.3224642828402743, 0.45055706570886966]}, "mutation_prompt": null}
{"id": "3e699e20-8533-4fb0-8d34-beeebb7a1a2f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * (self.best_score / (self.best_score + 1))  # Updated mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved convergence by refining the mutation factor to dynamically adapt based on the best solution's progress.", "configspace": "", "generation": 14, "fitness": 0.4802626393961873, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.480 with standard deviation 0.030. And the mean value of best solutions found was 0.420 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.5214909159557615, 0.44972142480332655, 0.46957557742947376], "final_y": [0.39026842954401897, 0.44297562048933303, 0.4282155531119064]}, "mutation_prompt": null}
{"id": "366212b3-71f2-415f-83fa-a682d07660c4", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.F = np.random.uniform(0.6, 0.9)  # Slightly increased lower bound of F\n        self.CR = np.random.uniform(0.6, 0.9)  # Slightly increased lower bound of CR\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity_factor = np.std(self.pop, axis=0).mean()  # Calculate population diversity\n            confidence_factor = np.random.uniform(0.5, 0.7) * (1 + diversity_factor)  # Enhance mutation with diversity\n            mutant = a + confidence_factor * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced global exploration and local refinement using diversity-based adaptive mutation and periodicity preservation.", "configspace": "", "generation": 14, "fitness": 0.5357158691447337, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.536 with standard deviation 0.140. And the mean value of best solutions found was 0.389 (0. is the best) with standard deviation 0.093.", "error": "", "parent_id": "8f0d8f5b-4004-4ad8-8ab7-2ad0b408382e", "metadata": {"aucs": [0.7338282463587475, 0.42592724136359306, 0.4473921197118603], "final_y": [0.2578109094197937, 0.46287852501556825, 0.4458537915461974]}, "mutation_prompt": null}
{"id": "3a31e754-db64-46ae-a060-b80339c7c2f9", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0)  # Calculate population diversity\n            self.F = 0.5 + 0.4 * diversity.mean()  # Update mutation factor based on diversity\n            self.CR = max(0.1, 0.9 - 0.4 * diversity.mean())  # Update crossover probability more precisely\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploration by adjusting crossover probability dynamically based on population diversity with better precision.", "configspace": "", "generation": 14, "fitness": 0.6241902443750983, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.624 with standard deviation 0.167. And the mean value of best solutions found was 0.336 (0. is the best) with standard deviation 0.091.", "error": "", "parent_id": "1d7858ec-7e69-4ef4-a78f-4e465d73b1e2", "metadata": {"aucs": [0.8595194598258248, 0.4922718174117584, 0.5207794558877118], "final_y": [0.20725595466759805, 0.4106266188352986, 0.38992609082363605]}, "mutation_prompt": null}
{"id": "c856063b-c1d0-4802-84c3-bc424489e7a2", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive reduction of mutation factor F based on diversity\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), constraints={'type': 'eq', 'fun': lambda x: np.sum(x[:self.dim//2] - x[self.dim//2:])}, method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))  # Dynamic population size\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced local search by augmenting the L-BFGS-B step with a periodicity-inducing constraint to encourage periodic solutions.", "configspace": "", "generation": 15, "fitness": 0.9658552823080573, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.022. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "metadata": {"aucs": [0.9819234230242382, 0.9810367753523793, 0.934605648547554], "final_y": [0.16485577482666391, 0.16485624947907718, 0.16485806396964897]}, "mutation_prompt": null}
{"id": "9e9546c5-b838-4ce4-8fde-82ed639c0df9", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.F = 0.8\n        self.CR = np.random.uniform(0.5, 0.9)\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        diversity = np.std(scores) / np.mean(scores)\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.7 * (1 - diversity)\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Refined differential evolution by introducing a diversity-driven adaptive greediness factor and hybrid recombination strategy to enhance exploration and exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.9200311147416406, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.920 with standard deviation 0.065. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "36027e3a-fca1-4ec9-9bf3-688d2c510611", "metadata": {"aucs": [0.8277028354868352, 0.9646498563039815, 0.9677406524341052], "final_y": [0.20044678917840808, 0.16485688115716968, 0.1648569981577992]}, "mutation_prompt": null}
{"id": "824249a0-97c9-4ce1-b1b5-36afa7651341", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = 0.5  # Initial crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive mutation factor based on diversity\n            diversity = np.std(self.pop, axis=0).mean()\n            self.F = 0.5 + 0.3 * (diversity / (ub - lb).mean())  # Adjust F based on normalized diversity\n            self.CR = 0.5 + 0.4 * (np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved exploration-exploitation balance by incorporating a diversity-based adaptive mutation strategy.", "configspace": "", "generation": 15, "fitness": 0.8539291843968703, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.029. And the mean value of best solutions found was 0.194 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "f0ae3e3f-c849-45a2-b727-c381203e5623", "metadata": {"aucs": [0.8128885024941725, 0.8739845036621458, 0.8749145470342925], "final_y": [0.1818796032688016, 0.2004500028353835, 0.20044800273027896]}, "mutation_prompt": null}
{"id": "0a935637-e414-4cb9-838f-464456f9af86", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0)  # Calculate diversity\n            self.F = 0.5 + 0.4 * (1 - diversity / np.max(diversity))  # Adjust mutation factor based on diversity\n            self.CR = 0.5 + 0.4 * (np.max(diversity) - diversity) / np.max(diversity)  # Adjust crossover probability\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved convergence by dynamically adjusting both mutation factor and crossover probability based on population diversity.", "configspace": "", "generation": 15, "fitness": 0.9723403595620308, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "29286eb3-15c1-425e-b975-a448f8dba48a", "metadata": {"aucs": [0.9826528740306073, 0.9479534586284726, 0.9864147460270127], "final_y": [0.16485770208247275, 0.16486071824754367, 0.16485666596441062]}, "mutation_prompt": null}
{"id": "996c9c12-9d8a-4791-9070-2bfa94bac5b9", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            # Add periodicity constraint\n            period_size = self.dim // 2  # Assuming a periodic solution over half the dimension\n            trial[:period_size] = trial[-period_size:]\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved convergence by introducing a periodicity constraint in the differential evolution step to encourage periodic solutions.", "configspace": "", "generation": 15, "fitness": 0.9639639075327658, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "metadata": {"aucs": [0.9573358601170632, 0.9648853408857472, 0.969670521595487], "final_y": [0.16485594729832098, 0.1648583036662793, 0.16486741239256086]}, "mutation_prompt": null}
{"id": "8b72d0e8-3df5-4aee-b0c9-a127d7535078", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.3, 0.7)  # Narrowed adaptive crossover probability range\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n            self.pop[np.argmin(scores)] = self.best_solution  # Ensure elitism preservation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))  # Dynamic population size\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved performance by adjusting crossover probability dynamically and ensuring elitism preservation after local search. ", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'scores' is not defined\").", "error": "NameError(\"name 'scores' is not defined\")", "parent_id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "metadata": {}, "mutation_prompt": null}
{"id": "34c9cc7d-1493-4378-b745-6c9bcb5f1275", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.4, 0.8)  # Refined adaptive mutation factor\n        self.CR = np.random.uniform(0.6, 0.95)  # Refined adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            confidence_factor = np.random.uniform(0.4, 0.6)  # Introduced confidence-based mutation\n            mutant = a + confidence_factor * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved solution by refining adaptive mutation factor and crossover probability ranges for better exploration-exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.8673404157622707, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.100. And the mean value of best solutions found was 0.208 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": "8f0d8f5b-4004-4ad8-8ab7-2ad0b408382e", "metadata": {"aucs": [0.8782130919760692, 0.9834152649634769, 0.740392890347266], "final_y": [0.2004464489171568, 0.16485582070133398, 0.2578100821158297]}, "mutation_prompt": null}
{"id": "b33d37fe-d941-4594-8ca9-763636a5431b", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.9 * (1 - np.std(scores) / np.mean(scores))  # Increase mutation factor\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(0.5 * cross_points, mutant, self.pop[i])  # Quasi-quadratic crossover\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))  # Dynamic population size\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved exploration by introducing quasi-quadratic crossover and an adaptive mutation factor to increase convergence speed and solution quality.", "configspace": "", "generation": 16, "fitness": 0.850557351989592, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.113. And the mean value of best solutions found was 0.212 (0. is the best) with standard deviation 0.051.", "error": "", "parent_id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "metadata": {"aucs": [0.906616455340802, 0.9524937811499204, 0.6925618194780537], "final_y": [0.1881307879276941, 0.16485577515566974, 0.28274669523335105]}, "mutation_prompt": null}
{"id": "cd69c1f0-d117-4ae8-a1cf-ab322acaede9", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            self.CR = 0.9 - 0.4 * (self.evaluations / self.budget)  # Progressive decrease of CR\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), constraints={'type': 'eq', 'fun': lambda x: np.sum(x[:self.dim//2] - x[self.dim//2:])}, method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced global search by dynamically adjusting crossover probability based on the evaluation progress.", "configspace": "", "generation": 16, "fitness": 0.919573679617616, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.920 with standard deviation 0.014. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "c856063b-c1d0-4802-84c3-bc424489e7a2", "metadata": {"aucs": [0.9073075691755161, 0.912286007388912, 0.9391274622884199], "final_y": [0.1818785889677491, 0.1648559327982525, 0.16485587873735363]}, "mutation_prompt": null}
{"id": "fda1fcc8-ec62-42c2-aaea-a7e8ed9409bd", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n        self.last_improvement = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n                    self.last_improvement = self.evaluations\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None and self.evaluations - self.last_improvement < 0.1 * self.budget:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n                self.last_improvement = self.evaluations\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced solution preservation by adapting local search frequency based on recent improvements to maintain diversity and reduce stagnation.", "configspace": "", "generation": 16, "fitness": 0.9242195593654379, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.004. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "metadata": {"aucs": [0.9277056418406091, 0.9191771970350158, 0.9257758392206887], "final_y": [0.1648557865814163, 0.16485909311407, 0.181878115145663]}, "mutation_prompt": null}
{"id": "099c0015-e589-4af0-8a0d-275307665290", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            confidence_factor = np.random.uniform(0.45, 0.65)  # Refined confidence-based mutation\n            mutant = a + confidence_factor * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced mutation strategy by introducing a refined confidence factor range to better balance exploration and exploitation.", "configspace": "", "generation": 17, "fitness": 0.8072173211140367, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.189. And the mean value of best solutions found was 0.234 (0. is the best) with standard deviation 0.098.", "error": "", "parent_id": "8f0d8f5b-4004-4ad8-8ab7-2ad0b408382e", "metadata": {"aucs": [0.8933227212455745, 0.9835498067811687, 0.5447794353153663], "final_y": [0.16485760217291046, 0.1648592417131275, 0.37188474217164347]}, "mutation_prompt": null}
{"id": "6e4c63ed-2900-44aa-9506-94ce1756a8e4", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive reduction of mutation factor F based on diversity and inertia\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores)) * (self.best_score / (self.best_score + 1))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))  # Dynamic population size\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploitation by incorporating inertia-based scaling of the mutation factor to favor convergence towards the best solutions.", "configspace": "", "generation": 17, "fitness": 0.46138831319385964, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.461 with standard deviation 0.007. And the mean value of best solutions found was 0.434 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "metadata": {"aucs": [0.47036258477305415, 0.45288062230008186, 0.46092173250844304], "final_y": [0.4265469562653208, 0.4400712036174008, 0.43547510569697223]}, "mutation_prompt": null}
{"id": "1b0dba3a-46c1-4701-b21f-cd17b0df9e15", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = 0.9  # Adjusted static crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            self.CR = 0.5 + 0.4 * (self.best_score / (self.best_score + np.std(scores)))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            # Add periodicity constraint\n            period_size = self.dim // 2  # Assuming a periodic solution over half the dimension\n            trial[:period_size] = trial[-period_size:]\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced effectiveness by adapting the crossover probability based on solution quality to improve the exploration-exploitation balance.", "configspace": "", "generation": 17, "fitness": 0.6122186182633476, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.612 with standard deviation 0.264. And the mean value of best solutions found was 0.364 (0. is the best) with standard deviation 0.142.", "error": "", "parent_id": "996c9c12-9d8a-4791-9070-2bfa94bac5b9", "metadata": {"aucs": [0.9838184111706244, 0.45288062230008186, 0.39995682131933663], "final_y": [0.16485590464022304, 0.4400712036174008, 0.4855896970440169]}, "mutation_prompt": null}
{"id": "9ba6fb6a-eca9-4925-80ca-0875971228fa", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            self.CR = 0.5 + 0.4 * np.std(scores) / np.mean(scores)  # Updated line\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            # Add periodicity constraint\n            period_size = self.dim // 2  # Assuming a periodic solution over half the dimension\n            trial[:period_size] = trial[-period_size:]\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved diversity by adapting crossover probability based on population variance to enhance solution exploration.", "configspace": "", "generation": 17, "fitness": 0.6635219689538765, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.664 with standard deviation 0.193. And the mean value of best solutions found was 0.312 (0. is the best) with standard deviation 0.105.", "error": "", "parent_id": "996c9c12-9d8a-4791-9070-2bfa94bac5b9", "metadata": {"aucs": [0.9359774338984408, 0.5081514357747101, 0.5464370371884786], "final_y": [0.1648566584478206, 0.3992702095910784, 0.3721630026403452]}, "mutation_prompt": null}
{"id": "dc08d30b-04ad-44fd-8e7c-e847a517dbb4", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)\n        self.CR = np.random.uniform(0.5, 0.9)\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n        self.use_local_search = True # Adaptive local search triggering\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def chaotic_map(self, current_value, max_value): # Chaotic map for parameter adaptation\n        return 0.7 * current_value * (1 - current_value / max_value)\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0)\n            self.F = self.chaotic_map(self.F, 1)  # Chaotic map for mutation factor\n            self.CR = self.chaotic_map(self.CR, 1)  # Chaotic map for crossover probability\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n                else:\n                    self.use_local_search = False # Skip local search if no improvement\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None and self.use_local_search:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced exploration and exploitation by incorporating chaotic maps for parameter adaptation and allowing adaptive local search triggering.", "configspace": "", "generation": 17, "fitness": 0.5182231911452607, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.518 with standard deviation 0.183. And the mean value of best solutions found was 0.394 (0. is the best) with standard deviation 0.146.", "error": "", "parent_id": "0a935637-e414-4cb9-838f-464456f9af86", "metadata": {"aucs": [0.7735482103166612, 0.4247570240256705, 0.35636433909345044], "final_y": [0.19084246738634003, 0.46439276560214127, 0.5257517022675344]}, "mutation_prompt": null}
{"id": "2775259d-dee0-4ea4-9cbe-98bb9250665f", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.9 * (1 - np.std(scores) / np.mean(scores))  # Adjusted\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            self.CR = 0.5 + 0.3 * (self.best_score / (np.mean(scores) + 1e-9))  # Adjusted\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            period_size = self.dim // 4  # Adjusted\n            trial[:period_size] = np.mean(trial[:period_size])  # Adjusted\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced performance by dynamically adjusting crossover probability and introducing a periodicity constraint that adapts to the current solution.", "configspace": "", "generation": 18, "fitness": 0.9067751283287695, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.907 with standard deviation 0.038. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "996c9c12-9d8a-4791-9070-2bfa94bac5b9", "metadata": {"aucs": [0.8531838432060201, 0.9404020064076913, 0.9267395353725968], "final_y": [0.18187921978911892, 0.16485742334068787, 0.18187992803911834]}, "mutation_prompt": null}
{"id": "acd387c6-32a4-48bf-ade7-b3e6816696fd", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = max(0.1, min(0.9, self.F * (1 - (np.std(scores) / np.mean(scores)))))  # Dynamic adjustment\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            # Add periodicity constraint\n            period_size = self.dim // 2  # Assuming a periodic solution over half the dimension\n            trial[:period_size] = trial[-period_size:]\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced mutation strategy by incorporating dynamic adjustment of the mutation factor based on the diversity of the population.", "configspace": "", "generation": 18, "fitness": 0.9593089034262502, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.033. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "996c9c12-9d8a-4791-9070-2bfa94bac5b9", "metadata": {"aucs": [0.9818928627482031, 0.9832373894520424, 0.912796458078505], "final_y": [0.16485845208685013, 0.16485668518695218, 0.16485628973962174]}, "mutation_prompt": null}
{"id": "3b13ba71-29e5-4a6a-b6b2-a9873b849d1a", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.F = 0.8\n        self.CR = np.random.uniform(0.5, 0.9)\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n        \n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 * (1 - np.std(scores) / (np.mean(scores) + 1e-9))\n            sinusoidal_perturbation = 0.1 * np.sin(np.linspace(0, np.pi, self.dim))\n            mutant = a + self.F * (b - c) + sinusoidal_perturbation\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), constraints={'type': 'eq', 'fun': lambda x: np.sum(x[:self.dim//2] - x[self.dim//2:])}, method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improve periodicity and convergence by integrating a sinusoidal perturbation and adaptive mutation strategy into the hybrid approach.", "configspace": "", "generation": 18, "fitness": 0.8308810734953878, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.108. And the mean value of best solutions found was 0.208 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": "c856063b-c1d0-4802-84c3-bc424489e7a2", "metadata": {"aucs": [0.7440846841906672, 0.9831009260780746, 0.7654576102174215], "final_y": [0.2578102240802639, 0.16485635129041443, 0.2004474635426322]}, "mutation_prompt": null}
{"id": "721a79bc-9c0f-4379-9f03-54a977f97f55", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim\n        self.F = np.random.uniform(0.5, 0.9)\n        self.CR = np.random.uniform(0.5, 0.9)\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0)\n            self.F = 0.5 + 0.4 * (1 - np.mean(diversity) / np.max(diversity))\n            self.CR = 0.5 + 0.4 * (np.max(diversity) - np.mean(diversity)) / np.max(diversity)\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            periodic_solution = self.enforce_periodicity(self.best_solution)\n            result = opt.minimize(func, periodic_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def enforce_periodicity(self, solution):\n        half_dim = self.dim // 2\n        periodic_solution = np.tile(solution[:half_dim], 2)\n        return periodic_solution\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced performance by introducing dynamic diversity-driven mutation and crossover strategies alongside a modular periodicity constraint for improved exploration and local search.", "configspace": "", "generation": 18, "fitness": 0.7793997894576566, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.779 with standard deviation 0.140. And the mean value of best solutions found was 0.246 (0. is the best) with standard deviation 0.058.", "error": "", "parent_id": "0a935637-e414-4cb9-838f-464456f9af86", "metadata": {"aucs": [0.9771062030598267, 0.6952093558508974, 0.6658838094622459], "final_y": [0.1648562244729863, 0.2827469213954883, 0.2918267043773213]}, "mutation_prompt": null}
{"id": "8455dfdc-379b-4d30-8b29-128a6fda0d2d", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            diversity = np.std(self.pop, axis=0)  # Calculate diversity\n            self.F = 0.5 + 0.4 * (1 - diversity / np.max(diversity))  # Adjust mutation factor based on diversity\n            self.CR = 0.5 + 0.2 * (np.max(diversity) - diversity) / np.max(diversity)  # Adjust crossover probability\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Enhanced performance by adjusting crossover probability using a more refined expression to better adapt to diversity.", "configspace": "", "generation": 18, "fitness": 0.8559011517963641, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.126. And the mean value of best solutions found was 0.198 (0. is the best) with standard deviation 0.048.", "error": "", "parent_id": "0a935637-e414-4cb9-838f-464456f9af86", "metadata": {"aucs": [0.8988592593165792, 0.9838050204899783, 0.6850391755825346], "final_y": [0.16486103872989788, 0.16485599212461421, 0.2657195088559452]}, "mutation_prompt": null}
{"id": "2ab23483-2bc8-46c3-a9f3-7d350d1836ae", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            # Add periodicity constraint\n            period_size = self.dim // 2  # Assuming a periodic solution over half the dimension\n            trial[:period_size] = trial[-period_size:] = (trial[:period_size] + trial[-period_size:]) / 2\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved handling of periodicity by ensuring full symmetry in periodic solutions to enhance constructive interference.", "configspace": "", "generation": 19, "fitness": 0.88616896886724, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.138. And the mean value of best solutions found was 0.204 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "996c9c12-9d8a-4791-9070-2bfa94bac5b9", "metadata": {"aucs": [0.9854556885470334, 0.9826868949633271, 0.6903643230913596], "final_y": [0.164858123009766, 0.1648580788534003, 0.28274712881360153]}, "mutation_prompt": null}
{"id": "4a242348-37eb-48df-9a50-e0a100b3c66c", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Stochastic adaptive mutation factor F\n            F = np.random.uniform(0.5, 1) * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))  # Dynamic population size\n            self.differential_evolution_step(func, scores, lb, ub)\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved exploration and exploitation through stochastic adaptive mutation factor and dynamic local search integration.", "configspace": "", "generation": 19, "fitness": 0.8908759848078832, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.063. And the mean value of best solutions found was 0.195 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "metadata": {"aucs": [0.8743681154488985, 0.8235111515339298, 0.9747486874408218], "final_y": [0.200445259172222, 0.22064869505825357, 0.16485655282065337]}, "mutation_prompt": null}
{"id": "8b90a81c-d40a-4407-a9a0-ac1e76051985", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(min(self.population_size, self.budget - self.evaluations)):  # Dynamically adjust population size\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            self.F = np.random.uniform(0.4, 0.8)  # Adjusted mutation factor based on diversity\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Refined exploration by dynamically adjusting mutation factor and crossover probability based on population diversity.", "configspace": "", "generation": 19, "fitness": 0.8631997580311203, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.123. And the mean value of best solutions found was 0.210 (0. is the best) with standard deviation 0.052.", "error": "", "parent_id": "8f0d8f5b-4004-4ad8-8ab7-2ad0b408382e", "metadata": {"aucs": [0.9288862962708737, 0.6911534231391084, 0.9695595546833785], "final_y": [0.18188012625527694, 0.28274669372431593, 0.16485643796446992]}, "mutation_prompt": null}
{"id": "ac02aacc-1992-4b6a-92b4-b134e9eee329", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive reduction of mutation factor F based on diversity\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores))\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), constraints={'type': 'eq', 'fun': lambda x: np.sum(x[:self.dim//2] - x[self.dim//2:]**2)}, method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))  # Dynamic population size\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Improved periodicity constraint measurement for enhanced local search efficacy.", "configspace": "", "generation": 19, "fitness": 0.8213719341408111, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.094. And the mean value of best solutions found was 0.204 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "c856063b-c1d0-4802-84c3-bc424489e7a2", "metadata": {"aucs": [0.8924514911838491, 0.8831625305061732, 0.6885017807324113], "final_y": [0.16485604745498816, 0.16485588152724417, 0.28274670264827095]}, "mutation_prompt": null}
{"id": "0df880f0-ddae-4962-86cf-9f2f102ba518", "solution": "import numpy as np\nimport scipy.optimize as opt\n\nclass HybridDeLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 5 * dim  # Adjusted for dynamic population\n        self.F = 0.8  # Mutation factor\n        self.CR = np.random.uniform(0.5, 0.9)  # Adaptive crossover probability\n        self.pop = None\n        self.best_solution = None\n        self.best_score = float('inf')\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        quasi_opposite_pop = lb + ub - self.pop\n        self.pop = np.vstack((self.pop, quasi_opposite_pop))\n\n    def evaluate_population(self, func):\n        scores = np.apply_along_axis(func, 1, self.pop)\n        self.evaluations += len(scores)\n        return scores\n\n    def differential_evolution_step(self, func, scores, lb, ub):\n        for i in range(self.population_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            # Adaptive scaling of mutation factor F with evaluation count\n            self.F = 0.8 * (1 - np.std(scores) / np.mean(scores)) * (1 - self.evaluations / self.budget)  \n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            f_trial = func(trial)\n            self.evaluations += 1\n            if f_trial < scores[i]:\n                scores[i] = f_trial\n                self.pop[i] = trial\n                if f_trial < self.best_score:\n                    self.best_score = f_trial\n                    self.best_solution = trial\n\n    def local_search(self, func, lb, ub):\n        if self.best_solution is not None:\n            result = opt.minimize(func, self.best_solution, bounds=list(zip(lb, ub)), method='L-BFGS-B')\n            if result.fun < self.best_score:\n                self.best_score = result.fun\n                self.best_solution = result.x\n                self.evaluations += result.nfev\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        scores = self.evaluate_population(func)\n        while self.evaluations < self.budget:\n            self.population_size = max(10, int((self.budget - self.evaluations) / (0.1 * self.budget)))  # Dynamic population size\n            self.differential_evolution_step(func, scores, lb, ub)\n            # Maintain elitism by preserving the best solution\n            self.pop[np.argmin(scores)] = self.best_solution\n            if self.evaluations < self.budget:\n                self.local_search(func, lb, ub)\n        return self.best_solution", "name": "HybridDeLocalSearch", "description": "Introduced adaptive scaling for the mutation factor F to enhance the convergence by factoring in the evaluation count.", "configspace": "", "generation": 19, "fitness": 0.9174441983053185, "feedback": "The algorithm HybridDeLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.917 with standard deviation 0.045. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b11cdfdc-0901-479b-992f-b7def1fc910b", "metadata": {"aucs": [0.9790826393570079, 0.8741710817575773, 0.8990788738013706], "final_y": [0.16485589429701852, 0.18187818555495483, 0.1818781560784395]}, "mutation_prompt": null}
