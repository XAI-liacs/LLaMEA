{"id": "32356a35-79d7-46ea-a762-bbf455d06902", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        mutation_vector = np.random.normal(scale=self.mutation_rate, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim, period):\n                    population[i][j:j+period] = population[i][:period]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "HPGA", "description": "A Hybrid Periodicity-Preserving Genetic Algorithm (HPGA) that combines global exploration with local exploitation and encourages periodicity to optimize multilayer structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 60, in __call__\n  File \"<string>\", line 50, in periodicity_heuristic\nValueError: could not broadcast input array from shape (4,) into shape (2,)\n.", "error": "ValueError('could not broadcast input array from shape (4,) into shape (2,)')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 60, in __call__\n  File \"<string>\", line 50, in periodicity_heuristic\nValueError: could not broadcast input array from shape (4,) into shape (2,)\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "c82e513c-27bd-4548-b701-88db4045b054", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        mutation_vector = np.random.normal(scale=self.mutation_rate, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim, period):\n                    population[i][j:j+period] = np.tile(population[i][:period], (self.dim // period + 1))[:period]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "HPGA", "description": "A Hybrid Periodicity-Preserving Genetic Algorithm (HPGA) that combines global exploration with local exploitation and encourages periodicity to optimize multilayer structures, with improved periodicity heuristic logic.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (4,) into shape (2,)').", "error": "ValueError('could not broadcast input array from shape (4,) into shape (2,)')", "parent_id": "32356a35-79d7-46ea-a762-bbf455d06902", "metadata": {}, "mutation_prompt": null}
{"id": "6c86bbcd-4673-4034-929a-2c72d202de7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        mutation_vector = np.random.normal(scale=self.mutation_rate, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim, period):\n                    end_index = j + min(period, self.dim - j)\n                    population[i][j:end_index] = population[i][:end_index]  # Fixed the broadcasting issue\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "HPGA", "description": "A Hybrid Periodicity-Preserving Genetic Algorithm (HPGA) with enhanced periodicity heuristic for structured optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (8,) into shape (4,)').", "error": "ValueError('could not broadcast input array from shape (8,) into shape (4,)')", "parent_id": "32356a35-79d7-46ea-a762-bbf455d06902", "metadata": {}, "mutation_prompt": null}
{"id": "4c082cc3-6b3d-40fa-aa06-031c8150af64", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, replace=False, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        mutation_vector = np.random.normal(scale=self.mutation_rate, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim - period + 1, period):\n                    population[i][j:j+period] = population[i][:period]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "HPGA", "description": "Enhanced HPGA with improved periodicity heuristic and selection strategy for better performance in complex search landscapes.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (2,) into shape (1,)').", "error": "ValueError('could not broadcast input array from shape (2,) into shape (1,)')", "parent_id": "32356a35-79d7-46ea-a762-bbf455d06902", "metadata": {}, "mutation_prompt": null}
{"id": "fec176a6-d4bf-490a-b04b-c5ee9df758b5", "solution": "import numpy as np\n\nclass ATV_PSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n        self.velocity_clamp = 0.1\n        self.n_iterations = budget // self.population_size\n        self.velocity = np.zeros((self.population_size, self.dim))\n\n    def initialize_particles(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_particles(self, func, particles):\n        return np.array([func(ind) for ind in particles])\n    \n    def update_velocity(self, particle, velocity, personal_best, global_best):\n        inertia = self.inertia_weight * velocity\n        cognitive_component = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best - particle)\n        social_component = self.social_coeff * np.random.rand(self.dim) * (global_best - particle)\n        new_velocity = inertia + cognitive_component + social_component\n        return np.clip(new_velocity, -self.velocity_clamp, self.velocity_clamp)\n    \n    def update_particle(self, particle, velocity, lb, ub):\n        new_position = particle + velocity\n        return np.clip(new_position, lb, ub)\n    \n    def enforce_periodicity(self, particle):\n        period = np.random.randint(1, self.dim // 2)\n        for j in range(0, self.dim, period):\n            particle[j:j+period] = particle[:period]\n        return particle\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        particles = self.initialize_particles(lb, ub)\n        personal_best = particles.copy()\n        personal_best_values = self.evaluate_particles(func, particles)\n        global_best = particles[np.argmin(personal_best_values)]\n        global_best_value = np.min(personal_best_values)\n\n        for iteration in range(self.n_iterations):\n            for i in range(self.population_size):\n                self.velocity[i] = self.update_velocity(particles[i], self.velocity[i], personal_best[i], global_best)\n                particles[i] = self.update_particle(particles[i], self.velocity[i], lb, ub)\n                particles[i] = self.enforce_periodicity(particles[i])\n\n            fitness = self.evaluate_particles(func, particles)\n            for i in range(self.population_size):\n                if fitness[i] < personal_best_values[i]:\n                    personal_best_values[i] = fitness[i]\n                    personal_best[i] = particles[i]\n            \n            current_global_best_value = np.min(personal_best_values)\n            if current_global_best_value < global_best_value:\n                global_best_value = current_global_best_value\n                global_best = personal_best[np.argmin(personal_best_values)]\n\n            # Adapt inertia weight for better exploration and exploitation balance\n            self.inertia_weight = 0.9 - (0.5 * (iteration / self.n_iterations))\n\n        return global_best", "name": "ATV_PSO", "description": "Adaptive Time-Varying Particle Swarm Optimization (ATV-PSO) that dynamically adjusts its parameters to balance exploration and exploitation, with a mechanism to preserve and enhance periodic structures for optimizing multilayer photonic designs.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('could not broadcast input array from shape (4,) into shape (2,)').", "error": "ValueError('could not broadcast input array from shape (4,) into shape (2,)')", "parent_id": "32356a35-79d7-46ea-a762-bbf455d06902", "metadata": {}, "mutation_prompt": null}
{"id": "7cf972c5-6f74-4183-bc5a-b2b7c379dd56", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        mutation_vector = np.random.normal(scale=self.mutation_rate, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = population[i][:end_index-j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "A Refined Hybrid Periodicity-Preserving Genetic Algorithm (RHPGA) integrating adaptive periodicity handling to optimize multilayer structures while resolving shape mismatches.", "configspace": "", "generation": 1, "fitness": 0.9010983782527348, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.901 with standard deviation 0.085. And the mean value of best solutions found was 0.190 (0. is the best) with standard deviation 0.035.", "error": "", "parent_id": "32356a35-79d7-46ea-a762-bbf455d06902", "metadata": {"aucs": [0.9236113716238523, 0.9922408000456184, 0.7874429630887334], "final_y": [0.1648586756421111, 0.16485682958917114, 0.2390632027411722]}, "mutation_prompt": null}
{"id": "b65d34c3-2b5d-4dbb-be76-2ca86ecca266", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        levy_flight = np.random.standard_cauchy(size=self.dim) * self.mutation_rate\n        mutated = np.clip(individual + levy_flight, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = population[i][:end_index-j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhanced RHPGA with improved mutation strategy using Lévy flight to explore the search space more effectively.", "configspace": "", "generation": 2, "fitness": 0.955387091420293, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.042. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "7cf972c5-6f74-4183-bc5a-b2b7c379dd56", "metadata": {"aucs": [0.9712760669331, 0.9964959836929785, 0.8983892236348001], "final_y": [0.16485682302100635, 0.1648568734592657, 0.18188030816129686]}, "mutation_prompt": null}
{"id": "9a322b86-1ff9-4f44-927d-64bdddff4d8a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        mutation_vector = np.random.normal(scale=self.mutation_rate, size=self.dim) + np.random.normal(0, 0.05)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = population[i][:end_index-j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhanced RHPGA with adaptive mutation rate and dynamic periodicity factor adjustment for improved exploration and solution quality.", "configspace": "", "generation": 2, "fitness": 0.8947754287901378, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.895 with standard deviation 0.101. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "7cf972c5-6f74-4183-bc5a-b2b7c379dd56", "metadata": {"aucs": [0.7548520398488845, 0.9910848566464506, 0.9383893898750784], "final_y": [0.20748941996746983, 0.16485883626511455, 0.1648563761981371]}, "mutation_prompt": null}
{"id": "4f122179-6d5d-469f-a0c0-34f70272614b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2  # This line remains unchanged, but we will adjust the mutation rate adaptively in the code\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub, mutation_rate):  # Changed to accept mutation_rate as a parameter\n        mutation_vector = np.random.normal(scale=mutation_rate, size=self.dim)  # Updated to use adaptive mutation_rate\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = population[i][:end_index-j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            current_mutation_rate = self.mutation_rate * (1 - generation / self.n_generations)  # Adaptive mutation rate\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub, current_mutation_rate)  # Updated to use adaptive mutation rate\n                child2 = self.mutate(child2, lb, ub, current_mutation_rate)  # Updated to use adaptive mutation rate\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance RHPGA by adjusting mutation rate adaptively based on generation, improving exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.957007125396459, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cf972c5-6f74-4183-bc5a-b2b7c379dd56", "metadata": {"aucs": [0.9232264852488191, 0.9822854356679955, 0.9655094552725627], "final_y": [0.16485997396725538, 0.16485818580313694, 0.16485881730780827]}, "mutation_prompt": null}
{"id": "1065475c-9a04-4218-b3b4-52607338fdb8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen)  # Adaptive mutation\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhanced Hybrid Genetic Algorithm (EHGA) with adaptive mutation and improved periodicity handling for optimized performance in multilayer structure design.", "configspace": "", "generation": 2, "fitness": 0.9638686613131782, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.029. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cf972c5-6f74-4183-bc5a-b2b7c379dd56", "metadata": {"aucs": [0.9232264852488191, 0.9820665742905592, 0.9863129244001563], "final_y": [0.16485997396725538, 0.16486076493896218, 0.16485965044951523]}, "mutation_prompt": null}
{"id": "4e77a70a-d7f5-4130-b9d1-e73f395d3fc1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        adaptive_crossover_prob = 0.7 + 0.2 * np.random.rand()  # Adaptive crossover\n        if np.random.rand() < adaptive_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        adaptive_mutation_rate = self.mutation_rate * np.random.rand()  # Adaptive mutation\n        mutation_vector = np.random.normal(scale=adaptive_mutation_rate, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                for j in range(0, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = population[i][:end_index-j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhanced RHPGA with adaptive crossover and mutation rates for improved exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.9495838219834439, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.950 with standard deviation 0.058. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cf972c5-6f74-4183-bc5a-b2b7c379dd56", "metadata": {"aucs": [0.8676979888210415, 0.9924868052013702, 0.9885666719279198], "final_y": [0.16485861921959588, 0.1648596594157059, 0.16485742399817382]}, "mutation_prompt": null}
{"id": "d8054454-4dd8-4aa2-80eb-b4044905dc61", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.85  # Increased probability for more crossovers\n        self.mutation_rate = 0.15  # Reduced mutation rate\n        self.local_search_prob = 0.2  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def dynamic_crossover(self, parent1, parent2, gen, max_gen):\n        if np.random.rand() < self.crossover_prob * (gen / max_gen):  # Dynamic crossover\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen)\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return np.clip(result.x, lb, ub)  # Ensure it's within bounds\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:\n                period = np.random.randint(1, self.dim // 3)  # Modified periodicity\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    pattern_to_apply = np.copy(pattern)[:end_index - j]\n                    pattern_to_apply += np.random.normal(0, 0.01, size=pattern_to_apply.shape)  # Minor random adjustment\n                    population[i][j:end_index] = np.clip(pattern_to_apply, func.bounds.lb, func.bounds.ub)\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.dynamic_crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Refined Enhanced Genetic Algorithm (REGA) improving performance through dynamic crossover and a novel periodicity encouragement mechanism tailored for multilayer photonic structure optimization.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "1065475c-9a04-4218-b3b4-52607338fdb8", "metadata": {}, "mutation_prompt": null}
{"id": "91696f0d-77d7-4932-a62d-56f1ecfef5e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.2  # Increased local search probability\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen)  # Adaptive mutation\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhanced local search probability to boost solution refinement capability in the RHPGA algorithm.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "1065475c-9a04-4218-b3b4-52607338fdb8", "metadata": {}, "mutation_prompt": null}
{"id": "ae8a6f92-9ad3-44e0-b820-032f16c06fb6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, replace=False, p=probabilities)  # no replacement\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen)  # Adaptive mutation\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.7:  # Increased probability to encourage periodicity\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Improved genetic diversity and periodicity encouragement by enhancing selection and periodicity strategies.", "configspace": "", "generation": 3, "fitness": 0.9730515436401127, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.029. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1065475c-9a04-4218-b3b4-52607338fdb8", "metadata": {"aucs": [0.9329246109106227, 0.9974608244625344, 0.9887691955471807], "final_y": [0.1648561867670857, 0.16485580528736132, 0.1648564742865536]}, "mutation_prompt": null}
{"id": "e0a6d10a-1267-4148-b4a1-62200d7f4c19", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2  # Enhanced mutation strategy\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Refined RHPGA with enhanced adaptive mutation and hybrid local search strategies to boost exploration and periodic pattern formation.", "configspace": "", "generation": 3, "fitness": 0.9805338291644062, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1065475c-9a04-4218-b3b4-52607338fdb8", "metadata": {"aucs": [0.9655824179284525, 0.9940700540425491, 0.9819490155222168], "final_y": [0.16485652243260573, 0.16485619230120618, 0.1648561153697511]}, "mutation_prompt": null}
{"id": "24933cc3-dfd9-4a63-b32d-916d2af6f16b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DPEA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1\n        self.n_generations = budget // self.population_size\n        self.intensification_factor = 0.5\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** self.intensification_factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='BFGS')\n        return result.x\n\n    def dynamic_periodicity_heuristic(self, population, gen):\n        period_length = max(1, self.dim // (2 + gen % 5))\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage periodicity with dynamic period length\n                pattern = population[i][:period_length]\n                for j in range(period_length, self.dim, period_length):\n                    end_index = min(j + period_length, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.dynamic_periodicity_heuristic(population, generation)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "DPEA", "description": "Dynamic Periodic Evolutionary Algorithm (DPEA) integrating varying periodic pattern lengths with adaptive search intensification for optimal multilayer structure design.", "configspace": "", "generation": 3, "fitness": 0.9155824213564557, "feedback": "The algorithm DPEA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.094. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "1065475c-9a04-4218-b3b4-52607338fdb8", "metadata": {"aucs": [0.7827475831867385, 0.9803593831172246, 0.9836402977654043], "final_y": [0.24183449287241698, 0.16486298920160958, 0.16486093946550473]}, "mutation_prompt": null}
{"id": "4c409929-58a1-4468-9452-a8501e39f9e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.2  # Adjusted probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Improved local search strategy by tuning the local_search_prob to balance exploration and exploitation.", "configspace": "", "generation": 4, "fitness": 0.9815295178645248, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0a6d10a-1267-4148-b4a1-62200d7f4c19", "metadata": {"aucs": [0.9655824179284525, 0.9919272084205891, 0.9870789272445332], "final_y": [0.16485652243260573, 0.16485631628417174, 0.1648564389811481]}, "mutation_prompt": null}
{"id": "50e43b63-e064-4be0-bf00-3b6d1f084f4e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2  # Enhanced mutation strategy\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.8:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhanced periodicity encouragement to improve convergence by focusing more on known optimal patterns.", "configspace": "", "generation": 4, "fitness": 0.9825105503855904, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0a6d10a-1267-4148-b4a1-62200d7f4c19", "metadata": {"aucs": [0.9632215307460046, 0.9972311931662335, 0.9870789272445332], "final_y": [0.1648567808269339, 0.16485604457054248, 0.1648564389811481]}, "mutation_prompt": null}
{"id": "dadf942d-6e8c-407c-b0dd-552b797537fe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2  # Enhanced mutation strategy\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a dynamic local search probability that increases over generations to intensify local refinement.", "configspace": "", "generation": 4, "fitness": 0.9878396509749785, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0a6d10a-1267-4148-b4a1-62200d7f4c19", "metadata": {"aucs": [0.9768142981448296, 0.9961753281603511, 0.9905293266197548], "final_y": [0.16485586122068896, 0.16485622594247307, 0.16485604690784839]}, "mutation_prompt": null}
{"id": "faf1e605-2036-47c3-ab74-654968f36d51", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2  # Enhanced mutation strategy\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n            \n            elite_index = np.argmin(fitness)\n            elite = population[elite_index].copy()  # Elitism strategy\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n            # Insert the elite back into the new population\n            population[np.random.randint(self.population_size)] = elite\n\n        return best_solution", "name": "RHPGA", "description": "Introducing an elitism strategy to preserve and carry forward the best individual across generations, enhancing convergence to optimal solutions.", "configspace": "", "generation": 4, "fitness": 0.983096022874414, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0a6d10a-1267-4148-b4a1-62200d7f4c19", "metadata": {"aucs": [0.9774465003827059, 0.9845527316719884, 0.9872888365685476], "final_y": [0.1648559192715866, 0.16485700559617333, 0.16485589356639951]}, "mutation_prompt": null}
{"id": "2d410c21-92b2-4559-8f0e-30a7e1a2b6e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Initial local search probability\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        for generation in range(self.n_generations):\n            # Adjust local search probability based on generation\n            self.local_search_prob = 0.05 + 0.1 * (generation / self.n_generations)\n\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            if np.random.rand() < self.local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduced adaptive local search probability to enhance exploitation in promising regions.", "configspace": "", "generation": 4, "fitness": 0.9704574469004758, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.031. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0a6d10a-1267-4148-b4a1-62200d7f4c19", "metadata": {"aucs": [0.9276067926409867, 0.9966866208159072, 0.9870789272445332], "final_y": [0.16485603620895717, 0.16485607691698834, 0.1648564389811481]}, "mutation_prompt": null}
{"id": "750dc624-fc7b-4aed-b463-3040ef164026", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2  # Enhanced mutation strategy\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def periodic_hill_climb(self, func, individual, period, lb, ub):\n        # Periodic hill climbing in small per-period increments\n        for start in range(0, self.dim, period):\n            end = min(start + period, self.dim)\n            segment = individual[start:end]\n            segment = minimize(lambda x: func(np.clip(individual[:start] + x + individual[end:], lb, ub)), \n                               segment, method='L-BFGS-B').x\n            individual[start:end] = segment\n        return individual\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            for i in range(self.population_size):\n                if np.random.rand() < dynamic_local_search_prob:\n                    population[i] = self.local_search(func, population[i], lb, ub)\n                else:\n                    period = np.random.randint(1, self.dim // 2)\n                    population[i] = self.periodic_hill_climb(func, population[i], period, lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a periodic hill-climbing strategy to enhance fine-tuning of periodic solutions during local refinement.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (0,) (3,) ').", "error": "ValueError('operands could not be broadcast together with shapes (0,) (3,) ')", "parent_id": "dadf942d-6e8c-407c-b0dd-552b797537fe", "metadata": {}, "mutation_prompt": null}
{"id": "79dd3080-9756-4593-91ef-908e08e94f3b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        elite_indices = np.argsort(fitness)[:5]  # Favor elite selection\n        indices = np.random.choice(elite_indices, size=2)  # Modified selection strategy\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance selection pressure by increasing elite selection probability to boost convergence.", "configspace": "", "generation": 5, "fitness": 0.9244065765288233, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.093. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "dadf942d-6e8c-407c-b0dd-552b797537fe", "metadata": {"aucs": [0.792357726029999, 0.9959686291025214, 0.9848933744539495], "final_y": [0.181878912638045, 0.16485648617145843, 0.16485656052832331]}, "mutation_prompt": null}
{"id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance adaptive mutation by introducing a decay factor to reduce mutation strength over generations.", "configspace": "", "generation": 5, "fitness": 0.9892222087189367, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "dadf942d-6e8c-407c-b0dd-552b797537fe", "metadata": {"aucs": [0.9835670006448299, 0.9879921180616313, 0.9961075074503491], "final_y": [0.1648562816188499, 0.16486000687663716, 0.16485607375197675]}, "mutation_prompt": null}
{"id": "6e3318e5-354f-4f2a-9e75-d1f271d6978b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        diversity_factor = np.std(individual) * 0.5\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2 + diversity_factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a diversity-promoting mechanism with adaptive mutation to enhance exploration while maintaining periodicity.", "configspace": "", "generation": 5, "fitness": 0.9875548032107497, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "dadf942d-6e8c-407c-b0dd-552b797537fe", "metadata": {"aucs": [0.9876345707495136, 0.9945124147472916, 0.9805174241354437], "final_y": [0.16485643701296826, 0.16485607273097402, 0.16485620517292077]}, "mutation_prompt": null}
{"id": "bed688c3-05b8-42e1-83e7-825c91e12fa7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        adaptive_crossover_prob = self.crossover_prob * (1 - gen / max_gen)  # Adaptive crossover probability\n        if np.random.rand() < adaptive_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (1 - gen / max_gen) ** 2  # Enhanced mutation strategy\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce adaptive crossover probability that decreases over generations to enhance solution diversity in early stages.", "configspace": "", "generation": 5, "fitness": 0.9591751397241439, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.042. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "dadf942d-6e8c-407c-b0dd-552b797537fe", "metadata": {"aucs": [0.8995274191196774, 0.9866751587976568, 0.9913228412550977], "final_y": [0.16485597290101928, 0.16485618987609119, 0.16485628602083757]}, "mutation_prompt": null}
{"id": "f7bafb13-1229-49af-85c6-fc864f7d1d2f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n        self.elitism_rate = 0.1  # Introduced elitism rate\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            elite_count = int(self.elitism_rate * self.population_size)  # Calculate elite count\n            elite_indices = fitness.argsort()[:elite_count]  # Select elite individuals\n            elites = population[elite_indices]  # Preserve top performers\n\n            for i in range(elite_count, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n            population[:elite_count] = elites  # Reinstate elite individuals\n\n        return best_solution", "name": "RHPGA", "description": "Introduce elitism to preserve top solutions, ensuring robust convergence by maintaining high-quality individuals across generations.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {}, "mutation_prompt": null}
{"id": "23f23f46-b44a-4851-ab07-8cc32b85cb77", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 3)  # Changed periodic pattern range\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (1 - np.min(fitness) / best_value)  # Dynamic local search probability\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduced a diversified periodicity pattern strategy and dynamic local search probability based on fitness improvement to enhance the algorithm's adaptability.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 50 is out of bounds for axis 0 with size 50').", "error": "IndexError('index 50 is out of bounds for axis 0 with size 50')", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {}, "mutation_prompt": null}
{"id": "69df7c31-ed54-4b1e-b2d5-5a87f20e2214", "solution": "import numpy as np\n\nclass MGSA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.initial_mutation_rate = 0.1\n        self.temperature = 1.0\n        self.decay_rate = 0.99\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def mutate(self, individual, lb, ub):\n        mutation_strength = self.initial_mutation_rate * self.temperature\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def modular_search(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.5:  # Encourage modular structures\n                module_size = np.random.randint(1, self.dim // 2)\n                module = np.random.uniform(-0.1, 0.1, module_size)\n                for j in range(0, self.dim, module_size):\n                    end_index = min(j + module_size, self.dim)\n                    population[i][j:end_index] += module[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.modular_search(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.mutate(child1, lb, ub)\n                child2 = self.mutate(child2, lb, ub)\n                population[i], population[i + 1] = child1, child2\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n            # Update temperature\n            self.temperature *= self.decay_rate\n\n        return best_solution", "name": "MGSA", "description": "Introduce a modular-based Genetic Simulated Annealing (MGSA) algorithm, combining modular construction with a simulated annealing-inspired temperature control for mutation intensity and acceptance probability.", "configspace": "", "generation": 6, "fitness": 0.5698427716012681, "feedback": "The algorithm MGSA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.570 with standard deviation 0.023. And the mean value of best solutions found was 0.354 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.5407700010338781, 0.5726932802073363, 0.5960650335625899], "final_y": [0.37553350923375994, 0.34932304387809354, 0.3378606094499279]}, "mutation_prompt": null}
{"id": "52bfa9e5-cd44-49d8-862d-f5da12f01553", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * np.sqrt(generation / self.n_generations)  # Modified adaptation strategy\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhanced local search probability adaptation to improve exploitation near promising regions.", "configspace": "", "generation": 6, "fitness": 0.9733591813292728, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9568875916178663, 0.9828623260182707, 0.9803276263516816], "final_y": [0.16485736582129773, 0.1648582688391541, 0.164856231754129]}, "mutation_prompt": null}
{"id": "7354e41f-924b-40cd-a6b3-efe726f95964", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population, gen, max_gen):\n        periodicity_prob = 0.6 * (1 - gen / max_gen)  # Adjusted periodicity probability dynamically\n        for i in range(self.population_size):\n            if np.random.rand() < periodicity_prob:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population, generation, self.n_generations)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Improved periodicity heuristic by adjusting the probability of periodic pattern formation based on generation progress.", "configspace": "", "generation": 6, "fitness": 0.9849745377341727, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9786635165765382, 0.9959324702742985, 0.9803276263516816], "final_y": [0.16485606570848577, 0.16485648400238795, 0.164856231754129]}, "mutation_prompt": null}
{"id": "237381af-7d47-40e2-86e9-a4b784882fa4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (1 - generation / self.n_generations)  # Increased exploration at early stages\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Improved local search by increasing dynamic exploration at early stages to enhance exploitation of promising regions.", "configspace": "", "generation": 7, "fitness": 0.9795099300433501, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9654514713219586, 0.989398066328685, 0.9836802524794066], "final_y": [0.1648564909275133, 0.16485628863024593, 0.16485608633503046]}, "mutation_prompt": null}
{"id": "b5102cff-a199-4a93-a101-bfa75c21b9c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.15  # Decreased initial mutation rate\n        self.local_search_prob = 0.1  # Decreased initial probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 1.5) * 0.95  # Adjusted decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob + (0.2 * generation / self.n_generations)  # Dynamic adjustment\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance adaptive mutation and local search probability with dynamic adjustments to improve exploitation and exploration balance.", "configspace": "", "generation": 7, "fitness": 0.9746768447298573, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9522228399516963, 0.9941713638827656, 0.9776363303551099], "final_y": [0.1648560868372405, 0.16485627468910047, 0.16485594770240275]}, "mutation_prompt": null}
{"id": "c310eca9-089c-401c-a363-7fc5d3a72b0e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.6  # Reduced initial crossover probability\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, generation, max_gen):\n        dynamic_crossover_prob = self.crossover_prob + (0.2 * generation / max_gen)  # Dynamic crossover adjustment\n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.7:  # Increased periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance exploration by introducing a dynamic crossover rate and an improved periodicity encouragement mechanism.", "configspace": "", "generation": 7, "fitness": 0.9760015569131406, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9532993935675934, 0.9910683081690952, 0.9836369690027333], "final_y": [0.1648559780429676, 0.1648560280452167, 0.16485728178593106]}, "mutation_prompt": null}
{"id": "43cd8281-504b-4377-b9d6-3932c563c7ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        adaptive_crossover_prob = self.crossover_prob * (0.5 + 0.5 * gen / max_gen)  # Adaptive crossover\n        if np.random.rand() < adaptive_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='Powell')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)  # Updated crossover\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i + 1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Incorporate adaptive crossover and improve local search exploration to enhance convergence and exploit exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.9535854356928897, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.046. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.8896169057888991, 0.9942691680062232, 0.9768702332835467], "final_y": [0.16486360401411948, 0.16485621195671052, 0.164856353596481]}, "mutation_prompt": null}
{"id": "a7db9503-4bb8-4d74-8d86-0c881dac297d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * np.log(1 + gen) / np.log(1 + max_gen)  # Refined mutation strategy with logarithmic decay\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance the adaptive mutation strategy by incorporating a logarithmic decay factor to refine solution exploration over generations.", "configspace": "", "generation": 7, "fitness": 0.9756481241335507, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9585521258533961, 0.9827550082343275, 0.9856372383129284], "final_y": [0.1648561856829276, 0.16485730128828024, 0.16485631953549262]}, "mutation_prompt": null}
{"id": "413bff6a-7cd6-4845-8078-6865d0938500", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        # Dynamic crossover probability\n        self.crossover_prob = 0.5 + 0.3 * (1 - generation / self.n_generations)\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce dynamic crossover probability to enhance exploration-exploitation balance.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'generation' is not defined\").", "error": "NameError(\"name 'generation' is not defined\")", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {}, "mutation_prompt": null}
{"id": "7594d077-5506-4ae7-b81e-f1719c7ab99a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        dynamic_crossover_prob = self.crossover_prob * ((max_gen - gen) / max_gen)\n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a dynamic crossover probability that decreases over generations to enhance solution refinement in later stages.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'generation' is not defined\").", "error": "NameError(\"name 'generation' is not defined\")", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {}, "mutation_prompt": null}
{"id": "c35ade90-81cb-4de0-8c02-cd8cbf11fb8b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 0.8) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Adjusted adaptive mutation to enhance exploration in the early stages of optimization.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'generation' is not defined\").", "error": "NameError(\"name 'generation' is not defined\")", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {}, "mutation_prompt": null}
{"id": "067190e6-b5de-476a-ae97-989ade81a294", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.85  # Increased crossover probability\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Increase crossover probability to enhance solution diversity and exploration.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'generation' is not defined\").", "error": "NameError(\"name 'generation' is not defined\")", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {}, "mutation_prompt": null}
{"id": "80aae9d3-dbd2-4d96-ae67-41a94af2b93a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA_Improved:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def detect_periodic_structure(self, individual):\n        period = np.random.randint(1, self.dim // 2)\n        pattern = individual[:period]\n        return pattern, period\n\n    def encourage_periodicity(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                pattern, period = self.detect_periodic_structure(population[i])\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.encourage_periodicity(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA_Improved", "description": "Incorporate periodic structure detection to guide crossover and mutation, enhancing convergence towards known optimal structures in multilayer mirrors.", "configspace": "", "generation": 8, "fitness": 0.972714275626493, "feedback": "The algorithm RHPGA_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.018. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9473372171392888, 0.9904622244297395, 0.9803433853104507], "final_y": [0.16485600508643639, 0.16485612395294702, 0.16485858648092766]}, "mutation_prompt": null}
{"id": "8fce2716-6de4-426f-b0de-5f5d971aee0d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        dynamic_crossover_prob = self.crossover_prob * (1 - gen / (2 * max_gen))  # Dynamic crossover probability\n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population, gen, max_gen):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6 + (gen / (2 * max_gen)):  # Enhanced periodicity encouragement over generations\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population, generation, self.n_generations)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce dynamic crossover probability and enhance local search periodicity to improve convergence.", "configspace": "", "generation": 9, "fitness": 0.9628242191640211, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.018. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9679875178468238, 0.9389819745043356, 0.9815031651409035], "final_y": [0.16485592473392308, 0.18333754002168234, 0.16485630631983395]}, "mutation_prompt": null}
{"id": "28ceb4f3-1438-451d-95b8-c90669ecb820", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.85  # Slightly increased crossover probability\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Increase exploration by tweaking crossover probability to enhance genetic diversity.", "configspace": "", "generation": 9, "fitness": 0.9735686487386896, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.959618521831787, 0.983489893454538, 0.9775975309297438], "final_y": [0.16485618389178214, 0.16485632621262736, 0.16485950309888353]}, "mutation_prompt": null}
{"id": "04e35c10-0c6e-4416-9b02-345d988588d4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        self.crossover_prob = 0.6 + 0.4 * (gen / max_gen)  # Adjusted crossover probability strategy\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Incorporate dynamic crossover probability to balance exploration and exploitation over generations.", "configspace": "", "generation": 9, "fitness": 0.9680876124464324, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.022. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.939314318098474, 0.9914950042294164, 0.9734535150114066], "final_y": [0.1648565175798682, 0.1648563046291429, 0.16485618283479053]}, "mutation_prompt": null}
{"id": "49061e58-b688-4761-874d-42d2d02a5a4f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** (1.5)) * 0.95  # Non-uniform decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce non-uniform mutation strength decay to enhance local exploration at early generations.", "configspace": "", "generation": 9, "fitness": 0.9102627119831469, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.910 with standard deviation 0.082. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.7956952507085663, 0.9839158585214378, 0.9511770267194365], "final_y": [0.2004450140422147, 0.16485637943563602, 0.16485604368973672]}, "mutation_prompt": null}
{"id": "c6e60734-f886-4a2e-a4ca-22e586b38f50", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        randomization_factor = np.random.rand() * 0.05  # Added randomization factor\n        mutation_vector = np.random.normal(scale=mutation_strength * (1 + randomization_factor), size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance adaptive mutation by adding a small randomization factor to further diversify the mutation process.", "configspace": "", "generation": 9, "fitness": 0.9389922328071396, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.035. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9038879124803699, 0.9861166578183284, 0.9269721281227206], "final_y": [0.16485644959366785, 0.16485613896844808, 0.16485593693426281]}, "mutation_prompt": null}
{"id": "a663c839-0019-4f17-96bb-0f428244e9ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 1.5) * 0.95  # Adjusted mutation decay rate\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.8:  # Increased periodicity encouragement probability\n                period = np.random.randint(1, self.dim // 3)  # Changed period calculation\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Optimize the periodicity strategy and adjust adaptive mutation to better balance exploration and exploitation.", "configspace": "", "generation": 10, "fitness": 0.9691498128078185, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.030. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9273570864229894, 0.9945572253285606, 0.9855351266719052], "final_y": [0.16485583950595506, 0.16486213453504972, 0.1648564875175199]}, "mutation_prompt": null}
{"id": "adf6311c-8105-408a-ac6f-e0ca953cf2f3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        dynamic_crossover_prob = self.crossover_prob * 0.9  # Dynamic adjustment for crossover probability\n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce dynamic crossover probability adjusting to enhance exploration and exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.9817694137605226, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9588321797617143, 0.9926360223356594, 0.9938400391841944], "final_y": [0.16485587459804396, 0.16486181263775368, 0.16485588647972893]}, "mutation_prompt": null}
{"id": "d5c82bc3-2221-442f-87c6-2b446bbf2fda", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.25  # Increased base mutation rate\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 3)  # Increased power for decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 3)  # Adjusted period length\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Fine-tune periodicity by optimizing the pattern length and enhance mutation rate reduction.", "configspace": "", "generation": 10, "fitness": 0.9835441410752855, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9849472804427802, 0.9973353992501033, 0.9683497435329731], "final_y": [0.16485630190123157, 0.16485589302977488, 0.16485604319557545]}, "mutation_prompt": null}
{"id": "911c90f7-9e2d-40ae-809b-1ee7128398ed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, fitness[i], best_value)  # Change in the mutation function call\n                child2 = self.adaptive_mutate(child2, lb, ub, fitness[i + 1], best_value)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce dynamic mutation rate scaling based on fitness improvement to intensify search around promising regions.", "configspace": "", "generation": 10, "fitness": 0.9831874350838378, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9677376118960374, 0.9943854732431744, 0.9874392201123013], "final_y": [0.16485614031814833, 0.1648567605298351, 0.16485606826191324]}, "mutation_prompt": null}
{"id": "da4e1903-b925-4ad6-9df2-475cf9547bdb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def dynamic_crossover(self, parent1, parent2, gen, max_gen):\n        dynamic_crossover_prob = self.crossover_prob * (1 - gen / max_gen)  # Dynamic crossover rate \n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def improved_local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='TNC')  # Changed local optimizer to 'TNC'\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.dynamic_crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.improved_local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance the exploration and exploitation balance by introducing dynamic crossover rates and implementing a more sophisticated local search strategy.", "configspace": "", "generation": 10, "fitness": 0.8863163805225658, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.145. And the mean value of best solutions found was 0.203 (0. is the best) with standard deviation 0.054.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.6813093456546133, 0.9915536880214582, 0.9860861078916258], "final_y": [0.27910079173128954, 0.1648616991729882, 0.16485990158328845]}, "mutation_prompt": null}
{"id": "7c91af67-df4c-4c20-823b-7b5528f38e8d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 1.5) * 0.95  # Adjusted decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        initial_guess = individual + np.random.normal(scale=0.01, size=self.dim)  # Small perturbation to initial guess\n        result = minimize(bounded_func, initial_guess, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Fine-tune the adaptive mutation strategy and local search initialization to enhance exploration and convergence.", "configspace": "", "generation": 11, "fitness": 0.9822047474520278, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9743391636279181, 0.9843661957196306, 0.9879088830085347], "final_y": [0.16485612680280815, 0.16485627471656017, 0.16485643035897402]}, "mutation_prompt": null}
{"id": "d7a5d1a1-1928-4419-ad30-bfd9d4925403", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        # Dynamically adjust crossover_prob based on generation progress\n        crossover_prob = self.crossover_prob * (1 - (1 / self.n_generations))\n        if np.random.rand() < crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Dynamically adjusts crossover probability based on generation progress to enhance exploration and exploitation balance.", "configspace": "", "generation": 11, "fitness": 0.9002041539129464, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.900 with standard deviation 0.112. And the mean value of best solutions found was 0.197 (0. is the best) with standard deviation 0.045.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.7419452156697024, 0.975275673678416, 0.9833915723907208], "final_y": [0.26016786054167107, 0.16486142760447486, 0.16485644979136005]}, "mutation_prompt": null}
{"id": "590d3800-fce3-44a4-8d8d-eb00b05bed15", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        adaptive_crossover_prob = self.crossover_prob * (gen / max_gen)  # Adaptive crossover probability\n        if np.random.rand() < adaptive_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduced adaptive crossover probability to balance exploration and exploitation dynamically.", "configspace": "", "generation": 11, "fitness": 0.9360978993747748, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.037. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.8832701677963214, 0.9660272230892817, 0.9589963072387213], "final_y": [0.16485614251320213, 0.16485712590522228, 0.1648559406057143]}, "mutation_prompt": null}
{"id": "5583db20-54d7-44fb-b423-0bf698c417db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        dynamic_crossover_prob = self.crossover_prob * (1 - gen / max_gen)  # Dynamic crossover probability\n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce dynamic crossover probability to adaptively balance exploration and exploitation.", "configspace": "", "generation": 11, "fitness": 0.9509269408531532, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.951 with standard deviation 0.040. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.8941848064863254, 0.9813592084480542, 0.9772368076250801], "final_y": [0.16485627561999994, 0.16485630121910877, 0.16485607607238328]}, "mutation_prompt": null}
{"id": "cda4093f-6042-4196-983a-dec84ecb33eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        crossover_prob_dynamic = self.crossover_prob * (gen / max_gen)  # Dynamic crossover probability\n        if np.random.rand() < crossover_prob_dynamic:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)  # Pass generation info\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a dynamic crossover probability that increases as the optimization progresses to enhance diversity.", "configspace": "", "generation": 11, "fitness": 0.9375463539109014, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.938 with standard deviation 0.050. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.8677520267024051, 0.9661214711840982, 0.9787655638462012], "final_y": [0.16485686834397317, 0.16485624023947543, 0.16485611650504806]}, "mutation_prompt": null}
{"id": "fa2b2a52-f9fb-4c4e-a144-0998e696c00d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        dynamic_crossover_prob = self.crossover_prob * (1 - gen / max_gen)\n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.7:  # Increased periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Refine RHPGA by dynamically adjusting crossover probability and improving periodicity encouragement for enhanced exploration.", "configspace": "", "generation": 12, "fitness": 0.9661112617120672, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.030. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9239858469867266, 0.9872003794425779, 0.9871475587068972], "final_y": [0.164856050046986, 0.1648565775569777, 0.16485717433729907]}, "mutation_prompt": null}
{"id": "40d23555-d3f7-41b1-977f-56b955f70d45", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob + (generation / self.n_generations) * 0.2  # Increase local search probability\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance local search exploration by increasing the probability dynamically as generations progress, improving fine-tuning of solutions.", "configspace": "", "generation": 12, "fitness": 0.955351299394727, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.042. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.8960416758231182, 0.9881172749445868, 0.9818949474164759], "final_y": [0.16485819561471549, 0.16485849567720556, 0.1648562092237117]}, "mutation_prompt": null}
{"id": "c13ddee5-94a5-4e41-926d-20a5c1a3c3e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen, fitness_var):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * (0.95 + 0.05 * fitness_var / (fitness_var + 1e-9))\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            fitness_var = np.var(fitness)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations, fitness_var)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations, fitness_var)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Refine the adaptive mutation strategy by dynamically adjusting mutation strength based on fitness variance, enhancing exploration in varied landscapes.", "configspace": "", "generation": 12, "fitness": 0.9185570856266887, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.919 with standard deviation 0.021. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.8923746182926654, 0.9427161027550267, 0.9205805358323742], "final_y": [0.16485657258431285, 0.16485677329530846, 0.18187889000399904]}, "mutation_prompt": null}
{"id": "7e7e8c48-20a3-47b8-9e6f-05507ef0b259", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (0.5 + 0.5 * np.sin(np.pi * gen / max_gen)) * 0.95  # Enhanced mutation strategy with sine decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Refine mutation strategy by introducing dynamic mutation strength based on a sine decay factor to enhance exploration in early generations.", "configspace": "", "generation": 12, "fitness": 0.9526925613907937, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.027. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9637392696885976, 0.9787550842449793, 0.9155833302388043], "final_y": [0.16485594776243062, 0.1648565612635272, 0.1648560013945003]}, "mutation_prompt": null}
{"id": "ee01593a-1356-428b-be5f-d9ac7fcb1b7b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, generation, max_gen):\n        dynamic_crossover_prob = self.crossover_prob * ((max_gen - generation) / max_gen) + 0.1\n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population, gen, max_gen):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                adaptive_period = np.random.randint(1, max(2, self.dim // (2 + int(3 * (gen / max_gen)))))\n                pattern = population[i][:adaptive_period]\n                for j in range(adaptive_period, self.dim, adaptive_period):\n                    end_index = min(j + adaptive_period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population, generation, self.n_generations)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce dynamic crossover probability and enhance periodicity heuristic with adaptive pattern length to improve exploration and exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.9287463428693945, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.929 with standard deviation 0.023. And the mean value of best solutions found was 0.175 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9574873082484315, 0.928009669443502, 0.9007420509162503], "final_y": [0.16485662116559163, 0.1648591541646689, 0.1962660074541549]}, "mutation_prompt": null}
{"id": "bdb2acf2-f8e5-4d08-840c-777aeafbeeae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.98  # Slightly increased mutation strategy\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance mutation strategy by adjusting mutation strength with a slight increase for more exploration.", "configspace": "", "generation": 13, "fitness": 0.9767054376925524, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9755427526899547, 0.9935574482292084, 0.961016112158494], "final_y": [0.16485613099086016, 0.1648579947232499, 0.1648562090309431]}, "mutation_prompt": null}
{"id": "61145e1f-9fc0-4aa7-af72-2f67f0db6d12", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * (np.exp(-gen / max_gen) + np.random.rand() * 0.05)  # Stochastic exponential decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Refine the mutation strategy by introducing a stochastic decay factor based on an exponential function to enhance exploration-exploitation balance in RHPGA.", "configspace": "", "generation": 13, "fitness": 0.9790276655283109, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9532854757089269, 0.9964788765647233, 0.9873186443112822], "final_y": [0.16485613473911676, 0.16485632526587557, 0.16485583427422124]}, "mutation_prompt": null}
{"id": "740c54f4-77ec-4fd0-9113-f50c800ee2b1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        dynamic_crossover_prob = self.crossover_prob * (1 - gen / max_gen)  # Dynamic crossover probability\n        if np.random.rand() < dynamic_crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.8:  # Increased periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a dynamic crossover probability and enhance periodicity encouragement to refine solution diversification.", "configspace": "", "generation": 13, "fitness": 0.893002509839674, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.893 with standard deviation 0.108. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9454499223897815, 0.9915509438854964, 0.7420066632437439], "final_y": [0.16485631347822693, 0.16485655781543485, 0.1881307554442757]}, "mutation_prompt": null}
{"id": "c202ac38-316b-48f8-abd8-75bce668c9d6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        crossover_prob = self.crossover_prob * (1 - gen / max_gen)  # Dynamic crossover probability\n        if np.random.rand() < crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduced a dynamic crossover probability to enhance exploration-exploitation balance over generations.", "configspace": "", "generation": 13, "fitness": 0.9375254360118445, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.938 with standard deviation 0.065. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.8451115749952083, 0.9887513855080996, 0.9787133475322256], "final_y": [0.1648566852993546, 0.16485677814638044, 0.16485593715218128]}, "mutation_prompt": null}
{"id": "b7736511-bdb0-4c11-9e7d-d8f7fd34be67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        if np.random.rand() < (self.crossover_prob * (1 - gen / max_gen)):  # Dynamic crossover probability\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j] * ((end_index - j) / period)  # Refined period replication\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a dynamic crossover probability and refined period replication for improved convergence.", "configspace": "", "generation": 13, "fitness": 0.9636885493484978, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.034. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9155334335106678, 0.9923594749558978, 0.9831727395789279], "final_y": [0.1648560393794548, 0.16485617381495266, 0.16485594282705196]}, "mutation_prompt": null}
{"id": "7f711f25-f87f-4c7f-9863-82fcc4de3c3b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        crossover_prob_adaptive = self.crossover_prob * ((1 - generation / self.n_generations) ** 0.5)\n        if np.random.rand() < crossover_prob_adaptive:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce adaptive crossover probability to enhance diversity and convergence speed over generations.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'generation' is not defined\").", "error": "NameError(\"name 'generation' is not defined\")", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {}, "mutation_prompt": null}
{"id": "d8d2d9ce-838e-4e8c-9a1d-647ac7b18e37", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 60  # Changed population size\n        self.crossover_prob = 0.85  # Adjusted crossover probability\n        self.mutation_rate = 0.25  # Increased mutation rate\n        self.local_search_prob = 0.2  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 3) * 0.9  # Modified mutation decay\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.65:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 3)  # Changed period range\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n            \n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a dual-phase optimization strategy combining adaptive mutation decay with a structured iterative refinement process to enhance convergence.", "configspace": "", "generation": 14, "fitness": 0.9714065941164715, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.017. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9491596418568825, 0.9740478684388345, 0.9910122720536974], "final_y": [0.17913226098910018, 0.1648560553511519, 0.16485672861667422]}, "mutation_prompt": null}
{"id": "c90aa8cc-e04a-4e5a-ab53-2c6794aa01e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen, fitness_improvement):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * (1 - fitness_improvement)  # Adjusted mutation strategy\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                fitness_improvement = (best_value - np.min(fitness)) / best_value if best_value != float('inf') else 0  # Calculate fitness improvement\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations, fitness_improvement)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations, fitness_improvement)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce dynamic mutation rate adjustment based on fitness improvement to enhance convergence speed. ", "configspace": "", "generation": 14, "fitness": 0.9842624522844133, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9697465635591979, 0.9920285212403449, 0.9910122720536974], "final_y": [0.16485606766878202, 0.16485585735980257, 0.16485672861667422]}, "mutation_prompt": null}
{"id": "22c1d296-6cc3-4212-9dab-0d56b6ba4f7b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        temperature = (1 - gen / max_gen) * 0.95  # Introduced temperature-based mutation control\n        mutation_strength = self.mutation_rate * temperature\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a temperature-controlled mutation strength to balance exploration and exploitation across generations.", "configspace": "", "generation": 14, "fitness": 0.9679680311355825, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9583558425914457, 0.9545359787616045, 0.9910122720536974], "final_y": [0.1648561019611704, 0.16485618520450984, 0.16485672861667422]}, "mutation_prompt": null}
{"id": "c882ec19-cbcd-4799-ae07-ef91421b3c6d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.7:\n                period = np.random.randint(2, self.dim // 2)  # Adjusted periodicity encouragement\n                pattern = np.tile(population[i][:period], (self.dim + period - 1) // period)[:self.dim]\n                population[i] = pattern\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        eval_count = 0  # New counter for evaluations\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            eval_count += self.population_size\n\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (1 - generation / self.n_generations)\n            if eval_count < self.budget * dynamic_local_search_prob:  # Modify local search condition\n                for i in range(self.population_size):\n                    if eval_count < self.budget:\n                        population[i] = self.local_search(func, population[i], lb, ub)\n                        eval_count += 1\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a layered periodicity strategy and adaptive local search decoupled from generation count to improve convergence.", "configspace": "", "generation": 14, "fitness": 0.9716523391303339, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9694087665757001, 0.9545359787616045, 0.9910122720536974], "final_y": [0.16485638836727634, 0.16485618520450984, 0.16485672861667422]}, "mutation_prompt": null}
{"id": "31602564-272a-48aa-a3fc-5867341eea9a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob_initial = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2, gen, max_gen):\n        crossover_prob = self.crossover_prob_initial * (1 - gen / max_gen)  # Dynamic crossover probability\n        if np.random.rand() < crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population, best_solution):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        if best_solution is not None:\n            population[0] = best_solution  # Elitism with periodicity consideration\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population, best_solution)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Refine RHPGA by introducing dynamic crossover probability and integrating a periodicity-based elitism strategy.", "configspace": "", "generation": 15, "fitness": 0.9610996260075798, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.038. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9076910278351928, 0.9890798768744541, 0.9865279733130924], "final_y": [0.164856419105462, 0.16485599415168117, 0.1648567141433731]}, "mutation_prompt": null}
{"id": "e485ab51-b2d8-4f7c-b049-7926a7e0cb52", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        diversity = np.std(individual)  # Calculate diversity as the standard deviation\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95 * (1 + diversity)  # Include diversity\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            # Implement elitism by keeping the best solution in the population\n            elite_index = np.argmin(fitness)\n            if not np.any(np.all(population == population[elite_index], axis=1)):\n                population[np.argmax(fitness)] = population[elite_index]\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Integrate elitism and adaptive mutation based on population diversity to improve convergence and exploration.", "configspace": "", "generation": 15, "fitness": 0.9863517042136127, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9786539408054192, 0.9942870240614584, 0.9861141477739607], "final_y": [0.16485651205484964, 0.16485604863538172, 0.16485686317475634]}, "mutation_prompt": null}
{"id": "5c200044-62e7-45de-9864-d75d5b05a9c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.98  # Enhanced mutation strategy with refined decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance the algorithm by adjusting the decay factor from 0.95 to 0.98 for more refined mutation control over generations.", "configspace": "", "generation": 15, "fitness": 0.9594694602243402, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9420432121661302, 0.9827697766276098, 0.9535953918792808], "final_y": [0.1648559868881746, 0.16485624665524523, 0.16485628248199502]}, "mutation_prompt": null}
{"id": "abc6c40e-fdd6-4fd3-97db-eea5779bce91", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (1 + generation / self.n_generations)  # Change here\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance local search application frequency by scaling local search probability with respect to the generation count.", "configspace": "", "generation": 15, "fitness": 0.9684436006203434, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.026. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9319256378932823, 0.9884530422209366, 0.9849521217468111], "final_y": [0.1648589658050631, 0.1648561254998011, 0.16485591218809204]}, "mutation_prompt": null}
{"id": "18e512e3-c051-4e10-9be6-372ddf3b26ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.9:  # Increased periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Strengthen periodicity enforcement by increasing its application probability to enhance solution quality.", "configspace": "", "generation": 15, "fitness": 0.9779166675341281, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9691775629655629, 0.9919855115397567, 0.9725869280970648], "final_y": [0.1648565176060106, 0.16485634424371176, 0.16485708765204954]}, "mutation_prompt": null}
{"id": "95787123-a7ba-4986-8a54-e01c4d6d263d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 3) * 0.95  # Improved nonlinear decay\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Improve adaptive mutation by using a nonlinear decay factor to enhance exploration capabilities.", "configspace": "", "generation": 16, "fitness": 0.9633980968206775, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9681507731435101, 0.9484893741822412, 0.9735541431362811], "final_y": [0.16485645444707564, 0.1648558363826903, 0.16485623800671434]}, "mutation_prompt": null}
{"id": "f2be44d7-bac4-44f9-af96-968349eaaffb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 1.8) * 0.95  # Adjusted decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Fine-tune adaptive mutation by adjusting the decay factor to improve exploration and exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.9821843430405751, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9604457892599065, 0.9940292900493656, 0.992077949812453], "final_y": [0.1648562727102938, 0.1648558694975405, 0.16485646874050286]}, "mutation_prompt": null}
{"id": "6ef09424-3cf3-4b96-a818-71f55bb19808", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        # Introduce periodic increase in mutation rate\n        periodic_factor = 1.2 if gen % 5 == 0 else 1.0\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * periodic_factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a dynamic mutation rate that increases periodically to escape local minima more effectively.", "configspace": "", "generation": 16, "fitness": 0.9182802642079664, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.918 with standard deviation 0.054. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9177664304238656, 0.9842631753559188, 0.8528111868441148], "final_y": [0.16485635911674523, 0.16485585306189565, 0.1648561217529022]}, "mutation_prompt": null}
{"id": "65894cd4-03a3-4705-8958-c88c2c081558", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    pattern = pattern + np.random.normal(0, 0.01, size=pattern.shape)  # Stochastic variation added\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce stochastic variation in the periodicity encouragement to balance exploration and exploitation.", "configspace": "", "generation": 16, "fitness": 0.9237678855233492, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.047. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.8681751535679851, 0.9198524446790759, 0.9832760583229866], "final_y": [0.16485648336244862, 0.16485624859392534, 0.16485621571506193]}, "mutation_prompt": null}
{"id": "f1af9f6f-9b76-499e-b46b-c9ad5dad8d8f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            best_current_value = np.min(fitness)\n            generation_best_index = np.argmin(fitness)\n            \n            if best_current_value < best_value:\n                best_value = best_current_value\n                best_solution = population[generation_best_index].copy()  # Elitism: copy best solution\n\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n        return best_solution", "name": "RHPGA", "description": "Introduce elitism by preserving the best solution from each generation to enhance convergence.", "configspace": "", "generation": 16, "fitness": 0.9800509008341013, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9800184087872887, 0.9760629223882991, 0.9840713713267157], "final_y": [0.16485585995584073, 0.1648559902688711, 0.1648581431617585]}, "mutation_prompt": null}
{"id": "bbdf8454-0039-4e5f-a231-ef53d34988e0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (1 if np.min(fitness) < best_value else generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Refine the algorithm by dynamically increasing the probability of local search based on fitness improvement rate.", "configspace": "", "generation": 17, "fitness": 0.978022094265037, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9704907315903204, 0.9907938955638429, 0.9727816556409477], "final_y": [0.16485649313763628, 0.16485669242116852, 0.16485607616803344]}, "mutation_prompt": null}
{"id": "0a000916-8954-4b79-af8b-04f33e95f91c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.85  # Increased crossover probability\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Increase crossover probability to improve solution diversity and convergence rate.", "configspace": "", "generation": 17, "fitness": 0.9817058167417931, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9688988277050826, 0.9825590118851452, 0.9936596106351517], "final_y": [0.16485610461432998, 0.16485755531142532, 0.16485638340035724]}, "mutation_prompt": null}
{"id": "76ce7317-669a-40ce-91d3-35a28904b585", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            self.crossover_prob = 0.8 - (0.5 * generation / self.n_generations)  # Dynamic crossover probability\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce a dynamic crossover probability to enhance exploration and exploitation balance in RHPGA.", "configspace": "", "generation": 17, "fitness": 0.9767013486524228, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9801359097356871, 0.9741420508998181, 0.9758260853217633], "final_y": [0.1648569372276849, 0.16485607785592304, 0.16485660650078993]}, "mutation_prompt": null}
{"id": "ded26685-8372-42c0-adbe-523c89dff868", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n\n            # Dynamic penalty for diversity preservation\n            niche_penalty = 1 + 0.1 * (generation / self.n_generations)\n            adjusted_fitness = fitness + niche_penalty * np.var(population, axis=0).sum()\n\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (adjusted_fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Integrate niche preservation with dynamic penalty to balance exploration-exploitation.", "configspace": "", "generation": 17, "fitness": 0.9145098236836624, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.915 with standard deviation 0.097. And the mean value of best solutions found was 0.183 (0. is the best) with standard deviation 0.026.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.7778311168452401, 0.984166614408454, 0.9815317397972929], "final_y": [0.22064875157045405, 0.16485627477165044, 0.16485761215511197]}, "mutation_prompt": null}
{"id": "851bcccb-a435-4684-81ae-f9d8c360dd6b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.1  # Reduced to balance exploration\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def adaptive_crossover(self, parent1, parent2, gen, max_gen):\n        crossover_prob = self.crossover_prob * (1 - gen / max_gen) ** 2  # Adaptive crossover probability\n        if np.random.rand() < crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def hybrid_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 1.5)  # Hybrid mutation with different decay\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.adaptive_crossover(parent1, parent2, generation, self.n_generations)\n                child1 = self.hybrid_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.hybrid_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "The algorithm introduces an adaptive crossover strategy and an enhanced hybrid mutation to balance exploration and exploitation effectively.", "configspace": "", "generation": 17, "fitness": 0.9397632611192179, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.033. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9019591183289721, 0.934915264293843, 0.9824154007348385], "final_y": [0.16485605080686883, 0.16485694266497608, 0.16485680504185907]}, "mutation_prompt": null}
{"id": "3be285a0-b6a7-4073-bffc-9d5b0e98f489", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95 * 0.98  # Enhanced mutation strategy with additional damping factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance mutation strategy by adding a damping factor to mutation strength, allowing finer adjustments as generations progress.", "configspace": "", "generation": 18, "fitness": 0.9810800334204456, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9708510875203311, 0.9904405472498063, 0.9819484654911993], "final_y": [0.1648561479098366, 0.1648561789007521, 0.1648568005267217]}, "mutation_prompt": null}
{"id": "14775da3-c079-40b5-9008-868eef2cf2b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            best_idx = np.argmin(fitness)  # Identify the best individual in the population\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[best_idx]  # Retain the best solution\n\n        return best_solution", "name": "RHPGA", "description": "Introduce elitism by retaining the best individual from each generation to maintain high-quality solutions.", "configspace": "", "generation": 18, "fitness": 0.9611522806483758, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.022. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9705756474775585, 0.9302766228835854, 0.9826045715839832], "final_y": [0.16485621092212, 0.16485615248332897, 0.16485607722966078]}, "mutation_prompt": null}
{"id": "5a8a3a56-9100-4418-9d67-4d6bcbaef9ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * ((generation / self.n_generations)**0.5)  # Modified dynamic local search probability\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Enhance the balance between exploration and exploitation by modifying the dynamic local search probability formula.", "configspace": "", "generation": 18, "fitness": 0.9694480316468846, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.969 with standard deviation 0.026. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9326126322869195, 0.9901886355212369, 0.9855428271324975], "final_y": [0.18187836377035072, 0.16485612345540834, 0.16485595906634598]}, "mutation_prompt": null}
{"id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0  # Initialize stagnation counter\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0  # Reset stagnation counter if improvement\n            else:\n                stagnation_counter += 1  # Increment stagnation counter if no improvement\n                if stagnation_counter > 5:  # Trigger increased local search on stagnation\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Improve exploitation by dynamically adjusting local search probability based on stagnation in fitness improvement.", "configspace": "", "generation": 18, "fitness": 0.9894673426739854, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9879566939299008, 0.9942671719283834, 0.9861781621636719], "final_y": [0.16485661569715226, 0.1648563982283746, 0.16485616340251164]}, "mutation_prompt": null}
{"id": "84a5f0d9-4f8b-44c9-863e-2ba898f9e858", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15  # Increased probability for local search\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def fitness_sharing(self, fitness, population, sigma_share=0.5):\n        shared_fitness = np.zeros_like(fitness)\n        for i in range(self.population_size):\n            niche_count = 1.0\n            for j in range(self.population_size):\n                distance = np.linalg.norm(population[i] - population[j])\n                if distance < sigma_share:\n                    niche_count += 1 - (distance / sigma_share) ** 2\n            shared_fitness[i] = fitness[i] / niche_count\n        return shared_fitness\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')  # Changed local optimizer\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:  # Adjusted periodicity encouragement\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            fitness = self.fitness_sharing(fitness, population)  # Fitness sharing to promote diversity\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n\n        return best_solution", "name": "RHPGA", "description": "Introduce fitness sharing to promote diversity and mitigate premature convergence.", "configspace": "", "generation": 18, "fitness": 0.9710617672360454, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.022. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "566266b6-eaf9-4ce9-98bf-c3aeb4e4abba", "metadata": {"aucs": [0.9400333109265769, 0.9895499833469525, 0.9836020074346068], "final_y": [0.1648561798633974, 0.16485664846445736, 0.16485679322977298]}, "mutation_prompt": null}
{"id": "75377a4b-0793-4056-a871-0054d3a65d98", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 1.5) * 0.9  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0  # Initialize stagnation counter\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0  # Reset stagnation counter if improvement\n            else:\n                stagnation_counter += 1  # Increment stagnation counter if no improvement\n                if stagnation_counter > 5:  # Trigger increased local search on stagnation\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhance exploitation by fine-tuning mutation strategy for improved convergence and solution accuracy.", "configspace": "", "generation": 19, "fitness": 0.9323457823238294, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.932 with standard deviation 0.069. And the mean value of best solutions found was 0.183 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.8343744372505337, 0.9766812345332605, 0.9859816751876939], "final_y": [0.21782962978939913, 0.16485648262447206, 0.1648561725590736]}, "mutation_prompt": null}
{"id": "e05c11bb-104e-40d3-96b0-fafe73d80c9d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Introduce quasi-oppositional initialization\n        opp_population = lb + ub - population\n        return np.vstack((population, opp_population))[:self.population_size]\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n                if stagnation_counter > 5:\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhance population diversity and adaptive criteria for improved exploration-exploitation balance in RHPGA.", "configspace": "", "generation": 19, "fitness": 0.9784482507507971, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.976587022338174, 0.99365542593471, 0.9651023039795072], "final_y": [0.16485624692436152, 0.16485591583142478, 0.16485593351231276]}, "mutation_prompt": null}
{"id": "912d38b2-d68a-4b04-af07-91fac43c89a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.15  # Changed from 0.2 to 0.15\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n                if stagnation_counter > 4:  # Changed from 5 to 4\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhance convergence speed by optimizing mutation strength and increasing local search probability during stagnation.", "configspace": "", "generation": 19, "fitness": 0.9784763785208052, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.9643445047323944, 0.9867178332155379, 0.9843667976144832], "final_y": [0.1648558894978408, 0.16485782560263895, 0.16485620595092354]}, "mutation_prompt": null}
{"id": "fd4052e7-cfb6-40a2-9def-0bd56d36adf7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def diversity_penalty(self, population, diversity_threshold=0.5):\n        centroid = np.mean(population, axis=0)\n        diversity = np.mean(np.linalg.norm(population - centroid, axis=1))\n        if diversity < diversity_threshold:\n            return 1 + (diversity_threshold - diversity) / diversity_threshold\n        return 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population) * self.diversity_penalty(population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n                if stagnation_counter > 5:\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Integrate a penalty-based diversity mechanism to prevent premature convergence and enhance global exploration.", "configspace": "", "generation": 19, "fitness": 0.9821644397641838, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.9770128730098093, 0.987880209412525, 0.981600236870217], "final_y": [0.1648558281155551, 0.16485605708801898, 0.16485642640442744]}, "mutation_prompt": null}
{"id": "e8be4b44-9342-40f0-a3ca-cb13df7d9883", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0  # Initialize stagnation counter\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations + 0.5)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0  # Reset stagnation counter if improvement\n            else:\n                stagnation_counter += 1  # Increment stagnation counter if no improvement\n                if stagnation_counter > 5:  # Trigger increased local search on stagnation\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhance exploitation by dynamically adjusting local search probability based on generation progress and stagnation.", "configspace": "", "generation": 19, "fitness": 0.8702700233831969, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.173. And the mean value of best solutions found was 0.217 (0. is the best) with standard deviation 0.074.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.6257179687533871, 0.9886701320285445, 0.9964219693676595], "final_y": [0.32260005935470815, 0.16485620417560598, 0.16485587673685242]}, "mutation_prompt": null}
{"id": "25fbd1f4-a00a-4bd9-8eaf-69fa38df7994", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub, best_solution=None):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        initial_point = (individual + best_solution) / 2 if best_solution is not None else individual\n        result = minimize(bounded_func, initial_point, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0  # Initialize stagnation counter\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub, best_solution)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0  # Reset stagnation counter if improvement\n            else:\n                stagnation_counter += 1  # Increment stagnation counter if no improvement\n                if stagnation_counter > 5:  # Trigger increased local search on stagnation\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhance local search effectiveness by considering historical best solutions for exploration.", "configspace": "", "generation": 20, "fitness": 0.9835164387248101, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.9758072390312498, 0.9911384447597965, 0.9836036323833838], "final_y": [0.16485620687915137, 0.1648566605755306, 0.16485663086871238]}, "mutation_prompt": null}
{"id": "63f03ea6-413a-4503-b6d1-1ce85bc82fdd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population, improvement_factor):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, max(2, int(self.dim / (2 * (1 + improvement_factor)))))\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0  # Initialize stagnation counter\n\n        for generation in range(self.n_generations):\n            improvement_factor = 1 / (stagnation_counter + 1)\n            population = self.improved_periodicity_heuristic(population, improvement_factor)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0  # Reset stagnation counter if improvement\n            else:\n                stagnation_counter += 1  # Increment stagnation counter if no improvement\n                if stagnation_counter > 5:  # Trigger increased local search on stagnation\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhance periodicity heuristic by dynamically adjusting pattern length based on fitness improvement trends.", "configspace": "", "generation": 20, "fitness": 0.9572530197943306, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.045. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.9881831212697699, 0.9901546097236529, 0.8934213283895688], "final_y": [0.16485616162675387, 0.16485595682010035, 0.16485621699799735]}, "mutation_prompt": null}
{"id": "1166dbdc-2ced-4636-b4dd-e44ebfdf78d6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 3) * 0.95  # Increase decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            elite_idx = np.argmin(fitness)\n            elite = population[elite_idx].copy()  # Retain elite individual\n            \n            for i in range(0, self.population_size, 2):\n                if i == elite_idx:\n                    continue  # Skip elite in crossover\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            population[elite_idx] = elite  # Reinsert elite\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n                if stagnation_counter > 5:\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhancing RHPGA with elite retention and improved mutation for better exploration and convergence.", "configspace": "", "generation": 20, "fitness": 0.9216562348516915, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.922 with standard deviation 0.086. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.9702989519493227, 0.9932248596940203, 0.8014448929117314], "final_y": [0.16485609362610731, 0.1648559461405974, 0.1648563978460157]}, "mutation_prompt": null}
{"id": "a52b1ea1-907a-44c9-bd55-09e72a5a422d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 2) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim)  # Changed from self.dim // 2 to self.dim\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0  # Initialize stagnation counter\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0  # Reset stagnation counter if improvement\n            else:\n                stagnation_counter += 1  # Increment stagnation counter if no improvement\n                if stagnation_counter > 5:  # Trigger increased local search on stagnation\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhance the periodicity heuristic by applying it to a more variable range within the population.", "configspace": "", "generation": 20, "fitness": 0.9746966276409986, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.9901840023216719, 0.9741978991733737, 0.9597079814279503], "final_y": [0.16485593546706878, 0.16486085985314813, 0.16485602974188118]}, "mutation_prompt": null}
{"id": "7aa9487d-e4c7-48f5-8d5f-ea738ef7d06e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RHPGA:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.crossover_prob = 0.8\n        self.mutation_rate = 0.2\n        self.local_search_prob = 0.15\n        self.n_generations = budget // self.population_size\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        return np.array([func(ind) for ind in population])\n\n    def select_parents(self, population, fitness):\n        probabilities = fitness / fitness.sum()\n        indices = np.random.choice(range(self.population_size), size=2, p=probabilities)\n        return population[indices]\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_prob:\n            crossover_point = np.random.randint(1, self.dim - 1)\n            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n            return child1, child2\n        else:\n            return parent1.copy(), parent2.copy()\n\n    def adaptive_mutate(self, individual, lb, ub, gen, max_gen):\n        mutation_strength = self.mutation_rate * ((1 - gen / max_gen) ** 1.5) * 0.95  # Enhanced mutation strategy with decay factor\n        mutation_vector = np.random.normal(scale=mutation_strength, size=self.dim)\n        mutated = np.clip(individual + mutation_vector, lb, ub)\n        return mutated\n\n    def local_search(self, func, individual, lb, ub):\n        def bounded_func(x):\n            return func(np.clip(x, lb, ub))\n        result = minimize(bounded_func, individual, method='L-BFGS-B')\n        return result.x\n\n    def improved_periodicity_heuristic(self, population):\n        for i in range(self.population_size):\n            if np.random.rand() < 0.6:\n                period = np.random.randint(1, self.dim // 2)\n                pattern = population[i][:period]\n                for j in range(period, self.dim, period):\n                    end_index = min(j + period, self.dim)\n                    population[i][j:end_index] = pattern[:end_index - j]\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)\n        best_solution = None\n        best_value = float('inf')\n        stagnation_counter = 0  # Initialize stagnation counter\n\n        for generation in range(self.n_generations):\n            population = self.improved_periodicity_heuristic(population)\n            fitness = self.evaluate_population(func, population)\n            for i in range(0, self.population_size, 2):\n                parent1, parent2 = self.select_parents(population, 1 / (fitness + 1e-9))\n                child1, child2 = self.crossover(parent1, parent2)\n                child1 = self.adaptive_mutate(child1, lb, ub, generation, self.n_generations)\n                child2 = self.adaptive_mutate(child2, lb, ub, generation, self.n_generations)\n                population[i], population[i+1] = child1, child2\n\n            dynamic_local_search_prob = self.local_search_prob * (generation / self.n_generations)\n            if np.random.rand() < dynamic_local_search_prob:\n                for i in range(self.population_size):\n                    population[i] = self.local_search(func, population[i], lb, ub)\n\n            generation_best = np.min(fitness)\n            if generation_best < best_value:\n                best_value = generation_best\n                best_solution = population[np.argmin(fitness)]\n                stagnation_counter = 0  # Reset stagnation counter if improvement\n            else:\n                stagnation_counter += 1  # Increment stagnation counter if no improvement\n                if stagnation_counter > 5:  # Trigger increased local search on stagnation\n                    self.local_search_prob = min(0.3, self.local_search_prob + 0.05)\n\n        return best_solution", "name": "RHPGA", "description": "Enhance mutation strategy by adjusting the decay factor for increased diversity and exploration.", "configspace": "", "generation": 20, "fitness": 0.9862743205540564, "feedback": "The algorithm RHPGA got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b79933b6-8dc4-4afa-a27e-4577340a328f", "metadata": {"aucs": [0.9800700775096785, 0.9903803951590343, 0.9883724889934565], "final_y": [0.16485594288549543, 0.1648572590852868, 0.16485609420341552]}, "mutation_prompt": null}
