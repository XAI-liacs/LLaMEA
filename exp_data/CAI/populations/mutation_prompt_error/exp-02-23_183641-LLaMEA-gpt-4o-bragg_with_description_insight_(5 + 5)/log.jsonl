{"id": "7f0556bb-bc2e-4579-be1c-e012992dc7a0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(20, budget // dim)\n        self.pop = None\n        self.best_solution = None\n        self.best_value = float('-inf')\n        self.bounds = None\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Quasi-oppositional initialization\n        opposite_pop = lb + ub - self.pop\n        self.pop = np.concatenate((self.pop, opposite_pop))\n        \n    def differential_evolution_step(self, func):\n        F = 0.8  # Differential weight\n        CR = 0.9 # Crossover probability\n        new_pop = np.copy(self.pop)\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            R = np.random.randint(self.dim)\n            trial = np.copy(self.pop[i])\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == R:\n                    trial[j] = self.pop[a][j] + F * (self.pop[b][j] - self.pop[c][j])\n                    # Enforce bounds\n                    trial[j] = np.clip(trial[j], self.bounds[0][j], self.bounds[1][j])\n            # Evaluate trial solution\n            trial_value = func(trial)\n            if trial_value > func(self.pop[i]):\n                new_pop[i] = trial\n                if trial_value > self.best_value:\n                    self.best_value = trial_value\n                    self.best_solution = trial\n        self.pop = new_pop\n        \n    def local_optimization(self, func):\n        if self.best_solution is not None:\n            result = minimize(lambda x: -func(x), self.best_solution, bounds=self.bounds, method='L-BFGS-B')\n            if -result.fun > self.best_value:\n                self.best_value = -result.fun\n                self.best_solution = result.x\n\n    def __call__(self, func):\n        self.bounds = (func.bounds.lb, func.bounds.ub)\n        # Initialize population\n        self.initialize_population(self.bounds)\n        evaluations = 0\n        \n        # Main optimization loop\n        while evaluations < self.budget:\n            self.differential_evolution_step(func)\n            evaluations += self.population_size\n            if evaluations + self.dim <= self.budget:\n                # Local optimization\n                self.local_optimization(func)\n                evaluations += self.dim\n\n        return self.best_solution", "name": "BraggOptimizer", "description": "The algorithm combines a symmetrically-initialized Differential Evolution with a local BFGS finetuning step, leveraging periodicity and modularity in the search for optimal multilayer structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 64, in __call__\n  File \"<string>\", line 47, in local_optimization\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nValueError: too many values to unpack (expected 2)\n.", "error": "ValueError('too many values to unpack (expected 2)')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 64, in __call__\n  File \"<string>\", line 47, in local_optimization\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nValueError: too many values to unpack (expected 2)\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "afd95abd-296d-41e9-ad1e-97b803e94117", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.local_refinement_budget = int(budget * 0.1)  # 10% of the budget for local refinement\n        self.global_budget = budget - self.local_refinement_budget  # Remaining budget for global search\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population with periodic solutions\n        population = self.initialize_population(bounds)\n        scores = np.array([func(individual) for individual in population])\n        for _ in range(self.global_budget // self.population_size):\n            for i in range(self.population_size):\n                # Mutation and crossover\n                indices = [idx for idx in range(self.population_size) if idx != i]\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + self.mutation_factor * (b - c), bounds.lb, bounds.ub)\n                crossover = np.random.rand(self.dim) < self.crossover_probability\n                trial = np.where(crossover, mutant, population[i])\n                # Selection\n                trial_score = func(trial)\n                if trial_score < scores[i]:\n                    population[i], scores[i] = trial, trial_score\n        return population[np.argmin(scores)], np.min(scores)\n\n    def initialize_population(self, bounds):\n        # Periodic initialization with small randomness\n        periodic_solution = np.tile([(bounds.ub - bounds.lb) / 2 + bounds.lb], self.dim // 2)\n        periodic_solution = np.concatenate((periodic_solution, periodic_solution[:self.dim % 2]))\n        population = np.array([periodic_solution + 0.1 * np.random.randn(self.dim) for _ in range(self.population_size)])\n        return np.clip(population, bounds.lb, bounds.ub)\n\n    def local_refinement(self, func, x0, bounds):\n        options = {'maxiter': self.local_refinement_budget, 'disp': False}\n        res = minimize(func, x0, method='L-BFGS-B', bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], options=options)\n        return res.x, res.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_global_solution, best_score = self.differential_evolution(func, bounds)\n        \n        # Apply local search around the best found solution\n        refined_solution, refined_score = self.local_refinement(func, best_global_solution, bounds)\n        if refined_score < best_score:\n            best_global_solution, best_score = refined_solution, refined_score\n        \n        return best_global_solution, best_score", "name": "HybridBraggOptimizer", "description": "A hybrid metaheuristic combining Differential Evolution with local refinement and periodicity encouragement to optimize black box functions with complex landscapes.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 46, in __call__\n  File \"<string>\", line 16, in differential_evolution\n  File \"<string>\", line 36, in initialize_population\n  File \"<string>\", line 36, in <listcomp>\nValueError: operands could not be broadcast together with shapes (1,50) (10,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (1,50) (10,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 46, in __call__\n  File \"<string>\", line 16, in differential_evolution\n  File \"<string>\", line 36, in initialize_population\n  File \"<string>\", line 36, in <listcomp>\nValueError: operands could not be broadcast together with shapes (1,50) (10,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "0a58d13e-d6c1-4157-8a9b-9946654cd234", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "A hybrid Differential Evolution approach enhanced with a local BFGS optimizer and periodicity encouragement to solve black box optimization problems with limited evaluations.", "configspace": "", "generation": 0, "fitness": 0.9261918578617626, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.926 with standard deviation 0.038. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": null, "metadata": {"aucs": [0.922625650014613, 0.8820904943110148, 0.9738594292596598], "final_y": [0.18187914027487673, 0.16485658491373123, 0.1648563086282172]}, "mutation_prompt": null}
{"id": "f54c6f54-5b2f-47b0-9c22-a5d870cb48db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicReflectivityOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.num_generations = int(budget / self.population_size)\n        self.local_refinement_threshold = 0.1 * budget  # Budget threshold to trigger local refinement\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = self.initialize_population(bounds)\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n\n        for generation in range(self.num_generations):\n            population_scores = np.array([self.evaluate_individual(ind, func) for ind in population])\n            evaluations += len(population)\n\n            if generation > self.num_generations * 0.5:  # Encourage periodicity in later generations\n                population_scores += self.periodicity_penalty(population)\n\n            generation_best_score = np.min(population_scores)\n            if generation_best_score < best_score:\n                best_score = generation_best_score\n                best_solution = population[np.argmin(population_scores)]\n\n            if evaluations >= self.local_refinement_threshold:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds)\n                if result.fun < best_score:\n                    best_score, best_solution = result.fun, result.x\n                evaluations += result.nfev\n                break\n\n            next_population = self.evolve_population(population, population_scores, bounds)\n            population = next_population\n\n        return best_solution\n\n    def initialize_population(self, bounds):\n        return np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n\n    def evaluate_individual(self, individual, func):\n        return func(individual)\n\n    def periodicity_penalty(self, population):\n        penalties = np.zeros(len(population))\n        for i, individual in enumerate(population):\n            period = 2  # Assuming known optimal period\n            penalty = np.sum((individual[i::period] - individual[i]) ** 2 for i in range(period))\n            penalties[i] = penalty\n        return penalties\n\n    def evolve_population(self, population, scores, bounds):\n        next_population = np.empty_like(population)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.mutation_factor * (b - c), bounds[:, 0], bounds[:, 1])\n            cross_points = np.random.rand(self.dim) < self.crossover_probability\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            next_population[i] = np.where(cross_points, mutant, population[i])\n        return next_population", "name": "PeriodicReflectivityOptimizer", "description": "The algorithm combines Differential Evolution with a periodicity-encouraging penalty and local gradient-based refinement to optimize the multilayer structure for high reflectivity.", "configspace": "", "generation": 0, "fitness": 0.9003073819347542, "feedback": "The algorithm PeriodicReflectivityOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.900 with standard deviation 0.034. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8957811524176724, 0.861209312825119, 0.9439316805614714], "final_y": [0.1818804245678617, 0.20044631940853397, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "c9457507-109e-4370-9857-8d8cda789fe6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SymmetricQuasiOppositionalDE:\n    def __init__(self, budget, dim, pop_size=50, F=0.8, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.evaluations = 0\n\n    def quasi_oppositional_initialization(self, lb, ub):\n        population = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        pop_opposite = lb + ub - population\n        return np.vstack((population, pop_opposite))\n\n    def mutation(self, population):\n        indices = np.random.choice(range(self.pop_size), 3, replace=False)\n        a, b, c = population[indices]\n        mutant = np.clip(a + self.F * (b - c), 0, 1)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.CR\n        offspring = np.where(crossover_mask, mutant, target)\n        return offspring\n\n    def select(self, candidate, target, func):\n        candidate_fitness = func(candidate)\n        target_fitness = func(target)\n        self.evaluations += 2\n        return candidate if candidate_fitness < target_fitness else target\n\n    def local_search(self, solution, func, bounds):\n        def bounded_func(x):\n            return func(np.clip(x, bounds[0], bounds[1]))\n\n        result = minimize(bounded_func, solution, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n        population = self.quasi_oppositional_initialization(lb, ub)\n        population = population[:self.pop_size]\n\n        best_solution = None\n        best_fitness = float('inf')\n\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                target = population[i]\n                mutant = self.mutation(population)\n                offspring = self.crossover(target, mutant)\n                population[i] = self.select(offspring, target, func)\n\n                # Local search on best found solution\n                if self.evaluations < self.budget:\n                    local_solution = self.local_search(population[i], func, bounds)\n                    local_fitness = func(local_solution)\n                    self.evaluations += 1\n\n                    if local_fitness < best_fitness:\n                        best_solution = local_solution\n                        best_fitness = local_fitness\n\n        return best_solution", "name": "SymmetricQuasiOppositionalDE", "description": "A hybrid metaheuristic algorithm combining a novel Symmetric Quasi-Oppositional DE and Local Search to optimize multilayered structures by leveraging symmetry and periodicity principles.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 60, in __call__\n  File \"<string>\", line 39, in local_search\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 738, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py\", line 386, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_optimize.py\", line 291, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 223, in __init__\n    self._update_fun()\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 295, in _update_fun\n    fx = self._wrapped_fun(self.x)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 21, in wrapped\n    fx = fun(np.copy(x), *args)\n  File \"<string>\", line 37, in bounded_func\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2169, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/_methods.py\", line 99, in _clip\n    return um.clip(a, min, max, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (10,) (2,) (2,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (10,) (2,) (2,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 60, in __call__\n  File \"<string>\", line 39, in local_search\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 738, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py\", line 386, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_optimize.py\", line 291, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 223, in __init__\n    self._update_fun()\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 295, in _update_fun\n    fx = self._wrapped_fun(self.x)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 21, in wrapped\n    fx = fun(np.copy(x), *args)\n  File \"<string>\", line 37, in bounded_func\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2169, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/_methods.py\", line 99, in _clip\n    return um.clip(a, min, max, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (10,) (2,) (2,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "9725b08f-5f9c-4ca3-ba9f-fb59eb59de3a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MemeticAlgorithmWithPeriodicityInjection:\n    def __init__(self, budget, dim, pop_size=50, F_base=0.8, CR=0.9, period_factor=4):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR = CR\n        self.period_factor = period_factor\n        self.evaluations = 0\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.pop_size, self.dim))\n\n    def adaptive_mutation(self, population, i):\n        indices = np.random.choice(range(self.pop_size), 3, replace=False)\n        a, b, c = population[indices]\n        F = self.F_base * (1 - self.evaluations / self.budget)\n        mutant = np.clip(a + F * (b - c), 0, 1)\n        return mutant\n\n    def crossover(self, target, mutant):\n        crossover_mask = np.random.rand(self.dim) < self.CR\n        offspring = np.where(crossover_mask, mutant, target)\n        return offspring\n\n    def periodicity_injection(self, vector, period):\n        pattern = vector[:period]\n        return np.tile(pattern, self.dim // period)[:self.dim]\n\n    def select(self, candidate, target, func):\n        candidate_fitness = func(candidate)\n        target_fitness = func(target)\n        self.evaluations += 2\n        return candidate if candidate_fitness < target_fitness else target\n\n    def local_search(self, solution, func, bounds):\n        def bounded_func(x):\n            return func(np.clip(x, bounds[0], bounds[1]))\n\n        result = minimize(bounded_func, solution, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n        population = self.initialize_population(lb, ub)\n\n        best_solution = None\n        best_fitness = float('inf')\n\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                target = population[i]\n                mutant = self.adaptive_mutation(population, i)\n                offspring = self.crossover(target, mutant)\n\n                # Inject periodicity\n                period = max(1, self.dim // self.period_factor)\n                offspring = self.periodicity_injection(offspring, period)\n\n                population[i] = self.select(offspring, target, func)\n\n                # Local search on best found solution\n                if self.evaluations < self.budget:\n                    local_solution = self.local_search(population[i], func, bounds)\n                    local_fitness = func(local_solution)\n                    self.evaluations += 1\n\n                    if local_fitness < best_fitness:\n                        best_solution = local_solution\n                        best_fitness = local_fitness\n\n        return best_solution", "name": "MemeticAlgorithmWithPeriodicityInjection", "description": "A novel Memetic Algorithm with Periodicity Injection (MAPI) leveraging adaptive mutation rates and periodic pattern reinforcement to solve complex multilayer optimization problems efficiently.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (2,) (2,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (2,) (2,) ')", "parent_id": "c9457507-109e-4370-9857-8d8cda789fe6", "metadata": {}, "mutation_prompt": null}
{"id": "58fadcdc-0a0e-4579-a2b7-4b7caf5127b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(20, budget // dim)\n        self.pop = None\n        self.best_solution = None\n        self.best_value = float('-inf')\n        self.bounds = None\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Quasi-oppositional initialization\n        opposite_pop = lb + ub - self.pop\n        self.pop = np.concatenate((self.pop, opposite_pop))\n        \n    def differential_evolution_step(self, func):\n        F = 0.8  # Differential weight\n        CR = 0.9 # Crossover probability\n        new_pop = np.copy(self.pop)\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            R = np.random.randint(self.dim)\n            trial = np.copy(self.pop[i])\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == R:\n                    trial[j] = self.pop[a][j] + F * (self.pop[b][j] - self.pop[c][j])\n                    # Enforce bounds\n                    trial[j] = np.clip(trial[j], self.bounds.lb[j], self.bounds.ub[j])  # Changed line\n            # Evaluate trial solution\n            trial_value = func(trial)\n            if trial_value > func(self.pop[i]):\n                new_pop[i] = trial\n                if trial_value > self.best_value:\n                    self.best_value = trial_value\n                    self.best_solution = trial\n        self.pop = new_pop\n        \n    def local_optimization(self, func):\n        if self.best_solution is not None:\n            result = minimize(lambda x: -func(x), self.best_solution, bounds=list(zip(self.bounds.lb, self.bounds.ub)), method='L-BFGS-B')  # Changed line\n            if -result.fun > self.best_value:\n                self.best_value = -result.fun\n                self.best_solution = result.x\n\n    def __call__(self, func):\n        self.bounds = func.bounds  # Changed line\n        # Initialize population\n        self.initialize_population((self.bounds.lb, self.bounds.ub))  # Changed line\n        evaluations = 0\n        \n        # Main optimization loop\n        while evaluations < self.budget:\n            self.differential_evolution_step(func)\n            evaluations += self.population_size\n            if evaluations + self.dim <= self.budget:\n                # Local optimization\n                self.local_optimization(func)\n                evaluations += self.dim\n\n        return self.best_solution", "name": "BraggOptimizer", "description": "Enhancements to BraggOptimizer for improved periodicity encouragement and boundary handling in local optimization.", "configspace": "", "generation": 1, "fitness": 0.4743708566539566, "feedback": "The algorithm BraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.474 with standard deviation 0.046. And the mean value of best solutions found was 0.396 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "7f0556bb-bc2e-4579-be1c-e012992dc7a0", "metadata": {"aucs": [0.4544971574686497, 0.537559774771345, 0.4310556377218753], "final_y": [0.42896079178713076, 0.3556420116084943, 0.4019208254736284]}, "mutation_prompt": null}
{"id": "062e379c-d86a-4d8d-afe6-1f87a4e5f282", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PeriodicReflectivityOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_probability = 0.7\n        self.num_generations = int(budget / self.population_size)\n        self.local_refinement_threshold = 0.1 * budget\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub]).T\n        population = self.initialize_population(bounds)\n        best_solution = None\n        best_score = float('inf')\n        evaluations = 0\n\n        for generation in range(self.num_generations):\n            population_scores = np.array([self.evaluate_individual(ind, func) for ind in population])\n            evaluations += len(population)\n\n            if generation > self.num_generations * 0.5:\n                population_scores += self.periodicity_penalty(population)\n\n            generation_best_score = np.min(population_scores)\n            if generation_best_score < best_score:\n                best_score = generation_best_score\n                best_solution = population[np.argmin(population_scores)]\n\n            if evaluations >= self.local_refinement_threshold:\n                result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds)\n                if result.fun < best_score:\n                    best_score, best_solution = result.fun, result.x\n                evaluations += result.nfev\n                break\n            \n            self.adaptive_mutation_and_crossover(generation)  # Adaptive strategy\n            next_population = self.evolve_population(population, population_scores, bounds)\n            population = next_population\n\n        return best_solution\n\n    def initialize_population(self, bounds):\n        return np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n\n    def evaluate_individual(self, individual, func):\n        return func(individual)\n\n    def periodicity_penalty(self, population):\n        penalties = np.zeros(len(population))\n        for i, individual in enumerate(population):\n            period = 2\n            penalty = np.sum((individual[i::period] - individual[i]) ** 2 for i in range(period))\n            penalties[i] = penalty\n        return penalties\n\n    def evolve_population(self, population, scores, bounds):\n        next_population = np.empty_like(population)\n        for i in range(self.population_size):\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.mutation_factor * (b - c), bounds[:, 0], bounds[:, 1])\n            cross_points = np.random.rand(self.dim) < self.crossover_probability\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            next_population[i] = np.where(cross_points, mutant, population[i])\n        return next_population\n\n    def adaptive_mutation_and_crossover(self, generation):\n        scale_factor = 0.5 + (generation / self.num_generations) * 0.5\n        self.mutation_factor *= scale_factor\n        self.crossover_probability *= scale_factor", "name": "PeriodicReflectivityOptimizer", "description": "Enhanced PeriodicReflectivityOptimizer by integrating adaptive mutation and crossover strategies for improved convergence.", "configspace": "", "generation": 1, "fitness": 0.9001155196797784, "feedback": "The algorithm PeriodicReflectivityOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.900 with standard deviation 0.041. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "f54c6f54-5b2f-47b0-9c22-a5d870cb48db", "metadata": {"aucs": [0.8512124333558359, 0.9524794830092925, 0.8966546426742071], "final_y": [0.20044589621232212, 0.16485629298510374, 0.18187822892568906]}, "mutation_prompt": null}
{"id": "4f7670e8-80c1-41c7-a47c-a0c7d7385c05", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n            \n            # Adaptive mutation factor: dynamically adjust based on budget usage\n            self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "A hybrid Differential Evolution approach enhanced with a local BFGS optimizer, periodicity encouragement, and adaptive mutation factor to solve black box optimization problems with limited evaluations.  ", "configspace": "", "generation": 1, "fitness": 0.9237545835539634, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.040. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "0a58d13e-d6c1-4157-8a9b-9946654cd234", "metadata": {"aucs": [0.9226238712207279, 0.8747868503123081, 0.9738530291288544], "final_y": [0.1818804245678617, 0.20044631940853397, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "08379375-9aa3-4410-8e0e-d7b5837549ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(20, budget // dim)\n        self.pop = None\n        self.best_solution = None\n        self.best_value = float('-inf')\n        self.bounds = None\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        # Quasi-oppositional initialization\n        opposite_pop = lb + ub - self.pop\n        self.pop = np.concatenate((self.pop, opposite_pop))\n        \n    def differential_evolution_step(self, func):\n        F = 0.8  # Differential weight\n        CR = 0.9 # Crossover probability\n        new_pop = np.copy(self.pop)\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            R = np.random.randint(self.dim)\n            trial = np.copy(self.pop[i])\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == R:\n                    trial[j] = self.pop[a][j] + F * (self.pop[b][j] - self.pop[c][j])\n                    # Enforce bounds\n                    trial[j] = np.clip(trial[j], self.bounds[0][j], self.bounds[1][j])\n            # Evaluate trial solution\n            trial_value = func(trial)\n            if trial_value > func(self.pop[i]):\n                new_pop[i] = trial\n                if trial_value > self.best_value:\n                    self.best_value = trial_value\n                    self.best_solution = trial\n        self.pop = new_pop\n        \n    def local_optimization(self, func):\n        if self.best_solution is not None:\n            bounds = [(self.bounds[0][i], self.bounds[1][i]) for i in range(self.dim)]  # Fixed bounds handling\n            result = minimize(lambda x: -func(x), self.best_solution, bounds=bounds, method='L-BFGS-B')\n            if -result.fun > self.best_value:\n                self.best_value = -result.fun\n                self.best_solution = result.x\n\n    def __call__(self, func):\n        self.bounds = (func.bounds.lb, func.bounds.ub)\n        # Initialize population\n        self.initialize_population(self.bounds)\n        evaluations = 0\n        \n        # Main optimization loop\n        while evaluations < self.budget:\n            self.differential_evolution_step(func)\n            evaluations += self.population_size\n            if evaluations + self.dim <= self.budget:\n                # Local optimization\n                self.local_optimization(func)\n                evaluations += self.dim\n\n        return self.best_solution", "name": "BraggOptimizer", "description": "The algorithm combines a symmetrically-initialized Differential Evolution with a local BFGS finetuning step, leveraging periodicity and modularity in the search for optimal multilayer structures, with enhanced bounds handling in local optimization.", "configspace": "", "generation": 1, "fitness": 0.5044086094853514, "feedback": "The algorithm BraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.504 with standard deviation 0.033. And the mean value of best solutions found was 0.379 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "7f0556bb-bc2e-4579-be1c-e012992dc7a0", "metadata": {"aucs": [0.5139158113886008, 0.4603946809207121, 0.5389153361467411], "final_y": [0.3915240642856337, 0.38035834163478555, 0.3661245366237065]}, "mutation_prompt": null}
{"id": "02f9b4ee-da79-472b-8e2b-0159ca210953", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MemeticBraggOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = min(20, budget // dim)\n        self.pop = None\n        self.best_solution = None\n        self.best_value = float('-inf')\n        self.bounds = None\n        \n    def initialize_population(self, bounds):\n        lb, ub = bounds\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        opposite_pop = lb + ub - self.pop\n        self.pop = np.concatenate((self.pop, opposite_pop))\n        \n    def differential_evolution_cc(self, func):\n        F = 0.8 \n        CR = 0.9 \n        new_pop = np.copy(self.pop)\n        for i in range(self.population_size):\n            indices = list(range(self.population_size))\n            indices.remove(i)\n            a, b, c = np.random.choice(indices, 3, replace=False)\n            R = np.random.randint(self.dim)\n            trial = np.copy(self.pop[i])\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == R:\n                    trial[j] = self.pop[a][j] + F * (self.pop[b][j] - self.pop[c][j])\n                    trial[j] = np.clip(trial[j], self.bounds[0][j], self.bounds[1][j])\n            trial_value = func(trial)\n            if trial_value > func(self.pop[i]):\n                new_pop[i] = trial\n                if trial_value > self.best_value:\n                    self.best_value = trial_value\n                    self.best_solution = trial\n        self.pop = new_pop\n        \n    def simulated_annealing_step(self, func, temp):\n        if self.best_solution is not None:\n            candidate = self.best_solution + np.random.uniform(-0.05, 0.05, self.dim)\n            candidate = np.clip(candidate, self.bounds[0], self.bounds[1])\n            candidate_value = func(candidate)\n            if candidate_value > self.best_value or np.exp((candidate_value - self.best_value) / temp) > np.random.rand():\n                self.best_value = candidate_value\n                self.best_solution = candidate\n                \n    def __call__(self, func):\n        self.bounds = (func.bounds.lb, func.bounds.ub)\n        self.initialize_population(self.bounds)\n        evaluations = 0\n        temperature = 1.0\n        cooling_rate = 0.95\n        \n        while evaluations < self.budget:\n            self.differential_evolution_cc(func)\n            evaluations += self.population_size\n            temperature *= cooling_rate\n            \n            if evaluations + self.dim <= self.budget:\n                self.simulated_annealing_step(func, temperature)\n                evaluations += self.dim\n\n        return self.best_solution", "name": "MemeticBraggOptimizer", "description": "The algorithm utilizes a memetic structure combining cooperative coevolutionary Differential Evolution with Simulated Annealing-inspired local search and periodicity-aware crossover to efficiently explore and exploit the search space for multilayer structure optimization.", "configspace": "", "generation": 2, "fitness": 0.5535047685035993, "feedback": "The algorithm MemeticBraggOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.554 with standard deviation 0.085. And the mean value of best solutions found was 0.325 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "08379375-9aa3-4410-8e0e-d7b5837549ea", "metadata": {"aucs": [0.6060876513120442, 0.43299096319737396, 0.6214356910013796], "final_y": [0.3283702010747688, 0.32487890674579245, 0.3231198866514776]}, "mutation_prompt": null}
{"id": "e544de39-a01d-4dce-92af-a37a02a4de37", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "A hybrid Differential Evolution approach enhanced with a local BFGS optimizer, adaptive periodicity enforcement, and a dynamic mutation factor to solve black box optimization problems efficiently.", "configspace": "", "generation": 2, "fitness": 0.9672887265203679, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.967 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0a58d13e-d6c1-4157-8a9b-9946654cd234", "metadata": {"aucs": [0.9649706804803212, 0.9744596792174618, 0.962435819863321], "final_y": [0.16485753044427298, 0.16485599325302003, 0.16485685669946837]}, "mutation_prompt": null}
{"id": "be29b3b5-0365-46fd-b3ad-e7e927326237", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        periodic_constraint = lambda x: np.sin(np.pi * (x[::2] - x[1::2])) # Encourage periodicity in optimization\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), constraints={'type': 'eq', 'fun': periodic_constraint})\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n            \n            self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n            \n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "A hybrid Differential Evolution with enhanced local optimization step using periodicity constraints and adaptive budget-based strategy.", "configspace": "", "generation": 2, "fitness": 0.9402999185270106, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.024. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "4f7670e8-80c1-41c7-a47c-a0c7d7385c05", "metadata": {"aucs": [0.9226244824076233, 0.923420815957796, 0.9748544572156125], "final_y": [0.181880165011859, 0.18187845913436984, 0.1648565964327231]}, "mutation_prompt": null}
{"id": "2d434a3f-9bb5-4a3e-a999-d67f0597e3bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            new_population = []  # Elitism: keep track of the new population\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    new_population.append(trial)  # Keep better solutions\n                    scores[i] = trial_score\n                else:\n                    new_population.append(target)  # Retain current if not improved\n            \n            population = np.array(new_population)  # Update the population with the new one\n            self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "Enhanced hybrid Differential Evolution with periodicity encouragement, adaptive mutation factor, and elitist replacement strategy to solve black box optimization problems effectively.", "configspace": "", "generation": 2, "fitness": 0.9505234721793178, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.951 with standard deviation 0.021. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "4f7670e8-80c1-41c7-a47c-a0c7d7385c05", "metadata": {"aucs": [0.9226238712207279, 0.9540931045410035, 0.9748534407762216], "final_y": [0.1818804245678617, 0.1648558545936769, 0.1648562814595267]}, "mutation_prompt": null}
{"id": "2c768a51-68f5-4a3f-add9-234f47a6620f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n        self.elite_fraction = 0.1  # Preserve 10% of the best solutions\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            elite_count = int(self.population_size * self.elite_fraction)\n            elite_indices = np.argsort(scores)[:elite_count]\n            new_population = population[elite_indices]\n            new_scores = scores[elite_indices]\n\n            for i in range(self.population_size - elite_count):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    new_population = np.vstack((new_population, trial))\n                    new_scores = np.append(new_scores, trial_score)\n                else:\n                    new_population = np.vstack((new_population, target))\n                    new_scores = np.append(new_scores, scores[i])\n            \n            population = new_population\n            scores = new_scores\n\n            # Enhanced adaptive mutation factor\n            self.mutation_factor = max(0.4, 1.4 * (1 - self.current_budget / self.budget))\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "An enhanced HybridDEBFGS algorithm that introduces an elite preservation strategy and improves adaptive mutation to optimize multilayer photonic structures with limited function evaluations.", "configspace": "", "generation": 2, "fitness": 0.8871298206917936, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.054. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4f7670e8-80c1-41c7-a47c-a0c7d7385c05", "metadata": {"aucs": [0.8222220828782988, 0.95450891336051, 0.884658465836572], "final_y": [0.16485640558056902, 0.16485586094626792, 0.16485716364942415]}, "mutation_prompt": null}
{"id": "c1e6fcf4-ae2b-44b1-a24d-41e4e7273620", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CooperativeCoevolutionDE:\n    def __init__(self, budget, dim, subcomponent_size=2):\n        self.budget = budget\n        self.dim = dim\n        self.subcomponent_size = subcomponent_size\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population, sub_idx):\n        scores = np.zeros(self.population_size)\n        for i, ind in enumerate(population):\n            sub_ind = np.copy(ind)\n            self.current_budget += 1\n            scores[i] = func(sub_ind) \n        return scores\n\n    def _local_optimization(self, target, func, bounds):\n        result = minimize(func, target, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else target\n\n    def _decompose_problem(self, dim):\n        subcomponents = []\n        for i in range(0, dim, self.subcomponent_size):\n            subcomponents.append((i, min(dim, i + self.subcomponent_size)))\n        return subcomponents\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        subcomponents = self._decompose_problem(self.dim)\n\n        while self.current_budget < self.budget:\n            for sub_idx, (start, end) in enumerate(subcomponents):\n                sub_population = population[:, start:end]\n                scores = self._evaluate_population(func, sub_population, sub_idx)\n\n                new_population = []  \n                for i in range(self.population_size):\n                    if self.current_budget >= self.budget:\n                        break\n                    target = sub_population[i]\n                    mutant = self._mutate(i, sub_population)\n                    trial = self._crossover(target, mutant, bounds)\n                    trial_score = func(trial)\n                    self.current_budget += 1\n                    if trial_score < scores[i]:\n                        new_population.append(trial)\n                        scores[i] = trial_score\n                    else:\n                        new_population.append(target)\n                \n                sub_population[:] = np.array(new_population)\n                population[:, start:end] = sub_population\n\n            best_score_idx = np.argmin(scores)\n            best_individual = population[best_score_idx]\n            target = best_individual\n            best_individual = self._local_optimization(target, func, bounds)\n\n        return best_individual", "name": "CooperativeCoevolutionDE", "description": "A Cooperative Coevolutionary Differential Evolution algorithm that splits the problem into subcomponents, optimizing each one independently with adaptive parallelization to improve convergence efficiency on complex landscapes.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (10,) (2,) (2,) ').", "error": "ValueError('operands could not be broadcast together with shapes (10,) (2,) (2,) ')", "parent_id": "2d434a3f-9bb5-4a3e-a999-d67f0597e3bb", "metadata": {}, "mutation_prompt": null}
{"id": "ccd61d9f-ab38-49c5-ab31-9d7b796d2414", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        # Quasi-oppositional initialization\n        opp_pop = bounds.ub + bounds.lb - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        scores = self._evaluate_population(func, combined_pop)\n        best_indices = np.argsort(scores)[:self.population_size]\n        return combined_pop[best_indices]\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n            \n            # Adaptive mutation factor: dynamically adjust based on budget usage\n            self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "A refined hybrid Differential Evolution approach with an adaptive mutation factor, strategically enhanced with quasi-oppositional initialization, and a local BFGS optimizer to efficiently explore black box optimization problems.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "4f7670e8-80c1-41c7-a47c-a0c7d7385c05", "metadata": {}, "mutation_prompt": null}
{"id": "a357f496-3d5c-4a89-830c-17b3fcd74035", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self.mutation_factor = 0.5 + 0.5 * (1 - self.current_budget / self.budget)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "A hybrid Differential Evolution approach enhanced with a local BFGS optimizer, periodicity encouragement, and dynamic mutation factor adjustment based on budget utilization to solve black box optimization problems efficiently.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "0a58d13e-d6c1-4157-8a9b-9946654cd234", "metadata": {}, "mutation_prompt": null}
{"id": "37657b0d-6d7b-480b-a52c-5cb7af7eb24a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        self.crossover_rate = 0.7 + 0.3 * np.sin(np.pi * self.current_budget / self.budget)  # Dynamic crossover\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        periodic_constraint = lambda x: np.sin(np.pi * (x[::2] - x[1::2])) # Encourage periodicity in optimization\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), constraints={'type': 'eq', 'fun': periodic_constraint})\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n            \n            self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n            \n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "Enhanced crossover strategy by dynamically adjusting crossover rate based on current budget to improve diversity and convergence.", "configspace": "", "generation": 3, "fitness": 0.9237545835539634, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.924 with standard deviation 0.040. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "be29b3b5-0365-46fd-b3ad-e7e927326237", "metadata": {"aucs": [0.9226238712207279, 0.8747868503123081, 0.9738530291288544], "final_y": [0.1818804245678617, 0.20044631940853397, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "afd2f0c8-63fe-4659-bddd-f8275ae3c229", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Increased population size for better exploration\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n            \n            # Adaptive mutation factor: dynamically adjust based on budget usage\n            self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "A hybrid Differential Evolution approach enhanced with a local BFGS optimizer, periodicity encouragement, and adaptive mutation factor to solve black box optimization problems with limited evaluations, improved by increasing the population size factor to enhance exploration.", "configspace": "", "generation": 3, "fitness": 0.9359231065263142, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.024. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "4f7670e8-80c1-41c7-a47c-a0c7d7385c05", "metadata": {"aucs": [0.9190463005880327, 0.9188594976973141, 0.9698635212935953], "final_y": [0.1818804245678617, 0.1818804950148526, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "aadfafb5-95e8-47c3-8cac-e1f47c52059c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self.population_size = max(4, int(10 * self.dim * (1 - self.current_budget / self.budget)))\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "An enhanced hybrid DE with local BFGS optimizer using dynamic population resizing to maintain exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8796723539220911, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.099. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "0a58d13e-d6c1-4157-8a9b-9946654cd234", "metadata": {"aucs": [0.7422753498402584, 0.9226892074053977, 0.9740525045206173], "final_y": [0.2578108477958787, 0.18188046291040139, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "c9ae5e70-9fea-4080-a996-80502397131d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population, best):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = best + self.mutation_factor * (a - b + c - best)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n    \n    def _adapt_mutation_factor(self):\n        self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            new_population = []  # Elitism: keep track of the new population\n            best, best_score = self._select_best(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population, best)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    new_population.append(trial)  # Keep better solutions\n                    scores[i] = trial_score\n                else:\n                    new_population.append(target)  # Retain current if not improved\n            \n            population = np.array(new_population)  # Update the population with the new one\n            self._adapt_mutation_factor()\n\n            best, _ = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "EnhancedHybridDEBFGS", "description": "Improved Hybrid DE with Adaptive Periodicity-Preserving DE Operator and Refinement via Gradient-Based Local Search for Enhanced Solution Accuracy and Convergence.", "configspace": "", "generation": 4, "fitness": 0.9371156300705458, "feedback": "The algorithm EnhancedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.937 with standard deviation 0.023. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2d434a3f-9bb5-4a3e-a999-d67f0597e3bb", "metadata": {"aucs": [0.9226238712207279, 0.9188594976973141, 0.9698635212935953], "final_y": [0.1818804245678617, 0.1818804950148526, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "8cfdc85c-2be4-4d72-a4fc-0fca92893bc0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "A hybrid Differential Evolution approach refined by enhancing the local optimization strategy to incorporate periodicity constraints more robustly, leveraging adaptive periodicity enforcement and dynamic mutation factors for efficient black box optimization.", "configspace": "", "generation": 4, "fitness": 0.9747445373429443, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e544de39-a01d-4dce-92af-a37a02a4de37", "metadata": {"aucs": [0.9779587645027585, 0.9662075480257675, 0.980067299500307], "final_y": [0.1648562770143015, 0.1648585500207017, 0.16485947507956245]}, "mutation_prompt": null}
{"id": "79a5074a-18da-4876-85e9-1232fcab6e90", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()  # Dynamic crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "An enhanced hybrid Differential Evolution approach with a local BFGS optimizer, adaptive periodicity enforcement, dynamic mutation factor, and dynamic crossover rate adjustment for efficient problem-solving.", "configspace": "", "generation": 4, "fitness": 0.9742467122511101, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e544de39-a01d-4dce-92af-a37a02a4de37", "metadata": {"aucs": [0.9684509881348067, 0.9775343064696801, 0.9767548421488438], "final_y": [0.16486005488729194, 0.1648562590128324, 0.1648559770460155]}, "mutation_prompt": null}
{"id": "21b48eab-6547-4cf1-8b23-1c68cebb5401", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Increased population size for diversity\n        self.mutation_factor = 0.7  # Adjusted mutation factor for better convergence\n        self.crossover_rate = 0.75  # Slightly increased crossover rate\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _dynamic_mutation_factor(self, diversity):\n        return 0.5 + 0.3 * (1 - diversity)  # New dynamic mutation factor based on diversity\n\n    def _population_diversity(self, population):\n        return np.mean(np.std(population, axis=0))  # Measure of population diversity\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            diversity = self._population_diversity(population)\n            self.mutation_factor = self._dynamic_mutation_factor(diversity)  # Adjust mutation factor\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "EnhancedHybridDEBFGS", "description": "A refined hybrid Differential Evolution strategy with enhanced diversity control, adaptive mutation, and periodic local search to optimize black box problems efficiently.", "configspace": "", "generation": 4, "fitness": 0.9359231065263142, "feedback": "The algorithm EnhancedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.024. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "e544de39-a01d-4dce-92af-a37a02a4de37", "metadata": {"aucs": [0.9190463005880327, 0.9188594976973141, 0.9698635212935953], "final_y": [0.1818804245678617, 0.1818804950148526, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "cd30e191-3d32-49a6-bf80-60e61ca6c582", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.9  # Adjusted crossover rate\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            new_population = []  # Elitism: keep track of the new population\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    new_population.append(trial)  # Keep better solutions\n                    scores[i] = trial_score\n                else:\n                    new_population.append(target)  # Retain current if not improved\n            \n            population = np.array(new_population)  # Update the population with the new one\n            self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "Enhanced Hybrid DE with periodicity encouragement and dynamic mutation factor, now with an adjusted crossover rate to optimize convergence.", "configspace": "", "generation": 5, "fitness": 0.9582729067736593, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "2d434a3f-9bb5-4a3e-a999-d67f0597e3bb", "metadata": {"aucs": [0.9226238712207279, 0.9783418199713957, 0.9738530291288544], "final_y": [0.1818804245678617, 0.16485701613160642, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "9aa05916-9b3d-4344-b1ca-f9fbe145d951", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        first_half = solution[:period]\n        second_half = solution[period:]\n        averaged_segment = (first_half + second_half) / 2  # Average segments for periodicity\n        solution[:period] = averaged_segment\n        solution[period:] = averaged_segment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()  # Dynamic crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "An enhanced hybrid Differential Evolution with a local BFGS optimizer; periodicity is reinforced by averaging periodic segments, adaptive periodicity enforcement, dynamic mutation factor, and dynamic crossover rate adjustment for efficient problem-solving.", "configspace": "", "generation": 5, "fitness": 0.9730792505710267, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "79a5074a-18da-4876-85e9-1232fcab6e90", "metadata": {"aucs": [0.980673033445856, 0.9616644392404367, 0.9769002790267874], "final_y": [0.16485654633195723, 0.16485610036371956, 0.16485773569891127]}, "mutation_prompt": null}
{"id": "3206868d-c17f-4923-93e0-2ca1b6fede3b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n        self.learning_rate = 0.1  # New line\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    self.crossover_rate = min(1.0, self.crossover_rate + self.learning_rate)  # Change line\n                else:  # Change line\n                    self.crossover_rate = max(0.1, self.crossover_rate - self.learning_rate)  # Change line\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced hybrid Differential Evolution with integrated learning rate adaptation and crossover rate adjustment based on performance feedback.", "configspace": "", "generation": 5, "fitness": 0.9739488113291216, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.974 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e544de39-a01d-4dce-92af-a37a02a4de37", "metadata": {"aucs": [0.9747083568184334, 0.971288755394035, 0.9758493217748963], "final_y": [0.1648560410376989, 0.16485743918902107, 0.16485673103314702]}, "mutation_prompt": null}
{"id": "06e25940-a421-41cc-b500-8f4b9ba4f8e6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        # Change here: Introduced a weighted difference vector to refine mutation\n        mutant = a + self.mutation_factor * ((b - c) + 0.5 * (a - np.mean(population, axis=0)))\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        periodic_constraint = lambda x: np.sin(np.pi * (x[::2] - x[1::2])) # Encourage periodicity in optimization\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), constraints={'type': 'eq', 'fun': periodic_constraint})\n        return result.x if result.success else best\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n            \n            self.mutation_factor = max(0.5, 1.5 * (1 - self.current_budget / self.budget))\n            \n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n\n        return best", "name": "HybridDEBFGS", "description": "Refined mutation strategy by introducing a weighted difference vector to enhance diversity and convergence efficiency.", "configspace": "", "generation": 5, "fitness": 0.9226356431893242, "feedback": "The algorithm HybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.923 with standard deviation 0.042. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "be29b3b5-0365-46fd-b3ad-e7e927326237", "metadata": {"aucs": [0.9226238712207279, 0.871420701844321, 0.9738623565029242], "final_y": [0.1818804245678617, 0.20044560988408444, 0.1648559862889315]}, "mutation_prompt": null}
{"id": "1210cbe7-0f27-4b24-8a2d-64757bb1aba1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n            population[np.argmax(scores)] = best  # Insert optimized best back into population\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "A hybrid Differential Evolution approach enhanced with a more adaptive local optimization strategy to incorporate periodicity constraints robustly, leveraging adaptive periodicity enforcement and dynamic mutation factors for efficient black box optimization.", "configspace": "", "generation": 5, "fitness": 0.9734838967829189, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cfdc85c-2be4-4d72-a4fc-0fca92893bc0", "metadata": {"aucs": [0.9714195065823302, 0.9751706319775844, 0.9738615517888423], "final_y": [0.16485759093378816, 0.16485595996505809, 0.16485596149453796]}, "mutation_prompt": null}
{"id": "c01ac96d-7f72-4d8b-9f92-c7aeea09a76a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        \n        # New strategy: Adaptive mutation factor\n        adaptive_mutation_factor = np.linspace(0.5, 1.0, self.budget//self.population_size)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                # Adaptive mutation factor\n                self.mutation_factor = adaptive_mutation_factor[self.current_budget // self.population_size]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n            \n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best) \n\n        return best", "name": "RefinedHybridDEBFGS", "description": "An enhanced hybrid Differential Evolution approach incorporating adaptive periodicity, dynamic mutation factors, and a multiobjective strategy balancing exploration and exploitation for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.9698276291296319, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cfdc85c-2be4-4d72-a4fc-0fca92893bc0", "metadata": {"aucs": [0.9657005120398116, 0.963407408190577, 0.9803749671585075], "final_y": [0.16485600347711793, 0.16485675430718094, 0.16485584701522427]}, "mutation_prompt": null}
{"id": "c7a44339-51a2-4ff1-a3f8-22fab8138c88", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = (solution[i] + solution[i + period]) / 2  # Average periodic segments\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:  # Adaptive mutation factor\n                    self.mutation_factor = min(1.0, self.mutation_factor + 0.01)\n                    population[i] = trial\n                    scores[i] = trial_score\n                else:\n                    self.mutation_factor = max(0.1, self.mutation_factor - 0.01)\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "An enhanced hybrid algorithm refining local optimization with dynamic periodicity enforcement based on performance and incorporating mutation factor adaptation, to exploit both exploration and exploitation efficiently.", "configspace": "", "generation": 6, "fitness": 0.9733387211326524, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.007. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cfdc85c-2be4-4d72-a4fc-0fca92893bc0", "metadata": {"aucs": [0.9634449538998444, 0.9776429459980156, 0.9789282635000972], "final_y": [0.16485636175958673, 0.1648575587944089, 0.16485600265547873]}, "mutation_prompt": null}
{"id": "8bd6fcde-ab82-4f9f-a14e-b013b1f06de1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        self.crossover_rate = 0.5 + 0.4 * np.random.rand()  # Dynamic mutation factor adjustment\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        solution[period:] = solution[:period]  # Enhanced symmetry enforcement\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n            population[np.argmax(scores)] = best  # Insert optimized best back into population\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced symmetry enforcement and dynamic mutation factor adjustment during crossover for improved efficiency.", "configspace": "", "generation": 6, "fitness": 0.9722543494980184, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1210cbe7-0f27-4b24-8a2d-64757bb1aba1", "metadata": {"aucs": [0.9770176027641783, 0.9636012852433199, 0.9761441604865568], "final_y": [0.1648558682313106, 0.16486257814484617, 0.16485652875226953]}, "mutation_prompt": null}
{"id": "600a1ccd-f60c-4bfc-bb51-812f6e9cf79a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        if np.random.rand() < 0.5:\n            solution = np.roll(solution, shift=np.random.randint(1, self.dim))  # Stochastic adjustment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced hybrid Differential Evolution utilizing a novel adaptive periodicity adjustment mechanism and stochastic local search to improve high-dimensional black box optimization performance.", "configspace": "", "generation": 6, "fitness": 0.9578464129523828, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.958 with standard deviation 0.023. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cfdc85c-2be4-4d72-a4fc-0fca92893bc0", "metadata": {"aucs": [0.9256300983405625, 0.9712414119709044, 0.9766677285456817], "final_y": [0.16485625328629017, 0.16485656906426904, 0.16485603053298636]}, "mutation_prompt": null}
{"id": "d4d60ec1-3659-4587-81aa-64e9e2e3d9b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n        self.learning_rate = 0.1\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    self.crossover_rate = min(1.0, self.crossover_rate + self.learning_rate * 0.5)  # Change line\n                else:\n                    self.crossover_rate = max(0.1, self.crossover_rate - self.learning_rate * 0.5)  # Change line\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced adaptive crossover rate adjustment and mutation factor reduction introduce improved exploration-exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.9650566661701513, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.015. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3206868d-c17f-4923-93e0-2ca1b6fede3b", "metadata": {"aucs": [0.9757798273374452, 0.94371703325204, 0.9756731379209691], "final_y": [0.16485638335067032, 0.16485725748219404, 0.1648559569043152]}, "mutation_prompt": null}
{"id": "b7e3ef7e-3876-40f4-9efd-c8734d6db978", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()  # Dynamic crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "A refined hybrid Differential Evolution approach with a local BFGS optimizer, featuring adaptive periodicity enforcement and dynamic mutation factor, with improved crossover logic for enhanced exploration.", "configspace": "", "generation": 7, "fitness": 0.9719614464279722, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "79a5074a-18da-4876-85e9-1232fcab6e90", "metadata": {"aucs": [0.9711895365069916, 0.9660929712289508, 0.9786018315479745], "final_y": [0.1648563256997161, 0.1648560674757913, 0.16485607950352643]}, "mutation_prompt": null}
{"id": "139f8725-9c77-42e4-930a-ccd974613ae6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.9  # Changed from 0.8\n        self.crossover_rate = 0.8  # Changed from 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        perturbation = np.random.normal(0, 0.1, size=self.dim)  # Added perturbation\n        mutant += perturbation\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        perturbation = np.random.normal(0, 0.05, size=self.dim)  # Added perturbation\n        trial = np.clip(trial + perturbation, bounds.lb, bounds.ub)\n        return trial\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = (solution[i] + solution[i + period]) / 2  # Average periodic segments\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:  # Adaptive mutation factor\n                    self.mutation_factor = min(1.0, self.mutation_factor + 0.01)\n                    population[i] = trial\n                    scores[i] = trial_score\n                else:\n                    self.mutation_factor = max(0.1, self.mutation_factor - 0.01)\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced mutation and crossover strategy with dynamic perturbation and elitism to balance exploration and exploitation in black box optimization.", "configspace": "", "generation": 7, "fitness": 0.9759433253231693, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c7a44339-51a2-4ff1-a3f8-22fab8138c88", "metadata": {"aucs": [0.9711659758431366, 0.9738066229703478, 0.9828573771560236], "final_y": [0.164856323973527, 0.16485587920962153, 0.16485767737629364]}, "mutation_prompt": null}
{"id": "2f8dfe35-6923-474e-b275-106568eb9841", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        self.mutation_factor = 0.5 + 0.3 * np.random.rand()  # Dynamic mutation factor\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()  # Dynamic crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced hybrid Differential Evolution with refined mutation factor dynamic strategy for improved exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.9765429171082042, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "79a5074a-18da-4876-85e9-1232fcab6e90", "metadata": {"aucs": [0.9783792837031283, 0.9773890475926992, 0.973860420028785], "final_y": [0.16485619765426884, 0.16485615703271483, 0.16485657501884987]}, "mutation_prompt": null}
{"id": "99f84f94-b7e1-47de-a45a-fc7bbe24b63c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population, best):  # Modified line: added 'best' parameter\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 2, replace=False)\n        a, b = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - best)  # Use 'best' in mutation\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                best, best_score = self._select_best(population, scores)\n                mutant = self._mutate(i, population, best)  # Pass 'best' to _mutate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n            population[np.argmax(scores)] = best  # Insert optimized best back into population\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhance mutation strategy by incorporating best individual's influence in the differential mutation to improve convergence speed and solution quality.", "configspace": "", "generation": 7, "fitness": 0.9720528363624396, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1210cbe7-0f27-4b24-8a2d-64757bb1aba1", "metadata": {"aucs": [0.9749428914171906, 0.9663126139116759, 0.9749030037584524], "final_y": [0.16485633678264866, 0.16485594170996942, 0.16485625059797093]}, "mutation_prompt": null}
{"id": "11087e4e-27c8-4e32-8614-15c88199020a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "A novel Cooperative Coevolutionary approach leveraging separate sub-components for global exploration and local exploitation, integrating random forest surrogate models for efficient convergence in complex landscapes.", "configspace": "", "generation": 7, "fitness": 0.9774441177487715, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "79a5074a-18da-4876-85e9-1232fcab6e90", "metadata": {"aucs": [0.9746873192163374, 0.971986585106889, 0.9856584489230881], "final_y": [0.16485865149758527, 0.16485606252319907, 0.1648559867690077]}, "mutation_prompt": null}
{"id": "d6feee34-0270-49e8-8e4b-e4aaad0db9a7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        self.mutation_factor = 0.5 + 0.3 * np.random.rand()  # Dynamic mutation factor\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            best = best - 0.01 * np.gradient(func(best))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.population_size = max(5, self.population_size // 2)  # Dynamic population scaling\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()  # Dynamic crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced local optimization with gradient-based periodicity adjustments and dynamic population scaling to improve efficiency.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"can't multiply sequence by non-int of type 'float'\").", "error": "TypeError(\"can't multiply sequence by non-int of type 'float'\")", "parent_id": "2f8dfe35-6923-474e-b275-106568eb9841", "metadata": {}, "mutation_prompt": null}
{"id": "5a5e2b04-a216-4676-83cd-1771ebf5a09e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = self.mutation_factor * (1 - self.current_budget / self.budget)  # Adaptive strategy\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "A refined hybrid Differential Evolution with an enhanced adaptive mutation factor strategy for improved performance in black box optimization.", "configspace": "", "generation": 8, "fitness": 0.9715919040565928, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cfdc85c-2be4-4d72-a4fc-0fca92893bc0", "metadata": {"aucs": [0.9655406099223746, 0.9753728851005202, 0.9738622171468835], "final_y": [0.16485610053640454, 0.16485590360014057, 0.16485601112182646]}, "mutation_prompt": null}
{"id": "7380a74c-8156-4836-a1f6-17e3e5fcf682", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        period = self.dim // 2\n        phase_shift = np.random.randint(0, period)\n        mutant = a + self.mutation_factor * (b - c)\n        mutant = np.roll(mutant, phase_shift)  # Apply phase shift\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "An enhanced hybrid DE with a new periodicity-based mutation strategy and local L-BFGS optimization to improve exploration and exploitation balance for black box optimization.", "configspace": "", "generation": 8, "fitness": 0.9679282343483576, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "79a5074a-18da-4876-85e9-1232fcab6e90", "metadata": {"aucs": [0.9767088526243258, 0.9765236734782212, 0.9505521769425256], "final_y": [0.1648590780803244, 0.16485748967254354, 0.1648558365105235]}, "mutation_prompt": null}
{"id": "d87fbc11-2a55-49d7-a340-ac740f0444bf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = max(1, self.dim // 3)  # Dynamic period adjustment\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Refined periodicity enforcement by dynamic period adjustment to improve convergence.", "configspace": "", "generation": 8, "fitness": 0.9573971341152164, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.957 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "8cfdc85c-2be4-4d72-a4fc-0fca92893bc0", "metadata": {"aucs": [0.9226256342218249, 0.9730093779395699, 0.9765563901842542], "final_y": [0.18187841990161402, 0.16485614069348542, 0.16485635114854036]}, "mutation_prompt": null}
{"id": "85af685c-dd11-4e07-a73a-72c1d0c55043", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.9  # Changed from 0.8\n        self.crossover_rate = 0.8  # Changed from 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        perturbation = np.random.normal(0, 0.1, size=self.dim)  # Added perturbation\n        mutant += perturbation\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        perturbation = np.random.normal(0, 0.05, size=self.dim)  # Added perturbation\n        trial = np.clip(trial + perturbation, bounds.lb, bounds.ub)\n        return trial\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = (solution[i] + solution[i + period]) / 2  # Average periodic segments\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:  # Adaptive mutation factor\n                    self.mutation_factor = min(1.0, self.mutation_factor + 0.02)  # Adjusted factor change\n                    population[i] = trial\n                    scores[i] = trial_score\n                else:\n                    self.mutation_factor = max(0.1, self.mutation_factor - 0.02)  # Adjusted factor change\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)  # Enforce symmetry more robustly\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Adjusted the mutation factor updating strategy for more dynamic adaptation during evolution.", "configspace": "", "generation": 8, "fitness": 0.9708351011120273, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "139f8725-9c77-42e4-930a-ccd974613ae6", "metadata": {"aucs": [0.9742462346291013, 0.9643981027616647, 0.9738609659453158], "final_y": [0.1648559392157395, 0.1648568438751582, 0.16485593752750527]}, "mutation_prompt": null}
{"id": "ef6d0d5d-af16-4cdf-83f3-8115fdb83469", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        self.mutation_factor = 0.5 + 0.3 * np.random.rand()  # Dynamic mutation factor\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self.population_size = int(max(10, self.population_size * 0.99))  # Dynamic population size\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()  # Dynamic crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Improved exploration-exploitation balance by dynamically adjusting the population size during optimization.", "configspace": "", "generation": 9, "fitness": 0.9776917978746106, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2f8dfe35-6923-474e-b275-106568eb9841", "metadata": {"aucs": [0.9752110707867023, 0.9769090428175455, 0.9809552800195841], "final_y": [0.16485600974526216, 0.1648560296732654, 0.16485668353494642]}, "mutation_prompt": null}
{"id": "da928bc1-e616-4434-a2e0-12e5635a75db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutation_variation = 0.05 * np.random.rand()  # Dynamic mutation factor adjustment\n        mutant = a + (self.mutation_factor + mutation_variation) * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()  # Dynamic crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "This version introduces dynamic mutation factor adjustment by incorporating a slight random variation to enhance exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.9771774791179201, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "79a5074a-18da-4876-85e9-1232fcab6e90", "metadata": {"aucs": [0.9742781973617166, 0.9809200028044284, 0.9763342371876153], "final_y": [0.16485636944384352, 0.1648564402552769, 0.16485636824069871]}, "mutation_prompt": null}
{"id": "812dc95e-5de4-42b1-a746-676b600d126f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = 0.5 * (solution[i] + solution[i + period])  # Harmonize periodicity\n        return solution\n\n    def _adaptive_mutation_factor(self, scores):\n        diversity = np.std(scores)\n        return 0.5 + 0.5 * diversity\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self.mutation_factor = self._adaptive_mutation_factor(scores)  # Adapt mutation factor\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "A refined hybrid DE-BFGS algorithm with improved periodicity enforcement and adaptive mutation factor based on population diversity to enhance black box optimization. ", "configspace": "", "generation": 9, "fitness": 0.9752657998358601, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "79a5074a-18da-4876-85e9-1232fcab6e90", "metadata": {"aucs": [0.9633170083394056, 0.9818137783601458, 0.980666612808029], "final_y": [0.1648560116242498, 0.1648559166764605, 0.16485628264650443]}, "mutation_prompt": null}
{"id": "47949178-bdd4-46df-b475-75d30440b279", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.5, 0.9)  # Adaptive mutation factor\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 4  # More flexible periodicity\n        for i in range(period):\n            solution[i + period] = solution[i]  # Enforce symmetry\n            solution[i + 2 * period] = solution[i]  # Further symmetry\n            solution[i + 3 * period] = solution[i]  # Further symmetry\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduction of adaptive mutation factor and enhanced periodicity enforcement for improved convergence and solution robustness.", "configspace": "", "generation": 9, "fitness": 0.9801876241411085, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8cfdc85c-2be4-4d72-a4fc-0fca92893bc0", "metadata": {"aucs": [0.9813445149121238, 0.9808562038370561, 0.9783621536741456], "final_y": [0.16485623232708702, 0.16485593467042192, 0.1648560132900102]}, "mutation_prompt": null}
{"id": "1686cc8c-b239-43d0-824d-2776c5495eba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.5 + 0.4 * var_pred\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced Cooperative Coevolutionary Optimization with adaptive mutation factor and crossover rate based on surrogate model prediction variance.", "configspace": "", "generation": 9, "fitness": 0.9823693080015153, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "11087e4e-27c8-4e32-8614-15c88199020a", "metadata": {"aucs": [0.9784084038557876, 0.9830448139757204, 0.9856547061730382], "final_y": [0.16485640103377286, 0.1648577381575418, 0.16485724634942478]}, "mutation_prompt": null}
{"id": "f3bd4faf-bd4a-43b9-8f9b-be79df0f43f6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.mutation_factor = 0.4 + 0.2 * (self.current_budget / self.budget)  # Adaptive mutation factor\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced convergence by introducing adaptive mutation factor based on current progress.", "configspace": "", "generation": 10, "fitness": 0.9816196472845308, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "11087e4e-27c8-4e32-8614-15c88199020a", "metadata": {"aucs": [0.981781302431053, 0.9801282179479976, 0.9829494214745417], "final_y": [0.16485605103628387, 0.16485672083321035, 0.1648559542130399]}, "mutation_prompt": null}
{"id": "3308fa45-c837-4106-90f1-2773d59fb95c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5 + 0.1 * np.random.rand()  # Dynamic mutation factor\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=15)  # More trees for better accuracy\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        # Improved periodic enforcement\n        period = self.dim // 2\n        solution[period:] = solution[:period]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced Cooperative Coevolutionary Optimization with dynamic mutation factor, adaptive surrogate modeling, and improved periodic enforcement for optimal convergence.", "configspace": "", "generation": 10, "fitness": 0.9798500655684891, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "11087e4e-27c8-4e32-8614-15c88199020a", "metadata": {"aucs": [0.976449751690359, 0.9806512218978942, 0.9824492231172138], "final_y": [0.16485614311416918, 0.16485622853047321, 0.1648559542130399]}, "mutation_prompt": null}
{"id": "d897ed48-5b81-4c7f-acc1-09aafaf3a9fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.5 * np.random.rand()  # Changed line\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced exploration by modifying crossover rate calculation for improved adaptability.", "configspace": "", "generation": 10, "fitness": 0.9802067990696332, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "11087e4e-27c8-4e32-8614-15c88199020a", "metadata": {"aucs": [0.975353498211027, 0.9805522239419079, 0.9847146750559649], "final_y": [0.1648562492274569, 0.16485609275659674, 0.1648559802411792]}, "mutation_prompt": null}
{"id": "682aef76-56dc-49c8-aade-ee95b6ff0d0a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Ensuring exact repetition for enhanced periodicity\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.55 + 0.45 * var_pred  # Enhanced adaptivity in mutation\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Improved adaptive mutation strategy and enhanced periodic enforcement to boost convergence performance.", "configspace": "", "generation": 10, "fitness": 0.9805043020079425, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9770976807028244, 0.9797717363736199, 0.9846434889473833], "final_y": [0.16485624882222005, 0.16485608709251331, 0.16485598566042925]}, "mutation_prompt": null}
{"id": "cc9b15e7-92b8-4fa5-8d5d-e9fc089ec757", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.5 + 0.2 * np.random.rand()  # Adaptive mutation factor\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=15)  # More estimators\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return np.clip(mutant, population.min(axis=0), population.max(axis=0))  # Diversity preservation\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 3  # Alter periodicity enforcement\n        for i in range(period):\n            solution[i + period] = solution[i]\n            solution[i + 2 * period] = solution[i]  # Enforce two periods for stability\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.75 + 0.15 * np.random.rand()  # Adjusted crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced Cooperative Coevolutionary Optimization with adaptive mutation factor, diversity preservation, and periodic constraint enforcement to improve convergence and robustness.", "configspace": "", "generation": 10, "fitness": 0.9783956770810076, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "11087e4e-27c8-4e32-8614-15c88199020a", "metadata": {"aucs": [0.9775192628424318, 0.9745858705599403, 0.9830818978406507], "final_y": [0.1648558125972257, 0.16485770232785635, 0.16485741747146643]}, "mutation_prompt": null}
{"id": "e9f127ee-0c4c-4b59-83f7-85fadcb53df4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.5, 0.9)\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 4)\n        for i in range(period):\n            solution[i + period] = solution[i]\n            solution[i + 2 * period] = solution[i]\n            solution[i + 3 * period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Integrating adaptive periodicity and dynamic crossover for enhanced global exploration and local refinement.", "configspace": "", "generation": 11, "fitness": 0.9803628659052976, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "47949178-bdd4-46df-b475-75d30440b279", "metadata": {"aucs": [0.9825097818179969, 0.9786430769986535, 0.9799357388992426], "final_y": [0.16485683762591608, 0.16485612484248957, 0.1648561363119304]}, "mutation_prompt": null}
{"id": "05ea3030-da50-401a-977e-05bb69ea9b4f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                surrogate_score = self._predict_surrogate(mutant)\n                # Changed line\n                self.mutation_factor = 0.5 + 0.5 * np.abs(surrogate_score - scores[i]) / np.max(np.abs(scores))\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced exploration by introducing adaptive mutation factor based on surrogate model prediction error.", "configspace": "", "generation": 11, "fitness": 0.9801387702091441, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d897ed48-5b81-4c7f-acc1-09aafaf3a9fd", "metadata": {"aucs": [0.9767337063535568, 0.982938447305812, 0.9807441569680632], "final_y": [0.16485631221028196, 0.1648564576780297, 0.16485612099287028]}, "mutation_prompt": null}
{"id": "ae945484-77fa-4e12-86a9-142bd8d8c474", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            diversity = np.mean(np.std(population, axis=0))  # New line\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.5 * np.random.rand()  # Changed line\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Improved exploration by adding a dynamic mutation factor adaptation based on population diversity.", "configspace": "", "generation": 11, "fitness": 0.9766463047002903, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d897ed48-5b81-4c7f-acc1-09aafaf3a9fd", "metadata": {"aucs": [0.9791432105746872, 0.9700543912214206, 0.9807413123047629], "final_y": [0.1648577600775023, 0.16485597851446865, 0.16485594316995178]}, "mutation_prompt": null}
{"id": "536b25fa-6a32-4d72-9475-4d3a4eeb499b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution, enforce_rate=0.5):\n        period = self.dim // 2\n        if np.random.rand() < enforce_rate:\n            for i in range(period):\n                solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.5 + 0.4 * var_pred\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial, enforce_rate=0.7)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Incorporate adaptive periodic enforcement frequency based on the convergence rate to balance exploration and exploitation.", "configspace": "", "generation": 11, "fitness": 0.9778478959520781, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9707425922697543, 0.9817088329861572, 0.981092262600323], "final_y": [0.16485627821406545, 0.16485638124493895, 0.16485624893164463]}, "mutation_prompt": null}
{"id": "ac657fbb-509e-4af3-bc9d-90fa6be8c007", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Ensuring exact repetition for enhanced periodicity\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.55 + 0.45 * var_pred * np.std(scores)  # Enhanced adaptivity in mutation\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Refined mutation strategy by adjusting mutation factor based on diversity measure to enhance exploration.", "configspace": "", "generation": 11, "fitness": 0.9791057648604742, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "682aef76-56dc-49c8-aade-ee95b6ff0d0a", "metadata": {"aucs": [0.9825386806073166, 0.9736364980437578, 0.9811421159303481], "final_y": [0.16485611744210737, 0.16485603092335666, 0.1648564108034567]}, "mutation_prompt": null}
{"id": "a4a2fbc7-2807-413f-9b21-254a1020e944", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = 0.5 + 0.4 * (self.current_budget / self.budget)  # Changed: Adaptive mutation factor based on iteration\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 4)\n        for i in range(period):\n            solution[i + period] = solution[i]\n            solution[i + 2 * period] = solution[i]\n            solution[i + 3 * period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Integrating adaptive periodicity and dynamic crossover for enhanced global exploration, local refinement, and adaptive mutation factor based on iteration.", "configspace": "", "generation": 12, "fitness": 0.9800719756434851, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9f127ee-0c4c-4b59-83f7-85fadcb53df4", "metadata": {"aucs": [0.9790696832155623, 0.9783075327232243, 0.982838710991669], "final_y": [0.16485618433553262, 0.16485613970873947, 0.16485600853905436]}, "mutation_prompt": null}
{"id": "aaed3b35-ab88-44f6-92b1-2db3956f61d2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        dynamic_mutation_factor = 0.4 + 0.2 * np.random.rand()  # Changed line\n        mutant = a + dynamic_mutation_factor * (b - c)  # Changed line\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        period_factor = 0.8 + 0.2 * np.random.rand()  # Changed line\n        for i in range(period):\n            solution[i + period] = solution[i] * period_factor  # Changed line\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.5 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Introducing a dynamic mutation factor and enhanced periodicity enforcement for improved exploration and convergence.", "configspace": "", "generation": 12, "fitness": 0.9790789092083205, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d897ed48-5b81-4c7f-acc1-09aafaf3a9fd", "metadata": {"aucs": [0.976988320502719, 0.9763490934925791, 0.9838993136296637], "final_y": [0.16485585123441027, 0.16485594027830408, 0.1648570519229099]}, "mutation_prompt": null}
{"id": "6fecc03b-88ee-4b23-b04f-783835fee75d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = (solution[i] + solution[i + period]) / 2  # Fine-tuned periodicity\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                surrogate_accuracy = np.mean(np.abs(scores - self.surrogate_model.predict(population)))\n                self.mutation_factor = 0.5 + 0.5 * (1 - surrogate_accuracy)  # Enhanced adaptivity\n                self.crossover_rate = 0.6 + 0.3 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                diversity_factor = np.std(scores) / np.mean(scores)\n                trial_score = surrogate_score + diversity_factor * (func(trial) - surrogate_score)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced adaptive mechanism in mutation and crossover leveraging surrogate accuracy and diversity, with fine-tuned periodicity enforcement.", "configspace": "", "generation": 12, "fitness": 0.9634241707485235, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.028. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "682aef76-56dc-49c8-aade-ee95b6ff0d0a", "metadata": {"aucs": [0.9242387999823554, 0.984591312239603, 0.981442400023612], "final_y": [0.1818804245678617, 0.16485640163867565, 0.1648559581675204]}, "mutation_prompt": null}
{"id": "094f6133-2599-444e-a3e0-ce2a8ea6e606", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=15)  # Increased estimators\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            diversity_metric = np.std(scores)  # Diversity metric\n            self.mutation_factor = 0.5 + 0.3 * diversity_metric  # Adaptive mutation factor\n            self.crossover_rate = 0.5 + 0.3 * diversity_metric  # Adaptive crossover rate\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = func(trial) if np.random.rand() < 0.2 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Adaptive Cooperative Coevolutionary Optimization with dynamic mutation-crossover scheme guided by surrogate-assisted diversity metrics.", "configspace": "", "generation": 12, "fitness": 0.9801785104746953, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9727939637048358, 0.9830868854130685, 0.9846546823061814], "final_y": [0.164856448014725, 0.16485697444506364, 0.16485706176408743]}, "mutation_prompt": null}
{"id": "6e706a2c-92b7-483e-b42b-417bc977781f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 3  # Enhanced periodic enforcement\n        for i in range(period):\n            solution[i + period] = solution[i]  # Ensuring exact repetition for enhanced periodicity\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.55 + 0.45 * var_pred  # Enhanced adaptivity in mutation\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced cooperative coevolutionary optimization integrating surrogate model uncertainty for better exploration-exploitation balance and periodic constraint adaptation.", "configspace": "", "generation": 12, "fitness": 0.9745520929827175, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "682aef76-56dc-49c8-aade-ee95b6ff0d0a", "metadata": {"aucs": [0.9615488770978021, 0.978507340073086, 0.9836000617772643], "final_y": [0.16485619990243727, 0.16485762323917053, 0.16485901878786358]}, "mutation_prompt": null}
{"id": "27875e31-b94d-4f3c-9c3c-d6d89dcb666c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.6, 0.85)  # Adjusted ranges\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)  # Slight change in the periodic strategy\n        for i in range(period):\n            solution[i::period] = solution[i]  # Refactored periodic assignment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced adaptive periodicity and mutation strategies to improve convergence and solution quality.", "configspace": "", "generation": 13, "fitness": 0.9859717330312124, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9f127ee-0c4c-4b59-83f7-85fadcb53df4", "metadata": {"aucs": [0.9869884578386351, 0.983521125634624, 0.9874056156203785], "final_y": [0.1648559121669798, 0.16485637670454778, 0.1648570153753185]}, "mutation_prompt": null}
{"id": "e474a21b-e7eb-49fd-bdf3-439226b5050b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        # Adjusted mutation strategy by enforcing periodicity based on current budget\n        period = self.dim // 2 + int(self.current_budget / self.budget * self.dim // 2)\n        mutant = np.copy(a)\n        mutant[:period] += self.mutation_factor * (b[:period] - c[:period])\n        mutant[period:] += self.mutation_factor * (b[period:] - c[period:])\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.mutation_factor = 0.4 + 0.2 * (self.current_budget / self.budget)  # Adaptive mutation factor\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Refined mutation strategy by introducing adaptive periodicity to enhance global exploration and local refinement.", "configspace": "", "generation": 13, "fitness": 0.978303921484049, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f3bd4faf-bd4a-43b9-8f9b-be79df0f43f6", "metadata": {"aucs": [0.9828002898943932, 0.9708682088255499, 0.981243265732204], "final_y": [0.16485603714363128, 0.16485581085683543, 0.16485598732941897]}, "mutation_prompt": null}
{"id": "3771334a-c574-4f61-b3be-145989b379ed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            variance = np.var(scores)  # New line\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                self.mutation_factor = 0.6 + 0.2 * np.random.rand() * variance  # Changed line\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.5 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Improved adaptability by utilizing dynamic mutation factor based on surrogate model variance for better convergence.", "configspace": "", "generation": 13, "fitness": 0.9799751917561398, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d897ed48-5b81-4c7f-acc1-09aafaf3a9fd", "metadata": {"aucs": [0.9814792670319368, 0.9744618732190542, 0.9839844350174283], "final_y": [0.16485621302814701, 0.1648568190913583, 0.1648567695214782]}, "mutation_prompt": null}
{"id": "35f2ae75-3f1f-421a-a726-5b87a936e5cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Ensuring exact repetition for enhanced periodicity\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.55 + 0.45 * np.sqrt(var_pred)  # Enhanced adaptivity in mutation\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced surrogate utilization by adjusting exploration based on surrogate model variance to improve convergence.", "configspace": "", "generation": 13, "fitness": 0.9751617054609228, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "682aef76-56dc-49c8-aade-ee95b6ff0d0a", "metadata": {"aucs": [0.9742924604758592, 0.9707552013586281, 0.9804374545482808], "final_y": [0.16485620461157213, 0.16485588360587433, 0.16485608897069493]}, "mutation_prompt": null}
{"id": "5b2deaeb-3af3-409f-bc2d-05c4e1c38c7c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=20)  # Changed line\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        # Adaptive mutation based on diversity\n        diversity_factor = np.std(population, axis=0).mean()  # Changed line\n        mutant = a + (self.mutation_factor + diversity_factor) * (b - c)  # Changed line\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            if self.current_budget % 50 == 0:  # Changed line\n                self._build_surrogate_model(population, scores)  # Changed line\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.crossover_rate = 0.5 + 0.5 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced exploration by integrating diversity in mutation strategy and incorporating dynamic surrogate model updates.", "configspace": "", "generation": 13, "fitness": 0.9626200218934229, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.024. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d897ed48-5b81-4c7f-acc1-09aafaf3a9fd", "metadata": {"aucs": [0.9282585449672214, 0.9794568302608576, 0.9801446904521898], "final_y": [0.1818804245678617, 0.16485586465780389, 0.16485632392334149]}, "mutation_prompt": null}
{"id": "75e41318-7cc4-4139-bcad-cc99b27e71f3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.65, 0.9)  # Adjusted range\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced adaptive periodicity and mutation strategies with refined L-BFGS-B optimization parameters to improve convergence and solution quality.", "configspace": "", "generation": 14, "fitness": 0.9847013465656826, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "27875e31-b94d-4f3c-9c3c-d6d89dcb666c", "metadata": {"aucs": [0.9867846989726717, 0.9832457831985039, 0.9840735575258724], "final_y": [0.1648570005983815, 0.1648569810587066, 0.16485621769460412]}, "mutation_prompt": null}
{"id": "d849271f-3af1-4910-93a0-6bb7a6c14061", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.9  # Changed from 0.8 to 0.9\n        self.crossover_rate = 0.8  # Changed from 0.7 to 0.8\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.6, 1.0)  # Adjusted range\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='Powell', bounds=list(zip(bounds.lb, bounds.ub)))  # Changed method\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)  # Changed from 4 to 5\n        for i in range(period):\n            solution[i + period] = solution[i]\n            solution[i + 2 * period] = solution[i]\n            solution[i + 3 * period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced periodicity adaptation and hybridized local search to boost search efficiency and convergence performance.", "configspace": "", "generation": 14, "fitness": 0.9766999231724522, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e9f127ee-0c4c-4b59-83f7-85fadcb53df4", "metadata": {"aucs": [0.9746162821270089, 0.9775039661273808, 0.977979521262967], "final_y": [0.16486561831199642, 0.16486665532531186, 0.16487175937913223]}, "mutation_prompt": null}
{"id": "e6859026-c21f-4752-9f70-743599197a7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.5 + 0.4 * var_pred\n                self.crossover_rate = 0.7 + 0.25 * np.random.rand()  # Slight increase in randomness\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced adaptive mutation and crossover strategy to improve convergence precision.", "configspace": "", "generation": 14, "fitness": 0.9783366724468693, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9792377133995883, 0.9713439940894273, 0.9844283098515924], "final_y": [0.16485587235906107, 0.16485830738468854, 0.16485704755654917]}, "mutation_prompt": null}
{"id": "fa036cfe-7413-417a-8971-f57492addb0c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return self._enforce_periodicity(result.x)\n        return best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.5 + 0.4 * var_pred\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Fine-tuned local optimization step with enhanced periodicity enforcement for improved solution quality.", "configspace": "", "generation": 14, "fitness": 0.9618738457033786, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9602157895643839, 0.9679120329306556, 0.9574937146150962], "final_y": [0.16485883568289672, 0.16485591759658658, 0.1648561411012951]}, "mutation_prompt": null}
{"id": "646f07ce-e795-45a8-b9d9-fa4ff6e9208e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=15)  # Updated number of estimators\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = (solution[i] + solution[period + i]) / 2  # Reinforce periodic pattern\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.5 + 0.3 * var_pred  # Adjust mutation factor range\n                self.crossover_rate = 0.7 + 0.1 * np.random.rand()  # Adjust crossover rate range\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.4 else surrogate_score  # Adjust exploration chance\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhancing exploration with adaptive surrogate model updates and periodic pattern reinforcement.", "configspace": "", "generation": 14, "fitness": 0.9761294073867878, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.976 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9714991046403196, 0.9718237028588272, 0.9850654146612164], "final_y": [0.1648558371395643, 0.1648561851932423, 0.16485628246344752]}, "mutation_prompt": null}
{"id": "96b549df-0969-4d81-8e91-813901695d40", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.65, 0.9)  # Adjusted range\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        # Adjust crossover rate based on the current budget\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced dynamic adjustment of the crossover rate based on function evaluations to enhance exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.9832616584472712, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "75e41318-7cc4-4139-bcad-cc99b27e71f3", "metadata": {"aucs": [0.9847298096086634, 0.9841412308604165, 0.980913934872734], "final_y": [0.16485618198125596, 0.1648567161824962, 0.16485784632526257]}, "mutation_prompt": null}
{"id": "d557717d-67f3-4bd3-b98c-1d55ba59cbff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n        self.inertia_weight = 0.9  # New: Initial inertia weight for adaptive mutation\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = self.inertia_weight * np.random.uniform(0.65, 0.9)  # Revised\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def _update_inertia_weight(self):  # New: Adaptive inertia weight adjustment\n        self.inertia_weight = max(0.4, self.inertia_weight - 0.05)  # Decays over iterations\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n            self._update_inertia_weight()  # New: Update inertia weight at the end of each generation\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced adaptive inertia weight and periodic mutation factor to enhance convergence speed and diversity maintenance.", "configspace": "", "generation": 15, "fitness": 0.982590083725678, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "75e41318-7cc4-4139-bcad-cc99b27e71f3", "metadata": {"aucs": [0.9805024721998925, 0.983603154084486, 0.9836646248926555], "final_y": [0.16485602222685203, 0.16485591326031623, 0.16485597853147138]}, "mutation_prompt": null}
{"id": "d5b3b7a8-5460-4a8f-8f9f-eec181c58b58", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.5 + 0.4 * var_pred\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                weight = 0.3 + 0.4 * var_pred  # Adjusted line for dynamic weighting\n                trial_score = surrogate_score + weight * (func(trial) - surrogate_score) if np.random.rand() < weight else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced Cooperative Coevolutionary Optimization with dynamic surrogate prediction weight adjustment based on prediction variance to improve exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.9540670876472879, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.021. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9496609614408774, 0.9303128673146885, 0.9822274341862977], "final_y": [0.16485685305988695, 0.1818804950148526, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "0a5cb056-3e3a-49ef-b5a5-56c4095e4062", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]  # Ensuring exact repetition for enhanced periodicity\n        solution[:period] = np.mean(solution[:period], axis=0)  # Smooth transition within period\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                if i > 0 and scores[i] > scores[i-1]:\n                    self.mutation_factor *= 1.1  # Adaptive mutation factor increase\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced periodicity enforcement and adaptive surrogate model update using Random Forest for improved convergence. ", "configspace": "", "generation": 15, "fitness": 0.9649576737574653, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.965 with standard deviation 0.026. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "682aef76-56dc-49c8-aade-ee95b6ff0d0a", "metadata": {"aucs": [0.9852054266880781, 0.9284336772992684, 0.981233917285049], "final_y": [0.16485799028802917, 0.1818804950148526, 0.16485805500365602]}, "mutation_prompt": null}
{"id": "c995807a-10d3-4050-8d7f-b4aeb58e0448", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 12 * dim  # Increased population size\n        self.mutation_factor = 0.5  # Adjusted initial mutation factor\n        self.crossover_rate = 0.8  # Adjusted initial crossover rate\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=20)  # Increased number of estimators\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i] * (0.9 + 0.2 * np.random.rand())  # Adjusted periodicity\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        if self.current_budget % 10 == 0:  # Periodically update the surrogate model\n            self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                self.mutation_factor = 0.3 + 0.3 * (self.current_budget / self.budget)  # More adaptive mutation factor\n                self.crossover_rate = 0.6 + 0.3 * np.random.rand()  # More adaptive crossover rate\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.2 else surrogate_score  # Adjusted surrogate blending\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced Cooperative Coevolutionary Optimization using dynamic surrogate modeling and adaptive periodic enforcement for improved performance.", "configspace": "", "generation": 15, "fitness": 0.9605336056650997, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.023. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f3bd4faf-bd4a-43b9-8f9b-be79df0f43f6", "metadata": {"aucs": [0.9713239435851289, 0.9290432547860866, 0.9812336186240835], "final_y": [0.1648560129727339, 0.16558640816621195, 0.16485914547443514]}, "mutation_prompt": null}
{"id": "4d8ac7de-f566-41f9-86ef-6d9d4b91a2a5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.65, 0.9)  # Adjusted range\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])  # Average initial block\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    if trial_score < best_score:  # Introduced elitism\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced adaptive periodic adjustment and introduced elitism to boost convergence and solution robustness.", "configspace": "", "generation": 16, "fitness": 0.9863379481953668, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "96b549df-0969-4d81-8e91-813901695d40", "metadata": {"aucs": [0.9864445086281671, 0.9846918293790546, 0.9878775065788786], "final_y": [0.1648570764710071, 0.16485605913754853, 0.1648564002812395]}, "mutation_prompt": null}
{"id": "3186e35f-2f20-48df-8263-47ae13f65e82", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.6, 0.85)  # Adjusted ranges\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // (5 + int(np.std(solution))))  # Enhanced dynamic periodic strategy\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced periodic strategy by introducing dynamic adjustment based on fitness variance to improve solution diversity and convergence.", "configspace": "", "generation": 16, "fitness": 0.9841832642246725, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "27875e31-b94d-4f3c-9c3c-d6d89dcb666c", "metadata": {"aucs": [0.9836189676044342, 0.985108660542777, 0.9838221645268059], "final_y": [0.16485594672020965, 0.16485661251715056, 0.1648561743809307]}, "mutation_prompt": null}
{"id": "1ba0eeef-7106-4114-958b-5757a8ad7599", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.5, 0.9)  # Adjusted ranges\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)  # Slight change in the periodic strategy\n        for i in range(period):\n            solution[i::period] = solution[i]  # Refactored periodic assignment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.population_size = max(5, self.population_size - (self.current_budget // (self.budget // 10)))  # Adaptive population size\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced an adaptive population size strategy and refined mutation factor to enhance exploration and convergence.", "configspace": "", "generation": 16, "fitness": 0.9836895627365804, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "27875e31-b94d-4f3c-9c3c-d6d89dcb666c", "metadata": {"aucs": [0.9805506695805916, 0.9864537931238538, 0.9840642255052956], "final_y": [0.16485742139182413, 0.16485712583730316, 0.16485830022282388]}, "mutation_prompt": null}
{"id": "2e4683f1-6af9-459d-87fe-61c24b2b8d2d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                fitness_var = np.var(scores)\n                self.mutation_factor = 0.5 + 0.4 * fitness_var  # Change 1\n                self.crossover_rate = 0.7 + 0.2 * fitness_var  # Change 2\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Enhanced mutation factor and crossover rate based on fitness variance to improve exploration-exploitation balance.", "configspace": "", "generation": 16, "fitness": 0.9721550774100306, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9812479103445144, 0.9531674641760386, 0.9820498577095387], "final_y": [0.16485614632731072, 0.16485641321185807, 0.1648565027922212]}, "mutation_prompt": null}
{"id": "7d19de37-9c94-4b9a-b38a-42f2e152ecc9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass CooperativeCoevolutionaryOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.6\n        self.crossover_rate = 0.9\n        self.current_budget = 0\n        self.surrogate_model = RandomForestRegressor(n_estimators=10)\n        \n    def _initialize_population(self, bounds):\n        pop_uniform = np.random.rand(self.population_size // 2, self.dim)\n        pop_uniform = bounds.lb + (bounds.ub - bounds.lb) * pop_uniform\n        pop_gaussian = np.random.randn(self.population_size // 2, self.dim)\n        pop_gaussian = np.clip(bounds.lb + (bounds.ub - bounds.lb) * (pop_gaussian / 3), bounds.lb, bounds.ub)\n        return np.vstack((pop_uniform, pop_gaussian))\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        mutant = a + self.mutation_factor * (b - c)\n        return mutant\n\n    def _crossover(self, target, mutant, bounds):\n        cross_points = np.random.rand(self.dim) < self.crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n    \n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _enforce_periodicity(self, solution):\n        period = self.dim // 2\n        for i in range(period):\n            solution[i + period] = solution[i]\n        return solution\n\n    def _build_surrogate_model(self, population, scores):\n        self.surrogate_model.fit(population, scores)\n\n    def _predict_surrogate(self, candidate):\n        return self.surrogate_model.predict(candidate.reshape(1, -1))[0]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            self._build_surrogate_model(population, scores)\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                var_pred = np.var(self.surrogate_model.predict(population))\n                self.mutation_factor = 0.5 + 0.4 * var_pred\n                self.crossover_rate = 0.7 + 0.2 * np.random.rand()\n                trial = self._crossover(target, mutant, bounds)\n                trial = self._enforce_periodicity(trial)\n                surrogate_score = self._predict_surrogate(trial)\n                trial_score = surrogate_score + (func(trial) - surrogate_score) if np.random.rand() < 0.3 else surrogate_score\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = population[np.argmin(scores)], np.min(scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._enforce_periodicity(best)\n\n        return best", "name": "CooperativeCoevolutionaryOptimization", "description": "Introduced diverse initialization that considers both uniform and Gaussian sampling to enhance the search space coverage.", "configspace": "", "generation": 16, "fitness": 0.9791544319542309, "feedback": "The algorithm CooperativeCoevolutionaryOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1686cc8c-b239-43d0-824d-2776c5495eba", "metadata": {"aucs": [0.9838662843255427, 0.9720643379879487, 0.9815326735492012], "final_y": [0.1648610065343139, 0.16485792929383225, 0.16485613926677423]}, "mutation_prompt": null}
{"id": "5378cf07-4489-4bcb-b523-eef1d0d76514", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.6, 0.85)\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0) * (1 - self.current_budget / self.budget)  # Adaptive adjustment\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced more dynamic crossover through adaptive adjustment based on dimensionality and current budget utilization to improve convergence quality.", "configspace": "", "generation": 17, "fitness": 0.984804617154473, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "27875e31-b94d-4f3c-9c3c-d6d89dcb666c", "metadata": {"aucs": [0.9869868797221412, 0.983306330347053, 0.9841206413942248], "final_y": [0.16485769303340403, 0.1648561568466509, 0.16485609716522098]}, "mutation_prompt": null}
{"id": "b72c44df-4bd0-47cf-9f8c-b76070313f1a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.6, 0.85)\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        fitness_variance = np.std(solution)\n        scaling_factor = 1 + fitness_variance  # Introduced variance-based scaling\n        period = max(1, self.dim // (5 + int(scaling_factor * np.std(solution))))\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced the dynamic periodicity factor by incorporating a fitness variance-dependent scaling to adaptively adjust exploration-exploitation balance.", "configspace": "", "generation": 17, "fitness": 0.9842382264087978, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3186e35f-2f20-48df-8263-47ae13f65e82", "metadata": {"aucs": [0.9842409470520371, 0.9826838934932624, 0.9857898386810938], "final_y": [0.16485621071908485, 0.16485808096237808, 0.16485655776376584]}, "mutation_prompt": null}
{"id": "f1ffca6a-de34-4b81-b5ab-253fe136ca79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.6, 0.85)  # Adjusted ranges\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // (5 + int(np.std(solution))))  # Enhanced dynamic periodic strategy\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            population[np.argmax(scores)] = best  # Introduced elitism\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced an elitism strategy to retain the best individual through generations, enhancing convergence and robustness.", "configspace": "", "generation": 17, "fitness": 0.9854984553028768, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3186e35f-2f20-48df-8263-47ae13f65e82", "metadata": {"aucs": [0.9859415795572376, 0.9858723378754624, 0.9846814484759304], "final_y": [0.16485585357116972, 0.1648560395979125, 0.16485605440143825]}, "mutation_prompt": null}
{"id": "63c4ff1a-e4ba-4697-8580-a177f675ce07", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.65, 0.9)  # Adjusted range\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])  # Average initial block\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    elitism_factor = 1 - (self.current_budget / self.budget)  # Adaptive elitism factor\n                    if trial_score < best_score * elitism_factor:  # Introduced adaptive elitism\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Implemented adaptive elitism by dynamically adjusting the elitism factor based on current budget usage to enhance convergence.", "configspace": "", "generation": 17, "fitness": 0.9856207921358222, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4d8ac7de-f566-41f9-86ef-6d9d4b91a2a5", "metadata": {"aucs": [0.9862462129994065, 0.9881832308067635, 0.9824329326012967], "final_y": [0.16485642240795828, 0.16485669862251406, 0.16485701600655922]}, "mutation_prompt": null}
{"id": "d4e4c51e-638e-48c3-a180-0b697102877c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.5, 0.9)  # Adjusted ranges\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            # Change: Apply trajectory-based step adjustment to fine-tune convergence\n            step_adjustment = np.clip(result.x * np.sin(np.pi * np.arange(self.dim) / self.dim), bounds.lb, bounds.ub)\n            return step_adjustment\n        return best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)  # Slight change in the periodic strategy\n        for i in range(period):\n            solution[i::period] = solution[i]  # Refactored periodic assignment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.population_size = max(5, self.population_size - (self.current_budget // (self.budget // 10)))  # Adaptive population size\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced a trajectory-based step adjustment in local optimization to refine convergence precision.", "configspace": "", "generation": 17, "fitness": 0.9819635066559531, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1ba0eeef-7106-4114-958b-5757a8ad7599", "metadata": {"aucs": [0.9819116328868673, 0.9809736581707926, 0.9830052289101993], "final_y": [0.16485689289907413, 0.16485750891400752, 0.16485728247831732]}, "mutation_prompt": null}
{"id": "8539e764-86c2-4a67-b060-8a8be5135c05", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population, best_score):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.clip(0.6 + (best_score - scores.min()) / best_score, 0.6, 0.85)  # Changed line\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // (5 + int(np.std(solution))))  # Enhanced dynamic periodic strategy\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population, scores.min())  # Pass current best score\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            population[np.argmax(scores)] = best  # Introduced elitism\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced adaptive mutation factor based on current best score improvement to enhance exploration and convergence.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'scores' is not defined\").", "error": "NameError(\"name 'scores' is not defined\")", "parent_id": "f1ffca6a-de34-4b81-b5ab-253fe136ca79", "metadata": {}, "mutation_prompt": null}
{"id": "04c09ea2-d9c4-4c25-b3fb-b0d1fc436300", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.5, 0.9)  # Adjusted mutation range\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 25.0)  # Adjusted crossover rate\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Conditional improvement\n            return result.x\n        return best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 4)  # Adjusted periodic strategy\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced local search, adaptive mutation, and periodicity strategies to target complex landscapes more effectively.", "configspace": "", "generation": 18, "fitness": 0.9845750291530946, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "27875e31-b94d-4f3c-9c3c-d6d89dcb666c", "metadata": {"aucs": [0.9863100081351004, 0.9832496506631844, 0.984165428660999], "final_y": [0.16485596169928418, 0.16485594406844362, 0.16485734973592525]}, "mutation_prompt": null}
{"id": "749e3177-9bfc-4846-813b-004b93f89621", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = self.mutation_factor * (1 - self.current_budget / self.budget) + 0.5  # Adjusted scaling\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget ** 0.5)  # Improved crossover logic\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])  # Average initial block\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    elitism_factor = 1 - (self.current_budget / self.budget)  # Adaptive elitism factor\n                    if trial_score < best_score * elitism_factor:  # Introduced adaptive elitism\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced exploration by introducing adaptive mutation factor scaling and improved crossover to balance exploration and exploitation phases.", "configspace": "", "generation": 18, "fitness": 0.9863402709397467, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "63c4ff1a-e4ba-4697-8580-a177f675ce07", "metadata": {"aucs": [0.9842462653647159, 0.9869411077888298, 0.9878334396656944], "final_y": [0.16485646422046962, 0.16485628459460844, 0.16485642635986886]}, "mutation_prompt": null}
{"id": "f9fabc10-0e2b-407a-9451-229896e66588", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.65, 0.9) * (1 - self.current_budget / self.budget)  # Adjusted mutation factor\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])  # Average initial block\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    elitism_factor = 1 - (self.current_budget / self.budget)  # Adaptive elitism factor\n                    if trial_score < best_score * elitism_factor:  # Introduced adaptive elitism\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced adaptive mutation scaling by dynamically adjusting mutation factor based on current budget usage to enhance convergence robustness.", "configspace": "", "generation": 18, "fitness": 0.9854263883911777, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "63c4ff1a-e4ba-4697-8580-a177f675ce07", "metadata": {"aucs": [0.9857178955066842, 0.9848030532012971, 0.9857582164655516], "final_y": [0.1648561707577748, 0.16485649205095065, 0.16485612410236195]}, "mutation_prompt": null}
{"id": "91890757-43b7-40b2-81d0-01066ac96902", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = self.mutation_factor * (1 - self.current_budget / self.budget)  # Updated line\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])  # Average initial block\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    elitism_factor = 1 - (self.current_budget / self.budget)  # Adaptive elitism factor\n                    if trial_score < best_score * elitism_factor:  # Introduced adaptive elitism\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced adaptive scaling to the mutation factor based on budget usage to enhance solution diversity and convergence.", "configspace": "", "generation": 18, "fitness": 0.9873803719258779, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "63c4ff1a-e4ba-4697-8580-a177f675ce07", "metadata": {"aucs": [0.9869814151307712, 0.9874740697325225, 0.9876856309143396], "final_y": [0.1648564527138271, 0.16485628459460844, 0.16485629316589512]}, "mutation_prompt": null}
{"id": "6212d529-ff7f-427f-b797-6a0eaf9274dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = self.mutation_factor * (1 - self.current_budget / self.budget) + 0.5\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget ** 0.5)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': int(100 * (self.current_budget / self.budget + 0.5))})  # Adaptive intensification\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        self.population_size = int(self.population_size * (1 + 0.1 * (self.current_budget / self.budget)))  # Dynamic population size\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    elitism_factor = 1 - (self.current_budget / self.budget)\n                    if trial_score < best_score * elitism_factor:\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced diversity and convergence by introducing dynamic population size scaling and adaptive local search intensification.", "configspace": "", "generation": 19, "fitness": 0.9859883748046547, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "749e3177-9bfc-4846-813b-004b93f89621", "metadata": {"aucs": [0.9847161690378529, 0.9868972835935572, 0.9863516717825538], "final_y": [0.1648564607817451, 0.16485605913754853, 0.16485762961563077]}, "mutation_prompt": null}
{"id": "ab42803a-6b05-44f9-b207-1dc7bfd8ab9f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = np.random.uniform(0.65, 0.9)  # Adjusted ranges\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dim_based_crossover_rate = self.crossover_rate * (self.dim / 20.0)\n        cross_points = np.random.rand(self.dim) < dim_based_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)  # Slight change in the periodic strategy\n        for i in range(period):\n            solution[i::period] = solution[i]  # Refactored periodic assignment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n\n            best, best_score = self._select_best(population, scores)\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced the adaptive mutation factor range to better balance exploration and exploitation.", "configspace": "", "generation": 19, "fitness": 0.9856026323760539, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "27875e31-b94d-4f3c-9c3c-d6d89dcb666c", "metadata": {"aucs": [0.9867786784133548, 0.9838559137220998, 0.9861733049927075], "final_y": [0.16485933134278175, 0.16485599199713097, 0.16485605130977599]}, "mutation_prompt": null}
{"id": "a69a2053-b256-4e1e-983e-cb085bdf3644", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = 0.6 + 0.3 * (1 - self.current_budget / self.budget)  # Adaptive scaling\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        noise = np.random.normal(0, 0.01, size=best.shape)  # Added noise\n        perturbed_best = best + noise\n        result = minimize(func, perturbed_best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])  # Average initial block\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    elitism_factor = 1 - (self.current_budget / self.budget)  # Adaptive elitism factor\n                    if trial_score < best_score * elitism_factor:  # Introduced adaptive elitism\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Introduced adaptive mutation factor scaling and added noise in local optimization step for better exploration-exploitation balance.", "configspace": "", "generation": 19, "fitness": 0.9866137320128313, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "63c4ff1a-e4ba-4697-8580-a177f675ce07", "metadata": {"aucs": [0.9861774912402523, 0.9871319094883192, 0.9865317953099224], "final_y": [0.16485632985438536, 0.16485605913754853, 0.16485606168443168]}, "mutation_prompt": null}
{"id": "64eef417-2fd2-41e8-ac9c-11131ce0f617", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = self.mutation_factor * (0.5 + 0.5 * np.cos(np.pi * self.current_budget / self.budget))  # Updated line\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (0.5 + 0.5 * np.sin(np.pi * self.current_budget / self.budget))  # Updated line\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])  # Average initial block\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    elitism_factor = 1 - (self.current_budget / self.budget)\n                    if trial_score < best_score * elitism_factor:\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Further refined adaptive mutation and crossover strategies with improved diversity preservation and convergence.", "configspace": "", "generation": 19, "fitness": 0.9865380348807368, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "91890757-43b7-40b2-81d0-01066ac96902", "metadata": {"aucs": [0.9869080421303373, 0.9857331587962368, 0.9869729037156361], "final_y": [0.16485646422046962, 0.16485648408110665, 0.16485703790930506]}, "mutation_prompt": null}
{"id": "62b5ce06-4677-46c5-90f6-9b12e6df08a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedHybridDEBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.current_budget = 0\n\n    def _initialize_population(self, bounds):\n        pop = np.random.rand(self.population_size, self.dim)\n        pop = bounds.lb + (bounds.ub - bounds.lb) * pop\n        return pop\n\n    def _select_best(self, population, scores):\n        best_idx = np.argmin(scores)\n        return population[best_idx], scores[best_idx]\n\n    def _mutate(self, target_idx, population):\n        selected_indices = np.random.choice(np.delete(np.arange(self.population_size), target_idx), 3, replace=False)\n        a, b, c = population[selected_indices]\n        adaptive_mutation_factor = self.mutation_factor * (1 - (self.current_budget / self.budget) * np.std(population, axis=0).mean()) # Updated line\n        mutant = a + adaptive_mutation_factor * (b - c)\n        return mutant\n\n    def _dynamic_crossover(self, target, mutant, bounds):\n        dynamic_crossover_rate = self.crossover_rate * (1 - self.current_budget / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_crossover_rate\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return np.clip(trial, bounds.lb, bounds.ub)\n\n    def _evaluate_population(self, func, population):\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget += len(population)\n        return scores\n\n    def _local_optimization(self, best, func, bounds):\n        result = minimize(func, best, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})\n        return result.x if result.success else best\n\n    def _adaptive_periodicity(self, solution):\n        period = max(1, self.dim // 5)\n        solution[:period] = np.mean(solution[:period])  # Average initial block\n        for i in range(period):\n            solution[i::period] = solution[i]\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self._initialize_population(bounds)\n        scores = self._evaluate_population(func, population)\n        best, best_score = self._select_best(population, scores)\n\n        while self.current_budget < self.budget:\n            for i in range(self.population_size):\n                if self.current_budget >= self.budget:\n                    break\n                target = population[i]\n                mutant = self._mutate(i, population)\n                trial = self._dynamic_crossover(target, mutant, bounds)\n                trial = self._adaptive_periodicity(trial)\n                trial_score = func(trial)\n                self.current_budget += 1\n                if trial_score < scores[i]:\n                    population[i] = trial\n                    scores[i] = trial_score\n                    elitism_factor = 1 - (self.current_budget / self.budget)  # Adaptive elitism factor\n                    if trial_score < best_score * elitism_factor:  # Introduced adaptive elitism\n                        best, best_score = trial, trial_score\n\n            best = self._local_optimization(best, func, bounds)\n            best = self._adaptive_periodicity(best)\n\n        return best", "name": "RefinedHybridDEBFGS", "description": "Enhanced solution diversity by refining adaptive mutation strategy based on budget usage and solution quality.", "configspace": "", "generation": 19, "fitness": 0.9863744084930367, "feedback": "The algorithm RefinedHybridDEBFGS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "91890757-43b7-40b2-81d0-01066ac96902", "metadata": {"aucs": [0.9864119746378732, 0.9865135570812216, 0.986197693760015], "final_y": [0.1648564409248401, 0.1648566814502439, 0.16485677700510615]}, "mutation_prompt": null}
