{"id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', bounds=func.bounds, options={'maxfev': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "The algorithm combines uniform random sampling for diverse initial guesses with a local optimization (Nelder-Mead) for fast convergence on smooth, low-dimensional landscapes.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 27, in __call__\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nTypeError: zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds\n.", "error": "TypeError('zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 27, in __call__\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nTypeError: zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "5aac261c-0eaf-4c7a-bbf7-0082a7553058", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Initial uniform random sampling\n        num_initial_samples = min(self.budget // 2, 10 * self.dim)\n        initial_solutions = np.random.uniform(lb, ub, (num_initial_samples, self.dim))\n        \n        # Evaluate initial solutions\n        for solution in initial_solutions:\n            value = func(solution)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_solution = solution\n\n        # Adaptive local optimization using BFGS\n        options = {'maxiter': self.budget, 'disp': False}\n        def callback(xk):\n            nonlocal best_value, best_solution\n            current_value = func(xk)\n            if current_value < best_value:\n                best_value = current_value\n                best_solution = xk\n\n        # Refining bounds around the best solution\n        bounds = [(max(lb[i], best_solution[i] - 0.1*(ub[i]-lb[i])), \n                   min(ub[i], best_solution[i] + 0.1*(ub[i]-lb[i]))) for i in range(self.dim)]\n\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options=options, callback=callback)\n        \n        return best_solution if result.success else result.x", "name": "AdaptiveLocalSearch", "description": "Adaptive Local Search (ALS): Combines uniform sampling for initial exploration with adaptive local optimization using BFGS, adjusting bounds based on solution quality to efficiently solve low-dimensional, smooth landscapes.", "configspace": "", "generation": 0, "fitness": 0.6129584248842379, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.613 with standard deviation 0.315. And the mean value of best solutions found was 0.300 (0. is the best) with standard deviation 0.424.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8255460886750444, 0.8456698375106952, 0.16765934846697406], "final_y": [3.8505615651422315e-08, 3.854257415472684e-08, 0.8999418067781022]}, "mutation_prompt": null}
{"id": "8ca00084-b66f-4cae-afdc-d5511b8786c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom pyDOE2 import lhs\n\nclass HybridSamplingLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial samples using Latin Hypercube Sampling\n        lhs_samples = lhs(self.dim, samples=num_samples)\n        initial_samples = lb + (ub - lb) * lhs_samples\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using Sequential Quadratic Programming (SQP)\n            result = minimize(func, sample, method='SLSQP', bounds=zip(lb, ub), options={'maxiter': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridSamplingLocalSearch", "description": "Hybrid Sampling and Local Search (HSLS): Integrates Latin Hypercube Sampling for diverse initial guesses with a Sequential Quadratic Programming (SQP) approach for precise local refinement in smooth, low-dimensional landscapes.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'pyDOE2'\").", "error": "ModuleNotFoundError(\"No module named 'pyDOE2'\")", "parent_id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "metadata": {}, "mutation_prompt": null}
{"id": "b25cf465-2fbb-4ee7-b17f-bac9f1ef99cc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Initial uniform random sampling\n        num_initial_samples = min(self.budget // 2, 10 * self.dim)\n        initial_solutions = np.random.uniform(lb, ub, (num_initial_samples, self.dim))\n        \n        # Evaluate initial solutions\n        for solution in initial_solutions:\n            value = func(solution)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_solution = solution\n\n        # Adaptive local optimization using BFGS\n        options = {'maxiter': self.budget, 'disp': False}\n        def callback(xk):\n            nonlocal best_value, best_solution\n            current_value = func(xk)\n            if current_value < best_value:\n                best_value = current_value\n                best_solution = xk\n\n        # Refining bounds around the best solution\n        exploration_factor = 0.15  # Changed exploration factor to be more aggressive\n        bounds = [(max(lb[i], best_solution[i] - exploration_factor*(ub[i]-lb[i])), \n                   min(ub[i], best_solution[i] + exploration_factor*(ub[i]-lb[i]))) for i in range(self.dim)]\n\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options=options, callback=callback)\n        \n        return best_solution if result.success else result.x", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search that refines bounds dynamically and employs a more aggressive exploration-exploitation balance using uniform random sampling and BFGS.", "configspace": "", "generation": 1, "fitness": 0.3609041904524249, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.361 with standard deviation 0.340. And the mean value of best solutions found was 2.190 (0. is the best) with standard deviation 2.303.", "error": "", "parent_id": "5aac261c-0eaf-4c7a-bbf7-0082a7553058", "metadata": {"aucs": [0.08919179789059528, 0.15337931020141715, 0.8401414632652622], "final_y": [5.373032659117729, 1.1975105392773535, 2.173773151765226e-08]}, "mutation_prompt": null}
{"id": "da016170-a2bb-4d2d-89f4-19ddf902f0a4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Convert bounds to a format compatible with minimize\n            bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n            \n            # Local optimization using a dynamically adjusted Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(self.budget - evaluations, 100)})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                \n            # Dynamically adjust bounds based on current best solution\n            if best_solution is not None:\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "The algorithm leverages uniform random sampling for initial guesses and a hybrid Nelder-Mead with dynamic bound adjustments for efficient convergence on smooth, low-dimensional landscapes.", "configspace": "", "generation": 1, "fitness": 0.7454799229036085, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "metadata": {"aucs": [0.7519751068717838, 0.7121475189823274, 0.7723171428567139], "final_y": [2.8502122859051246e-07, 1.0019687260213625e-06, 5.186349986036283e-07]}, "mutation_prompt": null}
{"id": "4aa40952-f240-433a-b5d5-7823ca6230d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', options={'maxfev': self.budget - evaluations})\n            result.x = np.clip(result.x, lb, ub)  # Ensure solution stays within bounds\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "Integrate uniform random sampling with Nelder-Mead local optimization, correcting bounds handling for compatibility with `minimize`.", "configspace": "", "generation": 1, "fitness": 0.4681226538348033, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.468 with standard deviation 0.284. And the mean value of best solutions found was 2.984 (0. is the best) with standard deviation 4.220.", "error": "", "parent_id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "metadata": {"aucs": [0.6809356261186159, 0.6565807674168087, 0.06685156796898517], "final_y": [2.6612568496636684e-06, 1.0833079849351878e-05, 8.952470345282297]}, "mutation_prompt": null}
{"id": "c8725296-1e34-4e86-bbb1-6385064559e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Boundary-checking local optimization using Nelder-Mead\n            result = minimize(\n                lambda x: func(self.boundary_check(x, lb, ub)), \n                sample, \n                method='Nelder-Mead', \n                options={'maxfev': self.budget - evaluations}\n            )\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution\n\n    def boundary_check(self, x, lb, ub):\n        \"\"\"Ensure solution stays within bounds.\"\"\"\n        return np.maximum(lb, np.minimum(x, ub))", "name": "PhotonicsOptimizer", "description": "The algorithm combines uniform random sampling with a boundary-checking Nelder-Mead approach, ensuring parameter bounds are respected while optimizing rapidly in smooth, low-dimensional spaces.", "configspace": "", "generation": 1, "fitness": 0.4629318619456486, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.463 with standard deviation 0.280. And the mean value of best solutions found was 2.984 (0. is the best) with standard deviation 4.220.", "error": "", "parent_id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "metadata": {"aucs": [0.665363250451152, 0.6565807674168087, 0.06685156796898517], "final_y": [6.942981538003276e-06, 1.0833079849351878e-05, 8.952470345282297]}, "mutation_prompt": null}
{"id": "00b2f882-b60f-47dd-99fe-5614201e661e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        standard_bounds = [(lb[i], ub[i]) for i in range(len(lb))]  # Convert bounds to standard format\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', bounds=standard_bounds, options={'maxfev': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "The algorithm enhances initial random sampling with Nelder-Mead local search, ensuring compatibility with the standard bounds format by converting custom bounds.", "configspace": "", "generation": 1, "fitness": 0.44738592345643147, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.447 with standard deviation 0.272. And the mean value of best solutions found was 3.136 (0. is the best) with standard deviation 4.436.", "error": "", "parent_id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "metadata": {"aucs": [0.6127427361325988, 0.06374646752393687, 0.6656685667127586], "final_y": [6.83595335702047e-06, 9.409166966122346, 7.4822282991142135e-06]}, "mutation_prompt": null}
{"id": "311ddbab-369b-4ba2-8b64-84465aaeccc6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using Nelder-Mead\n            # Change made here for bounds compatibility\n            result = minimize(func, sample, method='Nelder-Mead', bounds=[(l, u) for l, u in zip(lb, ub)], options={'maxfev': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "The algorithm integrates uniform random sampling for diverse initial guesses with a local optimization using Nelder-Mead, ensuring compatibility with the bounds handling.", "configspace": "", "generation": 1, "fitness": 0.6345063822777319, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.635 with standard deviation 0.113. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "metadata": {"aucs": [0.47470093465011454, 0.7235629893235448, 0.7052552228595366], "final_y": [1.4608694651396678e-06, 6.908140641256781e-07, 1.731120332188101e-06]}, "mutation_prompt": null}
{"id": "09d8c52d-d8e4-4823-8c67-4ae2e317ac67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import differential_evolution\n\nclass HybridEvolutionaryLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Differential Evolution for initial exploration\n        differential_budget = max(10, self.budget // 2)\n        bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n        result = differential_evolution(func, bounds, maxiter=differential_budget, disp=False)\n        self.budget -= result.nfev\n\n        if result.fun < best_value:\n            best_value = result.fun\n            best_solution = result.x\n\n        # Adaptive local optimization using BFGS\n        options = {'maxiter': self.budget, 'disp': False}\n        def callback(xk):\n            nonlocal best_value, best_solution\n            current_value = func(xk)\n            if current_value < best_value:\n                best_value = current_value\n                best_solution = xk\n\n        # Refining bounds around the best solution\n        bounds = [(max(lb[i], best_solution[i] - 0.1*(ub[i]-lb[i])), \n                   min(ub[i], best_solution[i] + 0.1*(ub[i]-lb[i]))) for i in range(self.dim)]\n\n        result = minimize(func, best_solution, method='L-BFGS-B', bounds=bounds, options=options, callback=callback)\n\n        return best_solution if result.success else result.x", "name": "HybridEvolutionaryLocalSearch", "description": "Hybrid Evolutionary Local Search (HELS): Combines differential evolution for robust exploration with BFGS for fast local convergence, iteratively shrinking search bounds based on solution quality to efficiently solve low-dimensional, smooth landscapes.", "configspace": "", "generation": 1, "fitness": 0.3591988760711109, "feedback": "The algorithm HybridEvolutionaryLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.359 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5aac261c-0eaf-4c7a-bbf7-0082a7553058", "metadata": {"aucs": [0.38347887356339383, 0.34876659896424667, 0.3453511556856922], "final_y": [6.9466522705859314e-06, 0.0001290594579929471, 0.00011917859420774978]}, "mutation_prompt": null}
{"id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Hybrid Gradient-Based and Heuristic Search: Combines Sobol sequence sampling for well-distributed initial guesses with L-BFGS-B for efficient local optimization and dynamic adjustment of bounds based on convergence patterns to solve low-dimensional, smooth landscapes.", "configspace": "", "generation": 1, "fitness": 0.8184598996843487, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "metadata": {"aucs": [0.8259597637699975, 0.7973355996360012, 0.8320843356470473], "final_y": [1.4434633133787231e-08, 1.2448841005109425e-07, 4.361617258839582e-08]}, "mutation_prompt": null}
{"id": "389a7731-019b-4d61-8a24-8e7412c01976", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedPhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds as tuples\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        bounds = [(l, u) for l, u in zip(lb, ub)]\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                \n                # Refine bounds around the best solution\n                refinement_factor = 0.1\n                new_lb = np.maximum(lb, best_solution - refinement_factor * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + refinement_factor * (ub - lb))\n                bounds = [(l, u) for l, u in zip(new_lb, new_ub)]\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "EnhancedPhotonicsOptimizer", "description": "Enhanced PhotonicsOptimizer with iterative bound refinement for improved convergence by adjusting local search bounds based on past solutions.", "configspace": "", "generation": 1, "fitness": 0.6419528764843189, "feedback": "The algorithm EnhancedPhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.642 with standard deviation 0.060. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "0713d005-2fa6-4b36-914d-8fa5b77c0ba9", "metadata": {"aucs": [0.6664496682579159, 0.6995141402745056, 0.559894820920535], "final_y": [7.071504975212195e-06, 3.5067047020234295e-06, 5.924585516044652e-06]}, "mutation_prompt": null}
{"id": "6417c63c-6e15-4647-9f0e-31ba3f6d6ffe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\nfrom skopt import gp_minimize  # Added line for Bayesian optimization\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        num_samples = max(5, self.budget // 10)\n        \n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            if evaluations >= self.budget:\n                break\n        \n        if evaluations < self.budget:  # Added line to use Bayesian optimization\n            result = gp_minimize(func, list(zip(lb, ub)), n_calls=self.budget-evaluations, x0=best_solution)\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Optimized Hybrid Search: Enhances the Hybrid Gradient-Based and Heuristic Search by incorporating Bayesian optimization for adaptive sampling and dynamic bound adjustment to efficiently explore and exploit low-dimensional, smooth landscapes.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'skopt'\").", "error": "ModuleNotFoundError(\"No module named 'skopt'\")", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {}, "mutation_prompt": null}
{"id": "000a0b35-3bf8-44f6-acb6-a63ba9c1ab44", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ImprovedPhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial adaptive random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Convert bounds to a format compatible with minimize\n            bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n            \n            # Local optimization using a dynamically adjusted Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(self.budget - evaluations, 100)})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n\n            # Use quadratic interpolation to refine the search around the best solution\n            if best_solution is not None:\n                new_samples = self.quadratic_interpolation(best_solution, lb, ub, 3)\n                for new_sample in new_samples:\n                    if evaluations >= self.budget:\n                        break\n                    result = minimize(func, new_sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(self.budget - evaluations, 100)})\n                    evaluations += result.nfev\n                    if result.fun < best_score:\n                        best_score = result.fun\n                        best_solution = result.x\n                \n                # Dynamically adjust bounds based on current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution\n\n    def quadratic_interpolation(self, best_solution, lb, ub, num_points):\n        # Generate points around the best solution using quadratic interpolation\n        perturbation = np.linspace(-0.05, 0.05, num_points) * (ub - lb)\n        new_samples = [np.clip(best_solution + delta, lb, ub) for delta in perturbation]\n        return new_samples", "name": "ImprovedPhotonicsOptimizer", "description": "Improved PhotonicsOptimizer using adaptive sampling and iterative quadratic interpolation for enhanced convergence in smooth, low-dimensional landscapes.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (3,) (2,) ').", "error": "ValueError('operands could not be broadcast together with shapes (3,) (2,) ')", "parent_id": "da016170-a2bb-4d2d-89f4-19ddf902f0a4", "metadata": {}, "mutation_prompt": null}
{"id": "224c444e-2404-4626-bdf2-ad5904e93ade", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial samples using Sobol sequence\n        sobol = Sobol(d=self.dim, scramble=True)\n        initial_samples = lb + (ub - lb) * sobol.random_base2(m=int(np.log2(num_samples)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Convert bounds to a format compatible with minimize\n            bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n            \n            # Local optimization using a dynamically adjusted Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(self.budget - evaluations, 100)})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                \n            # Dynamically adjust bounds based on current best solution\n            if best_solution is not None:\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "Enhanced PhotonicsOptimizer with Sobol sequence for more uniform initial sampling, improving initial exploration.", "configspace": "", "generation": 2, "fitness": 0.6768122972306272, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.677 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "da016170-a2bb-4d2d-89f4-19ddf902f0a4", "metadata": {"aucs": [0.6649490961424156, 0.7104928464310719, 0.6549949491183938], "final_y": [7.372023754431197e-06, 2.569488452122576e-06, 9.01611638378674e-06]}, "mutation_prompt": null}
{"id": "ad8eaa5e-ba83-46d0-a9c8-e582e9118ee9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial samples using Sobol sequence\n        sobol = Sobol(d=self.dim, scramble=True)\n        initial_samples = lb + (ub - lb) * sobol.random_base2(m=int(np.log2(num_samples)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Convert bounds to a format compatible with minimize\n            bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n            \n            # Local optimization using a dynamically adjusted Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(self.budget - evaluations, 100)})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                \n            # Dynamically adjust bounds based on current best solution\n            if best_solution is not None:\n                lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "Enhanced PhotonicsOptimizer using Sobol sequence for structured sampling and adaptive Nelder-Mead with dynamic exploration capabilities for improved convergence on smooth, low-dimensional landscapes.", "configspace": "", "generation": 2, "fitness": 0.6501555136126037, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.650 with standard deviation 0.048. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "da016170-a2bb-4d2d-89f4-19ddf902f0a4", "metadata": {"aucs": [0.6738954569311111, 0.6940132796626406, 0.5825578042440596], "final_y": [6.444069449701358e-06, 4.167615928943974e-06, 4.437338998796489e-05]}, "mutation_prompt": null}
{"id": "4a73c150-09ee-4199-abd5-7785c6d7f1d5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Set a random seed for reproducibility\n        np.random.seed(42)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Convert bounds to a format compatible with minimize\n            bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n            \n            # Local optimization using a dynamically adjusted Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(self.budget - evaluations, 100)})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                \n            # Dynamically adjust bounds based on current best solution\n            if best_solution is not None:\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "Enhanced PhotonicsOptimizer with random seed for reproducibility, ensuring consistent initial sampling and potentially improving convergence.", "configspace": "", "generation": 2, "fitness": 0.650919712033141, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.651 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "da016170-a2bb-4d2d-89f4-19ddf902f0a4", "metadata": {"aucs": [0.6458269735311151, 0.6655462160320209, 0.6413859465362871], "final_y": [1.3745698576177885e-05, 6.127290879473908e-06, 1.3769054778552596e-05]}, "mutation_prompt": null}
{"id": "c9fb2c39-b022-44c1-a408-1c2bee91ca9c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial random samples\n        initial_samples = lb + (ub - lb) * np.random.rand(num_samples, self.dim)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Convert bounds to a format compatible with minimize\n            bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n            \n            # Local optimization using a dynamically adjusted Nelder-Mead\n            if best_solution is not None and np.linalg.norm(sample - best_solution) < 0.01:\n                result = minimize(func, sample, method='Powell', bounds=bounds, options={'maxfev': min(self.budget - evaluations, 100)})\n            else:\n                result = minimize(func, sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(self.budget - evaluations, 100)})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                \n            # Dynamically adjust bounds based on current best solution\n            if best_solution is not None:\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "PhotonicsOptimizer", "description": "Enhances local optimization by switching to Powell's method when close to convergence, improving exploitation in smooth parameter spaces.", "configspace": "", "generation": 2, "fitness": 0.7970869550473155, "feedback": "The algorithm PhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.145. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "da016170-a2bb-4d2d-89f4-19ddf902f0a4", "metadata": {"aucs": [1.0, 0.669763371801754, 0.7214974933401923], "final_y": [0.0, 7.680387688607916e-06, 2.009374160809477e-06]}, "mutation_prompt": null}
{"id": "66c125f4-8992-4b34-8e65-831875872078", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass EnhancedPhotonicsOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Sobol sequence for low-discrepancy sampling\n        sobol_sampler = qmc.Sobol(d=self.dim, scramble=True)\n        initial_samples = lb + (ub - lb) * sobol_sampler.random(num_samples)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Convert bounds to a format compatible with minimize\n            bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n            \n            # Local optimization using L-BFGS-B with constrained bounds\n            result = minimize(func, sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget - evaluations, 100)})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                \n            # Dynamically adjust bounds based on current best solution\n            if best_solution is not None:\n                lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "EnhancedPhotonicsOptimizer", "description": "Enhanced PhotonicsOptimizer using adaptive Sobol sequence initial sampling and constrained L-BFGS-B for improved convergence and precision on smooth, low-dimensional landscapes.", "configspace": "", "generation": 2, "fitness": 0.7317180549798138, "feedback": "The algorithm EnhancedPhotonicsOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.732 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "da016170-a2bb-4d2d-89f4-19ddf902f0a4", "metadata": {"aucs": [0.7233537602326874, 0.7289708771090508, 0.7428295275977033], "final_y": [1.65101399366803e-07, 1.1093125732346658e-07, 6.051112513161997e-08]}, "mutation_prompt": null}
{"id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with adaptive Sobol sample scaling for improved local search precision.", "configspace": "", "generation": 2, "fitness": 0.8210288429511259, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.048. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {"aucs": [0.7693055488494134, 0.808152167778318, 0.8856288122256462], "final_y": [1.0139174847839055e-07, 5.0919121089351087e-08, 9.02993407549385e-09]}, "mutation_prompt": null}
{"id": "5dc30fb3-48a1-44d6-9033-780d1a8fdf02", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Changed from 0.1 to 0.2\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Based and Heuristic Search: Integrates Sobol sequence sampling for initial guesses with L-BFGS-B for local optimization and adaptive adjustment of dynamic bounds based on convergence patterns for low-dimensional, smooth landscapes.", "configspace": "", "generation": 2, "fitness": 0.25254212718332175, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.253 with standard deviation 0.344. And the mean value of best solutions found was 22.307 (0. is the best) with standard deviation 15.773.", "error": "", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {"aucs": [0.7395493306736329, 0.009037627136091353, 0.009039423740240915], "final_y": [4.755751833268035e-08, 33.460396823398135, 33.460396823398135]}, "mutation_prompt": null}
{"id": "5d727615-e1cb-4e0a-96f1-55e0d8043d72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(10, self.budget // 10)  # Changed from 5 to 10\n        \n        # Generate initial Sobol samples\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic: Integrates Sobol sequence sampling with L-BFGS-B and optimizes initial sample size for efficient convergence on smooth, low-dimensional landscapes.", "configspace": "", "generation": 2, "fitness": 0.7649710970470324, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.765 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {"aucs": [0.7593504192587818, 0.785221466226707, 0.7503414056556086], "final_y": [7.135147567090406e-08, 1.4796963762700722e-07, 1.1431178849168278e-07]}, "mutation_prompt": null}
{"id": "48473fe7-2237-4466-af2b-d2176e8ed066", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Dynamic exploration-exploitation balance\n            exploration_factor = 0.05 + 0.05 * (self.budget - evaluations) / self.budget\n            bounds = [(max(l, s - exploration_factor * (u - l)), min(u, s + exploration_factor * (u - l)))\n                      for s, l, u in zip(sample, lb, ub)]\n            \n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with adaptive Sobol sample scaling and dynamic exploration-exploitation balance for improved local search precision.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {}, "mutation_prompt": null}
{"id": "540afb09-a926-4a54-82ad-c477fb111535", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.02 * (ub - lb)))  # Changed 0.01 to 0.02 for better coverage\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Adaptive Hybrid Gradient-Heuristic with improved Sobol sample scaling precision for enhanced solution convergence.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {}, "mutation_prompt": null}
{"id": "9812fa06-7a5b-4f36-b998-f31947857c7e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.02 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with dynamic Sobol sample scaling and adaptive local search bounds to improve convergence precision.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {}, "mutation_prompt": null}
{"id": "3583251d-c6e1-4b82-82c8-285fd73d2efb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Adjust bounds to improve convergence precision\n            dynamic_bounds = list(zip(np.maximum(lb, sample - 0.1 * (ub - lb)), np.minimum(ub, sample + 0.1 * (ub - lb))))\n            \n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=dynamic_bounds, options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n            \n            # Early termination if the global minimum is found \n            if best_score < 1e-6 or evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Sobol-based Adaptive Bounds with Early Termination for Efficient Local Optimization", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {}, "mutation_prompt": null}
{"id": "a3f7874f-bfef-4958-8600-69d89491830a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 8)  # Increased initial samples for better exploration\n        \n        # Generate initial Sobol samples\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with adaptive Sobol sample scaling and increased Sobol sampling iterations for improved solution space exploration.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {}, "mutation_prompt": null}
{"id": "3457910e-61e8-4c05-a631-5f8df3926f10", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, int(0.15 * self.budget))  # Adjusted line: Changed sample scaling factor\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with adaptive Sobol sample scaling and strategic sampling size adjustment for improved precision.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {}, "mutation_prompt": null}
{"id": "ac4b3c0d-310d-463b-910f-582a3b50c349", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                bounds_adjustment_factor = 0.05  # Changed from 0.1 to improve convergence\n                lb = np.maximum(lb, best_solution - bounds_adjustment_factor * (ub - lb))\n                ub = np.minimum(ub, best_solution + bounds_adjustment_factor * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with improved dynamic bounds adjustment for better convergence.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {}, "mutation_prompt": null}
{"id": "1882e64c-2bc4-4d1f-a894-a0555a18abe5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))  # Changed from 0.1 to 0.05\n                ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))  # Changed from 0.1 to 0.05\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with dynamic sample scaling for improved Sobol distribution and refined local search precision.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {}, "mutation_prompt": null}
{"id": "3596927e-3315-4b94-a2e0-f3501c7961b4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(10, self.budget // 8)  # Adjusted sample count\n\n        # Generate initial Sobol samples\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Refined Hybrid Gradient-Heuristic with increased Sobol sample count for broader initial exploration.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {}, "mutation_prompt": null}
{"id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Adaptive Differential Evolution and Local Search: Integrates Differential Evolution with adaptive parameter control for exploration and employs L-BFGS-B for exploitation, allowing dynamic adjustment of population size and strategy based on fitness improvements for solving low-dimensional, smooth optimization landscapes.", "configspace": "", "generation": 3, "fitness": 0.8184707123880385, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.129. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "00631e25-bf4e-4352-98b6-a6c5d30950f5", "metadata": {"aucs": [1.0, 0.7458534504051231, 0.7095586867589925], "final_y": [0.0, 1.9823097779682604e-07, 2.8076303311341657e-07]}, "mutation_prompt": null}
{"id": "6803b0a5-e683-4790-a2c3-3b0d95c96559", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass GradientEnhancedDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Trust region based local optimization using BFGS\n                        bounds = Bounds(lb, ub)\n                        trust_radius = 0.1 * (ub - lb)\n                        result = minimize(func, best_solution, method='trust-constr', bounds=bounds, options={'maxiter': self.budget - evaluations, 'initial_trust_radius': np.min(trust_radius), 'verbose': 0})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "GradientEnhancedDE", "description": "Gradient-Enhanced Differential Evolution with Local Trust Region Search: Combines Differential Evolution with gradient information to refine solutions within a dynamically adjusted trust region, leveraging both global exploration and precise local exploitation.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"_minimize_trustregion_constr() got an unexpected keyword argument 'initial_trust_radius'\").", "error": "TypeError(\"_minimize_trustregion_constr() got an unexpected keyword argument 'initial_trust_radius'\")", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {}, "mutation_prompt": null}
{"id": "be1ee0b6-9557-49a1-9991-e0088ba19878", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))  # Changed line\n                ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))  # Changed line\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Improved Hybrid Gradient-Heuristic using adjusted dynamic bounds scaling for enhanced local search precision.", "configspace": "", "generation": 4, "fitness": 0.28018730340557885, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.280 with standard deviation 0.343. And the mean value of best solutions found was 11.425 (0. is the best) with standard deviation 8.079.", "error": "", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {"aucs": [0.7651449368653738, 0.03770230830752508, 0.03771466504383769], "final_y": [8.915918974151556e-08, 17.138240910046346, 17.138240910046346]}, "mutation_prompt": null}
{"id": "2704f931-3fba-4de5-82c1-84606cbcadbc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        scaling_factor = 0.02  # Adjusted scaling factor for improved precision\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + scaling_factor * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Sobol sample scaling within bounds through dynamic scaling factor adjustment to improve local search precision.", "configspace": "", "generation": 4, "fitness": 0.0064899847670206, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.006 with standard deviation 0.000. And the mean value of best solutions found was 34.652 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {"aucs": [0.0064895622359719685, 0.0064909739801566335, 0.006489418084933196], "final_y": [34.6522506749716, 34.6522506749716, 34.6522506749716]}, "mutation_prompt": null}
{"id": "b553cfb0-dac4-4189-a75a-c76f6a29defb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            fitness_variance = np.var(fitness)  # New line: dynamically calculate variance in fitness\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Improved Adaptive Differential Evolution by dynamically adjusting mutation factor based on fitness variance for better exploration.", "configspace": "", "generation": 4, "fitness": 0.7353334272926114, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.735 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.7052015766249031, 0.7646074240040907, 0.7361912812488407], "final_y": [9.573092279328208e-09, 4.00891330644205e-08, 2.3562322564977124e-07]}, "mutation_prompt": null}
{"id": "ff47474b-325c-4897-9463-d76207454264", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B with dynamic step size\n            step_size = 0.05 * (ub - lb)  # Dynamic step size calculation\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations, 'eps': step_size})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))  # Adjusted bound scaling\n                ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))  # Adjusted bound scaling\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Refined Hybrid Gradient-Heuristic with adaptive step size and dynamic bounds adjustment for enhanced convergence.", "configspace": "", "generation": 4, "fitness": 0.008403199127123026, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.008 with standard deviation 0.000. And the mean value of best solutions found was 33.460 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {"aucs": [0.009064707093808244, 0.008072394835275998, 0.008072495452284834], "final_y": [33.460396823398135, 33.460396823398135, 33.460396823398135]}, "mutation_prompt": null}
{"id": "8a47f63c-9f36-4754-9a08-7a184cd7a65f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control and reshuffling\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n                        if evaluations % 10 == 0:\n                            population = np.random.permutation(population)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Adaptive Differential Evolution with tuned exploration and exploitation, incorporating strategic population reshuffling for enhanced convergence in smooth optimization landscapes.", "configspace": "", "generation": 4, "fitness": 0.728275910133814, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.728 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.7247741716484013, 0.7641573521075289, 0.6958962066455114], "final_y": [7.837384612301762e-08, 1.3340794118292756e-07, 3.65010307258629e-07]}, "mutation_prompt": null}
{"id": "72b1dacb-4926-41a5-a490-43d3a8a67bae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - population[best_idx]), lb, ub)  # Change: Mutant vector update\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.5, 0.9)  # Change: CR range adjustment\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Improved DE with strategic mutation update and dynamic CR adjustment to enhance convergence speed and precision.", "configspace": "", "generation": 4, "fitness": 0.6772951676579354, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.677 with standard deviation 0.070. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.6937559349666418, 0.7541696360895564, 0.5839599319176079], "final_y": [1.206087275123708e-07, 1.3533884298016263e-07, 1.3128210926399626e-07]}, "mutation_prompt": null}
{"id": "d63707d3-537b-4a9f-8583-971d6dc868c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import OptimizeResult\nfrom scipy.stats import qmc\n\nclass AdaptiveQuasiRandomSearchLocalEscalation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Halton sequence sampler\n        sampler = qmc.Halton(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Halton samples\n        initial_samples = qmc.scale(sampler.random(num_samples), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using Nelder-Mead\n            result = minimize(func, sample, method='Nelder-Mead', options={'maxfev': self.budget - evaluations, 'adaptive': True})\n\n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                \n                # Adjust simplex size based on the improvement\n                simplex_size_factor = 0.05 if result.fun < 1e-3 else 0.1\n                lb = np.maximum(lb, best_solution - simplex_size_factor * (ub - lb))\n                ub = np.minimum(ub, best_solution + simplex_size_factor * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "AdaptiveQuasiRandomSearchLocalEscalation", "description": "Adaptive Quasi-Random Search and Local Escalation: Combines low-discrepancy Halton sequences for diverse initialization and adaptive Nelder-Mead for refined local exploration, dynamically adjusting simplex sizes based on interim solution quality to precisely navigate smooth optimization surfaces.", "configspace": "", "generation": 4, "fitness": 0.6010841603041728, "feedback": "The algorithm AdaptiveQuasiRandomSearchLocalEscalation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.601 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {"aucs": [0.5484552483361484, 0.6871740616716356, 0.5676231709047341], "final_y": [2.9984621135887784e-06, 5.646057048109743e-06, 5.571919437903914e-06]}, "mutation_prompt": null}
{"id": "186a070d-9fd5-4afb-93fd-558fbc2bc4ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization with dynamic scaling\n                        local_budget = min(50, self.budget - evaluations)\n                        if local_budget > 0:\n                            result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': local_budget})\n                            evaluations += result.nfev\n                            if result.fun < best_score:\n                                best_score = result.fun\n                                best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with Dynamic Local Search Scaling, utilizing covariance matrix adaptation for efficient exploration-exploitation balance in smooth, low-dimensional landscapes.", "configspace": "", "generation": 4, "fitness": 0.29292032483054103, "feedback": "The algorithm EnhancedAdaptiveDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.293 with standard deviation 0.030. And the mean value of best solutions found was 0.023 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.26359890395213126, 0.3341009454194017, 0.2810611251200902], "final_y": [0.03563426974661956, 0.0015497756699710456, 0.03071600386904187]}, "mutation_prompt": null}
{"id": "a7e2688d-505a-4642-8e54-976e320f1545", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B with adaptive triggering\n                        if evaluations < self.budget - 10:  # Only trigger if enough budget remains\n                            result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                            evaluations += result.nfev\n                            if result.fun < best_score:\n                                best_score = result.fun\n                                best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.6, 1.0)  # Adjusted range\n                        CR = np.random.uniform(0.6, 1.0)  # Adjusted range\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Improved Adaptive Differential Evolution with dynamic F and CR adjustments and adaptive local search triggering for enhanced exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.7842618283355511, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.153. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [1.0, 0.6924768616467949, 0.6603086233598583], "final_y": [0.0, 2.1018042812579698e-07, 1.868432231443306e-07]}, "mutation_prompt": null}
{"id": "66310614-442a-4def-8f51-1a41aa047329", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with adaptive Sobol sample scaling and dynamic bound adjustment for improved local search precision.", "configspace": "", "generation": 5, "fitness": 0.8375802661982806, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.065. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {"aucs": [0.8851564369050335, 0.8815150651763142, 0.7460692965134939], "final_y": [6.436565939296439e-09, 5.251712026170197e-09, 1.0927611790924742e-07]}, "mutation_prompt": null}
{"id": "76d32806-c064-4aed-9c62-2c637b83b170", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples; dynamically adjusted based on budget\n        num_samples = max(5, self.budget // 15)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with adaptive Sobol sample scaling and dynamic sample size for improved local search precision.", "configspace": "", "generation": 5, "fitness": 0.008072974397785168, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.008 with standard deviation 0.000. And the mean value of best solutions found was 33.460 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {"aucs": [0.008073193821381053, 0.008073613413461356, 0.008072115958513093], "final_y": [33.460396823398135, 33.460396823398135, 33.460396823398135]}, "mutation_prompt": null}
{"id": "390153d9-a48a-465e-a47a-1c0405b9e510", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamic population adaptation\n            if np.min(fitness) < best_score:\n                pop_size = min(pop_size + 1, self.budget // 10)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with dynamic population size adjustment based on fitness improvements for refined exploration.", "configspace": "", "generation": 5, "fitness": 0.7616971564305665, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.762 with standard deviation 0.058. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.7197360902504975, 0.7212177863131931, 0.8441375927280088], "final_y": [3.6721336862458966e-07, 1.4710927956458682e-07, 1.847424925029267e-08]}, "mutation_prompt": null}
{"id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Improved AdaptiveDELocalSearch with dynamic population size adjustment based on exploration progress for enhanced convergence.", "configspace": "", "generation": 5, "fitness": 0.8401115618776401, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.113. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.9981490934530588, 0.7440794357787752, 0.7781061564010865], "final_y": [0.0, 1.5153867510100895e-07, 1.6022407225913405e-08]}, "mutation_prompt": null}
{"id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)  # Changed: incorporated best_solution in mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Optimized Adaptive DE with enhanced mutation strategy using best individual for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.8382006509668916, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.113. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.9981490934530588, 0.7689677460068757, 0.7474851134407402], "final_y": [0.0, 1.2931123065133596e-07, 7.983833117861324e-08]}, "mutation_prompt": null}
{"id": "68bd0d35-2a7f-4a93-a2ed-44654b06986d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        if evaluations < self.budget * 0.75: # Adjusting local search trigger frequency\n                            result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                            evaluations += result.nfev\n                            if result.fun < best_score:\n                                best_score = result.fun\n                                best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n                        pop_size = max(5, pop_size + np.random.randint(-2, 3)) # Dynamic population size adjustment\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced Adaptive Differential Evolution with Dynamic Population Strategy and Adaptive Local Search Frequency for Efficient Exploration-Exploitation Balance.", "configspace": "", "generation": 5, "fitness": 0.8090245954413008, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.134. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.9990689392237291, 0.7127350561752119, 0.7152697909249613], "final_y": [0.0, 1.1750830675223726e-07, 1.4919631189338042e-07]}, "mutation_prompt": null}
{"id": "2a6cdf6e-4df4-4f77-82f5-c1d628044cce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, np.minimum(ub, lb + 0.01 * (ub - lb)))\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))  # Changed from 0.1 to 0.05\n                ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Enhanced Hybrid Gradient-Heuristic with improved dynamic bound adjustment by scaling factors for local refinement.", "configspace": "", "generation": 5, "fitness": 0.26141425297001086, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.261 with standard deviation 0.354. And the mean value of best solutions found was 21.429 (0. is the best) with standard deviation 15.153.", "error": "", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {"aucs": [0.7624841159949303, 0.010879560759486595, 0.010879082155615727], "final_y": [1.456405662626893e-08, 32.14380666119143, 32.14380666119143]}, "mutation_prompt": null}
{"id": "5c9ae8b7-4f13-4b66-be91-c370cf4ef41e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                diversity = np.std(population, axis=0).mean()\n                CR = 0.5 + 0.5 * (1 - diversity / (ub - lb).mean())\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with variable crossover rate based on population diversity for better exploration.", "configspace": "", "generation": 5, "fitness": 0.7207823841033248, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.721 with standard deviation 0.197. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [0.9990689392237291, 0.6004687868302332, 0.5628094262560122], "final_y": [0.0, 4.131808950418222e-07, 5.2547726277300614e-08]}, "mutation_prompt": null}
{"id": "a4a6b265-58f7-45b8-be92-fd6fc92bbffc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridGradientHeuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Retrieve bounds\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        \n        # Initialize Sobol sequence sampler\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        \n        # Number of initial samples\n        num_samples = max(5, self.budget // 10)\n        \n        # Generate initial Sobol samples with adaptive scaling\n        initial_samples = qmc.scale(sampler.random_base2(m=int(np.log2(num_samples))), lb, ub)\n        \n        best_solution = None\n        best_score = float('inf')\n        \n        # Evaluation counter\n        evaluations = 0\n\n        for sample in initial_samples:\n            # Local optimization using L-BFGS-B\n            result = minimize(func, sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), \n                              options={'maxfun': min(self.budget - evaluations, 100)})\n            \n            evaluations += result.nfev\n            \n            # Update best solution if improved\n            if result.fun < best_score:\n                best_score = result.fun\n                best_solution = result.x\n                # Dynamically adjust bounds around the current best solution\n                lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            \n            # Check if budget is exceeded\n            if evaluations >= self.budget:\n                break\n        \n        return best_solution", "name": "HybridGradientHeuristic", "description": "Hybrid Gradient-Heuristic with dynamic Sobol samples and iterative gradient refinement for enhancing local search accuracy and convergence efficiency.", "configspace": "", "generation": 5, "fitness": 0.7660271297513468, "feedback": "The algorithm HybridGradientHeuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.766 with standard deviation 0.001. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a36545d4-353e-4dcf-8a11-bc8e739d29eb", "metadata": {"aucs": [0.7651435479667249, 0.7651495052382961, 0.7677883360490193], "final_y": [8.915918974151556e-08, 8.915918974151556e-08, 8.915918974151556e-08]}, "mutation_prompt": null}
{"id": "03bd720e-1858-45c4-96cd-6996905fd1dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Rank-based selection\n                idxs = np.random.choice(range(pop_size), 3, p=fitness/fitness.sum(), replace=False)\n                a, b, c = population[idxs]\n                F = np.random.uniform(0.5, 1.2)  # Adaptive mutation factor\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with stochastic rank-based selection and adaptive mutation for improved exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.7603559842117145, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.760 with standard deviation 0.176. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4052cdcb-f59c-4e14-ba9e-3c1eb23e2635", "metadata": {"aucs": [1.0, 0.582310593868885, 0.6987573587662583], "final_y": [0.0, 2.593306920621042e-07, 2.1142421033771534e-07]}, "mutation_prompt": null}
{"id": "5f9edb7b-ee78-483b-b81f-7b57a335b791", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(best_solution + F * (b - c), lb, ub)  # Modified mutation strategy\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced mutation strategy using weighted combination of best and random individuals for improved exploration and exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.663497168025168, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.663 with standard deviation 0.052. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.6146543912596301, 0.7351807986651419, 0.640656314150732], "final_y": [2.421970208033431e-07, 1.0113664310403811e-07, 1.5704204875763614e-07]}, "mutation_prompt": null}
{"id": "853f51a7-c126-4a27-a297-6d42b231f158", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n            pop_size = max(5, int(pop_size * (1 - evaluations / self.budget)))  # Dynamic population size reduction\n            F = np.random.uniform(0.5, 1.0)\n            CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Introduced dynamic population size reduction to enhance convergence efficiency as evaluations approach the budget limit.", "configspace": "", "generation": 6, "fitness": 0.7800901578494172, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.051. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.8179500239602966, 0.8140351532469188, 0.7082852963410362], "final_y": [1.2555875793276375e-08, 3.3178004359915346e-08, 1.2013656454375944e-07]}, "mutation_prompt": null}
{"id": "cf4fa5cd-f0a5-45a5-ab39-dc8ea1ce894e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            improvement = False  # New line for early stopping\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    improvement = True  # Track improvement for early stopping\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size and check for early stopping\n            if not improvement:\n                break  # Stop if no improvement in the current loop\n            pop_size = max(5, (self.budget - evaluations) // 20)\n            F = 0.5 + 0.5 * np.random.rand()  # Dynamic mutation factor adjustment\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with strategic early stopping and dynamic mutation control for improved convergence efficiency.", "configspace": "", "generation": 6, "fitness": 0.7080804732207829, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.708 with standard deviation 0.070. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.6184138111336326, 0.7899620730708681, 0.7158655354578484], "final_y": [2.2942755665822156e-07, 7.960602173650595e-08, 2.320620712968566e-07]}, "mutation_prompt": null}
{"id": "19c0fdea-947f-4d36-af75-b3f8d0fa2668", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)] \n                mutant = np.clip(best_solution + F * (a - b), lb, ub) \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial) * 0.999  # Modified trial fitness for enhanced selection\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced Adaptive DE algorithm with a novel trial solution selection mechanism for improved convergence efficiency.", "configspace": "", "generation": 6, "fitness": 0.7023067987428154, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.702 with standard deviation 0.057. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7695002171231774, 0.6307150315359281, 0.7067051475693408], "final_y": [6.64555805860527e-08, 1.816383709630761e-07, 3.077105630830263e-07]}, "mutation_prompt": null}
{"id": "856090e3-17b0-485b-920a-e6cdf684b785", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)  # Changed: incorporated best_solution in mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using Nelder-Mead\n                        result = minimize(func, best_solution, method='Nelder-Mead', bounds=list(zip(lb, ub)), options={'maxiter': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced convergence by replacing L-BFGS-B with Nelder-Mead for local optimization and replacing CR with dynamic CR.", "configspace": "", "generation": 6, "fitness": 0.7553849732411727, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.755 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7709710028636219, 0.7364674261177608, 0.7587164907421354], "final_y": [1.1781776502547725e-07, 4.1788685613828697e-08, 1.1012337440230475e-07]}, "mutation_prompt": null}
{"id": "229b999e-a2ff-4623-af84-03c4f43ce180", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + F * (a - b) + 0.1 * (best_solution - population[i]), lb, ub)  # Changed: added perturbation to prevent stagnation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced Adaptive DE with fitness-weighted mutation and improved diversity control for better convergence.", "configspace": "", "generation": 6, "fitness": 0.7245354738283041, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.725 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7640126729700951, 0.7410519480285715, 0.6685418004862456], "final_y": [1.3962106371769617e-07, 2.683032161202372e-07, 2.2950722574893876e-07]}, "mutation_prompt": null}
{"id": "3c36fc96-a010-4d75-8913-831702ac238d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 15)  # Changed: Adjusted initial population size\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations - 5})  # Changed: Added a buffer to maxfun\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control and population size\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n                        pop_size = min(pop_size + 1, self.budget // 10)  # Changed: Dynamically adjust population size\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with dynamic control of population size and improved local search for faster convergence.", "configspace": "", "generation": 6, "fitness": 0.8351630947874403, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.097. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.9543112069763298, 0.8352142094386557, 0.7159638679473356], "final_y": [0.0, 5.344972030601372e-10, 1.9090531659076579e-07]}, "mutation_prompt": null}
{"id": "564b997d-e8f9-482f-a3f4-5c04dbeea857", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': (self.budget - evaluations) // 2})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = 0.5 + 0.5 * (best_score / (fitness.mean() + 1e-8))  # Dynamic CR\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced adaptive DE with dynamic CR adjustment and improved local search initiation for faster convergence.", "configspace": "", "generation": 6, "fitness": 0.7807921177357305, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7818965190567531, 0.8041345595340657, 0.756345274616373], "final_y": [6.737616431971521e-08, 4.1866274136012316e-08, 8.290842230197302e-08]}, "mutation_prompt": null}
{"id": "8d23d402-f0de-4318-a6d3-29b4fdcfcff9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                dynamic_F = 0.5 + (0.5 * (1 - fitness[i] / best_score))  # Changed: dynamic mutation factor based on fitness\n                mutant = np.clip(best_solution + dynamic_F * (a - b), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Improved convergence by enhancing mutation with dynamic factor based on best solutions' fitness.", "configspace": "", "generation": 6, "fitness": 0.7838011007403792, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.127. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.9525661317543038, 0.7536262142050429, 0.6452109562617911], "final_y": [0.0, 1.6599394922089253e-07, 5.020330338345373e-08]}, "mutation_prompt": null}
{"id": "5c3ef296-6fee-4434-bb15-9ae29ca5d59d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        if evaluations < self.budget * 0.8:  # Delay local search\n                            result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                            evaluations += result.nfev\n                            if result.fun < best_score:\n                                best_score = result.fun\n                                best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 0.9)  # Refined range\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Refined AdaptiveDELocalSearch with adaptive mutation step size and improved local search initiation for enhanced convergence.", "configspace": "", "generation": 6, "fitness": 0.7021262323842449, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.702 with standard deviation 0.052. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.6286626426573889, 0.7329129172356679, 0.7448031372596777], "final_y": [1.0705956817682678e-08, 1.8609855450843815e-07, 1.6194166419800531e-07]}, "mutation_prompt": null}
{"id": "0f5b7b4f-df54-4f67-b2cd-db6ce924f6c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector with stochasticity\n                trial_fitness = func(trial + np.random.normal(0, 0.01, self.dim))\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n        \n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced Adaptive DE with stochastic local search and diverse initialization strategies for robust convergence.", "configspace": "", "generation": 7, "fitness": 0.7237438976424041, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.724 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.6501054776689597, 0.7216192558763908, 0.7995069593818619], "final_y": [3.8727240744643194e-07, 1.2300955077607562e-07, 5.613813354489768e-08]}, "mutation_prompt": null}
{"id": "04d33016-779b-4432-9d69-9790819d88cb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)  # Changed: incorporated best_solution in mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5 + np.std(fitness) / np.mean(fitness), 1.0)  # Changed: adaptive scaling factor\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Further refined Adaptive DE Local Search by incorporating adaptive scaling factor control based on fitness diversity for enhanced exploration.", "configspace": "", "generation": 7, "fitness": 0.7433799389803178, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.743 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7552903729619524, 0.760291399985369, 0.7145580439936319], "final_y": [3.652929908988111e-08, 1.1487558572148284e-07, 1.2704194846992875e-07]}, "mutation_prompt": null}
{"id": "07701550-74ee-47f9-adca-01d3077f6d19", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n            # Dynamic adjustment based on fitness landscape\n            F = 0.5 + 0.5 * (1 - best_score / max(fitness))  # Modified line\n            CR = 0.7 + 0.3 * (best_score / max(fitness))  # Modified line\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with dynamic mutation factor and crossover rate based on fitness landscape for improved convergence.", "configspace": "", "generation": 7, "fitness": 0.6962850306313609, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.696 with standard deviation 0.084. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.6646056185658015, 0.6128517062537366, 0.8113977670745447], "final_y": [9.70792449819061e-08, 8.583484460297098e-08, 2.9025316834934248e-08]}, "mutation_prompt": null}
{"id": "0cf860b9-5768-4066-81d2-8dcc9cfc24a1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Dynamic local optimization using L-BFGS-B\n                        if evaluations / self.budget < 0.5:  # Change 1: Local search frequency based on convergence speed\n                            result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                            evaluations += result.nfev\n                            if result.fun < best_score:\n                                best_score = result.fun\n                                best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = 1 - (best_score / max(fitness))  # Change 2: Dynamic adaptation of CR based on convergence\n            \n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Introduced a dynamic crossover rate and local search frequency based on convergence speed to enhance exploitation.", "configspace": "", "generation": 7, "fitness": 0.7417901546100442, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.742 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.6848626823290057, 0.7608927036891253, 0.7796150778120015], "final_y": [1.360661317784148e-07, 8.264276629227393e-08, 1.196553373616625e-07]}, "mutation_prompt": null}
{"id": "367106ab-253c-46dc-857e-4720b8c3b339", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n\n                        # Adjusting CR based on population diversity\n                        CR = 0.7 + 0.3 * np.std(population) / (np.std(population) + 1e-10)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch by adjusting crossover rate based on population diversity for improved convergence.", "configspace": "", "generation": 7, "fitness": 0.7295767475647202, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.730 with standard deviation 0.044. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7770952423892954, 0.6714050851622915, 0.7402299151425739], "final_y": [8.775301830816291e-08, 1.9693875730431954e-07, 2.0656649075012465e-07]}, "mutation_prompt": null}
{"id": "7aed04c7-6086-4af8-a67c-0a4b554311c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust control parameters based on feedback\n            if evaluations < self.budget * 0.5:\n                F = np.random.uniform(0.6, 0.9)\n                CR = np.random.uniform(0.8, 1.0)\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced local exploration and convergence control through feedback-based parameter tuning and trial vector differentiation.", "configspace": "", "generation": 7, "fitness": 0.7378948367780046, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.738 with standard deviation 0.049. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.6896924901737875, 0.8048023619021065, 0.7191896582581196], "final_y": [2.1013940252956405e-07, 3.422022338005845e-08, 2.9742071748933165e-07]}, "mutation_prompt": null}
{"id": "e866bfac-f45d-472f-a791-3767de18bedc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F = np.clip(F + np.random.normal(0, 0.1), 0.5, 1.0)  # Adjust mutation factor\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with adaptive mutation factor and crossover rate adjustment for improved exploration and exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.7209698173610292, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.721 with standard deviation 0.042. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7043635323205684, 0.6794818471881903, 0.7790640725743287], "final_y": [3.899920033059948e-08, 7.232282980293123e-08, 8.417354756153621e-08]}, "mutation_prompt": null}
{"id": "5c0a185b-f746-4750-a397-3d9cedee1448", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n        \n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        \n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 0.9)  # Adjusted range for more effective search\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with dynamic mutation factor adjustment based on fitness improvements for better exploration-exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.7551444086652168, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.755 with standard deviation 0.054. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.8281461170007812, 0.7000267668892382, 0.7372603421056312], "final_y": [9.58039164547929e-10, 1.2312632406446985e-07, 1.7823699522561328e-07]}, "mutation_prompt": null}
{"id": "c18010b3-d685-49eb-b644-fcdfaa24a238", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0) * (best_score / trial_fitness)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with improved local exploitation by adjusting the crossover rate dynamically based on fitness improvement.", "configspace": "", "generation": 7, "fitness": 0.750165039475673, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.750 with standard deviation 0.078. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7463499781959554, 0.6572074766988392, 0.8469376635322247], "final_y": [2.0081633403214986e-07, 1.6280205491960105e-07, 2.5820505523613003e-09]}, "mutation_prompt": null}
{"id": "f924f720-4357-48a4-acff-c5fe6fc3edf7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[np.random.choice(pop_size)])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with a diversified trial vector selection to improve exploration and convergence.", "configspace": "", "generation": 7, "fitness": 0.741663418470624, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.742 with standard deviation 0.083. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.6531225795401681, 0.7193696009871415, 0.8524980748845628], "final_y": [2.2502849281486243e-07, 3.713078028546115e-07, 1.2979508703937998e-08]}, "mutation_prompt": null}
{"id": "c39f1a06-e3c7-400c-81dc-14f65e55db50", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + F * (a - b) + 0.1 * (best_score - fitness[i]), lb, ub)  # Change: added best score differential\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with improved mutation by incorporating the best score differential to boost convergence.", "configspace": "", "generation": 8, "fitness": 0.7040709507939636, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.704 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.6318617615442063, 0.6773936232048223, 0.8029574676328626], "final_y": [1.5728755522981936e-07, 5.663657541363221e-07, 2.2653809965249862e-08]}, "mutation_prompt": null}
{"id": "9ab4043d-10d5-4064-a152-3cf76d09368d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)  # Changed: incorporated best_solution in mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, trial, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})  # Changed: use trial for local search\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = 0.6 + 0.4 * (best_score / fitness[i])  # Changed: dynamic CR adjustment\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with gradient-based local search step selection and dynamic CR adjustment for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.7550108214077245, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.755 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7483782976950055, 0.7972136106191139, 0.7194405559090539], "final_y": [1.9035475505590163e-07, 3.7890739369558315e-08, 3.0922693639528664e-07]}, "mutation_prompt": null}
{"id": "91d5b8a6-cb4b-4d8f-90d8-77f157311d18", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)  # Changed: incorporated best_solution in mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, trial + np.random.normal(0, 0.01, self.dim), method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})  # Added perturbation\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced Adaptive DE with strategic perturbation for improved local search capabilities.", "configspace": "", "generation": 8, "fitness": 0.7525735516465265, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.753 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7273629185633218, 0.772439521829786, 0.7579182145464715], "final_y": [1.3461882160296163e-07, 1.390606594193158e-07, 6.229215798093079e-08]}, "mutation_prompt": null}
{"id": "306978e3-259f-4e45-8e54-66be9c425495", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n            \n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n            \n            # Adjust F based on fitness variance\n            F = 0.5 + 0.5 * np.var(fitness) / (np.var(fitness) + 1e-6)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch by adjusting the mutation strategy factor dynamically based on fitness variance for better exploitation.", "configspace": "", "generation": 8, "fitness": 0.6148682990027489, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.615 with standard deviation 0.177. And the mean value of best solutions found was 0.002 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7432015675393374, 0.7361141012480077, 0.3652892282209015], "final_y": [1.3665423199292523e-07, 3.099641113146035e-07, 0.006045612403042241]}, "mutation_prompt": null}
{"id": "8fc656af-74b9-43d9-8c36-7b865704ad60", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                # Change: Weighted combination of best_solution and a\n                mutant = np.clip(0.5 * best_solution + 0.5 * a + F * (a - b), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Improved mutation strategy using weighted combination of best and random individuals for better diversity.", "configspace": "", "generation": 8, "fitness": 0.7536416451879665, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.754 with standard deviation 0.043. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7294690433459581, 0.8140893968426, 0.7173664953753411], "final_y": [3.6104096215366406e-07, 3.3178004359915346e-08, 3.443147903097675e-07]}, "mutation_prompt": null}
{"id": "2e30c706-e1ad-45b5-b8ec-35680b22cfa9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 15)  # Changed: adjusted population size calculation\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(population[i] + F * (best_solution - a + b - population[i]), lb, ub)  # Changed: new mutation strategy\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control based on learning\n                        F = 0.9 * F + 0.1 * np.random.uniform(0.5, 1.0)  # Changed: learning-based F update\n                        CR = 0.9 * CR + 0.1 * np.random.uniform(0.7, 1.0)  # Changed: learning-based CR update\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced Adaptive DE with learning-based parameter tuning and improved local exploitation for better convergence.", "configspace": "", "generation": 8, "fitness": 0.7227304708652023, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.723 with standard deviation 0.055. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7379780320800838, 0.6485789747244233, 0.7816344057910996], "final_y": [1.74349851469463e-07, 7.242936614109169e-08, 5.5792730746787526e-08]}, "mutation_prompt": null}
{"id": "2f6eb02d-9ac9-46cc-82b0-8327b1d80f76", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n        restarts = 0\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations - restarts})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 0.95)  # slightly more conservative upper bound\n                        CR = np.random.uniform(0.6, 1.0)  # widened range for broader exploration\n\n            # Restart mechanism\n            if evaluations % (self.budget // 4) == 0:\n                restarts += 1\n                population = np.random.uniform(lb, ub, (pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                evaluations += pop_size\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced Adaptive Differential Evolution with Restart Mechanism and Adaptive Mutation Strategy for Improved Convergence.", "configspace": "", "generation": 8, "fitness": 0.7005266323333464, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.701 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7343325009843665, 0.6959053017517546, 0.6713420942639181], "final_y": [9.40246425680771e-08, 2.9205869589535333e-07, 1.4441281480197773e-07]}, "mutation_prompt": null}
{"id": "41282e3a-543e-4bfb-9c74-c6222dab5749", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n        success_rates = np.zeros(pop_size)\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n                    success_rates[i] += 1\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n            # Adaptive parameter control based on success rates\n            F = 0.5 + 0.5 * (success_rates.mean() / (success_rates.mean() + 1))\n            CR = 0.7 + 0.3 * (success_rates.mean() / (success_rates.mean() + 1))\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with improved adaptive parameter tuning using local success rates for better convergence.", "configspace": "", "generation": 8, "fitness": 0.6882141086053067, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.688 with standard deviation 0.039. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.6340836083506817, 0.7250762374170936, 0.7054824800481448], "final_y": [1.824168672717097e-07, 1.9311067032244846e-07, 9.892926046410825e-08]}, "mutation_prompt": null}
{"id": "98289f2c-d892-43ba-a925-cd67372929a6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.std(population)  # Change made here\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Incorporate dynamic crossover rate adjustment based on the diversity of the population for enhanced exploration-exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.7564839518142813, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.756 with standard deviation 0.055. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.6942594723462279, 0.7480453852027897, 0.827146997893826], "final_y": [5.88010002314354e-08, 2.683032161202372e-07, 1.7940454980735126e-08]}, "mutation_prompt": null}
{"id": "80564eae-29eb-4d12-aa1c-492d4010fc55", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                    # Adaptive parameter control refined\n                    F = F + 0.1 * np.abs(fitness[i] - trial_fitness)\n                    CR = CR - 0.1 * np.abs(fitness[i] - trial_fitness)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Introducing adaptive F and CR updates based on individual and global improvements for more effective convergence.", "configspace": "", "generation": 8, "fitness": 0.7684084114529682, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.768 with standard deviation 0.064. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7329160115875601, 0.7145068876615037, 0.857802335109841], "final_y": [1.233150297084286e-07, 3.140222126565304e-07, 1.2979508703937998e-08]}, "mutation_prompt": null}
{"id": "cbe7815e-4609-4793-864d-c5a51aa754c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)  # Changed: incorporated best_solution in mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0) * (1 - np.mean(fitness) / np.max(fitness))  # Changed: memory-based crossover rate adjustment\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced convergence using an adaptive crossover rate with memory-based adjustments.", "configspace": "", "generation": 9, "fitness": 0.7685477898654524, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.058. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7155287192435071, 0.7410190441515969, 0.849095606201253], "final_y": [3.8538330097865433e-07, 2.646289521210991e-08, 9.41315401873865e-09]}, "mutation_prompt": null}
{"id": "dfec8948-a3f6-47a9-ac7f-049330601e44", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]  # Changed: used 3 individuals\n                mutant = np.clip(a + F * (b - c), lb, ub)  # Changed: mutant vector strategy\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                # New adaptive parameter control based on population diversity\n                diversity = np.std(population, axis=0).mean()\n                F = np.clip(F + np.random.normal(0, 0.1) * (1 - diversity), 0.5, 1.0)  # Changed: adaptive F\n                CR = np.clip(CR + np.random.normal(0, 0.1) * diversity, 0.7, 1.0)  # Changed: adaptive CR\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced Adaptive DE with adaptive scaling factor and crossover rates based on diversity metrics for improved convergence and exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.7692643881128737, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.055. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.846703256182155, 0.7232730831987103, 0.7378168249577557], "final_y": [1.254275824071967e-08, 1.2431438970364532e-07, 1.3531866096351102e-07]}, "mutation_prompt": null}
{"id": "f9618652-253f-4104-82f6-ce97066f8a10", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Stochastic ranking: randomly decide if trial replaces based on relative improvement\n                if trial_fitness < fitness[i] or np.random.rand() < 0.1:  # Changed: stochastic ranking condition\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with stochastic ranking for improved selection pressure control.", "configspace": "", "generation": 9, "fitness": 0.7642471418592311, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.082. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.6549662578344111, 0.7854102267853368, 0.8523649409579452], "final_y": [1.2859797122969868e-07, 8.330032707545149e-08, 7.989311630216065e-09]}, "mutation_prompt": null}
{"id": "edd72792-4373-4a54-844f-637565355d9c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b) + F * (best_solution - population[i]), lb, ub)  # Changed: added global best-guided mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch by introducing a global best-guided mutation strategy for improved convergence.", "configspace": "", "generation": 9, "fitness": 0.7725283125419597, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.046. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7399901754825552, 0.7406700709070657, 0.836924691236258], "final_y": [1.2990137937747453e-07, 1.6161100036797446e-07, 6.493544290121758e-09]}, "mutation_prompt": null}
{"id": "400be6c4-1263-49d5-96d6-711a3c7559ba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations, 'ftol': 1e-10})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with improved local search precision by reducing `tol` in L-BFGS-B for better convergence.", "configspace": "", "generation": 9, "fitness": 0.7393643921257614, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.739 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7390344354468033, 0.7175542820575691, 0.761504458872912], "final_y": [3.4058212658412515e-08, 2.0011584079935438e-07, 1.747318170310759e-08]}, "mutation_prompt": null}
{"id": "00cc6422-0bd8-447e-b548-94fc31dea4a1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n            # Random restarts for global exploration\n            if evaluations < self.budget and np.random.rand() < 0.1:\n                population = np.random.uniform(lb, ub, (pop_size, self.dim))\n                fitness = np.array([func(ind) for ind in population])\n                evaluations += pop_size\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with integrated random restarts for escaping local optima and improved global exploration.", "configspace": "", "generation": 9, "fitness": 0.7500858439744597, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.750 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7111977753219141, 0.7707173063058208, 0.7683424502956441], "final_y": [3.408057270954302e-07, 3.883973314573657e-08, 1.0945292013826784e-07]}, "mutation_prompt": null}
{"id": "f7feaf1e-963e-46ad-bc45-0e9787f4f749", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                F_adaptive = np.random.uniform(0.5, 1.2)  # Adaptive mutation factor\n                mutant = np.clip(a + F_adaptive * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Improved AdaptiveDELocalSearch by adding adaptive mutation factor and trial selection for better exploration and convergence.", "configspace": "", "generation": 9, "fitness": 0.7552839078174017, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.755 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7518709663270406, 0.7648709772494635, 0.7491097798757014], "final_y": [5.5103870763876394e-08, 5.617320401570143e-08, 1.2113767404296404e-07]}, "mutation_prompt": null}
{"id": "779187ac-d799-47a6-922f-94237f580e35", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 0.9)\n                        CR = np.random.uniform(0.6, 0.9)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with dynamic F and CR adjustment for refined convergence in smooth landscapes.", "configspace": "", "generation": 9, "fitness": 0.7809490990660987, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.8114469446256583, 0.7873307261951207, 0.7440696263775173], "final_y": [3.684346267994638e-08, 1.0646702680479391e-07, 1.8469956479848955e-07]}, "mutation_prompt": null}
{"id": "c2bafc3e-df18-40ae-83df-e9ef53726bbd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                # Changed line: Enhanced mutation strategy\n                mutant = np.clip(best_solution + F * (b - c) + np.random.normal(0, 0.1, self.dim), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced mutation strategy by using best individual and adding noise for better exploration.", "configspace": "", "generation": 9, "fitness": 0.794865231258318, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.034. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.8214040221754291, 0.8158669249988375, 0.7473247466006874], "final_y": [2.3774844553056884e-08, 3.819483383795544e-08, 1.8021760693906703e-07]}, "mutation_prompt": null}
{"id": "e56129a9-f708-4a95-8152-c8935b074708", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = np.random.uniform(0.5, 1.0)  # Changed line for stochastic tuning\n        CR = np.random.uniform(0.7, 1.0)  # Changed line for stochastic tuning\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Introduced stochastic tuning for F and CR factors in Differential Evolution to enhance adaptability and convergence. ", "configspace": "", "generation": 9, "fitness": 0.8179215819307476, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.034. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.8373451469982534, 0.8458314827892699, 0.7705881160047192], "final_y": [7.2214133765978545e-09, 1.4541286791976981e-08, 1.40161954353097e-07]}, "mutation_prompt": null}
{"id": "ead72515-ad36-454a-b678-3285db985700", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]  # Changed: used 3 individuals\n                mutant = np.clip(best_solution + F * (a - b + c - best_solution), lb, ub)  # Changed: incorporated additional term for targeted mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Refined AdaptiveDELocalSearch by incorporating best individual's difference for a more targeted mutation strategy.", "configspace": "", "generation": 10, "fitness": 0.7466400339685073, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.747 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.6644815517290499, 0.8085544615267376, 0.7668840886497345], "final_y": [1.1773483189305948e-07, 1.0477577840654467e-09, 2.142686810635337e-09]}, "mutation_prompt": null}
{"id": "90ee9642-b0c1-4a9b-b2f7-4521ab81aaf9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        temperature = 1.0  # Added: Initial temperature for adaptive acceptance\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)  # Changed: incorporated best_solution in mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i] or np.exp((fitness[i] - trial_fitness) / temperature) > np.random.rand():  # Modified: Added temperature-based acceptance\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Added a temperature-based adaptive acceptance mechanism to enhance exploration in the existing DE local search.", "configspace": "", "generation": 10, "fitness": 0.7667524271954438, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.767 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7309064261672997, 0.7650887383214307, 0.804262117097601], "final_y": [9.629195752234885e-08, 1.5521865404160446e-08, 4.265116557847489e-09]}, "mutation_prompt": null}
{"id": "8f65db5a-82dd-42da-b2c6-3bbfd4d06b87", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n            F = 0.5 + 0.5 * (best_score / fitness.mean())  # New line 1\n            CR = 0.8 + 0.2 * (1 - (best_score / fitness.min()))  # New line 2\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with dynamic F and CR adjustment based on fitness improvement for better exploration-exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.7992992251426848, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.006. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7940877956260735, 0.7960190656839812, 0.8077908141179999], "final_y": [8.046438965278638e-09, 3.0694739540544965e-08, 2.5599122935043595e-09]}, "mutation_prompt": null}
{"id": "e4caf273-e673-4fd7-b684-3f96992362d5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass QuantumInspiredDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        # Quantum-inspired initialization\n        population = np.random.uniform(0, 1, (pop_size, self.dim))\n        fitness = np.array([func(self.decode(ind, lb, ub)) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), 0, 1)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Decode and evaluate trial vector\n                trial_decoded = self.decode(trial, lb, ub)\n                trial_fitness = func(trial_decoded)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, self.decode(best_solution, lb, ub), method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = self.encode(result.x, lb, ub)\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return self.decode(best_solution, lb, ub)\n\n    def decode(self, individual, lb, ub):\n        return lb + individual * (ub - lb)\n\n    def encode(self, solution, lb, ub):\n        return (solution - lb) / (ub - lb)", "name": "QuantumInspiredDE", "description": "Quantum-Inspired Differential Evolution (QIDE) combines quantum-inspired superposition states with differential evolution for enhanced exploration and exploitation.", "configspace": "", "generation": 10, "fitness": 0.728601662270311, "feedback": "The algorithm QuantumInspiredDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.729 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7201254371745875, 0.7426822514438772, 0.7229972981924685], "final_y": [1.224015201536366e-07, 6.712293132686361e-08, 6.893282243849424e-08]}, "mutation_prompt": null}
{"id": "dec8cf69-0b9f-4fbb-a636-e7f1a63d08ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n\n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + 0.9 * (a - b), lb, ub)  # Changed: Increased weight from F to 0.9\n                cross_points = np.random.rand(self.dim) < 0.85  # Changed: Updated CR to 0.85 for enhanced crossover\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Refined Adaptive DE with enhanced crossover and trial vector strategies for improved convergence and diversity.", "configspace": "", "generation": 10, "fitness": 0.7916563471936443, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.035. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.745443383028953, 0.7979281563936469, 0.831597502158333], "final_y": [8.404051320997016e-08, 2.161424557871102e-08, 2.683416061919416e-09]}, "mutation_prompt": null}
{"id": "f416757a-8129-42c2-aba8-60ea46feb986", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.5, 1.0) * (1 - np.std(fitness) / (np.mean(fitness) + 1e-9))\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch by dynamically adjusting crossover rate based on population diversity for improved convergence.", "configspace": "", "generation": 10, "fitness": 0.7572044681680542, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.757 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7437789528198533, 0.7782511731777635, 0.7495832785065456], "final_y": [2.668763191789742e-07, 1.4153172283908493e-09, 6.794670242421784e-10]}, "mutation_prompt": null}
{"id": "8a47b9c7-a712-4082-b9cd-3281d21ebdd2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]  # Changed: used 2 individuals\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)  # Changed: incorporated best_solution in mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0) * (best_score / fitness.mean())  # Updated: F now adapts based on historical performance\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced convergence by introducing a self-adaptive mutation factor influenced by historical performance.", "configspace": "", "generation": 10, "fitness": 0.7995465890527845, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.7633474257387721, 0.8241799284500206, 0.8111124129695607], "final_y": [3.001283609379992e-08, 2.9695159064962355e-09, 5.982234606172263e-09]}, "mutation_prompt": null}
{"id": "3a3ade26-4812-4a5f-a3df-217b2fbba2c8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(best_solution + F * (a - b), lb, ub)\n                diversity = np.std(population, axis=0).mean()  # Calculate population diversity\n                CR = 0.9 - 0.5 * diversity  # Adjusted: adaptive crossover rate based on diversity\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = np.random.uniform(0.5, 1.0)\n                        CR = np.random.uniform(0.7, 1.0)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with adaptive crossover rate adjusted based on population diversity for better performance.", "configspace": "", "generation": 10, "fitness": 0.792252079868001, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ee81384d-dc42-4231-85a1-75fcd8852bb2", "metadata": {"aucs": [0.781998255662379, 0.7822767923892625, 0.8124811915523613], "final_y": [7.024863844355899e-08, 1.9202533696334093e-08, 1.6242141365594047e-08]}, "mutation_prompt": null}
{"id": "e38035ab-da68-4808-8b66-10bdbbcc99f5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        F = 0.5 + 0.5 * (1 - (trial_fitness / best_score))  # Changed line\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Introduced adaptive mutation factor based on the best solution's fitness improvement to enhance convergence.", "configspace": "", "generation": 10, "fitness": 0.7689462488185681, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.001. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7692725619619746, 0.7679526137202839, 0.7696135707734462], "final_y": [7.18662062418139e-08, 1.204954305652039e-07, 7.197272767105908e-08]}, "mutation_prompt": null}
{"id": "d515f5fa-37c1-4174-93ca-ac7a48452c09", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        pop_size = max(5, self.budget // 20)\n        F = 0.8\n        CR = 0.9\n        population = np.random.uniform(lb, ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        evaluations = pop_size\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        best_score = fitness[best_idx]\n\n        while evaluations < self.budget:\n            for i in range(pop_size):\n                if evaluations >= self.budget:\n                    break\n                \n                # Mutation and crossover\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation of trial vector\n                trial_fitness = func(trial)\n                evaluations += 1\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                    # Update best solution if improved\n                    if trial_fitness < best_score:\n                        best_solution = trial\n                        best_score = trial_fitness\n\n                        # Local optimization using L-BFGS-B\n                        result = minimize(func, best_solution, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxfun': self.budget - evaluations})\n                        evaluations += result.nfev\n                        if result.fun < best_score:\n                            best_score = result.fun\n                            best_solution = result.x\n\n                        # Adaptive parameter control\n                        population_std = np.std(population, axis=0).mean()  # Added line\n                        F = np.clip(population_std / 2, 0.5, 1.0)  # Modified line\n                        CR = np.random.uniform(0.7, 1.0)\n\n            # Dynamically adjust population size based on evaluations\n            pop_size = max(5, (self.budget - evaluations) // 20)\n\n        return best_solution", "name": "AdaptiveDELocalSearch", "description": "Enhanced AdaptiveDELocalSearch with adaptive scaling factor adjustment based on population diversity for refined exploration.", "configspace": "", "generation": 10, "fitness": 0.7731280036515514, "feedback": "The algorithm AdaptiveDELocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1e70eeb0-a5fc-4317-8535-e6631b9b8936", "metadata": {"aucs": [0.7315948288020455, 0.7892729216517739, 0.7985162605008347], "final_y": [2.5798392632348873e-08, 1.948325761301971e-09, 1.2055681149899727e-08]}, "mutation_prompt": null}
