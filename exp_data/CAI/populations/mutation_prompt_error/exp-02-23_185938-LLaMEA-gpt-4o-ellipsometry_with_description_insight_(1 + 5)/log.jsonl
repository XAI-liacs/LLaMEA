{"id": "d39f9ea8-e002-4e6d-bb8c-cedce3a616db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "The algorithm combines uniform sampling for initial exploration and a fast-converging local optimization technique (BFGS) for exploitation, iteratively refining parameter bounds based on previous estimates to efficiently solve smooth, low-dimensional optimization problems within a limited budget.", "configspace": "", "generation": 0, "fitness": 0.7609071282076899, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.761 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7851839967799252, 0.7476851965029507, 0.7498521913401939], "final_y": [4.5367636303777576e-08, 1.136733895323998e-07, 8.384260042802179e-08]}, "mutation_prompt": null}
{"id": "a7a88c61-5cce-4ec2-b3ea-b80153100815", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\nfrom pyswarm import pso\n\nclass SobolPSOOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples with Sobol sequence\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Generate Sobol sequence samples within bounds\n        sobol_sampler = Sobol(d=self.dim, seed=0)\n        sobol_samples = sobol_sampler.random_base2(m=int(np.log2(initial_sample_count)))\n        \n        # Scale Sobol samples to fit within bounds\n        scaled_samples = []\n        for sample in sobol_samples:\n            scaled_sample = [\n                lb + sample[i] * (ub - lb) for i, (lb, ub) in enumerate(zip(func.bounds.lb, func.bounds.ub))\n            ]\n            scaled_samples.append(scaled_sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in scaled_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Define the objective function for PSO\n        def objective(x):\n            return func(x)\n\n        # Use Particle Swarm Optimization (PSO) from pyswarm within bounds\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        xopt, fopt = pso(objective, lb, ub, swarmsize=min(20, self.budget), maxiter=self.budget//10)\n\n        return xopt", "name": "SobolPSOOptimizer", "description": "The algorithm employs quasi-random sampling using Sobol sequences for diverse initial exploration, followed by particle swarm optimization (PSO) to efficiently converge to optimal solutions in smooth, low-dimensional spaces under constrained evaluation budgets.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'pyswarm'\").", "error": "ModuleNotFoundError(\"No module named 'pyswarm'\")", "parent_id": "d39f9ea8-e002-4e6d-bb8c-cedce3a616db", "metadata": {}, "mutation_prompt": null}
{"id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhancing the local search phase by adding a stopping criterion based on the improvement threshold to better utilize the limited budget.", "configspace": "", "generation": 1, "fitness": 0.7928227022322515, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.147. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d39f9ea8-e002-4e6d-bb8c-cedce3a616db", "metadata": {"aucs": [1.0, 0.6850696166652008, 0.6933984900315535], "final_y": [0.0, 8.394255648206208e-07, 6.464329322247618e-07]}, "mutation_prompt": null}
{"id": "a39db635-feb8-4c59-bbde-a881150ad500", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds using Sobol sequences\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random(n=initial_sample_count)\n        initial_samples = initial_samples * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "The algorithm enhances exploration by using Sobol sequences for low-discrepancy initial sampling and exploits smooth landscapes with fast-converging local BFGS optimization.", "configspace": "", "generation": 1, "fitness": 0.6665787895920628, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.667 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d39f9ea8-e002-4e6d-bb8c-cedce3a616db", "metadata": {"aucs": [0.7461308571399471, 0.6548708602774527, 0.5987346513587888], "final_y": [1.1358957697653839e-07, 1.585005848242777e-06, 7.509903381691017e-06]}, "mutation_prompt": null}
{"id": "2e1d1fe8-37f4-41a1-a80a-0fbfcbf6efd8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RefinedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Adaptive sampling: refine bounds around the best sample\n        reduction_factor = 0.1\n        bounds = [(max(lb, x - reduction_factor * (ub - lb)), min(ub, x + reduction_factor * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Dual local optimization: BFGS followed by Nelder-Mead\n        res_bfgs = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget // 2})\n        self.budget -= res_bfgs.nfev\n\n        if self.budget > 0:\n            res_nelder = minimize(objective, x0=res_bfgs.x, method='Nelder-Mead', options={'maxfev': self.budget})\n            self.budget -= res_nelder.nfev\n            final_result = res_nelder.x\n        else:\n            final_result = res_bfgs.x\n\n        return final_result", "name": "RefinedMetaheuristicOptimizer", "description": "Introduce adaptive sampling to dynamically focus search in promising regions, followed by dual local optimizers (BFGS and Nelder-Mead) to balance speed and robustness, iteratively adjusting bounds for refined exploration within a limited budget.", "configspace": "", "generation": 1, "fitness": 0.7343532461810711, "feedback": "The algorithm RefinedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.734 with standard deviation 0.182. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d39f9ea8-e002-4e6d-bb8c-cedce3a616db", "metadata": {"aucs": [0.9905466969531717, 0.5877992893727331, 0.6247137522173087], "final_y": [0.0, 7.227279087195617e-06, 3.920414011405774e-06]}, "mutation_prompt": null}
{"id": "b6734d4a-38d0-4378-8c66-afc534cfffff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize an adaptive sample size\n        initial_sample_count = 10\n        \n        # Function to dynamically adjust the sample size\n        def dynamic_sample_size(remaining_budget):\n            return max(10, remaining_budget // 10)\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Dynamically expand initial exploration based on variance reduction\n        while self.budget > self.budget // 2:\n            new_sample_count = dynamic_sample_size(self.budget)\n            for _ in range(new_sample_count):\n                sample = np.array([\n                    np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n                ])\n                value = func(sample)\n                self.budget -= 1\n                if value < best_value:\n                    best_value = value\n                    best_sample = sample\n                if self.budget <= 0:\n                    return best_sample\n\n        # Memory-based approach to refine bounds\n        history = [best_sample]\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for local optimizers\n        def objective(x):\n            return func(x)\n\n        # Alternate between Nelder-Mead and BFGS for local optimization\n        for _ in range(2):\n            res = minimize(objective, x0=best_sample, method='Nelder-Mead', options={'maxfev': self.budget // 2})\n            best_sample = res.x\n            history.append(best_sample)\n            self.budget -= res.nfev\n\n            if self.budget <= 0:\n                return best_sample\n\n            res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget})\n            best_sample = res.x\n            history.append(best_sample)\n            self.budget -= res.nfev\n\n            if self.budget <= 0:\n                return best_sample\n\n            # Adaptive bounds refinement based on memory\n            bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                      for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        return best_sample", "name": "EnhancedMetaheuristicOptimizer", "description": "The algorithm enhances the initial exploration by expanding the sample size dynamically based on variance reduction, then utilizes a blend of Nelder-Mead and BFGS methods for efficient exploitation, leveraging a memory-based approach to adaptively refine the search space boundaries for improved convergence in smooth, low-dimensional landscapes.", "configspace": "", "generation": 1, "fitness": 0.48232760475056313, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.482 with standard deviation 0.361. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.139.", "error": "", "parent_id": "d39f9ea8-e002-4e6d-bb8c-cedce3a616db", "metadata": {"aucs": [0.9908440169232505, 0.2645066094461055, 0.1916321878823336], "final_y": [0.0, 0.11249298900446915, 0.3356527095227608]}, "mutation_prompt": null}
{"id": "da21eb84-2b4a-48b8-ac07-1d950bcb4857", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Introduce adaptive narrowing of bounds\n        factor = 0.05 + 0.05 * (np.random.rand())  # New sampling strategy\n        bounds = [(max(lb, x - factor * (ub - lb)), min(ub, x + factor * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce adaptive sampling based on previous evaluations to refine initial guesses and enhance convergence.", "configspace": "", "generation": 2, "fitness": 0.5386066199893041, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.539 with standard deviation 0.222. And the mean value of best solutions found was 0.050 (0. is the best) with standard deviation 0.071.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.6752858243540998, 0.7150932061007345, 0.2254408295130781], "final_y": [1.0029841960560833e-06, 2.2689988959885036e-07, 0.15111349675808827]}, "mutation_prompt": null}
{"id": "104e610d-3911-4e30-8473-d565d223c2bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample with adaptive adjustment\n        adaptive_factor = 0.05  # Adjusted factor\n        bounds = [(max(lb, x - adaptive_factor * (ub - lb)), min(ub, x + adaptive_factor * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce an adaptive adjustment to the bounds narrowing factor for improved budget efficiency.", "configspace": "", "generation": 2, "fitness": 0.5202846989329245, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.520 with standard deviation 0.202. And the mean value of best solutions found was 0.053 (0. is the best) with standard deviation 0.075.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.23519712356503664, 0.6793578050563349, 0.6462991681774021], "final_y": [0.15869308752056138, 2.895638222340281e-07, 7.20154779677508e-07]}, "mutation_prompt": null}
{"id": "cd2c9175-1455-42ef-86be-095f3394357f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, int(0.2 * self.budget))\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        refinement_factor = 0.05 + 0.1 * (best_value / initial_sample_count)\n        bounds = [(max(lb, x - refinement_factor * (ub - lb)), \n                   min(ub, x + refinement_factor * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrating adaptive sampling and dynamic bounds refinement to enhance local search efficiency and convergence speed.", "configspace": "", "generation": 2, "fitness": 0.6183733466942959, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.618 with standard deviation 0.290. And the mean value of best solutions found was 0.050 (0. is the best) with standard deviation 0.071.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.9145860044690752, 0.7150932061007345, 0.2254408295130781], "final_y": [0.0, 2.2689988959885036e-07, 0.15111349675808827]}, "mutation_prompt": null}
{"id": "e0d84c9d-5ad8-491d-851e-c22e1dd3462e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(5, int(0.15 * self.budget))  # Changed line\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Refining the initial sample count computation to ensure a more balanced exploration-exploitation trade-off.", "configspace": "", "generation": 2, "fitness": 0.5154034113163406, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.515 with standard deviation 0.194. And the mean value of best solutions found was 0.031 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.620461043479321, 0.6821006018444107, 0.24364858862529015], "final_y": [1.9188423604916197e-06, 3.474194903783092e-07, 0.09358435849017419]}, "mutation_prompt": null}
{"id": "4f7389d1-2ade-4efa-87f3-a5a13b8cb39c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.05 * (ub - lb)), min(ub, x + 0.05 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Introducing adaptive bounds adjustment to focus local search on promising regions and better exploit smooth cost landscapes.", "configspace": "", "generation": 2, "fitness": 0.6403146380115415, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.640 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.6059933841622918, 0.6624289432614419, 0.6525215866108909], "final_y": [2.605352132595941e-06, 5.502857798049804e-07, 4.280680694164025e-07]}, "mutation_prompt": null}
{"id": "802d8ffd-4933-4f57-9aec-c9425ad6b1ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, min(self.budget // 10, 30))  # Changed from max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Incorporate adaptive sample size for initial sampling to better explore the parameter space given the budget.", "configspace": "", "generation": 3, "fitness": 0.5027510776759901, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.503 with standard deviation 0.243. And the mean value of best solutions found was 0.381 (0. is the best) with standard deviation 0.540.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.1584966172694776, 0.6769062553009473, 0.6728503604575454], "final_y": [1.144486705187225, 7.932848840217571e-07, 8.591959912550062e-07]}, "mutation_prompt": null}
{"id": "3a12a900-9d0f-4756-807a-c12bad61f0a9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        # Adaptive bounds tightening based on objective value\n        if res.fun < best_value:\n            bounds = [(max(lb, x - 0.05 * (ub - lb)), min(ub, x + 0.05 * (ub - lb)))\n                      for x, lb, ub in zip(res.x, func.bounds.lb, func.bounds.ub)]\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Incorporate adaptive bounds tightening based on objective value to enhance local search exploitation efficiency.", "configspace": "", "generation": 3, "fitness": 0.7866924510379524, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.132. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.9727276401312105, 0.6852786840172532, 0.7020710289653934], "final_y": [0.0, 9.091027801791762e-07, 4.907521287444681e-07]}, "mutation_prompt": null}
{"id": "24120848-e655-41f9-9348-b1f8a5c55932", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 15)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce a dynamic adjustment to the initial sample size based on remaining budget to better balance exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.48746884133671137, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.487 with standard deviation 0.250. And the mean value of best solutions found was 0.613 (0. is the best) with standard deviation 0.868.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.6513174151854327, 0.6773263529435822, 0.13376275588111908], "final_y": [1.072812843072164e-06, 9.503370350175278e-07, 1.840469985151271]}, "mutation_prompt": null}
{"id": "dccf26a7-143f-4a57-8cc0-bdcb040faa2d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, int(self.budget / (self.dim + 1)))  # Adjusted line\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Incorporating a dynamic adjustment mechanism for the initial sampling size based on remaining budget to improve sample quality.", "configspace": "", "generation": 3, "fitness": 0.700357573730809, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.700 with standard deviation 0.037. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.7520356832664989, 0.6767568150906142, 0.6722802228353137], "final_y": [3.4325189500260526e-07, 6.880228273457976e-07, 7.162753125866088e-07]}, "mutation_prompt": null}
{"id": "b6081ba9-41db-49e5-ac43-97def25ebc7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Adjusted to dynamically scale with budget\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local search by dynamically adjusting initial sample size based on remaining budget.", "configspace": "", "generation": 3, "fitness": 0.6528701587012741, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.653 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.6293701568820986, 0.6509132311376524, 0.6783270880840713], "final_y": [1.7549387412413584e-06, 2.0160373124643287e-06, 7.166649877913477e-07]}, "mutation_prompt": null}
{"id": "8bcf3710-73af-4f9e-9ddf-d9e97786d91e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        scale_factor = 0.05  # Changed line: reduced the scale factor for narrowing bounds\n        bounds = [(max(lb, x - scale_factor * (ub - lb)), min(ub, x + scale_factor * (ub - lb)))  # Changed line\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Incorporating an adaptive sampling strategy to refine search space convergence within budget constraints.", "configspace": "", "generation": 4, "fitness": 0.5194548024453441, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.519 with standard deviation 0.249. And the mean value of best solutions found was 0.278 (0. is the best) with standard deviation 0.394.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.6926403969069785, 0.6989962963008759, 0.1667277141281781], "final_y": [5.900313531410305e-07, 4.997332807223092e-07, 0.8350821966521177]}, "mutation_prompt": null}
{"id": "980ebc84-6a25-4b48-be04-a57d07adac01", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization with adaptive step size\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-8})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Utilize an adaptive step size mechanism to dynamically adjust the search space during the local optimization phase for improved convergence.", "configspace": "", "generation": 4, "fitness": 0.5338784785498921, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.534 with standard deviation 0.273. And the mean value of best solutions found was 0.445 (0. is the best) with standard deviation 0.629.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.7196278598033803, 0.7346999873616444, 0.14730758848465164], "final_y": [2.7325012539118027e-07, 3.917569179960264e-07, 1.3344707414339612]}, "mutation_prompt": null}
{"id": "10dc7611-d36a-4b85-a719-a2180b6533e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-8})\n\n        return res.x", "name": "NovelMetaheuristicOptimizer", "description": "Incorporating a stopping criterion based on the minimum change in the objective function to efficiently utilize the budget.", "configspace": "", "generation": 4, "fitness": 0.5556666552469117, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.556 with standard deviation 0.151. And the mean value of best solutions found was 0.002 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.7244355429515379, 0.5851182911296953, 0.357446131659502], "final_y": [1.6797280789893723e-07, 1.3710679580157321e-05, 0.006441055535474095]}, "mutation_prompt": null}
{"id": "060affdd-c82e-40b7-a1d6-fc106845c035", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveBoundaryMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = np.array([\n            [np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n            for _ in range(initial_sample_count)\n        ])\n\n        # Evaluate initial samples and find the best one\n        sample_values = np.array([func(sample) for sample in initial_samples])\n        self.budget -= initial_sample_count\n        best_value_index = np.argmin(sample_values)\n        best_sample = initial_samples[best_value_index]\n\n        if self.budget <= 0:\n            return best_sample\n\n        # Calculate sample spread to adaptively scale bounds\n        sample_spread = np.std(initial_samples, axis=0)\n        adaptive_scaling = 0.1 + 0.5 * sample_spread / (np.array(func.bounds.ub) - np.array(func.bounds.lb))\n        \n        # Narrow bounds adaptively around the best initial sample\n        bounds = [(max(lb, best_sample[i] - adaptive_scaling[i] * (ub - lb)), \n                   min(ub, best_sample[i] + adaptive_scaling[i] * (ub - lb)))\n                  for i, (lb, ub) in enumerate(zip(func.bounds.lb, func.bounds.ub))]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization within the new adaptive bounds\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-6})\n\n        return res.x", "name": "AdaptiveBoundaryMetaheuristicOptimizer", "description": "Introduce adaptive boundary scaling based on initial sample spread to enhance exploration before local optimization.", "configspace": "", "generation": 4, "fitness": 0.6946972587663177, "feedback": "The algorithm AdaptiveBoundaryMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.681728192194171, 0.7149298349153328, 0.6874337491894491], "final_y": [7.010595141271482e-07, 3.3726693530581293e-07, 5.388768474615629e-07]}, "mutation_prompt": null}
{"id": "1156b459-960c-408c-8d56-73034c8c949f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Integrating adaptive sampling with local search and dynamic budget allocation to improve convergence in smooth landscapes.", "configspace": "", "generation": 4, "fitness": 0.8061494922501207, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.077. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e0e94da2-a640-49e1-becf-44b8b09d842a", "metadata": {"aucs": [0.9145860044690752, 0.7628020626280951, 0.7410604096531916], "final_y": [0.0, 9.392923381717664e-08, 5.341785760823254e-08]}, "mutation_prompt": null}
{"id": "464352cc-9dfe-4190-a398-62b51f5cbe5c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(10, self.budget // 6)  # Slightly increase the initial sample count\n\n        # Randomly sample initial points within bounds with a refined strategy\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds and refine more aggressively with a dynamic shrinking ratio\n        shrink_ratio = 0.1 + 0.05 * np.random.rand()  # Introduce randomness to avoid local minima\n        bounds = [(max(lb, x - shrink_ratio * (ub - lb)), min(ub, x + shrink_ratio * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.75), 'ftol': 1e-8})  # Adjust budget allocation\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing a dynamic sampling and shrinking strategy to enhance convergence and robustness in local optimizations.", "configspace": "", "generation": 5, "fitness": 0.7854567588310147, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.076. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.8910592759220155, 0.7123249830063505, 0.7529860175646781], "final_y": [0.0, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "0a5027ae-c931-4680-b82f-c68f839f6e16", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced initial sample count and local optimization bounds adjustment to improve coverage and convergence.", "configspace": "", "generation": 5, "fitness": 0.7361706757204264, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.736 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7438043149028862, 0.7117216946937152, 0.7529860175646781], "final_y": [6.524980536255546e-08, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "d7b6c972-2f30-4c7b-b057-0244a05e005b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(15, self.budget // 8)  # Increase initial sample count more significantly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjust budget usage and ftol\n        \n        if res.success:\n            return res.x\n        else:\n            return best_sample", "name": "EnhancedMetaheuristicOptimizer", "description": "Modify initial sample strategy and tweak optimization options for enhanced convergence efficiency.", "configspace": "", "generation": 5, "fitness": 0.7381034044414486, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.738 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7427469649545325, 0.7140152301178057, 0.757548018252008], "final_y": [4.5043482750519614e-08, 1.3118465503193522e-07, 2.5089994264028325e-08]}, "mutation_prompt": null}
{"id": "933df734-b2c6-4f67-9dbe-f4c009a40f91", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Introduce Differential Evolution with adaptive mutation\n        population_size = min(20, self.budget // 5)\n        population = [best_sample + 0.01 * np.random.randn(self.dim) for _ in range(population_size)]\n        \n        def differential_evolution(population, best_value):\n            F = 0.5 + 0.5 * np.random.rand()  # Adaptive mutation factor\n            CR = 0.9  # Crossover rate\n            for i in range(population_size):\n                a, b, c = np.random.choice(population_size, 3, replace=False)\n                mutant = np.clip(population[a] + F * (population[b] - population[c]), func.bounds.lb, func.bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, population[i])\n                trial_value = func(trial)\n                self.budget -= 1\n                if trial_value < best_value:\n                    best_value = trial_value\n                    population[i] = trial\n                if self.budget <= 0:\n                    break\n            return best_value\n        \n        best_value = differential_evolution(population, best_value)\n\n        # Refine with local optimization\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        def objective(x):\n            return func(x)\n\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample", "name": "EnhancedMetaheuristicOptimizer", "description": "Incorporating differential evolution with adaptive mutation rates and elitism to enhance exploration and exploitation balance for black box optimization.", "configspace": "", "generation": 5, "fitness": 0.7321352753439762, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.732 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7301067076443931, 0.7133141765050393, 0.752984941882496], "final_y": [6.524980536255546e-08, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "4da6fa61-9684-458a-be5c-00bc10e3986c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(10, self.budget // 8)\n\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        def objective(x):\n            return func(x)\n\n        res1 = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.4), 'ftol': 1e-8})\n        \n        if res1.success:\n            best_sample = res1.x\n            best_value = res1.fun\n            self.budget -= res1.nfev\n\n        bounds = [(max(lb, x - 0.05 * (ub - lb)), min(ub, x + 0.05 * (ub - lb))) for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        res2 = minimize(objective, x0=best_sample, method='Nelder-Mead', bounds=bounds, options={'maxfev': int(self.budget * 0.6)})\n        \n        if res2.success and res2.fun < best_value:\n            return res2.x\n        else:\n            return best_sample", "name": "EnhancedMetaheuristicOptimizer", "description": "The algorithm refines adaptive sampling with dynamic bounds adjustment and employs dual local optimizers to enhance convergence in smooth landscapes.", "configspace": "", "generation": 5, "fitness": 0.7353884648998577, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.735 with standard deviation 0.046. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7984864884997118, 0.6900405807159982, 0.7176383254838627], "final_y": [2.3688183031675448e-08, 2.2685678287819244e-07, 5.197914030948284e-08]}, "mutation_prompt": null}
{"id": "cbf9c682-a45f-4db6-a5c1-4faafccd11be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # More aggressive adjustment\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced convergence through refined adaptive bound adjustment in local search.", "configspace": "", "generation": 6, "fitness": 0.7746199079710676, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.100. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.9148493033248625, 0.7206374911028408, 0.6883729294854997], "final_y": [0.0, 6.48015059967296e-08, 1.1546933967249994e-07]}, "mutation_prompt": null}
{"id": "a0e7229d-9869-4bb1-9be0-ebb8f32062c4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(20, self.budget // 8)  # Increase initial sample count to improve exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhancing the initial sampling strategy by doubling the initial sample count to improve exploration of the parameter space.", "configspace": "", "generation": 6, "fitness": 0.7239705177811614, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.724 with standard deviation 0.071. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.8193830989487306, 0.6510684269744557, 0.7014600274202978], "final_y": [1.6899074107934e-09, 3.699775432860044e-07, 6.319825956593422e-08]}, "mutation_prompt": null}
{"id": "0d911a0e-8592-42ee-9153-e772ba819f98", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 6)  # Adjust initial sample count for more exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Introduce additional stochastic perturbations around the best initial sample\n        perturbed_samples = [best_sample + np.random.normal(0, 0.05, self.dim) for _ in range(3)]\n        for sample in perturbed_samples:\n            if self.budget <= 0:\n                break\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n\n        # Narrow bounds around the best sample with more aggressive adjustment\n        bounds = [(max(lb, x - 0.2 * (ub - lb)), min(ub, x + 0.2 * (ub - lb))) \n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with early stopping\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.7), 'ftol': 1e-7})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Incorporating stochastic perturbations and early stopping criteria to enhance convergence in the local optimization phase.", "configspace": "", "generation": 6, "fitness": 0.5462550849890436, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.546 with standard deviation 0.255. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.210.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.719742685251064, 0.1857911980175484, 0.7332313716985183], "final_y": [5.118046519518103e-08, 0.4445051330095076, 4.3904991131010264e-08]}, "mutation_prompt": null}
{"id": "e9e3a329-f89d-4434-97af-25a1a4aa0fa7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 5)  # Adjust initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Improved initial sampling strategy to enhance initial guess quality.", "configspace": "", "generation": 6, "fitness": 0.7170270357628153, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.717 with standard deviation 0.035. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7575279846247497, 0.7209283206936801, 0.6726248019700167], "final_y": [4.8096079935292653e-08, 9.160165630192476e-08, 2.0659171292922343e-07]}, "mutation_prompt": null}
{"id": "bf8eb5b5-d789-4cab-b4fe-4f786c6fe2e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhancing adaptive sampling and local search with refined initial sampling and optimized budget allocation.", "configspace": "", "generation": 6, "fitness": 0.7337415242962839, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.734 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7438043149028862, 0.6924047232930732, 0.765015534692892], "final_y": [6.524980536255546e-08, 2.32856451272013e-07, 3.6930447744249074e-09]}, "mutation_prompt": null}
{"id": "c3ebaed8-c9f5-4888-83e8-163d5e7643f9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(10, self.budget // 8)\n        \n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        def objective(x):\n            return func(x)\n\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-8})  # Increase precision\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhancing local search efficiency by incorporating gradient-based step size adaptation and iterative narrowing of search space.", "configspace": "", "generation": 7, "fitness": 0.5630528165809224, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.563 with standard deviation 0.226. And the mean value of best solutions found was 0.032 (0. is the best) with standard deviation 0.045.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7019801809159036, 0.7429683853440414, 0.24420988348282235], "final_y": [3.355879511086346e-07, 9.875164950539414e-08, 0.0946199892728326]}, "mutation_prompt": null}
{"id": "f86c3c68-76bc-47b0-bea0-0a5d1d56bc12", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Integrating adaptive sampling with local search and aggressive dynamic bound adjustment to improve convergence in smooth landscapes.", "configspace": "", "generation": 7, "fitness": 0.5176631814940927, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.518 with standard deviation 0.282. And the mean value of best solutions found was 0.811 (0. is the best) with standard deviation 1.148.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7307332566652065, 0.11848133120225435, 0.7037749566148175], "final_y": [6.955025173951917e-08, 2.4343619010238084, 1.7914683489811546e-07]}, "mutation_prompt": null}
{"id": "7311b25b-3f46-40e7-8c03-fc52151412be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Use adaptive reduction of bounds around the best sample to enhance local exploitation.", "configspace": "", "generation": 7, "fitness": 0.7454502015193322, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7438043149028862, 0.723273198062111, 0.7692730915929991], "final_y": [6.524980536255546e-08, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "46fd842b-dceb-46ec-a8cc-56dd7a26c9bc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7, 'maxiter': 100})  # Added 'maxiter' for early stopping\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Utilize early stopping in local optimization to improve computational efficiency and convergence speed.", "configspace": "", "generation": 7, "fitness": 0.7198063173254767, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.720 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7366517055124966, 0.7189477579790879, 0.7038194884848457], "final_y": [1.352018154902472e-07, 1.5504429093161722e-07, 2.3309657505772898e-07]}, "mutation_prompt": null}
{"id": "02961461-fe15-4c00-8bdc-490278b4d8bf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-8})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhance convergence by increasing the precision of the local optimizer's stopping criterion.", "configspace": "", "generation": 7, "fitness": 0.7607010255524088, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.761 with standard deviation 0.037. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7995374864442751, 0.7711410573521654, 0.7114245328607859], "final_y": [1.4904301880257648e-08, 2.5602624140718523e-08, 1.9029056833474794e-07]}, "mutation_prompt": null}
{"id": "e0ab6def-be9d-47fc-b363-d7e4cd9c0da0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.10 * (ub - lb)), min(ub, x + 0.10 * (ub - lb)))  # Adjust bounds more gradually\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing a gradual reduction of the bounds narrowing factor to improve convergence towards the optimal solution.", "configspace": "", "generation": 8, "fitness": 0.4072697988392755, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.407 with standard deviation 0.295. And the mean value of best solutions found was 0.220 (0. is the best) with standard deviation 0.206.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.8242740621200424, 0.203945540953836, 0.1935897934439481], "final_y": [8.435928089334119e-09, 0.1636331287578498, 0.49592601888096055]}, "mutation_prompt": null}
{"id": "a3c3560b-d531-43e0-89fb-558d355ddd89", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 6)  # Slightly increase initial sample count again\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Integrating adaptive sampling with local search and dynamic budget allocation to improve convergence in smooth landscapes with slightly adjusted initial sample calculations for enhanced exploration.", "configspace": "", "generation": 8, "fitness": 0.4206839395875878, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.421 with standard deviation 0.333. And the mean value of best solutions found was 0.323 (0. is the best) with standard deviation 0.234.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.8910592759220155, 0.1883557369871135, 0.18263680585363462], "final_y": [0.0, 0.4239080359611992, 0.5464524168229791]}, "mutation_prompt": null}
{"id": "bd6c35a0-b753-4cce-8d93-8f56d03ecbed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Improved initial sampling strategy to enhance exploration capabilities and convergence efficiency.", "configspace": "", "generation": 8, "fitness": 0.38296140204423224, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.383 with standard deviation 0.291. And the mean value of best solutions found was 0.318 (0. is the best) with standard deviation 0.283.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7943953839715887, 0.18574411978706318, 0.16874470237404482], "final_y": [2.686379499286396e-08, 0.2659093070288864, 0.6875085814714473]}, "mutation_prompt": null}
{"id": "8895ff73-4bb9-4263-80f6-2835a802f730", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 6)  # Slightly increase initial sample count\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.2 * (ub - lb)))  # Refine bounds for precision\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-8})  # Adjust budget allocation\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced adaptive sampling and refined local search with dynamic bounds adjustment for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.36807246499577123, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.368 with standard deviation 0.283. And the mean value of best solutions found was 0.437 (0. is the best) with standard deviation 0.482.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7675840728638025, 0.1821492742580777, 0.15448404786543357], "final_y": [2.3688183031675448e-08, 0.2019166950384148, 1.1084007859960479]}, "mutation_prompt": null}
{"id": "1a99a2fc-a297-43f9-84b4-6b5377758cab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.kernel = Matern(nu=2.5)\n        self.gp = GaussianProcessRegressor(kernel=self.kernel)\n\n    def __call__(self, func):\n        initial_sample_count = max(10, self.budget // 8)\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        X = np.array(initial_samples)\n        y = np.array([func(sample) for sample in initial_samples])\n        self.budget -= len(initial_samples)\n\n        self.gp.fit(X, y)\n        best_sample = X[np.argmin(y)]\n        best_value = np.min(y)\n\n        while self.budget > 0:\n            candidate_sample = self.propose_location(func.bounds.lb, func.bounds.ub)\n            candidate_value = func(candidate_sample)\n            self.budget -= 1\n\n            if candidate_value < best_value:\n                best_value = candidate_value\n                best_sample = candidate_sample\n\n            X = np.vstack((X, candidate_sample))\n            y = np.append(y, candidate_value)\n            self.gp.fit(X, y)\n\n            if self.budget <= 0:\n                return best_sample\n\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        def objective(x):\n            return func(x)\n\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample\n\n    def propose_location(self, lb, ub):\n        random_point = np.random.uniform(lb, ub, size=self.dim)\n        return random_point", "name": "EnhancedMetaheuristicOptimizer", "description": "Hybridize Bayesian optimization with adaptive local search for efficient exploration and exploitation in smooth landscapes.", "configspace": "", "generation": 8, "fitness": 0.4160676345681313, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.416 with standard deviation 0.336. And the mean value of best solutions found was 0.410 (0. is the best) with standard deviation 0.297.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.8910592759220155, 0.17489905737800593, 0.18224457040437225], "final_y": [0.0, 0.6961128612328757, 0.5340668748148677]}, "mutation_prompt": null}
{"id": "e31e86a3-2d3f-45c6-a2ce-9133058bac7f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveGradientBoostingOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples, ensuring a portion of the budget is reserved\n        initial_sample_count = max(10, self.budget // 10)\n        \n        # Randomly sample initial points within bounds with a diversification factor\n        diversification_factor = 0.25\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb + diversification_factor * (ub - lb), ub - diversification_factor * (ub - lb))\n                for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Perform a secondary sampling around the best initial sample\n        secondary_sample_count = initial_sample_count // 2\n        secondary_samples = []\n        for _ in range(secondary_sample_count):\n            sample = np.array([\n                np.random.uniform(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)\n            ])\n            secondary_samples.append(sample)\n\n        for sample in secondary_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Adaptive L-BFGS-B optimization with remaining budget\n        bounds = [(max(lb, x - 0.05 * (ub - lb)), min(ub, x + 0.05 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n        \n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': self.budget, 'ftol': 1e-8})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "AdaptiveGradientBoostingOptimizer", "description": "Multi-Stage Constrained Sampling and Adaptive Gradient Boosting to enhance exploration and exploitation balance in smooth optimization landscapes.", "configspace": "", "generation": 9, "fitness": 0.14026688953232783, "feedback": "The algorithm AdaptiveGradientBoostingOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.140 with standard deviation 0.006. And the mean value of best solutions found was 1.534 (0. is the best) with standard deviation 0.262.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.1328746726624086, 0.14861283340120912, 0.1393131625333658], "final_y": [1.851134091700802, 1.2098212806079778, 1.5403203461736663]}, "mutation_prompt": null}
{"id": "1ea4f4ff-9a61-467b-bed2-1ed9016bbd01", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        values = []\n        for sample in initial_samples:\n            value = func(sample)\n            values.append(value)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Early stop if variance reduction is significant\n        if np.var(values) < 1e-5:\n            return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "EnhancedMetaheuristicOptimizer+ with early stopping based on variance reduction in function values during initial sampling.", "configspace": "", "generation": 9, "fitness": 0.8061494922501207, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.077. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.9145860044690752, 0.7628020626280951, 0.7410604096531916], "final_y": [0.0, 9.392923381717664e-08, 5.341785760823254e-08]}, "mutation_prompt": null}
{"id": "12213b7a-6ea3-4f68-91a7-9713a70eb7ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Dynamically adjust initial sample count based on remaining budget\n        initial_sample_count = min(initial_sample_count, self.budget // 2)  # Adjusted line\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introduce dynamic adjustment of initial sample count based on remaining budget to enhance initial exploration phase.", "configspace": "", "generation": 9, "fitness": 0.7454502015193322, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7438043149028862, 0.723273198062111, 0.7692730915929991], "final_y": [6.524980536255546e-08, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "aae71b65-8f7f-4d4f-9d54-6e52ee58e9a6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-8})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Incorporating adaptive learning rates and mixed exploration strategies to enhance convergence efficiency in smooth landscapes.", "configspace": "", "generation": 9, "fitness": 0.7139544178569142, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.714 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7159391400040117, 0.6964124453288515, 0.7295116682378793], "final_y": [1.6039976852864924e-07, 2.682782427937522e-07, 8.253115522640151e-08]}, "mutation_prompt": null}
{"id": "9bc94021-6d38-4962-806c-cc51ce156d72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, int(self.budget / 6))  # Dynamically adjust initial sample count based on budget\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introduce dynamic adjustment of initial sample size based on remaining budget to improve exploration.", "configspace": "", "generation": 9, "fitness": 0.7607010255524088, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.761 with standard deviation 0.037. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7995374864442751, 0.7711410573521654, 0.7114245328607859], "final_y": [1.4904301880257648e-08, 2.5602624140718523e-08, 1.9029056833474794e-07]}, "mutation_prompt": null}
{"id": "12b403c9-1a1a-4abf-90c7-7e8f9131267c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(15, self.budget // 6)  # Increase initial sample count slightly\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced initialization and adaptive budget allocation for improved convergence in smooth landscapes.", "configspace": "", "generation": 10, "fitness": 0.8021253998160308, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.081. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.9138299097929825, 0.723273198062111, 0.7692730915929991], "final_y": [0.0, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "888cad60-1379-40d5-b52d-a04db20cc2c7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Improved boundary adjustment to enhance local exploration in smooth landscapes.", "configspace": "", "generation": 10, "fitness": 0.753997403142729, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.754 with standard deviation 0.053. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.8242740621200424, 0.7404758480386139, 0.6972422992695304], "final_y": [8.435928089334119e-09, 9.089217099156418e-08, 3.73473871310239e-07]}, "mutation_prompt": null}
{"id": "579c5341-6479-4a66-a1bf-9cf437141c5d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            penalty = 1e6 * np.sum(np.maximum(0, np.array(func.bounds.lb) - x) + np.maximum(0, x - np.array(func.bounds.ub)))\n            return func(x) + penalty  # Dynamic penalty for infeasible solutions\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Incorporating dynamic penalty for infeasible solutions to enhance convergence in constrained optimization.", "configspace": "", "generation": 10, "fitness": 0.5176631814940927, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.518 with standard deviation 0.282. And the mean value of best solutions found was 0.811 (0. is the best) with standard deviation 1.148.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7307332566652065, 0.11848133120225435, 0.7037749566148175], "final_y": [6.955025173951917e-08, 2.4343619010238084, 1.7914683489811546e-07]}, "mutation_prompt": null}
{"id": "a6cab991-efb2-409b-8caa-ee913289fed3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhancing initial sampling strategy and fine-tuning convergence criteria to improve performance in smooth landscapes.", "configspace": "", "generation": 10, "fitness": 0.8100924447619993, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.100. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.9477117440730464, 0.7711410573521654, 0.7114245328607859], "final_y": [0.0, 2.5602624140718523e-08, 1.9029056833474794e-07]}, "mutation_prompt": null}
{"id": "bf11d030-4d3a-4f50-ac1d-0ad4827f59b8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 8)  # Increase initial sample count slightly\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-7})  # Reserve some budget\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhancing initial sampling diversity by stratifying the random samples within bounds to improve convergence.", "configspace": "", "generation": 10, "fitness": 0.7454502015193322, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1156b459-960c-408c-8d56-73034c8c949f", "metadata": {"aucs": [0.7438043149028862, 0.723273198062111, 0.7692730915929991], "final_y": [6.524980536255546e-08, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "76522b3b-2c4c-41aa-82f1-56d100f5080e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(15, self.budget // 8)  # Increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.90), 'ftol': 1e-7})  # Increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Optimize initial sampling precision and local optimization strategy to boost performance in smooth landscapes.", "configspace": "", "generation": 11, "fitness": 0.5879274536288184, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.588 with standard deviation 0.346. And the mean value of best solutions found was 0.811 (0. is the best) with standard deviation 1.148.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.9415260730693832, 0.11848133120225435, 0.7037749566148175], "final_y": [0.0, 2.4343619010238084, 1.7914683489811546e-07]}, "mutation_prompt": null}
{"id": "88fbba7e-29e4-4810-97f8-baaed52c1b29", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.10 * (ub - lb)), min(ub, x + 0.10 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.90), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Adaptive sampling and adjusted bounds to enhance local optimization in smooth landscapes.", "configspace": "", "generation": 11, "fitness": 0.753997403142729, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.754 with standard deviation 0.053. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.8242740621200424, 0.7404758480386139, 0.6972422992695304], "final_y": [8.435928089334119e-09, 9.089217099156418e-08, 3.73473871310239e-07]}, "mutation_prompt": null}
{"id": "6d6e8251-e56d-462c-90ac-0a29e06abae2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.2 * (ub - lb)), min(ub, x + 0.2 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Refining initial sampling and bounds adjustment for enhanced local optimization performance.", "configspace": "", "generation": 11, "fitness": 0.7425636643522134, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.743 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7351447034015302, 0.723273198062111, 0.7692730915929991], "final_y": [1.0640492092829553e-07, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "61af14db-8066-4295-a359-a543769672f9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(16, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhance initial sampling count further to improve initial solution quality.", "configspace": "", "generation": 11, "fitness": 0.7454502015193322, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7438043149028862, 0.723273198062111, 0.7692730915929991], "final_y": [6.524980536255546e-08, 3.012259439557228e-07, 2.7991490650865255e-08]}, "mutation_prompt": null}
{"id": "be180883-0937-4815-a673-052b184f5f9f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(15, self.budget // 7)  # Increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Increase budget for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing an adaptive sampling strategy and fine-tuning the convergence method for improved performance in smooth landscapes.", "configspace": "", "generation": 11, "fitness": 0.7609720824925357, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.761 with standard deviation 0.037. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.8003506572646557, 0.7711410573521654, 0.7114245328607859], "final_y": [1.4904301880257648e-08, 2.5602624140718523e-08, 1.9029056833474794e-07]}, "mutation_prompt": null}
{"id": "f4dc01a6-0b3f-461e-95b5-b9667e83c78a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-7})  # Adjust budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Strategically enhance local optimization by adjusting initial sample coverage, improving convergence near optimal regions.", "configspace": "", "generation": 12, "fitness": 0.7980382053403314, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.088. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.9145860044690752, 0.7765323472669612, 0.7029962642849579], "final_y": [0.0, 3.854257415472684e-08, 9.540180681103266e-08]}, "mutation_prompt": null}
{"id": "5718b24f-00c2-4e99-868b-911fb61b7958", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples with hybrid approach\n        initial_sample_count = max(15, self.budget // 7)  # Adjusted sample count\n        \n        # Randomly sample initial points within bounds (hybrid strategy)\n        initial_samples = [\n            np.random.uniform(func.bounds.lb, func.bounds.ub) for _ in range(initial_sample_count // 2)\n        ]\n        halton_samples = self.halton_sequence(initial_sample_count // 2, func.bounds.lb, func.bounds.ub)\n        initial_samples.extend(halton_samples)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Dynamic bounds adjustment strategy\n        bounds = [(max(lb, x - 0.10 * (ub - lb)), min(ub, x + 0.10 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-8})  # Fine-tuned ftol\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails\n\n    def halton_sequence(self, count, lb, ub):\n        \"\"\"Generate Halton sequence samples within given bounds.\"\"\"\n        # Implementation of a basic Halton sequence generator for 2D\n        samples = []\n        for i in range(count):\n            base2 = self.van_der_corput(i, 2)\n            base3 = self.van_der_corput(i, 3)\n            sample = [(ub[j] - lb[j]) * base + lb[j] for j, base in enumerate([base2, base3])]\n            samples.append(sample)\n        return samples\n\n    def van_der_corput(self, n, base):\n        \"\"\"Van der Corput sequence implementation.\"\"\"\n        vdc = 0\n        denom = 1\n        while n > 0:\n            denom *= base\n            n, remainder = divmod(n, base)\n            vdc += remainder / denom\n        return vdc", "name": "EnhancedMetaheuristicOptimizer", "description": "Refine sampling and optimization strategy using hybrid initialization and dynamic bounds adjustment for improved local search in smooth landscapes.", "configspace": "", "generation": 12, "fitness": 0.7361943069475863, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.736 with standard deviation 0.001. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7356855292646284, 0.7371726589136223, 0.7357247326645081], "final_y": [9.376139980246714e-08, 9.376139980246714e-08, 9.376139980246714e-08]}, "mutation_prompt": null}
{"id": "4967b6b7-37e8-4569-bd4c-0ecc3eaaf776", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 10)  # Adjust initial samples to balance exploration\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        bounds = [(max(lb, x - 0.2 * (ub - lb)), min(ub, x + 0.2 * (ub - lb)))  # Adaptive expansion of bounds\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        def objective(x):\n            return func(x)\n\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-9})  # Adjust ftol for precision\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing adaptive step-size and dynamic sampling based on convergence rate to enhance local search efficiency and robustness.", "configspace": "", "generation": 12, "fitness": 0.7614349412347184, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.761 with standard deviation 0.023. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7914025757357945, 0.7354312264749809, 0.7574710214933796], "final_y": [4.69862970259982e-08, 1.2134184467623174e-07, 4.020074953632673e-08]}, "mutation_prompt": null}
{"id": "1d892aa5-8781-4250-82ab-7a4b14b5609e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.12 * np.ptp([sample[i] for sample in initial_samples])), min(ub, x + 0.12 * np.ptp([sample[i] for sample in initial_samples])))\n                  for i, (x, lb, ub) in enumerate(zip(best_sample, func.bounds.lb, func.bounds.ub))]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Improving precision by adjusting the bounds more dynamically based on initial sampling variance.", "configspace": "", "generation": 12, "fitness": 0.7373187253120314, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.737 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7084495702259112, 0.7513156323625745, 0.7521909733476086], "final_y": [2.4492595793680775e-07, 3.0106934360747446e-08, 3.0106934360747446e-08]}, "mutation_prompt": null}
{"id": "296bce92-14e2-4169-bd1a-162c5b9b7208", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)\n\n        # Hybrid initial sampling strategy\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            if np.random.rand() > 0.5:  # Alternate between uniform and normal sampling\n                sample = np.array([\n                    np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n                ])\n            else:\n                center = np.array([(lb + ub) / 2 for lb, ub in zip(func.bounds.lb, func.bounds.ub)])\n                scale = np.array([(ub - lb) / 6 for lb, ub in zip(func.bounds.lb, func.bounds.ub)])  # 6 for 99.7% coverage\n                sample = np.random.normal(center, scale)\n                sample = np.clip(sample, func.bounds.lb, func.bounds.ub)\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Dynamically adjust bounds around best initial sample\n        bounds = [(max(lb, x - 0.20 * (ub - lb)), min(ub, x + 0.20 * (ub - lb))) \n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with refined options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.90), 'ftol': 1e-8})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample", "name": "EnhancedMetaheuristicOptimizer", "description": "An enhanced local optimization strategy leveraging hybrid sampling and variable bounds adaptation for improved convergence.", "configspace": "", "generation": 12, "fitness": 0.7375477869283374, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.738 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7091367550748294, 0.7513156323625745, 0.7521909733476086], "final_y": [1.846816878674009e-07, 3.0106934360747446e-08, 3.0106934360747446e-08]}, "mutation_prompt": null}
{"id": "21c8bd2d-7aad-47c3-954d-79aca2decf02", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Adjusted bounds update strategy\n        bounds = [(max(lb, x - 0.20 * (ub - lb)), min(ub, x + 0.20 * (ub - lb)))  \n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Adjusts bounds update strategy for better exploration-exploitation balance in smooth landscapes. ", "configspace": "", "generation": 13, "fitness": 0.7451143860296531, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7587782922421975, 0.7478201802859537, 0.728744685560808], "final_y": [3.1909435094216654e-08, 8.209057597881497e-08, 8.382792381289441e-08]}, "mutation_prompt": null}
{"id": "72659d07-76bb-41b7-967f-6ef9721a72a4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Fine-tune bounds adjustment\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Further fine-tune the bounds adjustment around the best initial sample for enhanced optimization precision.", "configspace": "", "generation": 13, "fitness": 0.7494806879710089, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.749 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7572890953194844, 0.7544994050377609, 0.7366535635557814], "final_y": [4.8096079935292653e-08, 9.778890280860107e-08, 5.740440979511552e-08]}, "mutation_prompt": null}
{"id": "48c129ef-b445-4334-9ff1-262509412595", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(15, self.budget // 8)  # Adjust initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Fine-tuning random initial sampling and adjusting budget distribution for better optimization.", "configspace": "", "generation": 13, "fitness": 0.7007428879852058, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.701 with standard deviation 0.035. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.6531978562570144, 0.7366305894821064, 0.7124002182164966], "final_y": [1.7013103387416462e-07, 8.859394400207286e-08, 1.6937410895038083e-07]}, "mutation_prompt": null}
{"id": "52a7a9de-5cf6-4fb0-8950-c050cd181199", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Apply an adaptive mutation to the best initial sample\n        mutated_sample = best_sample + 0.05 * np.random.randn(*best_sample.shape)\n\n        # Narrow bounds around the mutated sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(mutated_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=mutated_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introduce an adaptive mutation step to refine initial samples, enhancing diversity and exploration.", "configspace": "", "generation": 13, "fitness": 0.745316950692748, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.043. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7352023873366342, 0.698212090568938, 0.8025363741726717], "final_y": [1.0600664828928131e-07, 2.57931056156917e-07, 3.1594847404616838e-09]}, "mutation_prompt": null}
{"id": "5b36e81f-1724-4042-9f0e-21d5312a86b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced initial sampling strategy and convergence criteria with refined boundary adjustments for better performance.", "configspace": "", "generation": 13, "fitness": 0.7439824894937365, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.744 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7407944998876672, 0.7544994050377609, 0.7366535635557814], "final_y": [1.0588797065277743e-07, 9.778890280860107e-08, 5.740440979511552e-08]}, "mutation_prompt": null}
{"id": "ea35ae16-c8c1-439a-a6be-3b85b084f42a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use BFGS for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='BFGS', options={'maxiter': int(self.budget * 0.85), 'gtol': 1e-7})  # Adjusted method and option\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhance local optimization by utilizing the BFGS method instead of L-BFGS-B for potentially better performance in smooth cost function landscapes.", "configspace": "", "generation": 14, "fitness": 0.48613183853954306, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.486 with standard deviation 0.200. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7687743531138543, 0.33983829039177704, 0.34978287211299786], "final_y": [5.0875415143475034e-08, 1.6519566363247183e-05, 1.6519566363247183e-05]}, "mutation_prompt": null}
{"id": "b12f468e-47c7-4702-a435-1ee2c75d9d55", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Adjust initial sample selection to ensure more thorough exploration of the parameter space.", "configspace": "", "generation": 14, "fitness": 0.5007681379494997, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.501 with standard deviation 0.187. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7646844791661417, 0.37080752039095544, 0.3668124142914019], "final_y": [3.637337556810344e-08, 1.462245606780428e-05, 1.462245606780428e-05]}, "mutation_prompt": null}
{"id": "e7da3fc5-c3ff-4ffb-a659-598840264d28", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(10, self.budget // 7)  # Adjust initial sample count for better exploration-exploitation\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Adjust initial sample count to better balance exploration and exploitation across the budget.", "configspace": "", "generation": 14, "fitness": 0.4750293416089799, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.475 with standard deviation 0.200. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7572890953194844, 0.3343449539800428, 0.33345397552741274], "final_y": [4.8096079935292653e-08, 2.8808288566313998e-05, 2.8808288566313998e-05]}, "mutation_prompt": null}
{"id": "023c7be5-595e-4465-a580-b946668e3899", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.10 * (ub - lb)), min(ub, x + 0.10 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Improved adaptive locality by refining bounds adjustment strategy for enhanced convergence in smooth landscapes.", "configspace": "", "generation": 14, "fitness": 0.3753630333442372, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.375 with standard deviation 0.003. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.37565956868442196, 0.37839879740020455, 0.37203073394808506], "final_y": [2.5552812543407265e-05, 2.5552812543407265e-05, 2.5552812543407265e-05]}, "mutation_prompt": null}
{"id": "e007ad33-7861-40ba-935b-de1df43d9090", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import differential_evolution\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(10, self.budget // 10)  # Adjust sample count for better global coverage\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Use Differential Evolution for global optimization refinement\n        bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        result = differential_evolution(func, bounds, maxiter=int(self.budget * 0.25), seed=42, disp=False)\n        self.budget -= result.nfev\n        if result.fun < best_value:\n            best_sample = result.x\n            best_value = result.fun\n\n        # Narrow bounds around the best sample found\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb))) \n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.75), 'ftol': 1e-7})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Optimizing initial sample strategy and leveraging hybrid local-global optimization to enhance performance in low-dimensional smooth landscapes.", "configspace": "", "generation": 14, "fitness": 0.33502844546454263, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.335 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.3372864068861724, 0.3343449539800428, 0.33345397552741274], "final_y": [2.8808288566313998e-05, 2.8808288566313998e-05, 2.8808288566313998e-05]}, "mutation_prompt": null}
{"id": "f08ab2b6-a4b3-42e7-9aa4-1afeb6aa7e9f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-5})  # Adjust 'ftol' for precision\n\n        if res.success and res.fun < 1e-5:  # Early termination if function value is sufficiently low\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introduce early termination and dynamic adjustment of the local optimizer's precision to enhance convergence efficiency.", "configspace": "", "generation": 15, "fitness": 0.4190525600175598, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.419 with standard deviation 0.360. And the mean value of best solutions found was 0.707 (0. is the best) with standard deviation 0.589.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.9280670580855649, 0.1839213345590457, 0.14516928740806878], "final_y": [0.0, 0.6785973353621007, 1.4410556987504026]}, "mutation_prompt": null}
{"id": "6fd129e6-8824-4c4b-a959-f9e0ac0b6e7e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // (6 * self.dim))  # Adjust sample count proportional to dimensionality\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Refine sampling strategy by adjusting the sample count proportional to the dimensionality to enhance exploration.", "configspace": "", "generation": 15, "fitness": 0.5214605750900517, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.521 with standard deviation 0.112. And the mean value of best solutions found was 0.002 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.644643514544513, 0.5467808333857516, 0.37295737733989065], "final_y": [3.039056569698796e-06, 2.0409045422023707e-05, 0.005427254835298545]}, "mutation_prompt": null}
{"id": "36d8880d-cc44-4b45-a3e8-c695a6c1e04c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(15, self.budget // 6)  # Increase initial sample count for broader coverage\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.2 * (ub - lb)), min(ub, x + 0.2 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Integrating adaptive sampling and dynamic bound adjustments to enhance solution accuracy in smooth landscapes.", "configspace": "", "generation": 15, "fitness": 0.712729434569216, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.713 with standard deviation 0.126. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.8910592759220155, 0.6269535467088203, 0.6201754810768121], "final_y": [0.0, 4.5718724218689915e-06, 2.6465924518502e-06]}, "mutation_prompt": null}
{"id": "5e51b020-2d93-4632-b2ce-83765e34c563", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.90), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Slightly increase the budget percentage for the local optimizer to enhance exploitation around the best initial sample.", "configspace": "", "generation": 15, "fitness": 0.6634752247716487, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.663 with standard deviation 0.116. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.8258125375294453, 0.5991365834169261, 0.5654765533685746], "final_y": [2.686379499286396e-08, 5.438270630339916e-06, 9.89144790014477e-06]}, "mutation_prompt": null}
{"id": "bf4e0d2e-123b-4ce2-80dc-0276a2fc8a96", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(14, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Slightly adjust initial sample count calculation for improved initial coverage and convergence.", "configspace": "", "generation": 15, "fitness": 0.6097491534561063, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.610 with standard deviation 0.080. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7221556579122851, 0.5381354784355752, 0.5689563240204587], "final_y": [8.833150691911567e-08, 3.490511533887963e-05, 1.2353173266715229e-05]}, "mutation_prompt": null}
{"id": "d5232e36-e606-4183-b54d-081dfc31b221", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.2 * (ub - lb)), min(ub, x + 0.2 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Refining the initial sample selection strategy to improve the performance of the optimizer.", "configspace": "", "generation": 16, "fitness": 0.7266825586072758, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.727 with standard deviation 0.077. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.6193752817131017, 0.7622750348456785, 0.7983973592630473], "final_y": [3.039056569698796e-06, 1.3824676234766895e-07, 4.642203846300867e-08]}, "mutation_prompt": null}
{"id": "abd97fec-26ab-4cd0-84da-caf44e4f7ea9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.25 * (ub - lb)), min(ub, x + 0.25 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introduce a diversified local search strategy by dynamically adjusting the bounds based on initial sample distribution.", "configspace": "", "generation": 16, "fitness": 0.8044153133339474, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7943953839715887, 0.8140054383857407, 0.8048451176445128], "final_y": [2.686379499286396e-08, 5.438749409886883e-08, 4.343984241694585e-08]}, "mutation_prompt": null}
{"id": "8dccc079-4555-44bf-92d5-76572338e3b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate the number of initial samples\n        initial_sample_count = max(12, self.budget // 8)  # Slightly increase initial sample count for better coverage\n        \n        # Randomly sample initial points within bounds using Sobol sequence\n        sobol_sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sobol_sampler.random_base2(m=int(np.log2(initial_sample_count)))\n\n        # Scale Sobol samples to the given bounds\n        initial_samples = func.bounds.lb + initial_samples * (func.bounds.ub - func.bounds.lb)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-7})  # Slightly increase budget percentage for L-BFGS-B\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Adjust the initial sampling strategy to include Sobol sequence sampling for improved coverage.", "configspace": "", "generation": 16, "fitness": 0.7447234310866039, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7485351484875995, 0.7559306376310233, 0.729704507141189], "final_y": [2.162442667587319e-07, 2.0202363192504902e-07, 3.0718980187728664e-07]}, "mutation_prompt": null}
{"id": "1480b7ef-43e2-445a-b398-c22296a43903", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 6)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing an adaptive sampling strategy and dynamically adjusting convergence parameters to improve efficiency in smooth landscapes.", "configspace": "", "generation": 16, "fitness": 0.8215761656454187, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.792626026359011, 0.8642205202601588, 0.8078819503170865], "final_y": [8.435928089334119e-09, 1.742772774448665e-09, 3.318634113183431e-08]}, "mutation_prompt": null}
{"id": "13d9a6ce-ad98-4619-9615-6019825b1054", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Calculate initial sample count using dimensionality\n        initial_sample_count = max(5, self.budget // 10)\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and select the top performers\n        evaluated_samples = []\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            evaluated_samples.append((value, sample))\n            if self.budget <= 0:\n                break\n\n        # Sort by performance and select top samples\n        evaluated_samples.sort()\n        top_samples = [s for _, s in evaluated_samples[:min(3, len(evaluated_samples))]]\n\n        # Define a local search using multiple top samples\n        def local_search(x0):\n            bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                      for x, lb, ub in zip(x0, func.bounds.lb, func.bounds.ub)]\n            res = minimize(func, x0=x0, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.7 / len(top_samples)), 'ftol': 1e-7})\n            return res\n\n        best_value = float('inf')\n        best_solution = None\n        for sample in top_samples:\n            if self.budget <= 0:\n                break\n            res = local_search(sample)\n            if res.success and res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n\n        return best_solution if best_solution is not None else top_samples[0]", "name": "AdaptiveHybridOptimizer", "description": "Adaptive Hybrid Sampling and Local Optimization combines adaptive sampling with hybrid local optimization to efficiently navigate smooth landscapes.", "configspace": "", "generation": 16, "fitness": 0.7604936639279506, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.760 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a6cab991-efb2-409b-8caa-ee913289fed3", "metadata": {"aucs": [0.7765176085214395, 0.748760401759818, 0.7562029815025946], "final_y": [4.8096079935292653e-08, 5.3961015055114294e-08, 5.740440979511552e-08]}, "mutation_prompt": null}
{"id": "db92145f-88bb-465f-a35c-19f8d40d78ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 6)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.05 * (ub - lb)), min(ub, x + 0.05 * (ub - lb)))  # More aggressive bound adjustment\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing a more aggressive bound narrowing strategy around the best sample to enhance convergence speed in smooth landscapes.", "configspace": "", "generation": 17, "fitness": 0.7731053401681267, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.065. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.8646492868308266, 0.7271174631762212, 0.7275492704973323], "final_y": [0.0, 5.818019750319735e-10, 1.4895716862233917e-09]}, "mutation_prompt": null}
{"id": "883d78a6-d2c7-4dfc-84ca-25586dee289f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(15, self.budget // 5)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more for refined search\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.8), 'ftol': 1e-9})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing a hybrid sampling strategy with adaptive local optimization to enhance performance in smooth landscapes.", "configspace": "", "generation": 17, "fitness": 0.7817272364335928, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.8552146386150543, 0.706475937504057, 0.783491133181667], "final_y": [0.0, 7.514725580831788e-10, 2.163367540597146e-09]}, "mutation_prompt": null}
{"id": "adc03341-aadf-4b93-8fb8-5810829ff87c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 6)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.05 * (ub - lb)), min(ub, x + 0.05 * (ub - lb)))  # Slightly shrink bounds\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.95), 'ftol': 1e-9})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing adaptive constraint shrinking and dynamic sample refinement for improved convergence.", "configspace": "", "generation": 17, "fitness": 0.742917128890371, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.743 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.7637866155685308, 0.7273291025158988, 0.7376356685866835], "final_y": [7.893698703597067e-10, 5.918059772809668e-10, 1.072258584843337e-08]}, "mutation_prompt": null}
{"id": "e3dd1149-8f02-46d8-91e7-daa8bf8c2f8d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(15, self.budget // 5)  # Adjusted initial sample count for enhanced exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.2 * (ub - lb)), min(ub, x + 0.2 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use dual-phase optimization: first L-BFGS-B then Nelder-Mead for fine-tuning\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.7), 'ftol': 1e-8})\n        \n        if res.success and self.budget > 0:\n            # Further exploit with Nelder-Mead\n            res_nm = minimize(objective, x0=res.x, method='Nelder-Mead', options={'maxiter': int(self.budget), 'xatol': 1e-8})\n            return res_nm.x if res_nm.success else res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Refine the algorithm by implementing an adaptive sampling strategy and dual-phase optimization with both exploration and exploitation phases to enhance convergence in smooth search landscapes.", "configspace": "", "generation": 17, "fitness": 0.7069470110583724, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.707 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.6669436113712449, 0.7386626976195731, 0.7152347241842991], "final_y": [1.540456931521003e-07, 5.408415847992742e-10, 6.297302158329407e-10]}, "mutation_prompt": null}
{"id": "3e015765-62f7-4bea-93e9-4c40de32ef90", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(18, self.budget // 6)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced sampling strategy by increasing the initial sample count for better exploration and convergence.", "configspace": "", "generation": 17, "fitness": 0.7247799580891078, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.725 with standard deviation 0.001. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.7234151669279103, 0.7244957114918049, 0.7264289958476078], "final_y": [7.822551813792396e-10, 1.5210412801641201e-09, 8.970809035499408e-10]}, "mutation_prompt": null}
{"id": "804b21aa-c671-4b29-8623-0895e5ae33ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 5)  # Slightly increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Refine enhanced exploration and balanced exploitation through increased sampling diversity and adaptive convergence for improved optimization.", "configspace": "", "generation": 18, "fitness": 0.7604330739881013, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.760 with standard deviation 0.094. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.8910592759220155, 0.673463691003707, 0.7167762550385817], "final_y": [0.0, 3.012259439557228e-07, 2.602748968373873e-08]}, "mutation_prompt": null}
{"id": "2336e583-cbbf-441c-a997-ad7900760cfb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(20, self.budget // 5)  # Increased initial sample count for better exploration\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Improved sampling and exploitation with dynamic exploration-exploitation tradeoff to enhance convergence in smooth landscapes.", "configspace": "", "generation": 18, "fitness": 0.7124335562438425, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.712 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.7984864884997127, 0.662074593448978, 0.676739586782837], "final_y": [2.3688183031675448e-08, 1.514816120506746e-07, 8.32070641062512e-08]}, "mutation_prompt": null}
{"id": "f7ef3587-50e2-4f6e-aeeb-35d94af387f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(15, self.budget // 5)  # Increase initial sample count further for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.15 * (ub - lb)), min(ub, x + 0.15 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing a hybrid adaptive sampling and local optimization strategy to enhance exploration and exploitation balance in smooth landscapes.", "configspace": "", "generation": 18, "fitness": 0.5692570963105003, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.569 with standard deviation 0.205. And the mean value of best solutions found was 0.014 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.2797494685643841, 0.7184849495329256, 0.7095368708341908], "final_y": [0.042974922087540196, 1.4050931196738524e-08, 1.1646227808639294e-07]}, "mutation_prompt": null}
{"id": "e2c67a83-3650-4318-b279-715364a4167b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = min(max(12, self.budget // 8), self.budget)  # Adjusted sample count formula for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhance adaptive sampling by modifying the initial sample count formula to improve exploration.", "configspace": "", "generation": 18, "fitness": 0.7740626034411079, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.055. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.8515394211078597, 0.72740180053366, 0.7432465886818039], "final_y": [0.0, 9.656817279813577e-09, 1.1465850962284952e-08]}, "mutation_prompt": null}
{"id": "18f50278-26cf-4a0f-aa13-f4a8facc7044", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 6)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced adaptive sampling and dynamic parameter tuning to improve convergence efficiency.", "configspace": "", "generation": 18, "fitness": 0.6529818666227883, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.653 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.6829559460993366, 0.6553821094331604, 0.6206075443358678], "final_y": [1.546566484751403e-07, 2.9595217194362397e-07, 3.668578690315117e-07]}, "mutation_prompt": null}
{"id": "482744c0-c607-44c1-96bd-be9018333fcf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 6)\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function with stochastic perturbation\n        def objective(x):\n            perturbation = np.random.normal(0, 0.01, size=x.shape)\n            return func(x + perturbation)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing stochastic perturbation in the local optimization phase to avoid premature convergence and improve exploration in smooth landscapes.", "configspace": "", "generation": 19, "fitness": 0.20231958505893247, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.202 with standard deviation 0.066. And the mean value of best solutions found was 0.768 (0. is the best) with standard deviation 0.519.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.29500903751154184, 0.15679638993527512, 0.15515332772998047], "final_y": [0.03533963642688233, 1.1637547234314545, 1.1048940316002174]}, "mutation_prompt": null}
{"id": "de3862ec-dd17-49b3-a3bc-d4e6b462ba04", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 6)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            if self.budget > 0.1 * int(self.budget) and np.random.rand() < 0.5:  # Adaptive restart mechanism\n                return self.__call__(func)  # Recursive restart\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introduce an adaptive restart mechanism to escape local minima and enhance exploration capabilities.", "configspace": "", "generation": 19, "fitness": 0.3336858400518044, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.334 with standard deviation 0.246. And the mean value of best solutions found was 0.752 (0. is the best) with standard deviation 0.683.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.6808643375330745, 0.18132802226792133, 0.13886516035441732], "final_y": [2.0186766924028356e-07, 0.6024732660354903, 1.653691032159868]}, "mutation_prompt": null}
{"id": "8b724ce3-5be9-47ed-b4e2-f3800204eb92", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(15, self.budget // 5)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.2 * (ub - lb)), min(ub, x + 0.2 * (ub - lb)))  # Adjust bounds more aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced exploration by expanding the initial sampling count and adjusting the bounds around the best sample more aggressively.", "configspace": "", "generation": 19, "fitness": 0.3446693915541421, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.345 with standard deviation 0.224. And the mean value of best solutions found was 0.363 (0. is the best) with standard deviation 0.318.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.6599704180288125, 0.2069517709072226, 0.1670859857263911], "final_y": [1.255134868721406e-07, 0.315748250846789, 0.77437100044026]}, "mutation_prompt": null}
{"id": "6cee3c36-00d8-4112-9138-aa6359aac30d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = min(max(12, self.budget // 6), self.budget // 2)  # Adjusted initial sample count based on remaining budget\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing a dynamic adjustment of sample size based on remaining budget for improved efficiency in smooth landscapes.", "configspace": "", "generation": 19, "fitness": 0.3354913490889481, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.335 with standard deviation 0.251. And the mean value of best solutions found was 0.689 (0. is the best) with standard deviation 0.561.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.6898508312844787, 0.17354598783459496, 0.14307722814777069], "final_y": [1.1995524501378412e-07, 0.6925847250991688, 1.3741047649316007]}, "mutation_prompt": null}
{"id": "56fee506-c943-44c3-b673-41afa866f2bc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(15, self.budget // 5)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-9})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhancing exploration by increasing initial sampling diversity and refining convergence criteria for improved optimization in smooth landscapes.", "configspace": "", "generation": 19, "fitness": 0.32046641515765484, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.320 with standard deviation 0.236. And the mean value of best solutions found was 0.752 (0. is the best) with standard deviation 0.568.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.6537862079351275, 0.164453923318177, 0.14315911421966], "final_y": [2.076438796218177e-07, 0.8828088841650654, 1.3716961331210193]}, "mutation_prompt": null}
{"id": "fcb47609-275c-4535-93bc-ed86d8aa1f72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = min(20, self.budget // 5)  # Adjust sample count for better budget utilization\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Refining the initial sample count strategy to adaptively balance exploration and budget efficiency, enhancing convergence in smooth landscapes.", "configspace": "", "generation": 20, "fitness": 0.8219234220701835, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.022. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.7902409941208088, 0.8366688910896256, 0.8388603810001162], "final_y": [1.5405122149078877e-07, 3.76337611313909e-10, 9.45094966641003e-10]}, "mutation_prompt": null}
{"id": "9cef9164-4308-40b6-b3c3-270a99241c8a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(15, self.budget // 5)  # Adjust sample count for better initial approximation\n\n        # Sobol sequence for quasi-random initial points within bounds\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        initial_points = sampler.random_base2(m=int(np.log2(initial_sample_count)))\n        initial_samples = [func.bounds.lb + (func.bounds.ub - func.bounds.lb) * point for point in initial_points]\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            if self.budget <= 0:\n                return best_sample\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.2 * (ub - lb)), min(ub, x + 0.2 * (ub - lb)))\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Hybrid optimization: try Nelder-Mead first, then L-BFGS-B\n        res = minimize(objective, x0=best_sample, method='Nelder-Mead', options={'maxfev': int(self.budget * 0.3), 'xatol': 1e-8})\n        if not res.success:\n            res = minimize(objective, x0=res.x, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.6), 'ftol': 1e-8})\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Improved exploration through Sobol sampling and refined local optimization with a hybrid Nelder-Mead and L-BFGS-B approach.", "configspace": "", "generation": 20, "fitness": 0.8383166006372985, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.8406757212965992, 0.8363517322052142, 0.8379223484100826], "final_y": [3.732416628170493e-10, 4.290707729404221e-10, 5.339737120470221e-10]}, "mutation_prompt": null}
{"id": "8902ddf6-50aa-4f5e-9158-778f543c4351", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 6)  # Increase initial sample count for better exploration\n        \n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        mid_point = [(lb + ub) / 2 for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        res = minimize(objective, x0=mid_point, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Introducing a hybrid approach by combining local optimization with a strategic midpoint refinement for improved convergence.", "configspace": "", "generation": 20, "fitness": 0.8778201740208639, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.9794590068989777, 0.8274311918674435, 0.8265703232961703], "final_y": [0.0, 4.031483671505678e-09, 1.8395684711581735e-09]}, "mutation_prompt": null}
{"id": "38a72478-9554-473d-978e-20f24cb31b67", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 5)  # Adjusted initial sample count for improved exploration\n\n        # Randomly sample initial points within bounds\n        initial_samples = []\n        for _ in range(initial_sample_count):\n            sample = np.array([\n                np.random.uniform(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)\n            ])\n            initial_samples.append(sample)\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.85), 'ftol': 1e-8})  # Adjusted budget utilization\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Refined adaptive sampling strategy and convergence parameters for enhanced efficiency in smooth landscapes.", "configspace": "", "generation": 20, "fitness": 0.7715135737492828, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.082. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.6558604221017419, 0.8287914359493489, 0.8298888631967576], "final_y": [1.871740648356173e-07, 2.7938568425210154e-09, 2.076942606645038e-09]}, "mutation_prompt": null}
{"id": "43d0dd6b-9e01-41ed-b0dd-7824f5913fd5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass EnhancedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        initial_sample_count = max(12, self.budget // 6)  # Increase initial sample count for better exploration\n        \n        # Generate initial points using Sobol sequence for better exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        sobol_samples = sampler.random_base2(m=int(np.log2(initial_sample_count)))\n        initial_samples = [\n            func.bounds.lb + sample * (func.bounds.ub - func.bounds.lb)\n            for sample in sobol_samples\n        ]\n\n        # Evaluate initial samples and find the best one\n        best_sample = None\n        best_value = float('inf')\n        for sample in initial_samples:\n            value = func(sample)\n            self.budget -= 1\n            if value < best_value:\n                best_value = value\n                best_sample = sample\n            if self.budget <= 0:\n                return best_sample\n\n        # Narrow bounds around the best initial sample\n        bounds = [(max(lb, x - 0.1 * (ub - lb)), min(ub, x + 0.1 * (ub - lb)))  # Adjust bounds less aggressively\n                  for x, lb, ub in zip(best_sample, func.bounds.lb, func.bounds.ub)]\n\n        # Define the objective function for the local optimizer\n        def objective(x):\n            return func(x)\n\n        # Use L-BFGS-B for local optimization with adaptive options\n        res = minimize(objective, x0=best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': int(self.budget * 0.9), 'ftol': 1e-8})  # Adjusted budget and tolerance\n\n        if res.success:\n            return res.x\n        else:\n            return best_sample  # Fallback if optimization fails", "name": "EnhancedMetaheuristicOptimizer", "description": "Enhanced initial sampling by introducing quasi-random Sobol sequences for improved exploration.", "configspace": "", "generation": 20, "fitness": 0.8851805682239576, "feedback": "The algorithm EnhancedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.885 with standard deviation 0.068. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1480b7ef-43e2-445a-b398-c22296a43903", "metadata": {"aucs": [0.9810091195648313, 0.8357790144579349, 0.8387535706491066], "final_y": [0.0, 3.397627287318593e-09, 1.3908173434016826e-09]}, "mutation_prompt": null}
