{"id": "b0162038-67cf-49cb-ad17-b9e6de258f79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, 0, 1)  # Ensure modular constraints\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_search(self, candidate, func):\n        res = minimize(func, candidate, bounds=func.bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                trial = lb + trial * (ub - lb)  # Scale back to actual bounds\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func)\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "SEDE", "description": "A Symmetry-Enhanced Differential Evolution (SEDE) algorithm that utilizes quasi-oppositional initialization, modular mutation, and local optimization to efficiently explore and exploit the search space for maximizing reflectivity in multilayer photonic structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 68, in __call__\n  File \"<string>\", line 31, in local_search\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nTypeError: zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds\n.", "error": "TypeError('zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 68, in __call__\n  File \"<string>\", line 31, in local_search\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nTypeError: zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "fc38ba05-8179-489d-815f-87117e8f7e12", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, 0, 1)  # Ensure modular constraints\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_search(self, candidate, func):\n        bounds = [(low, high) for low, high in zip(func.bounds.lb, func.bounds.ub)]\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                trial = lb + trial * (ub - lb)  # Scale back to actual bounds\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func)\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "SEDE", "description": "Symmetry-Enhanced Differential Evolution (SEDE) algorithm with a refined local search to address bounds handling for more robust photonic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.9356994432274456, "feedback": "The algorithm SEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.001. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b0162038-67cf-49cb-ad17-b9e6de258f79", "metadata": {"aucs": [0.937714985747934, 0.9347147728892219, 0.9346685710451809], "final_y": [0.181887594931183, 0.1818797723782697, 0.18188202218259308]}, "mutation_prompt": null}
{"id": "c9c70f62-b92d-43f8-a06d-8f678ee92477", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, 0, 1)  # Ensure modular constraints\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_search(self, candidate, func):\n        res = minimize(func, candidate, bounds=[(l, u) for l, u in zip(func.bounds.lb, func.bounds.ub)], method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                trial = lb + trial * (ub - lb)  # Scale back to actual bounds\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func)\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "SEDE", "description": "Enhanced SEDE with adjusted handling of bounds in local search for improved compatibility and performance.", "configspace": "", "generation": 1, "fitness": 0.8725291018872564, "feedback": "The algorithm SEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.090. And the mean value of best solutions found was 0.207 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "b0162038-67cf-49cb-ad17-b9e6de258f79", "metadata": {"aucs": [0.937714985747934, 0.9346142568906838, 0.7452580630231511], "final_y": [0.181887594931183, 0.1818797723782697, 0.2578124253687365]}, "mutation_prompt": null}
{"id": "4fda19da-21a4-4831-beb6-bbc9063412ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass E_SEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, 0, 1)  # Ensure modular constraints\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def periodicity_penalty(self, solution):\n        period_length = 2\n        penalty = 0\n        for i in range(self.dim - period_length):\n            penalty += np.sum(np.abs(solution[i:i+period_length] - solution[i+period_length:i+2*period_length]))\n        return penalty\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) + self.periodicity_penalty(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                trial = lb + trial * (ub - lb)  # Scale back to actual bounds\n\n                trial_fitness = func(trial) + self.periodicity_penalty(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, bounds)\n\n                if local_fitness + self.periodicity_penalty(best_candidate) < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness + self.periodicity_penalty(best_candidate)\n                    budget_spent += 1\n                    self.history.append(local_fitness + self.periodicity_penalty(best_candidate))\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "E_SEDE", "description": "An Enhanced Symmetry-Enhanced Differential Evolution (E-SEDE) algorithm that addresses the existing issues by correctly handling bounds in local optimization and introducing a periodicity encouragement mechanism for improving the structural design of multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": 0.48223737224171975, "feedback": "The algorithm E_SEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.482 with standard deviation 0.126. And the mean value of best solutions found was 0.429 (0. is the best) with standard deviation 0.095.", "error": "", "parent_id": "b0162038-67cf-49cb-ad17-b9e6de258f79", "metadata": {"aucs": [0.6418869110903805, 0.33513677613680815, 0.4696884294979706], "final_y": [0.31342984217431435, 0.5465460285937667, 0.4278468661738024]}, "mutation_prompt": null}
{"id": "b29f7127-9cb4-40e1-87ff-b5fca3f6dcf6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, 0, 1)  # Ensure modular constraints\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_search(self, candidate, func):\n        bounds = [(l, u) for l, u in zip(func.bounds.lb, func.bounds.ub)]  # Fixed bounds handling\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')  # Correct bounds\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                trial = lb + trial * (ub - lb)  # Scale back to actual bounds\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func)\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "SEDE", "description": "A refined Symmetry-Enhanced Differential Evolution (SEDE) algorithm that fixes bound handling in local search, preserving its modular and adaptive search capabilities for maximizing reflectivity in multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": 0.8724414485532334, "feedback": "The algorithm SEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.090. And the mean value of best solutions found was 0.207 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "b0162038-67cf-49cb-ad17-b9e6de258f79", "metadata": {"aucs": [0.937714985747934, 0.934429179741845, 0.7451801801699207], "final_y": [0.181887594931183, 0.1818797723782697, 0.2578124253687365]}, "mutation_prompt": null}
{"id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity by averaging adjacent elements\n        trial = (trial + np.roll(trial, 1)) / 2\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "An Enhanced Symmetry-Enhanced Differential Evolution (ESEDE) algorithm using quasi-oppositional initialization, modular mutation, periodicity-promoting crossover, and robust local search to optimize multilayer photonic structures for maximum reflectivity.", "configspace": "", "generation": 1, "fitness": 0.9606371370161019, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.961 with standard deviation 0.006. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b0162038-67cf-49cb-ad17-b9e6de258f79", "metadata": {"aucs": [0.9683605949719295, 0.9552629779477949, 0.9582878381285813], "final_y": [0.16486175946207748, 0.1648584144149403, 0.16485843438099257]}, "mutation_prompt": null}
{"id": "62cefef0-9d2f-4eba-8897-fd672952709c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity and average adjacent elements\n        trial = (trial + np.roll(trial, -1)) / 2  # Changed line for better periodicity\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "Improved ESEDE by adjusting crossover to better maintain periodicity in multilayer designs.", "configspace": "", "generation": 2, "fitness": 0.948428166036431, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.948 with standard deviation 0.014. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.967018236795667, 0.9346142568906838, 0.943652004422942], "final_y": [0.16485717493626872, 0.1818797723782697, 0.1648559340826583]}, "mutation_prompt": null}
{"id": "bcb0bea3-5309-441b-a537-5e57e6d26105", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity by averaging adjacent elements\n        trial = (trial + np.roll(trial, 1)) / 2\n        trial = np.sin(trial)  # Hybrid crossover strategy: adding sine-based perturbations\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "A refined ESEDE algorithm enhancing exploitation by integrating hybrid crossover strategies to optimize multilayer photonic structures for maximum reflectivity.", "configspace": "", "generation": 2, "fitness": 0.9536076716497384, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.937714985747934, 0.9346142568906838, 0.9884937723105971], "final_y": [0.181887594931183, 0.1818797723782697, 0.1648571803346487]}, "mutation_prompt": null}
{"id": "259a88c3-aaf5-41bb-a3df-38128db80d19", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity by averaging adjacent elements\n        trial = (trial + np.roll(trial, 1)) / 2\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='TNC')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "Enhanced Symmetry-Enhanced Differential Evolution (ESEDE) algorithm with a refined local search strategy for improved optimization of multilayer photonic structures.", "configspace": "", "generation": 2, "fitness": 0.9346548112138825, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.004. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.9309959199022318, 0.9406223184666503, 0.9323461952727655], "final_y": [0.18187814594421103, 0.16485747581173915, 0.18188013199538955]}, "mutation_prompt": null}
{"id": "31467219-3d07-42b1-8917-43337bb372a4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity by averaging adjacent elements\n        trial = (trial + np.roll(trial, 1)) / 2\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                adaptive_F = 0.4 + 0.6 * (1 - budget_spent / self.budget)  # Adapt F based on budget\n                mutant = self.mutate(i, pop, F=adaptive_F)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "Enhanced ESEDE algorithm through adaptive mutation factor to dynamically balance exploration and exploitation.", "configspace": "", "generation": 2, "fitness": 0.9589175264138525, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.959 with standard deviation 0.023. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.937714985747934, 0.9908998196526823, 0.9481377738409416], "final_y": [0.181887594931183, 0.16485594350114163, 0.16486370409128104]}, "mutation_prompt": null}
{"id": "3e762b4a-97a3-481f-9058-c736b1e97f14", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        # Adaptive mutation scaling based on population diversity\n        F = 0.5 + 0.3 * (np.std(pop) / np.mean(pop)) \n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Weighted average to promote periodicity\n        trial = 0.7 * trial + 0.3 * np.roll(trial, 1)\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "A refined ESEDE algorithm with adaptive mutation scaling and enhanced crossover to improve exploration while respecting periodicity for optimizing multilayer photonic structures.", "configspace": "", "generation": 2, "fitness": 0.9437333217787579, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.944 with standard deviation 0.011. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.937714985747934, 0.9347147728892219, 0.958770206699118], "final_y": [0.181887594931183, 0.1818797723782697, 0.16486084101892395]}, "mutation_prompt": null}
{"id": "b5f44677-4189-48ce-ac81-29f47ce2d4e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity by averaging adjacent elements\n        trial = (trial + np.roll(trial, 1)) / 2\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='Powell')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "A refined ESEDE algorithm enhancing local search performance by switching to a more precise 'Powell' optimization method for maximum reflectivity in multilayer photonic structures.", "configspace": "", "generation": 3, "fitness": 0.9800591392961574, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.9855018595912398, 0.9767350773399341, 0.9779404809572982], "final_y": [0.164855771904728, 0.1648557719047039, 0.16485577190469847]}, "mutation_prompt": null}
{"id": "c95b59b7-ad92-44ba-b906-e69cb54072be", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity by averaging adjacent elements\n        trial = (trial + np.roll(trial, 1)) / 2\n\n        # Sinusoidal adjustment for enhanced periodicity\n        trial = trial + 0.1 * np.sin(2 * np.pi * np.arange(self.dim) / self.dim)\n\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "Improved crossover strategy by incorporating periodic sinusoidal adjustments to further enhance solution periodicity and performance.", "configspace": "", "generation": 3, "fitness": 0.9732050975192826, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.973 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.9710742885287177, 0.9601884331198182, 0.9883525709093121], "final_y": [0.16486291010485643, 0.1648564754081202, 0.16485589703860715]}, "mutation_prompt": null}
{"id": "d97571ff-5af9-4ab5-aa04-08ac68e9157c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def crossover(self, target, mutant, CR=0.9):\n        if len(self.history) > 2 and self.history[-1] > self.history[-2]:  # Adaptive CR tuning\n            CR *= 0.95\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity by averaging adjacent elements\n        trial = (trial + np.roll(trial, 1)) / 2\n        return trial\n\n    def local_search(self, candidate, func, bounds):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop)\n                trial = self.crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "ESEDE", "description": "A refined Enhanced Symmetry-Enhanced Differential Evolution (ESEDE) algorithm incorporating adaptive crossover rate tuning to dynamically balance exploration and exploitation for optimized multilayer photonic structures.", "configspace": "", "generation": 3, "fitness": 0.9631001842458021, "feedback": "The algorithm ESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.969059464013307, 0.9622215003256486, 0.9580195883984511], "final_y": [0.16485709509171464, 0.16485800317316213, 0.16485845802835764]}, "mutation_prompt": null}
{"id": "31a9096d-40f7-44a8-95a7-0af373aa2efe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def adaptive_mutate(self, target_idx, pop, F_min=0.4, F_max=0.9):\n        F = np.random.uniform(F_min, F_max)\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def periodic_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Promote periodicity with enhanced averaging\n        trial = (0.5 * trial + 0.5 * np.roll(trial, 1))\n        return trial\n\n    def targeted_local_search(self, candidate, func, bounds, max_iter=10):\n        res = minimize(func, candidate, bounds=bounds, method='L-BFGS-B', options={'maxiter': max_iter})\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)  # Scale population to actual bounds\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.adaptive_mutate(i, pop)\n                trial = self.periodic_crossover(pop[i], mutant)\n\n                # Ensure trial is within bounds\n                trial = np.clip(trial, lb, ub)\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            # Local search on the best candidate\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.targeted_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "AESEDE", "description": "An Adaptive Enhanced Symmetry-Enhanced Differential Evolution (AESEDE) algorithm that integrates adaptive parameter control, enhanced periodicity promotion, and targeted local search to optimize multilayer photonic structures for maximum reflectivity.", "configspace": "", "generation": 3, "fitness": 0.9543482723610563, "feedback": "The algorithm AESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.954 with standard deviation 0.033. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.9602722586001393, 0.9915429627338397, 0.91122959574919], "final_y": [0.16485947111532406, 0.1648799206334426, 0.16485956729157647]}, "mutation_prompt": null}
{"id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "A Reflectivity-Enhanced Symmetry-Enhanced Differential Evolution (RESEDE) algorithm that incorporates Fourier-domain constraints for periodicity, stochastic local search, and adaptive crossover/mutation rates to optimize multilayer photonic structures for maximum reflectivity.", "configspace": "", "generation": 3, "fitness": 0.9923298809143071, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "449e272e-5337-42ce-b18c-bc06f8fadd9a", "metadata": {"aucs": [0.9921972749960315, 0.9922895759442403, 0.9925027918026496], "final_y": [0.1648561688955238, 0.16485611046225945, 0.16485581502616808]}, "mutation_prompt": null}
{"id": "0667a6c6-66c1-4673-b843-50c3e810525f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 3, 0, transformed)  # Slightly optimized constraint\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "An enhanced RESEDE algorithm with optimized Fourier constraint tuning for improved reflectivity optimization.", "configspace": "", "generation": 4, "fitness": 0.9892239130279586, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.989 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9920053062401089, 0.9835648583815926, 0.9921015744621745], "final_y": [0.1648557938804921, 0.16485598603856866, 0.16485599839295784]}, "mutation_prompt": null}
{"id": "2782bc04-44c1-46ac-b352-1767eaf8f1ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        F = np.random.uniform(0.4, 0.9)  # Dynamic mutation factor\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "Enhanced RESEDE algorithm by introducing a dynamic mutation factor to balance exploration and exploitation adaptively.", "configspace": "", "generation": 4, "fitness": 0.9916319849040368, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9889844466787345, 0.9924738118136229, 0.9934376962197531], "final_y": [0.16485586118147622, 0.16485621882460288, 0.16485607058595553]}, "mutation_prompt": null}
{"id": "6ff8cd9a-9b2c-459b-a415-6a73eb30f87a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * ((b - c) * np.random.uniform(0.5, 1.5))  # Adjusted mutation strategy\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "The Reflectivity-Enhanced Symmetry-Enhanced Differential Evolution (RESEDE) algorithm is improved by adjusting the mutation strategy to incorporate a weighted difference vector, enhancing exploration capabilities.", "configspace": "", "generation": 4, "fitness": 0.9920156592579183, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9929292502510695, 0.9902573616602814, 0.9928603658624042], "final_y": [0.16485636080947108, 0.16485784291689898, 0.16485701550802534]}, "mutation_prompt": null}
{"id": "8fb85c98-b6a5-4cd2-b6fd-e5ecee747efb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.95):  # Changed CR from 0.9 to 0.95\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "Enhanced the algorithm by tweaking the adaptive crossover rate to improve exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.9907917184693336, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9919513217858874, 0.991387119471426, 0.9890367141506873], "final_y": [0.16485598236262955, 0.16485637153232569, 0.16485666137575494]}, "mutation_prompt": null}
{"id": "242c6e0f-de75-4891-ab67-2a31686394f1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                diversity_metric = np.mean([np.dot(trial, other) / (np.linalg.norm(trial) * np.linalg.norm(other)) for other in pop])\n                \n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness + diversity_metric)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "Enhanced exploration by introducing cosine similarity-based diversity maintenance in RESEDE.", "configspace": "", "generation": 4, "fitness": 0.9916561772956674, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9915740647024497, 0.9913931751818285, 0.9920012920027238], "final_y": [0.1648574208244623, 0.16485601026171803, 0.16485587995611206]}, "mutation_prompt": null}
{"id": "189be7ba-41be-4040-b08c-56b5b94eb986", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 3, 0, transformed)  # Adjusted filter\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "An enhanced RESEDE algorithm with improved periodicity promotion by adjusting the Fourier-domain filtering strategy.", "configspace": "", "generation": 5, "fitness": 0.8905936383395621, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.105. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.992005305198887, 0.9346100600826968, 0.7451655497371025], "final_y": [0.1648557945823983, 0.18187810518571979, 0.25781012211043675]}, "mutation_prompt": null}
{"id": "85f7bd54-fa03-423b-b28b-f0a70b89fb79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        improvement_factor = max(0.1, 1.0 - (self.history[-1] / (self.history[-1] + 1e-9)))  # Adaptive mutation\n        mutant = a + F * improvement_factor * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "Enhance reflectivity optimization by adapting mutation strength based on fitness improvement to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.8906897002665092, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.106. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9922538574409521, 0.9346455208978715, 0.7451697224607037], "final_y": [0.16485633508547115, 0.18187810420834616, 0.257810120913172]}, "mutation_prompt": null}
{"id": "af728aa8-784f-4327-93cc-50521e3520ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filter_limit = int(len(transformed) * min(0.25, 0.05 + 0.2 * (1 - np.var(self.history[-10:]))))\n        filtered = np.where(np.arange(len(transformed)) > filter_limit, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "An enhanced version of RESEDE that adjusts the Fourier constraint to dynamically adapt based on convergence history for improved optimization performance.", "configspace": "", "generation": 5, "fitness": 0.8912749354756295, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.106. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9939989606583725, 0.9346466815854154, 0.7451791641831007], "final_y": [0.16485716297220787, 0.1818781096446973, 0.2578101046594975]}, "mutation_prompt": null}
{"id": "de6f2c8d-0253-485c-993a-1cd7177c3a0b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n                trial = np.sin(trial)  # Periodicity enhancement with trigonometric function\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "Enhancing RESEDE by incorporating an additional periodicity enhancement step with trigonometric transformation to boost convergence and performance.", "configspace": "", "generation": 5, "fitness": 0.8095078392195649, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.092. And the mean value of best solutions found was 0.232 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9392545095117218, 0.7440986686920881, 0.7451703394548852], "final_y": [0.1818781036599244, 0.25781013483345583, 0.2578101070075304]}, "mutation_prompt": null}
{"id": "692a96a0-4aba-477a-bf5b-ccf947223f5c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        dynamic_scale = 0.01 + (0.05 - 0.01) * np.random.rand()  # Line changed\n        perturbation = np.random.normal(0, dynamic_scale, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "Enhanced RESEDE incorporating dynamic perturbation scale in stochastic local search for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.8875415906903331, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.888 with standard deviation 0.103. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9828289754660215, 0.934606581475275, 0.7451892151297026], "final_y": [0.1648568980369135, 0.18187809999030458, 0.25781009157468004]}, "mutation_prompt": null}
{"id": "8d290718-9f55-4b79-95fb-4d6eeacdbf61", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        # Perform a simple wavelet transformation for periodicity\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "A Wavelet-Driven Adaptive Particle Swarm Optimization (WDAPSO) algorithm that combines wavelet transformations to encourage periodicity in candidates, adaptive learning factors for global and local exploration, and iterative refinement for optimizing multilayer photonic structures.", "configspace": "", "generation": 6, "fitness": 0.9960296135633634, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9960239704438122, 0.9960740316606337, 0.9959908385856442], "final_y": [0.1648558637122255, 0.16485628692059195, 0.16485604025042355]}, "mutation_prompt": null}
{"id": "1235ed90-1604-4cc9-95b4-238d77fc0113", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        # Dynamic mutation factor adjustment\n        convergence_factor = np.std(pop) / (np.ptp(pop) + 1e-9)\n        F = convergence_factor * F\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "Enhanced RESEDE algorithm by balancing exploration and exploitation through dynamic mutation factor adjustment based on population convergence.", "configspace": "", "generation": 6, "fitness": 0.9105175669655267, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.911 with standard deviation 0.117. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9932584778718068, 0.9931365363476348, 0.7451576866771387], "final_y": [0.1648567484346577, 0.1648559208260596, 0.2578101012281686]}, "mutation_prompt": null}
{"id": "44959610-9ee3-4711-8eb9-8da9cd6cc8f5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.abs(transformed) < np.max(np.abs(transformed)) * 0.2, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "Enhancing RESEDE by modifying Fourier-domain constraints for improved reflectivity optimization.", "configspace": "", "generation": 6, "fitness": 0.8909846976041979, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.106. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9931698317587807, 0.934595021649807, 0.7451892394040058], "final_y": [0.16485647437462514, 0.181878105846224, 0.25781008519031234]}, "mutation_prompt": null}
{"id": "5772ea16-9dc8-4e91-991d-143e960797c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        threshold = self.dim // 4 - np.random.randint(0, 2)  # Self-adaptive threshold\n        filtered = np.where(np.arange(len(transformed)) > threshold, 0, transformed)\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim) * np.random.uniform(0.8, 1.2)  # Dynamic scaling\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "RESEDE+ algorithm enhances RESEDE by incorporating a self-adaptive Fourier constraint threshold and dynamic perturbation scaling in local search for improved optimization. ", "configspace": "", "generation": 6, "fitness": 0.890750624132306, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.106. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9924496570342317, 0.9346143178258828, 0.7451878975368036], "final_y": [0.16485718729531995, 0.1818781036869651, 0.25781009492817053]}, "mutation_prompt": null}
{"id": "b9ca26f7-9c4f-42db-bcb8-164e94f448af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RESEDE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = np.random.uniform(low=lb, high=ub, size=(size, self.dim))\n        opp_pop = lb + ub - pop\n        combined_pop = np.vstack((pop, opp_pop))\n        return combined_pop\n\n    def mutate(self, target_idx, pop, F=0.5):\n        idxs = [idx for idx in range(len(pop)) if idx != target_idx]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + F * (b - c)\n        return mutant\n\n    def adaptive_crossover(self, target, mutant, CR=0.9):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        trial = (trial + np.roll(trial, 1)) / 2  # Promote periodicity\n        return trial\n\n    def fourier_constraint(self, candidate):\n        transformed = np.fft.fft(candidate)\n        filtered = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        filtered = (filtered + np.conj(filtered[::-1])) / 2  # Enforcing symmetry in Fourier domain\n        return np.real(np.fft.ifft(filtered))\n\n    def stochastic_local_search(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        pop = self.quasi_oppositional_initialization(scaled_lb, scaled_ub, pop_size)\n        pop = lb + pop * (ub - lb)\n\n        fitness = np.array([func(ind) for ind in pop])\n        self.history.extend(fitness)\n        budget_spent = len(pop)\n\n        while budget_spent < self.budget:\n            for i in range(pop_size):\n                if budget_spent >= self.budget:\n                    break\n\n                mutant = self.mutate(i, pop, F=np.random.uniform(0.5, 1.0))\n                trial = self.adaptive_crossover(pop[i], mutant, CR=np.random.uniform(0.7, 1.0))\n\n                trial = np.clip(trial, lb, ub)\n                trial = self.fourier_constraint(trial)  # Apply Fourier constraint\n\n                trial_fitness = func(trial)\n                budget_spent += 1\n                self.history.append(trial_fitness)\n\n                if trial_fitness < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = trial_fitness\n\n            if budget_spent < self.budget:\n                best_idx = np.argmin(fitness)\n                best_candidate, local_fitness = self.stochastic_local_search(pop[best_idx], func, list(zip(lb, ub)))\n\n                if local_fitness < fitness[best_idx]:\n                    pop[best_idx] = best_candidate\n                    fitness[best_idx] = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        best_idx = np.argmin(fitness)\n        return pop[best_idx], fitness[best_idx]", "name": "RESEDE", "description": "A modification to the RESEDE algorithm introducing an additional constraint to further enhance periodicity by enforcing symmetry in the Fourier domain.", "configspace": "", "generation": 6, "fitness": 0.8725023959376611, "feedback": "The algorithm RESEDE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.090. And the mean value of best solutions found was 0.207 (0. is the best) with standard deviation 0.036.", "error": "", "parent_id": "872f9f5b-a456-45e3-9a24-4a3d17fca854", "metadata": {"aucs": [0.9377394479435126, 0.9345984007010493, 0.7451693391684213], "final_y": [0.18187811298703072, 0.18187810536979176, 0.2578101130113857]}, "mutation_prompt": null}
{"id": "50814173-7918-48db-976e-0f08e1c44a80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An improved WDAPSO that enhances local search with adaptive wavelet refinement to increase precision and convergence speed.", "configspace": "", "generation": 7, "fitness": 0.9963995256209178, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8d290718-9f55-4b79-95fb-4d6eeacdbf61", "metadata": {"aucs": [0.9960234330163042, 0.9965265125482613, 0.996648631298188], "final_y": [0.16485596745319153, 0.16485595157228017, 0.16485599279765684]}, "mutation_prompt": null}
{"id": "22854a95-a12d-4aeb-9f95-59c7e1a065de", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        # Perform a simple wavelet transformation for periodicity\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=2.0, social=1.0):  # Modified line\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An enhanced version of WDAPSO by adjusting cognitive and social factors to adaptively enhance exploration and exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.9962871728522874, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8d290718-9f55-4b79-95fb-4d6eeacdbf61", "metadata": {"aucs": [0.9960938684480801, 0.9963315308505561, 0.9964361192582263], "final_y": [0.1648559353180763, 0.16485581361810908, 0.16485589008654178]}, "mutation_prompt": null}
{"id": "1363bd30-113d-47a6-abbb-3f3ca18bf8c4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        # Perform a simple wavelet transformation for periodicity\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * np.random.rand())  # Adaptive inertia factor\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "A refined WDAPSO algorithm integrating an adaptive inertia factor to enhance global and local search balance, thereby improving convergence in multilayer photonic structure optimization.", "configspace": "", "generation": 7, "fitness": 0.9962366437646221, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8d290718-9f55-4b79-95fb-4d6eeacdbf61", "metadata": {"aucs": [0.995824341359992, 0.996331827711281, 0.9965537622225928], "final_y": [0.16485582583953418, 0.1648560104961705, 0.16485590442130849]}, "mutation_prompt": null}
{"id": "bc6f4702-533b-47f6-ad21-72482cfd64b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        # Perform a simple wavelet transformation for periodicity\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.05, size=self.dim)  # Changed perturbation scale\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        if res.fun < func(candidate):  # Random restart policy\n            return res.x, res.fun\n        return candidate, func(candidate)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An enhanced WDAPSO algorithm with improved local refinement by adjusting perturbation scale and using random restart policy for escaping local optima.", "configspace": "", "generation": 7, "fitness": 0.9959809837679563, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8d290718-9f55-4b79-95fb-4d6eeacdbf61", "metadata": {"aucs": [0.9956478379815829, 0.9963319152280815, 0.9959631980942044], "final_y": [0.16485592029920904, 0.16485601667396987, 0.16485595240517947]}, "mutation_prompt": null}
{"id": "1cfba6a6-5a0b-4e84-b861-b2c8d62e7104", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EWDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        return res.x, res.fun\n\n    def chaotic_exploration(self, particle_position, lb, ub):\n        # Apply a chaotic map (e.g., logistic map) for exploration\n        r = 4.0  # Parameter for logistic map\n        chaos = np.random.rand(self.dim)\n        chaos = r * chaos * (1 - chaos)\n        chaotic_step = lb + chaos * (ub - lb)\n        return np.clip(particle_position + chaotic_step, lb, ub)\n\n    def adaptive_differential_mutation(self, particle, global_best, F=0.5):\n        # Differential mutation with adaptive factor F\n        indices = np.random.choice(len(particle[\"position\"]), 3, replace=False)\n        mutated = particle[\"position\"].copy()\n        mutated[indices[0]] = particle[\"position\"][indices[0]] + F * (particle[\"position\"][indices[1]] - particle[\"position\"][indices[2]])\n        return np.clip(mutated, -1, 1)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                # Apply chaotic exploration\n                p[\"position\"] = self.chaotic_exploration(p[\"position\"], lb, ub)\n\n                # Apply differential mutation\n                p[\"position\"] = self.adaptive_differential_mutation(p, global_best_position)\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "EWDAPSO", "description": "The Enhanced Wavelet-Driven Adaptive Particle Swarm Optimization (EWDAPSO) integrates a frequency-domain filtering approach with an adaptive differential mutation operator and chaos-induced exploration to improve convergence and solution quality for optimizing multilayer photonic structures.", "configspace": "", "generation": 7, "fitness": 0.890211183573597, "feedback": "The algorithm EWDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.104. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "8d290718-9f55-4b79-95fb-4d6eeacdbf61", "metadata": {"aucs": [0.9364518026131844, 0.9876995809974543, 0.7464821671101521], "final_y": [0.18188041953771084, 0.1648565671610449, 0.25781025009014413]}, "mutation_prompt": null}
{"id": "e681d767-0da9-44b8-8452-5b03e6f4f629", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 3, 0, transformed)  # Adjusted low-pass threshold\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "A refined WDAPSO algorithm with enhanced wavelet transformation to further boost convergence by better preserving periodicity.", "configspace": "", "generation": 8, "fitness": 0.9954774844050212, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9959796798602235, 0.9940652730250743, 0.9963875003297658], "final_y": [0.1648560120216015, 0.1648559831348344, 0.16485604109737195]}, "mutation_prompt": null}
{"id": "cfdd53ec-afdc-4946-ab45-4395e1719fff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4 * 0.9, 0, transformed)  # Fine-tuned low pass\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by fine-tuning the wavelet refinement process with an adaptive bandwidth parameter.", "configspace": "", "generation": 8, "fitness": 0.9945764755922298, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9961004704583922, 0.9908904298185729, 0.9967385264997243], "final_y": [0.16485600067014317, 0.1648559593382456, 0.16485615165509426]}, "mutation_prompt": null}
{"id": "d1d5a5ee-9283-419e-b088-dd7888d51b80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.2, social=1.8):  # Adjusted factor values\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Refined WDAPSO by adjusting the cognitive and social factors dynamically for improved adaptability.", "configspace": "", "generation": 8, "fitness": 0.9956433219945904, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960065609574049, 0.994177768589157, 0.9967456364372091], "final_y": [0.16485599685545105, 0.16485638632421806, 0.16485588980449284]}, "mutation_prompt": null}
{"id": "dd8de430-59ad-4b0b-8b99-b3a8f75a2aa2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 3, 0, transformed)  # Changed from dim // 4\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.6, cognitive=1.5, social=1.5):  # Changed inertia from 0.5\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by modifying wavelet low-pass filter and refining velocity adaptive factors.", "configspace": "", "generation": 8, "fitness": 0.9963645458667649, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9962898026166321, 0.9960653124978944, 0.9967385224857679], "final_y": [0.16485586854821055, 0.16485583117008407, 0.16485591446683623]}, "mutation_prompt": null}
{"id": "13ae34d8-d59b-4a9d-9bd4-43f11ea667c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia *= (0.9 - 0.4) * (self.budget - len(self.history)) / self.budget + 0.4  # Non-linear inertia reduction\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.random.uniform(scaled_lb - 0.1, scaled_ub + 0.1, self.dim),  # Bounded random initial velocity\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhancing WDAPSO by refining velocity updating with non-linear inertia and bounded mutation for improved diversity.", "configspace": "", "generation": 8, "fitness": 0.9879774860578466, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9949295160171044, 0.972249314703767, 0.9967536274526683], "final_y": [0.1648559647905462, 0.16485592316316722, 0.16485611511331344]}, "mutation_prompt": null}
{"id": "777d8749-67f4-49f4-9c48-bc58855b9d59", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nimport pywt  # Added for multi-scale wavelet analysis\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        coeffs = pywt.wavedec(candidate, 'db1', level=2)  # Use multi-level wavelet\n        coeffs[1:] = [np.zeros_like(c) for c in coeffs[1:]]  # Zero detail coeffs for smoothing\n        transformed = pywt.waverec(coeffs, 'db1')  # Inverse transform\n        return transformed\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced adaptive wavelet refinement using multi-scale wavelet analysis to boost local search precision and convergence speed.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'pywt'\").", "error": "ModuleNotFoundError(\"No module named 'pywt'\")", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {}, "mutation_prompt": null}
{"id": "b59d60ec-1910-4302-8982-c8859cbb0e7c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.005, size=self.dim)  # Changed noise scale from 0.01 to 0.005\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhance wavelet refinement by replacing the perturbation with a smaller, adaptive noise scale.", "configspace": "", "generation": 9, "fitness": 0.9960515012763427, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9959928654707466, 0.9964155285768181, 0.995746109781463], "final_y": [0.16485600853349747, 0.16485587650345956, 0.1648560295513124]}, "mutation_prompt": null}
{"id": "92cb46a0-750e-4e65-8b67-715c8b440eb6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 5, 0, transformed)  # Change from self.dim // 4 to self.dim // 5\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Minor adjustment in the wavelet transformation for enhanced periodicity in position updates.", "configspace": "", "generation": 9, "fitness": 0.995919738425148, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960233982368223, 0.9964885816309259, 0.9952472354076959], "final_y": [0.16485631222740216, 0.16485598619987396, 0.16485634601268917]}, "mutation_prompt": null}
{"id": "613c7706-80e1-4c0d-98d0-1339d0f8a05f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.9 - 0.4) * (len(self.history) / self.budget)  # Dynamic inertia adjustment\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An improved WDAPSO with dynamic inertia adjustment to maintain balance between global exploration and local exploitation.", "configspace": "", "generation": 9, "fitness": 0.9962532028841388, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.996023149147605, 0.9965144738873473, 0.9962219856174643], "final_y": [0.16485600146247537, 0.16485606791751384, 0.16485616307191475]}, "mutation_prompt": null}
{"id": "08ff2534-c2f6-4a6b-812e-f3c472b86320", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.9 - 0.4) * (len(self.history) / self.budget)  # Adaptive inertia weight\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Introduced an adaptive inertia weight strategy to enhance convergence speed and avoid local minima.", "configspace": "", "generation": 9, "fitness": 0.996358283728093, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9963259261747636, 0.9963170827719344, 0.9964318422375812], "final_y": [0.16485590077625856, 0.1648562929349464, 0.16485595447009704]}, "mutation_prompt": null}
{"id": "adbf7059-0bdb-4ca6-976b-66a0fa25d262", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"]) + np.random.normal(0, 0.001, size=self.dim)  # Small perturbation\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Introduce a small random perturbation in the wavelet-transformed position to further enhance exploration near local optima.", "configspace": "", "generation": 10, "fitness": 0.995862982161798, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9949598130341941, 0.9961439442778477, 0.9964851891733523], "final_y": [0.16485602996912174, 0.16485582530816634, 0.16485613781050723]}, "mutation_prompt": null}
{"id": "cd63a7fb-7973-4b12-b082-9f6a67fe277d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 3, 0, transformed)  # Optimized wavelet cutoff\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by optimizing wavelet cutoff for improved balance between exploration and exploitation.", "configspace": "", "generation": 10, "fitness": 0.9963969177348614, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9962039014382458, 0.9965590360659458, 0.9964278157003926], "final_y": [0.1648560462778147, 0.16485602554845113, 0.16485600483176055]}, "mutation_prompt": null}
{"id": "77b9561e-336a-4d22-8ba4-b28beaa13084", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5, iter_count=1, max_iter=100):\n        r1, r2 = np.random.rand(), np.random.rand()\n        cognitive = cognitive * (1 - iter_count / max_iter)  # Adjust cognitive factor dynamically\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        iter_count = 0\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, iter_count=iter_count, max_iter=self.budget)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n            iter_count += 1\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Improved WDAPSO by adjusting cognitive factor dynamically based on iteration count for better convergence.", "configspace": "", "generation": 10, "fitness": 0.9963671670150559, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960241740372908, 0.9966250762316655, 0.9964522507762115], "final_y": [0.16485592975658048, 0.1648558963207305, 0.16485632903255176]}, "mutation_prompt": null}
{"id": "57bd07e6-2793-4d9f-8bd9-fc522a5704bf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        # Introduce adaptive threshold based on candidate standard deviation\n        threshold = np.std(transformed) / 2  # Changed line\n        low_pass = np.where(np.abs(transformed) < threshold, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Introducing an adaptive wavelet threshold for refining the wavelet transformation, enhancing the periodical solution refinement.", "configspace": "", "generation": 10, "fitness": 0.9955610810641282, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9961594029258416, 0.9965469130311134, 0.9939769272354299], "final_y": [0.16485602423620171, 0.16485589207596996, 0.1648559768045924]}, "mutation_prompt": null}
{"id": "8425b3c8-865a-4329-b9a7-29cd185c9460", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                # Adaptive inertia adjustment\n                inertia = 0.4 + 0.6 * (1 - budget_spent / self.budget)\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhance WDAPSO by introducing adaptive inertia adjustment to intensify exploration and exploitation balance for faster convergence.", "configspace": "", "generation": 10, "fitness": 0.9962542514449794, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9961494615391819, 0.9964422993244636, 0.9961709934712925], "final_y": [0.16485586990549284, 0.1648558366024947, 0.16485606577848144]}, "mutation_prompt": null}
{"id": "566d3803-2aa2-4eee-a186-38a8a1c00275", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.001, size=self.dim)  # Changed from 0.01 to 0.001\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced wavelet refinement with a smaller perturbation in stochastic refinement for better local search precision.", "configspace": "", "generation": 11, "fitness": 0.9962667166719742, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9959745225933716, 0.9963284066115602, 0.9964972208109908], "final_y": [0.16485637006551268, 0.16485603072122668, 0.16485587709567506]}, "mutation_prompt": null}
{"id": "f3c760a4-7e06-4153-b229-c11648e413ef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                inertia = 0.9 - (0.5 * (budget_spent / self.budget))  # Dynamic inertia\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia=inertia)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n                p[\"position\"] = lb + (p[\"position\"] - lb) % (ub - lb)  # Periodic boundary handling\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by incorporating periodic boundary handling and dynamic inertia to improve exploration and convergence.", "configspace": "", "generation": 11, "fitness": 0.9963851136786358, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960226158186043, 0.9966907328265585, 0.9964419923907446], "final_y": [0.16485607519801493, 0.16485613328360327, 0.1648558814311376]}, "mutation_prompt": null}
{"id": "6d29d920-e381-4b3d-92d3-ad61197118ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate, refinement_factor=0.5):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim * refinement_factor, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        adaptive_inertia = inertia * (1 - len(self.history) / self.budget)  # Adaptive inertia\n        velocity = (\n            adaptive_inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x, refinement_factor=0.3)  # Dynamic refinement factor\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO with adaptive inertia and dynamic wavelet refinement for improved convergence and exploration.", "configspace": "", "generation": 11, "fitness": 0.9961093626652886, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9954730933324396, 0.996427081295119, 0.9964279133683072], "final_y": [0.16485590558292684, 0.16485605040854912, 0.16485598971584325]}, "mutation_prompt": null}
{"id": "8c888c4b-2a45-4249-a05e-8a7ecb85a650", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x) + np.random.normal(0, 0.005, size=self.dim)  # Enhanced local search with wavelet refinement and noise\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Modify the stochastic refinement to increase diversity by adding random noise to the refined candidate.", "configspace": "", "generation": 11, "fitness": 0.9962561963173204, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960231343991823, 0.9963274368860996, 0.9964180176666797], "final_y": [0.16485597342585634, 0.16485604436147516, 0.16485584267424558]}, "mutation_prompt": null}
{"id": "41d22e57-c138-4601-acde-f1534ab22bd0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        # Mutation-like operation added to increase diversity\n        mutation = np.random.uniform(-0.01, 0.01, size=self.dim)\n        perturbed_candidate += mutation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced wavelet refinement by adding a mutation-like operation to increase exploration diversity.", "configspace": "", "generation": 11, "fitness": 0.9960056341209724, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9952184130034368, 0.9965073850678892, 0.9962911042915911], "final_y": [0.16485587447144, 0.16485591837895341, 0.16485584648585005]}, "mutation_prompt": null}
{"id": "a680c4fe-868f-4bfb-961d-7ccfb17a9034", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.6, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = self.wavelet_transform(np.random.normal(0, 0.01, size=self.dim))\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO with wavelet-based perturbation and adaptive inertia for improved convergence and precision.", "configspace": "", "generation": 12, "fitness": 0.9124885722775419, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.912 with standard deviation 0.117. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960010058947741, 0.9949816027045283, 0.7464831082333234], "final_y": [0.16485593641363627, 0.1648560202684023, 0.25781027898004705]}, "mutation_prompt": null}
{"id": "f59064a5-0159-4e7f-b514-43c8e7e19d54", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 5, 0, transformed)  # Changed from self.dim // 4 to self.dim // 5\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by fine-tuning wavelet transformation parameters to better preserve periodicity.", "configspace": "", "generation": 12, "fitness": 0.8923678745848597, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.106. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960234838789349, 0.9345826543506162, 0.746497485525028], "final_y": [0.1648559781853145, 0.18187874208034227, 0.2578102880922566]}, "mutation_prompt": null}
{"id": "a85422f2-e2a6-4395-9b20-123303c79927", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 3, 0, transformed)  # Adjusted low-pass filter\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        scale = 1 / (1 + np.std(particle[\"position\"]))  # Added scale adjustment\n        return scale * velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An adaptive WDAPSO with improved periodicity control by modifying the wavelet transformation and adding scale adjustment in the learning factors.", "configspace": "", "generation": 12, "fitness": 0.9068678435667903, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.907 with standard deviation 0.114. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9948039180580689, 0.9793098556587393, 0.7464897569835627], "final_y": [0.16485588293591058, 0.16485652031915699, 0.25781028302557574]}, "mutation_prompt": null}
{"id": "4bfbc0a8-9027-46ae-89a7-2e3148ddc4c7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                # Dynamic inertia adjustment for better exploration and convergence\n                inertia = 0.9 - (0.9 - 0.4) * (budget_spent / self.budget)\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia=inertia)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by dynamically adjusting the inertia weight to improve exploration and convergence balance.", "configspace": "", "generation": 12, "fitness": 0.9101663076383533, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.910 with standard deviation 0.116. And the mean value of best solutions found was 0.196 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9958201785874208, 0.9878933820520083, 0.7467853622756311], "final_y": [0.16485617562338484, 0.16485657871554615, 0.2578142086960188]}, "mutation_prompt": null}
{"id": "7ff89d28-8af1-42c4-b851-84c2689d092a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def trigonometric_refinement(self, candidate):  # New function for trigonometric refinement\n        return np.cos(candidate * np.pi / 2)  # Encourages periodicity\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        refined = self.trigonometric_refinement(refined)  # Apply trigonometric refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n                p[\"position\"] = self.trigonometric_refinement(p[\"position\"])  # Apply trigonometric refinement\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Improved WDAPSO using hybrid wavelet-trigonometric position refinement for enhanced periodicity and local search accuracy.", "configspace": "", "generation": 12, "fitness": 0.9723074704662825, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.024. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9388891338647618, 0.9878513962032045, 0.9901818813308811], "final_y": [0.18187817668293393, 0.16485657223392147, 0.16485693493025355]}, "mutation_prompt": null}
{"id": "79eea944-2205-43ac-be23-914cf2779f03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                inertia_weight = 0.9 - 0.5 * (budget_spent / self.budget)  # Adaptive inertia weight\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia=inertia_weight)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "A refined WDAPSO integrating adaptive inertia weights and dynamic population resizing based on convergence to enhance exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.9962640527454395, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960231412982646, 0.9963183411621765, 0.9964506757758771], "final_y": [0.1648559605442832, 0.16485605985714147, 0.16485633559167856]}, "mutation_prompt": null}
{"id": "293aca49-6699-454c-967c-1d6beba823f2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        decay_rate = 0.99  # New decay for inertia\n        inertia *= decay_rate  # Apply decay to inertia\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced velocity update to include non-linear inertia decay for improved exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.9963008536327674, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960237163940052, 0.996565319773775, 0.9963135247305223], "final_y": [0.16485599454998723, 0.1648560809233971, 0.16485591934826693]}, "mutation_prompt": null}
{"id": "715eef3b-dc2f-4e32-9ee0-c6e223df60df", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                # Adaptive inertia weight based on progress\n                inertia = 0.9 - 0.5 * (budget_spent / self.budget)\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia=inertia)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Incorporate an adaptive inertia weight strategy in WDAPSO to enhance convergence by dynamically adjusting the exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.9928537952286455, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960240096177694, 0.9858715279663568, 0.99666584810181], "final_y": [0.16485594709130225, 0.16485591951271994, 0.16485603181785435]}, "mutation_prompt": null}
{"id": "b1ad570f-7c07-4ff3-a882-e90241b5e166", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        # Modified line: Implement a time-varying inertia weight\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "WDAPSO now uses a time-varying inertia weight for improved exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.9962510577078323, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9961066078001878, 0.9963485527492857, 0.9962980125740234], "final_y": [0.164855861876127, 0.16485624523646036, 0.16485602380203213]}, "mutation_prompt": null}
{"id": "786a34f2-5889-4127-a067-05e19fef6975", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 5, 0, transformed)  # Slightly modified\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Improved WDAPSO by optimizing wavelet selection for enhancing periodicity in solutions.", "configspace": "", "generation": 13, "fitness": 0.9963327984420945, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9962996993804077, 0.9964780903172193, 0.9962206056286568], "final_y": [0.16485589869902173, 0.16485586742153435, 0.1648560951660336]}, "mutation_prompt": null}
{"id": "244be5b5-d914-4342-ae8d-a5a4653d38e1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An improved WDAPSO that enhances local search with adaptive wavelet refinement to increase precision and convergence speed.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960234330163042, 0.9965265125482613, 0.996648631298188], "final_y": [0.16485596745319153, 0.16485595157228017, 0.16485599279765684]}, "mutation_prompt": null}
{"id": "d9276bdb-faa2-404e-9983-9bce7fca5479", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                # Additional adaptive wavelet-based perturbation\n                if np.random.rand() < 0.1:  # New line\n                    best_candidate = self.wavelet_transform(best_candidate)\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Apply an additional adaptive wavelet-based perturbation to improve exploration near the best-known solution.", "configspace": "", "generation": 14, "fitness": 0.996264565967487, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960231189400774, 0.996316852197191, 0.9964537267651924], "final_y": [0.16485586037657718, 0.1648562008741009, 0.1648560200301341]}, "mutation_prompt": null}
{"id": "fb8ea9b3-dc8c-4f21-91a1-bfd5ba6cd16d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        # Introduce dynamic inertia weight adjustment\n        inertia = 0.9 - 0.4 * len(self.history) / self.budget\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhances global search by implementing a dynamic inertia weight adjustment in the velocity update to improve exploration and convergence.", "configspace": "", "generation": 14, "fitness": 0.9949591840890545, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960232170447276, 0.9952136335982468, 0.9936407016241888], "final_y": [0.16485598864267237, 0.16485596853971196, 0.16485596152799775]}, "mutation_prompt": null}
{"id": "0665f137-d260-474c-850c-4a9d8072ac06", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": self.wavelet_transform(np.random.uniform(-1, 1, self.dim)),  # Change\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhance the initial diversity of the population by initializing particles with random positions and wavelet-transformed velocities.", "configspace": "", "generation": 14, "fitness": 0.9684296491857157, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.968 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9949387092627181, 0.9345589054841289, 0.9757913328103001], "final_y": [0.16485585512652412, 0.18187850825684038, 0.16485595615306248]}, "mutation_prompt": null}
{"id": "1654e4cc-273a-4556-acb9-caad5fda8efb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.abs(transformed) < np.max(np.abs(transformed)) * 0.1, 0, transformed)  # Adaptive thresholding\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.9, cognitive=1.5, social=1.5):  # Dynamic inertia\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Improved WDAPSO with adaptive wavelet thresholding and dynamic inertia weight for better periodicity and convergence.", "configspace": "", "generation": 14, "fitness": 0.9937811779134903, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9935831888692145, 0.9938970608537528, 0.9938632840175036], "final_y": [0.16485585448934936, 0.164855772751742, 0.16485595255452135]}, "mutation_prompt": null}
{"id": "07d6a208-9b70-4095-a319-0652d7bbb08e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + 1.5 * r1 * (personal_best - particle[\"position\"])\n            + 1.5 * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            inertia = 0.9 - 0.5 * (budget_spent / self.budget)  # Dynamic inertia\n\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An enhanced WDAPSO with adaptive periodic wavelet refinement and dynamic inertia to improve convergence and solution quality.", "configspace": "", "generation": 15, "fitness": 0.9953080351771835, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960227283792223, 0.9953589072653652, 0.9945424698869628], "final_y": [0.16485600434757997, 0.16485618259075108, 0.1648561476412801]}, "mutation_prompt": null}
{"id": "ce7a0ee1-dcbb-45ee-82d0-03f20a60e65c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        # Adjusted perturbation standard deviation for improved exploration\n        perturbation = np.random.normal(0, 0.02, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO with improved stochastic refinement to maintain diversity and convergence speed.", "configspace": "", "generation": 15, "fitness": 0.9954806949863791, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.995 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960128608276909, 0.9953590890348178, 0.9950701350966287], "final_y": [0.1648558228499959, 0.16485604502883766, 0.16485594496155986]}, "mutation_prompt": null}
{"id": "a278e3ba-905c-4894-bcff-9af6d7f49279", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        t = len(self.history) / self.budget  # Normalized time factor\n        cognitive = 2.5 - 1.5 * t  # Time-varying cognitive coefficient\n        social = 0.5 + 1.5 * t  # Time-varying social coefficient\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO with time-varying acceleration coefficients to improve global exploration and local exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.99566484303423, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9957807793077882, 0.9953603840639252, 0.9958533657309767], "final_y": [0.16485595704734268, 0.16485604984695568, 0.16485598771042764]}, "mutation_prompt": null}
{"id": "94643e98-0f45-4f63-a03c-a7628a400bff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 3, 0, transformed)  # Adjusted cutoff for enhanced periodicity\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5 + 0.3*np.random.rand(), cognitive=1.5, social=1.5):  # Introduced adaptive inertia\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  \n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhancing WDAPSO with adaptive inertia and refined wavelet-based periodicity to improve convergence and solution quality.", "configspace": "", "generation": 15, "fitness": 0.9961839659841756, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9957836477480114, 0.9963173200586818, 0.9964509301458334], "final_y": [0.16485607462838547, 0.16485587877100194, 0.16485604388389263]}, "mutation_prompt": null}
{"id": "b242010c-54be-4b8a-853b-eeda24b38c11", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                inertia = max(0.4, 1 - budget_spent / self.budget)  # Change: dynamically adjust inertia\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia=inertia)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhance WDAPSO by introducing a feedback mechanism to adjust inertia dynamically based on convergence speed, improving its ability to escape local optima.", "configspace": "", "generation": 15, "fitness": 0.9962647414854864, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960237768521244, 0.9963174518262532, 0.9964529957780817], "final_y": [0.1648559914812695, 0.1648558546319142, 0.1648558895968184]}, "mutation_prompt": null}
{"id": "f8a54817-9f8b-4b6f-802a-4c498c814d73", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n                p[\"position\"] = (p[\"position\"] + ub) % ub  # Ensure periodic boundary conditions\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced particle position update with periodic boundary handling to promote constructive interference.", "configspace": "", "generation": 16, "fitness": 0.996261641744724, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960075611584831, 0.996332342898461, 0.9964450211772279], "final_y": [0.16485600449099624, 0.1648559682541051, 0.1648559975625873]}, "mutation_prompt": null}
{"id": "805df4ae-8d20-4807-a117-260b0f1443fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 6, 0, transformed)  # Changed filter threshold from dim // 4 to dim // 6\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Modified WDAPSO to incorporate a noise-tolerant wavelet transform filter for enhanced robustness against stochastic noise.", "configspace": "", "generation": 16, "fitness": 0.9961550229352693, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9961013300426208, 0.996170323949555, 0.9961934148136324], "final_y": [0.1648558823405124, 0.16485580794234678, 0.16485582874490712]}, "mutation_prompt": null}
{"id": "6775bc77-8eaf-4584-951e-346afa59885c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        # Changed line: incorporating a decay factor to adjust inertia\n        inertia = inertia * 0.99  # Adding decay factor to inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhances convergence by incorporating a decay factor to dynamically adjust inertia during optimization.", "configspace": "", "generation": 16, "fitness": 0.9963329862118119, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960230429216905, 0.9965220760088452, 0.9964538397048998], "final_y": [0.1648561039203964, 0.16485616290202942, 0.16485590117059568]}, "mutation_prompt": null}
{"id": "d07a8478-cb07-4f1b-8385-4e18419ef7c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (sum(self.history) / self.budget))  # Adaptive inertia weight\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO with adaptive inertia weights for improved convergence balance.", "configspace": "", "generation": 16, "fitness": 0.9935273501867069, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9880889809624309, 0.9963176405606347, 0.9961754290370548], "final_y": [0.1648561021777688, 0.16485588457884626, 0.16485614970375695]}, "mutation_prompt": null}
{"id": "f0fb44cd-6e8f-4973-91fe-8f42c8e8c0b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedWDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def periodic_potential(self, candidate):\n        periodicity_factor = np.cos(2 * np.pi * np.arange(self.dim) / self.dim)\n        return candidate * periodicity_factor\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        refined = self.periodic_potential(refined)  # Encourage periodic solutions\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation and periodic potential to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n                p[\"position\"] = self.periodic_potential(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "EnhancedWDAPSO", "description": "Enhanced WDAPSO with adaptive periodic potential to encourage periodicity and symmetry in multilayer structures, leveraging constructive interference principles for improved optimization.", "configspace": "", "generation": 16, "fitness": 0.8874179990806227, "feedback": "The algorithm EnhancedWDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.101. And the mean value of best solutions found was 0.202 (0. is the best) with standard deviation 0.040.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9364546900636112, 0.979309407568011, 0.7464898996102459], "final_y": [0.18188046216688225, 0.1648565607572997, 0.2578101852607517]}, "mutation_prompt": null}
{"id": "13bd8930-d5b7-450d-889d-e81803e2a574", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n                else:  # Reinitialize if no improvement\n                    p[\"position\"] = np.random.uniform(lb, ub, self.dim)\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhance the WDAPSO algorithm by adding periodic position reinitialization for particles that haven't improved to invigorate exploration.", "configspace": "", "generation": 17, "fitness": 0.9962855532457452, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9960608297284593, 0.9963435636578873, 0.9964522663508892], "final_y": [0.1648560289409019, 0.16485670825017507, 0.16485601852334342]}, "mutation_prompt": null}
{"id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by introducing self-adaptive inertia and refining wavelet transformation to further improve convergence speed and precision.", "configspace": "", "generation": 17, "fitness": 0.9964421841134469, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9964017455945751, 0.9964707160476508, 0.9964540906981149], "final_y": [0.1648561587440589, 0.1648560028317947, 0.1648558724981941]}, "mutation_prompt": null}
{"id": "cba711c9-db36-410e-ab15-63c7e69f18e3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                # Updated line: Adaptive inertia decreases over time\n                inertia = 0.9 - (0.7 * (budget_spent / self.budget))\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia=inertia)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "WDAPSO enhanced with adaptive inertia to improve exploration-exploitation balance and convergence rate.", "configspace": "", "generation": 17, "fitness": 0.9962081981485839, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9958671592574752, 0.9963161697647492, 0.9964412654235272], "final_y": [0.16485609000856272, 0.164856664814371, 0.16485608681782482]}, "mutation_prompt": null}
{"id": "fae811b3-399a-44b0-b32b-eccf76e8de6b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.6):  # Increased the social component slightly\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An improved WDAPSO that enhances local search with adaptive wavelet refinement to increase precision and convergence speed, along with a slight increase in social component to foster better convergence. ", "configspace": "", "generation": 17, "fitness": 0.9961322693543345, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9958901470449413, 0.9963181999558727, 0.9961884610621893], "final_y": [0.16485589634813813, 0.16485598460283135, 0.16485617542473974]}, "mutation_prompt": null}
{"id": "22e21722-c24d-4849-a38e-28d1dcc036e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                # Dynamic inertia adjustment\n                inertia = 0.9 - 0.5 * (budget_spent / self.budget)\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p, inertia=inertia)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "An improved WDAPSO that enhances local search with adaptive wavelet refinement and dynamic inertia to increase precision and convergence speed.", "configspace": "", "generation": 17, "fitness": 0.9956741821667049, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "50814173-7918-48db-976e-0f08e1c44a80", "metadata": {"aucs": [0.9940040245946143, 0.9966260056296458, 0.9963925162758546], "final_y": [0.1648559305213173, 0.16485588981750288, 0.16485589842815296]}, "mutation_prompt": null}
{"id": "af6e18f7-40b6-4aea-a98c-15e9cce2afde", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.4 * (len(self.history) / self.budget))  # Adjusted self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform((res.x + candidate) / 2)  # Symmetrized wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Optimized WDAPSO by introducing gradient-based inertia adjustment and refined wavelet-symmetrized local search to enhance convergence and precision.", "configspace": "", "generation": 18, "fitness": 0.9961942784556133, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.996024181776253, 0.9961054073426783, 0.9964532462479087], "final_y": [0.16485583937377868, 0.16485590009907825, 0.1648559311261839]}, "mutation_prompt": null}
{"id": "c33036ee-ad7c-412b-a21a-8083bbad7a1c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        threshold = self.dim // 4 + (len(self.history) / self.budget) * (self.dim // 2)  # Adaptive threshold\n        low_pass = np.where(np.arange(len(transformed)) > threshold, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Introduced adaptive wavelet thresholds to enhance periodicity reinforcement, improving solution quality.", "configspace": "", "generation": 18, "fitness": 0.9963611411867047, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9960222056917462, 0.9966072176934608, 0.9964540001749074], "final_y": [0.16485607021845694, 0.1648559705783902, 0.1648559066828763]}, "mutation_prompt": null}
{"id": "d87f5020-fca8-49c7-89b5-c73f423121cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n        \n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        # Adjust low_pass filter dynamically based on iteration\n        cutoff_frequency = int(self.dim * (0.25 + 0.25 * (len(self.history) / self.budget)))  \n        low_pass = np.where(np.arange(len(transformed)) > cutoff_frequency, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhance the algorithm with an adaptive wavelet cutoff to dynamically adjust the transformation for improved convergence.", "configspace": "", "generation": 18, "fitness": 0.9963677546544559, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9960232997220734, 0.9966260770930306, 0.9964538871482638], "final_y": [0.16485613195803395, 0.16485613659916698, 0.16485584890907723]}, "mutation_prompt": null}
{"id": "8164dcb8-0289-4eb4-98e1-96fd24aef34e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass_thresh = int(self.dim * (0.25 + 0.05 * (len(self.history) / self.budget)))  # Adaptive low-pass filter\n        low_pass = np.where(np.arange(len(transformed)) > low_pass_thresh, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by optimizing wavelet transformation with adaptive low-pass filtering to improve periodicity alignment.", "configspace": "", "generation": 18, "fitness": 0.9958029933100364, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9952084728252237, 0.9960748590799456, 0.9961256480249401], "final_y": [0.16485602356002726, 0.1648561207312368, 0.16485590580494003]}, "mutation_prompt": null}
{"id": "f80ba8aa-2518-418f-9741-161d8d3965a7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.7 * (len(self.history) / self.budget))  # Dynamic inertia modification\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def adaptive_random_search(self, func, lb, ub, num_samples=5):\n        best_sample = None\n        best_fitness = float(\"inf\")\n        for _ in range(num_samples):\n            sample = np.random.uniform(lb, ub, self.dim)\n            fitness = func(sample)\n            if fitness < best_fitness:\n                best_fitness = fitness\n                best_sample = sample\n        return best_sample, best_fitness\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n            if budget_spent < self.budget:\n                candidate, fitness = self.adaptive_random_search(func, lb, ub)\n                if fitness < global_best_fitness:\n                    global_best_position = candidate\n                    global_best_fitness = fitness\n                    budget_spent += 1\n                    self.history.append(fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Introduce hybrid dynamic inertia with wavelet-enhanced local search and adaptive random search to refine convergence and balance exploration-exploitation.", "configspace": "", "generation": 18, "fitness": 0.9958916861134185, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9955244763763247, 0.9963163244275355, 0.9958342575363952], "final_y": [0.16485600774030662, 0.1648558986521278, 0.16485592613404842]}, "mutation_prompt": null}
{"id": "a19a9697-2d90-4ebb-a32c-aa8ecf5280f1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.9 - 0.4) * (len(self.history) / self.budget) ** 2  # Nonlinear inertia adaptation\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Improved WDAPSO by incorporating nonlinear inertia weight adaptation and enhanced wavelet-based refinement for superior convergence.", "configspace": "", "generation": 19, "fitness": 0.9962577565461187, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9960222471108687, 0.9961604758818113, 0.9965905466456759], "final_y": [0.16485608597190982, 0.16485591691164325, 0.1648560034064973]}, "mutation_prompt": null}
{"id": "7b1ba46f-e678-4fa7-b441-37dc143ccfcf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x + 0.1 * perturbation)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO with improved local search through stochastic refinement using adaptive wavelet coefficients to enhance convergence precision.", "configspace": "", "generation": 19, "fitness": 0.9961959446055952, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9958405806929782, 0.9961605922123952, 0.9965866609114125], "final_y": [0.1648560067649366, 0.1648558111351538, 0.16485618544376823]}, "mutation_prompt": null}
{"id": "3aefed3e-9b2a-4d2b-83df-b532cb12cb1d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20 if self.budget > 50 else 10  # Dynamic population adjustment\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Optimized WDAPSO by introducing dynamic population adjustment to enhance diversity and convergence speed.", "configspace": "", "generation": 19, "fitness": 0.9963844143991833, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9960244799417584, 0.9965181319524796, 0.9966106313033118], "final_y": [0.16485597876472236, 0.16485585878257658, 0.1648561817558425]}, "mutation_prompt": null}
{"id": "0c41896f-d8d5-4865-905f-7f2aa120da73", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        social = 1.5 + (0.5 * (len(self.history) / self.budget))   # Dynamic social factor adjustment\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by incorporating dynamic social factor adjustment based on iteration to improve convergence precision and speed.", "configspace": "", "generation": 19, "fitness": 0.995737646291816, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9941284145478052, 0.99646778043258, 0.9966167438950629], "final_y": [0.16485609735342077, 0.1648559882889603, 0.16485598938889656]}, "mutation_prompt": null}
{"id": "da3a214d-3d52-49fa-b15b-618105a2b079", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)\n        return refined, res.fun\n\n    def adapt_population_size(self, current_size, max_size):\n        # Adaptively adjust population size\n        if len(self.history) % 10 == 0:  # Check every 10 evaluations\n            return min(max_size, current_size + 1)\n        return current_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        max_pop_size = 30  # Define a maximum population size\n\n        while budget_spent < self.budget:\n            pop_size = self.adapt_population_size(pop_size, max_pop_size)\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n                # Introduce a random restart for stuck particles\n                if np.random.rand() < 0.1:\n                    p[\"position\"] = np.random.uniform(lb, ub, self.dim)\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Introduced adaptive population size and ensured solution diversity by incorporating a random restart strategy for stuck particles.", "configspace": "", "generation": 19, "fitness": 0.9957937567051678, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9946534139949877, 0.9966589888236774, 0.9960688672968381], "final_y": [0.1648558276958394, 0.16485590646788473, 0.16485591887873208]}, "mutation_prompt": null}
{"id": "b13c2467-7073-4c74-a0b8-4cc2501a9fa1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        high_pass = np.where(np.arange(len(transformed)) <= self.dim // 8, transformed, 0)  # Adaptive high-pass filter\n        return np.real(np.fft.ifft(low_pass + high_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced wavelet transformation by implementing an adaptive high-pass filter to refine search space exploration.", "configspace": "", "generation": 20, "fitness": 0.9926676456617405, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9852941899659274, 0.9963073979973122, 0.996401349021982], "final_y": [0.1648560736631376, 0.16485614570248008, 0.16485597753783088]}, "mutation_prompt": null}
{"id": "de85aef5-f0eb-4cca-bfba-8d6da21b2907", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        dynamic_threshold = self.dim // (4 + len(self.history) // (self.budget // 4))  # Dynamic threshold\n        low_pass = np.where(np.arange(len(transformed)) > dynamic_threshold, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO with a dynamic wavelet transformation threshold for improved adaptability and convergence.", "configspace": "", "generation": 20, "fitness": 0.9962630686213988, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.996023460786194, 0.9963249684850444, 0.9964407765929583], "final_y": [0.16485599013560404, 0.1648558792929986, 0.16485595215163307]}, "mutation_prompt": null}
{"id": "371e612a-a540-44a8-a328-16db763137e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate, phase):\n        transformed = np.fft.fft(candidate)\n        # Adaptive wavelet thresholds based on phase\n        threshold_index = max(1, int(self.dim // (4 + phase)))\n        low_pass = np.where(np.arange(len(transformed)) > threshold_index, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds, phase):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x, phase)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        phase = 1\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"], phase)\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)), phase)\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n            phase += 1  # Increment phase to adapt thresholds\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Introduced adaptive wavelet thresholds and multi-phase stochastic refinement to enhance periodicity and local searching efficiency in WDAPSO.", "configspace": "", "generation": 20, "fitness": 0.9962507080820648, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9960238406513197, 0.9963267841913223, 0.9964014994035527], "final_y": [0.16485587828581827, 0.16485597315207423, 0.1648559333262093]}, "mutation_prompt": null}
{"id": "7809d0f1-7f55-4738-8b68-bc5bc038f96f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        periodic_enforcement = np.real(np.fft.ifft(low_pass))\n        return np.clip(periodic_enforcement, np.min(candidate), np.max(candidate))  # Enhanced periodicity\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.5, cognitive=1.5, social=1.5):\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        cognitive = 1.0 + 0.5 * np.sin(len(self.history) / self.budget * np.pi)  # Self-adaptive cognitive factor\n        social = 1.0 + 0.5 * np.cos(len(self.history) / self.budget * np.pi)  # Self-adaptive social factor\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.01, size=self.dim)\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Refined WDAPSO by incorporating self-adaptive cognitive and social factors and enhancing periodicity enforcement in wavelet transformation to improve convergence robustness.", "configspace": "", "generation": 20, "fitness": 0.9960888744365392, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9960415066192835, 0.9964523084219141, 0.9957728082684199], "final_y": [0.16485592708730645, 0.16485627037347306, 0.16485587543106384]}, "mutation_prompt": null}
{"id": "87a8c1f5-8d6b-4993-b125-f80e3355d841", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass WDAPSO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.history = []\n\n    def wavelet_transform(self, candidate):\n        transformed = np.fft.fft(candidate)\n        low_pass = np.where(np.arange(len(transformed)) > self.dim // 4, 0, transformed)\n        return np.real(np.fft.ifft(low_pass))\n\n    def adaptive_learning_factors(self, personal_best, global_best, particle, inertia=0.6, cognitive=1.7, social=1.5):  # Changed inertia and cognitive\n        r1, r2 = np.random.rand(), np.random.rand()\n        inertia = 0.9 - (0.5 * (len(self.history) / self.budget))  # Self-adaptive inertia\n        velocity = (\n            inertia * particle[\"velocity\"]\n            + cognitive * r1 * (personal_best - particle[\"position\"])\n            + social * r2 * (global_best - particle[\"position\"])\n        )\n        return velocity\n\n    def stochastic_refinement(self, candidate, func, bounds):\n        perturbation = np.random.normal(0, 0.02, size=self.dim)  # Changed perturbation std from 0.01 to 0.02\n        perturbed_candidate = candidate + perturbation\n        res = minimize(func, perturbed_candidate, bounds=bounds, method='L-BFGS-B')\n        refined = self.wavelet_transform(res.x)  # Enhanced local search with wavelet refinement\n        return refined, res.fun\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        scaled_lb = np.zeros(self.dim)\n        scaled_ub = np.ones(self.dim)\n\n        pop_size = 20\n        particles = [{\n            \"position\": np.random.uniform(lb, ub, self.dim),\n            \"velocity\": np.zeros(self.dim),\n            \"best_position\": None,\n            \"best_fitness\": float(\"inf\")\n        } for _ in range(pop_size)]\n\n        global_best_position = None\n        global_best_fitness = float(\"inf\")\n\n        fitness = np.array([func(p[\"position\"]) for p in particles])\n        for i, p in enumerate(particles):\n            p[\"best_position\"] = p[\"position\"]\n            p[\"best_fitness\"] = fitness[i]\n            if fitness[i] < global_best_fitness:\n                global_best_position = p[\"position\"]\n                global_best_fitness = fitness[i]\n        self.history.extend(fitness)\n        budget_spent = len(particles)\n\n        while budget_spent < self.budget:\n            for i, p in enumerate(particles):\n                if budget_spent >= self.budget:\n                    break\n\n                p[\"velocity\"] = self.adaptive_learning_factors(p[\"best_position\"], global_best_position, p)\n                p[\"position\"] = np.clip(p[\"position\"] + p[\"velocity\"], lb, ub)\n\n                # Apply wavelet transformation to encourage periodicity\n                p[\"position\"] = self.wavelet_transform(p[\"position\"])\n\n                current_fitness = func(p[\"position\"])\n                budget_spent += 1\n                self.history.append(current_fitness)\n\n                if current_fitness < p[\"best_fitness\"]:\n                    p[\"best_position\"] = p[\"position\"]\n                    p[\"best_fitness\"] = current_fitness\n\n                    if current_fitness < global_best_fitness:\n                        global_best_position = p[\"position\"]\n                        global_best_fitness = current_fitness\n\n            if budget_spent < self.budget:\n                best_particle = min(particles, key=lambda x: x[\"best_fitness\"])\n                best_candidate, local_fitness = self.stochastic_refinement(best_particle[\"best_position\"], func, list(zip(lb, ub)))\n\n                if local_fitness < global_best_fitness:\n                    global_best_position = best_candidate\n                    global_best_fitness = local_fitness\n                    budget_spent += 1\n                    self.history.append(local_fitness)\n\n        return global_best_position, global_best_fitness", "name": "WDAPSO", "description": "Enhanced WDAPSO by refining perturbation in stochastic refinement and adjusting adaptive learning factors.", "configspace": "", "generation": 20, "fitness": 0.9964280156128571, "feedback": "The algorithm WDAPSO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.996 with standard deviation 0.000. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "910f3b40-cd59-4535-a9b5-ff328a12608f", "metadata": {"aucs": [0.9958039356001803, 0.996613644568456, 0.996866466669935], "final_y": [0.16485589820967117, 0.16485598995869544, 0.1648559169375372]}, "mutation_prompt": null}
