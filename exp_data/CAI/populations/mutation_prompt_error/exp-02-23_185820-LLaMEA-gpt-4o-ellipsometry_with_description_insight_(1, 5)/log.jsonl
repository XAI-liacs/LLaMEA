{"id": "f1f09fc1-a44c-4e35-b41b-593542206807", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.2\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Hybrid Local Sampling with Dynamic Bound Adjustment for Efficient Smooth Cost Function Optimization.", "configspace": "", "generation": 0, "fitness": 0.8535169463927442, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8515578824075499, 0.8373897774325774, 0.8716031793381058], "final_y": [2.6266187392845664e-08, 4.0383189857653896e-08, 3.299961192869691e-08]}, "mutation_prompt": null}
{"id": "e31c5808-10a5-4dec-8970-15d7b93a13c4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom pyDOE import lhs  # Import Latin Hypercube Sampling\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        # Change: Use Latin Hypercube Sampling for better initial coverage\n        initial_samples = lhs(self.dim, samples=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.2\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhanced Initial Sampling with Latin Hypercube Sampling for Improved Coverage in Low-Dimensional Spaces.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'pyDOE'\").", "error": "ModuleNotFoundError(\"No module named 'pyDOE'\")", "parent_id": "f1f09fc1-a44c-4e35-b41b-593542206807", "metadata": {}, "mutation_prompt": null}
{"id": "34710db1-08f6-40b0-b8fe-0ce427cc156f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveBoundaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using Nelder-Mead\n        result = minimize(\n            func,\n            best_guess,\n            method='Nelder-Mead',\n            options={'maxfev': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # Adaptive boundary exploration\n        while self.evals < self.budget:\n            # Adjust bounds progressively around the best solution\n            exploration_factor = 0.1\n            adjusted_bounds = [\n                (max(lb, result.x[i] - exploration_factor * (ub - lb)), \n                 min(ub, result.x[i] + exploration_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with progressively adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='Nelder-Mead',\n                bounds=adjusted_bounds,\n                options={'maxfev': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "AdaptiveBoundaryOptimizer", "description": "Adaptive Boundary Exploration with Progressive Refinement for Efficient Smooth Landscape Optimization.", "configspace": "", "generation": 1, "fitness": 0.6202090270835808, "feedback": "The algorithm AdaptiveBoundaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.620 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f1f09fc1-a44c-4e35-b41b-593542206807", "metadata": {"aucs": [0.6302070358565588, 0.6026942542505139, 0.62772579114367], "final_y": [1.3885830109761544e-05, 3.191982098075992e-05, 2.0574828907999304e-05]}, "mutation_prompt": null}
{"id": "d6029e5e-f4dd-403d-a806-e6f02ef4c03a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.1  # Adjusted scaling factor for refined exploration\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhanced adaptive scaling for bound adjustment in smooth cost function optimization.", "configspace": "", "generation": 1, "fitness": 0.8442932419818189, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.027. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f1f09fc1-a44c-4e35-b41b-593542206807", "metadata": {"aucs": [0.8477862944991361, 0.8099966119918971, 0.8750968194544232], "final_y": [4.102645462959513e-08, 1.3960073927005789e-07, 1.6255133378790614e-10]}, "mutation_prompt": null}
{"id": "54fd90a2-4d12-4010-a1be-15726cd9a412", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveMultistartSGDOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_initial_samples = min(10, self.budget // 4)\n        \n        # Step 1: Uniform sampling for diverse initial guesses\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_initial_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_initial_samples\n        \n        # Step 2: Multi-start local search with gradient-based refinement\n        best_solution = None\n        best_value = float('inf')\n        \n        for i in range(num_initial_samples):\n            guess = initial_samples[i]\n            \n            # Local optimizer using L-BFGS-B\n            result = minimize(\n                func,\n                guess,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={'maxfun': (self.budget - self.evals) // (num_initial_samples - i)}\n            )\n            self.evals += result.nfev\n            \n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n            \n            # Early exit if budget is exhausted\n            if self.evals >= self.budget:\n                break\n        \n        # Step 3: Stochastic Gradient Descent for final exploitation phase\n        alpha = 0.01  # learning rate\n        current_solution = best_solution\n        current_value = best_value\n        \n        while self.evals < self.budget:\n            gradient = self._estimate_gradient(func, current_solution)\n            next_solution = current_solution - alpha * gradient\n            \n            # Clip to bounds\n            next_solution = np.clip(next_solution, bounds[:, 0], bounds[:, 1])\n            \n            next_value = func(next_solution)\n            self.evals += 1\n            \n            if next_value < current_value:\n                current_solution = next_solution\n                current_value = next_value\n            \n        return current_solution\n    \n    def _estimate_gradient(self, func, x):\n        epsilon = 1e-8\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_step = np.array(x)\n            x_step[i] += epsilon\n            grad[i] = (func(x_step) - func(x)) / epsilon\n        return grad", "name": "AdaptiveMultistartSGDOptimizer", "description": "Adaptive Multi-Start Local Search with Stochastic Gradient Descent for Enhanced Exploration and Exploitation in Smooth Landscapes.", "configspace": "", "generation": 1, "fitness": 0.5765389441013895, "feedback": "The algorithm AdaptiveMultistartSGDOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.577 with standard deviation 0.056. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f1f09fc1-a44c-4e35-b41b-593542206807", "metadata": {"aucs": [0.5562641148421441, 0.5199774890087032, 0.6533752284533212], "final_y": [2.7235187029795554e-05, 0.00022621910191818472, 6.981300874154825e-07]}, "mutation_prompt": null}
{"id": "8ca5ab34-12ff-4d82-8746-48e6325c3c2a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = np.random.uniform(0.1, 0.3)\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhanced Hybrid Local Sampling with Stochastic Bound Adjustment for Improved Smooth Cost Function Optimization.", "configspace": "", "generation": 1, "fitness": 0.8126116335735523, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.067. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f1f09fc1-a44c-4e35-b41b-593542206807", "metadata": {"aucs": [0.9030930785986394, 0.7918096485942705, 0.7429321735277468], "final_y": [3.955962958937068e-10, 1.8136850779333294e-07, 3.848191877757973e-07]}, "mutation_prompt": null}
{"id": "077055a9-b00d-457f-bf62-9dc58b47ad77", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamic initial sample size\n        num_samples = min(max(5, self.budget // 4), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.1  # Adjusted scaling factor for refined exploration\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce dynamic sampling size in initial exploration for improved convergence.  ", "configspace": "", "generation": 2, "fitness": 0.8918800427998307, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.075. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6029e5e-f4dd-403d-a806-e6f02ef4c03a", "metadata": {"aucs": [0.9905466969531717, 0.8099966119918971, 0.8750968194544232], "final_y": [0.0, 1.3960073927005789e-07, 1.2230614821419e-09]}, "mutation_prompt": null}
{"id": "694b12b9-16f1-4c7a-a0e6-2391cb5f5501", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveGaussianMutator:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform Adaptive Gaussian Mutation\n        while self.evals < self.budget:\n            # Generate a Gaussian noise centered at the best solution found\n            mutation_std_dev = 0.05 * (bounds[:, 1] - bounds[:, 0])\n            gaussian_mutation = np.random.normal(0, mutation_std_dev, self.dim)\n            new_guess = np.clip(result.x + gaussian_mutation, bounds[:, 0], bounds[:, 1])\n            \n            # Evaluate the new guess\n            new_value = func(new_guess)\n            self.evals += 1\n            \n            # If the new guess is better, update the result\n            if new_value < result.fun:\n                result.x = new_guess\n                result.fun = new_value\n        \n        return result.x", "name": "AdaptiveGaussianMutator", "description": "Adaptive Gaussian Mutation for refined local exploitation in smooth cost function landscapes.", "configspace": "", "generation": 2, "fitness": 0.7999793432493378, "feedback": "The algorithm AdaptiveGaussianMutator got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6029e5e-f4dd-403d-a806-e6f02ef4c03a", "metadata": {"aucs": [0.7802778611325124, 0.8135007836156668, 0.8061593849998341], "final_y": [1.104681484067186e-07, 1.0745750671700037e-07, 1.1026790199191566e-07]}, "mutation_prompt": null}
{"id": "51af1c0e-c2b7-4c02-879a-74a03618547b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # Adaptive bound adjustment based on the remaining budget\n        while self.evals < self.budget:\n            # Dynamically adjust the scale factor based on remaining evaluations\n            remaining_budget = self.budget - self.evals\n            scale_factor = 0.05 + 0.1 * (remaining_budget / self.budget)  # Adjusts more aggressively with more budget left\n            \n            # Narrow the bounds based on the best-found solution\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': remaining_budget}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhanced adaptive scaling with dynamic local exploration based on residual budget and convergence rate.", "configspace": "", "generation": 2, "fitness": 0.7809324026829673, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.021. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6029e5e-f4dd-403d-a806-e6f02ef4c03a", "metadata": {"aucs": [0.769810653830179, 0.7621351110717745, 0.8108514431469482], "final_y": [2.571884018041915e-07, 1.0088024239188043e-07, 1.8075925487352536e-07]}, "mutation_prompt": null}
{"id": "3d72e4e4-e7a0-4c2d-8b40-006b75f0a70c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.05  # Adjusted scaling factor for refined exploration\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduced dynamic scale factor adjustment in local search to enhance exploitation efficiency with minimal changes.", "configspace": "", "generation": 2, "fitness": 0.8126116335735523, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.067. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6029e5e-f4dd-403d-a806-e6f02ef4c03a", "metadata": {"aucs": [0.9030930785986394, 0.7918096485942705, 0.7429321735277468], "final_y": [1.1862436046978647e-09, 1.8136850779333294e-07, 3.848191877757973e-07]}, "mutation_prompt": null}
{"id": "f8d247ae-1c63-4023-beba-fb0f13c28d18", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        num_samples = min(10, self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.05  # Adjusted scaling factor for refined exploration\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhanced adaptive bound adjustment using dynamic scaling for improved convergence in smooth landscapes.", "configspace": "", "generation": 2, "fitness": 0.8442932415077652, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.027. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6029e5e-f4dd-403d-a806-e6f02ef4c03a", "metadata": {"aucs": [0.8477862930769753, 0.8099966119918971, 0.8750968194544232], "final_y": [4.102645671201029e-08, 1.3960073927005789e-07, 1.2230614821419e-09]}, "mutation_prompt": null}
{"id": "f50d91e5-8d33-4ea0-a7fa-9e495e260bdc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nimport math\n\nclass CosineAdaptiveLocalSearchOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Step 1: Initial uniform sampling for a broad exploration\n        num_samples = min(10, self.budget // 3)\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        result = None\n        while self.evals < self.budget:\n            # Step 3: Cosine annealing for dynamically shrinking bounds\n            cosine_factor = (1 + math.cos(math.pi * self.evals / self.budget)) / 2\n            scale_factor = 0.3 * cosine_factor\n            narrowed_bounds = [\n                (max(lb, best_guess[i] - scale_factor * (ub - lb)), min(ub, best_guess[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Local search with narrowed bounds using BFGS\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': min(10, self.budget - self.evals)}\n            )\n            \n            # Update evaluations and best guess\n            self.evals += result.nfev\n            if result.fun < func(best_guess):\n                best_guess = result.x\n\n        return best_guess", "name": "CosineAdaptiveLocalSearchOptimizer", "description": "Introduce an adaptive local search phase using cosine annealing to balance exploration and exploitation dynamically.", "configspace": "", "generation": 3, "fitness": 0.1856011235447483, "feedback": "The algorithm CosineAdaptiveLocalSearchOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.186 with standard deviation 0.047. And the mean value of best solutions found was 0.092 (0. is the best) with standard deviation 0.048.", "error": "", "parent_id": "077055a9-b00d-457f-bf62-9dc58b47ad77", "metadata": {"aucs": [0.23072152341229324, 0.20466307481114288, 0.12141877241080878], "final_y": [0.0265959647486274, 0.11285425403225481, 0.1376448458836277]}, "mutation_prompt": null}
{"id": "5d10c7a7-213f-4b21-a0f8-46e4e023f884", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamic initial sample size\n        num_samples = min(max(5, self.budget // 4), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Incorporate a dynamic scaling factor for adaptive bound adjustments to enhance the exploration-exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.8109490210820841, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.127. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "077055a9-b00d-457f-bf62-9dc58b47ad77", "metadata": {"aucs": [0.9905466969531717, 0.7229517025225279, 0.7193486637705531], "final_y": [0.0, 1.2054887508388761e-08, 2.4586876497799555e-09]}, "mutation_prompt": null}
{"id": "dba504d8-1aec-43a4-944d-7e6700e011e2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamic initial sample size\n        num_samples = min(max(5, self.budget // 4), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.1  # Adjusted scaling factor for refined exploration\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Intermediate local search with adjusted bounds\n            intermediate_guess = result.x + np.random.uniform(-0.05, 0.05, self.dim)\n            result = minimize(\n                func,\n                intermediate_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': (self.budget - self.evals) // 2}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate intermediate local search phases for progressive refinement of solutions.", "configspace": "", "generation": 3, "fitness": 0.6907575834276861, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.691 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "077055a9-b00d-457f-bf62-9dc58b47ad77", "metadata": {"aucs": [0.7066405124828472, 0.6635895927856568, 0.7020426450145543], "final_y": [4.631451606604483e-09, 5.23189184937585e-08, 6.85029332216514e-09]}, "mutation_prompt": null}
{"id": "9ae89c64-acd2-4a91-a272-55b74951153c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for variance-based dynamic initial sample size\n        num_samples = min(max(5, int(np.var(bounds) * self.budget // 3)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.1  # Adjusted scaling factor for refined exploration\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance dynamic sampling in the initial exploration by introducing a variance-based adaptive sampling size for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.7032908013140687, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.703 with standard deviation 0.063. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "077055a9-b00d-457f-bf62-9dc58b47ad77", "metadata": {"aucs": [0.7914328804518432, 0.6655183896690267, 0.6529211338213363], "final_y": [0.0, 2.352190310195904e-08, 2.5980379914081366e-08]}, "mutation_prompt": null}
{"id": "d9278b4b-c056-4837-bcc0-268f63c03365", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamic initial sample size\n        num_samples = min(max(5, self.budget // 4), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = 0.1 if result.success else 0.2  # Adjusted scaling factor for refined exploration\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce adaptive scaling factor based on solution improvement for better convergence.", "configspace": "", "generation": 3, "fitness": 0.6913171029595674, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.691 with standard deviation 0.078. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "077055a9-b00d-457f-bf62-9dc58b47ad77", "metadata": {"aucs": [0.7998629952838806, 0.6200871009525649, 0.6540012126422565], "final_y": [0.0, 6.44077930885539e-08, 3.2635395222251365e-08]}, "mutation_prompt": null}
{"id": "5313cea2-2105-43c9-b44b-4f43a8438663", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(5, self.budget // 4), self.budget // 2)\n        \n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.12 * (1 - self.evals / self.budget))  # Adjusted dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Fine-tune the dynamic scaling factor for adaptive bound adjustments, enhancing convergence precision.", "configspace": "", "generation": 4, "fitness": 0.6094126045700744, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.609 with standard deviation 0.049. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5d10c7a7-213f-4b21-a0f8-46e4e023f884", "metadata": {"aucs": [0.6775290357973467, 0.5826582261202453, 0.5680505517926311], "final_y": [1.7645992435605006e-08, 5.991566987707053e-08, 1.0386695257937388e-07]}, "mutation_prompt": null}
{"id": "09467527-56fc-4dab-95f5-e0754f4fefc7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamic initial sample size\n        num_samples = min(max(5, self.budget // 4), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B with dynamic tolerance adjustment\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals, 'ftol': 1e-8 * (self.budget / 1000)}  # Updated line\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce an adaptive convergence criterion by integrating dynamic tolerance adjustments based on budget usage to enhance optimization efficiency.", "configspace": "", "generation": 4, "fitness": 0.6256411627960924, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.626 with standard deviation 0.062. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5d10c7a7-213f-4b21-a0f8-46e4e023f884", "metadata": {"aucs": [0.7126553498196899, 0.578390819732489, 0.5858773188360982], "final_y": [5.681379894003114e-09, 3.9476388838995994e-08, 3.2635395222251365e-08]}, "mutation_prompt": null}
{"id": "a641599c-16e8-4079-9354-9188bcda4185", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamic initial sample size\n        num_samples = min(max(5, self.budget // 4), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = max(0.02, 0.1 * (1 - self.evals / self.budget))  # Adjusted dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Adjusted the dynamic scaling factor formula for adaptive bound narrowing to enhance precision in fine-tuning.", "configspace": "", "generation": 4, "fitness": 0.6256411627960924, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.626 with standard deviation 0.062. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5d10c7a7-213f-4b21-a0f8-46e4e023f884", "metadata": {"aucs": [0.7126553498196899, 0.578390819732489, 0.5858773188360982], "final_y": [5.681379894003114e-09, 3.9476388838995994e-08, 3.2635395222251365e-08]}, "mutation_prompt": null}
{"id": "83e414ae-8cba-4655-9cd3-a39c00618171", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamic initial sample size\n        num_samples = min(max(5, self.budget // 4), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='BFGS',  # Changed to BFGS for better gradient refinement near optimum\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce a gradient-based refinement step to improve convergence near optimal points.", "configspace": "", "generation": 4, "fitness": 0.6256411627960924, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.626 with standard deviation 0.062. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5d10c7a7-213f-4b21-a0f8-46e4e023f884", "metadata": {"aucs": [0.7126553498196899, 0.578390819732489, 0.5858773188360982], "final_y": [5.681379894003114e-09, 3.9476388838995994e-08, 3.2635395222251365e-08]}, "mutation_prompt": null}
{"id": "c27d7cc8-0eb2-4841-b9a5-adee193a61fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, self.budget // 3), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploration by increasing the initial sample size to improve the initial guess accuracy.", "configspace": "", "generation": 4, "fitness": 0.6547103779508225, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.655 with standard deviation 0.103. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5d10c7a7-213f-4b21-a0f8-46e4e023f884", "metadata": {"aucs": [0.7998629952838806, 0.578390819732489, 0.5858773188360982], "final_y": [0.0, 3.9476388838995994e-08, 3.2635395222251365e-08]}, "mutation_prompt": null}
{"id": "31882bd0-8f78-4280-b365-a35d95510c44", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, self.budget // 3), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget)) \n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            # Modified line: Switching to 'trust-constr' method for enhanced convergence\n            result = minimize(\n                func,\n                result.x,\n                method='trust-constr',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Utilize dynamic adaptation of local bounds to improve convergence speed and solution accuracy.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"_minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\").", "error": "TypeError(\"_minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\")", "parent_id": "c27d7cc8-0eb2-4841-b9a5-adee193a61fc", "metadata": {}, "mutation_prompt": null}
{"id": "2477b09d-3f5a-428b-b202-ec64b5e2f5f1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamic initial sample size based on remaining budget\n        num_samples = min(max(7, (self.budget - self.evals) // 3), (self.budget - self.evals) // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce a dynamic sampling strategy by adjusting the number of initial samples based on the remaining budget to enhance exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.6022974374834492, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.602 with standard deviation 0.023. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c27d7cc8-0eb2-4841-b9a5-adee193a61fc", "metadata": {"aucs": [0.5986514237294382, 0.6320508697423389, 0.5761900189785705], "final_y": [6.528407528955343e-08, 2.256544066955098e-09, 5.591943764574693e-08]}, "mutation_prompt": null}
{"id": "55d96df7-3eea-401e-ad33-5af3a36a7806", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for adaptive initial sample size\n        num_samples = min(max(7, self.budget // 3), self.budget // (2 + self.evals // 50))\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Narrow the bounds based on the best-found solution\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Implement adaptive sampling size by dynamically adjusting the initial sample size based on the remaining budget.  ", "configspace": "", "generation": 5, "fitness": 0.5845624292915974, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.585 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c27d7cc8-0eb2-4841-b9a5-adee193a61fc", "metadata": {"aucs": [0.5701067480076567, 0.5799620230736904, 0.6036185167934451], "final_y": [1.599238741922587e-07, 5.6318319439103416e-08, 3.771038635118406e-08]}, "mutation_prompt": null}
{"id": "8f2996b5-c7c9-4670-807b-83a01e30c46e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, self.budget // 3), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce a refined dynamic scaling factor for adaptive bound adjustments to increase convergence efficiency.", "configspace": "", "generation": 5, "fitness": 0.6088383394152616, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.609 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c27d7cc8-0eb2-4841-b9a5-adee193a61fc", "metadata": {"aucs": [0.5986514237294382, 0.6320508697423389, 0.5958127247740075], "final_y": [6.528407528955343e-08, 2.256544066955098e-09, 3.6765519041035817e-09]}, "mutation_prompt": null}
{"id": "fb33df31-bcb4-4086-b0dc-6ba17a321aab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, self.budget // 3), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Introduce dynamic adjustment of initial guess diversity\n            diversity_adjustment = np.random.normal(0, 0.01, self.dim)\n            dynamic_guess = result.x + diversity_adjustment\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                dynamic_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve local search efficiency by introducing a strategy to dynamically adjust the initial guess diversity based on current solution quality.", "configspace": "", "generation": 5, "fitness": 0.6052916648154123, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c27d7cc8-0eb2-4841-b9a5-adee193a61fc", "metadata": {"aucs": [0.5986514237294382, 0.6320508697423389, 0.5851727009744598], "final_y": [6.528407528955343e-08, 2.256544066955098e-09, 7.324766075178066e-08]}, "mutation_prompt": null}
{"id": "289f4ec3-689d-4e90-81a8-ad08b7944f49", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, self.budget // 3), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.18 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Fine-tune the dynamic scaling factor for adaptive bound adjustments to enhance convergence rate.", "configspace": "", "generation": 6, "fitness": 0.5076582280522671, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.508 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8f2996b5-c7c9-4670-807b-83a01e30c46e", "metadata": {"aucs": [0.5631025198461099, 0.46531993738143185, 0.4945522269292598], "final_y": [8.902108130509981e-08, 6.859719040199672e-08, 7.423835686140827e-09]}, "mutation_prompt": null}
{"id": "eecaf422-12eb-4333-ab31-631bd6a24629", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Implement a dynamic sampling strategy to enhance initial exploration by adjusting the initial sample size based on remaining budget.", "configspace": "", "generation": 6, "fitness": 0.5557345020519429, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.556 with standard deviation 0.118. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8f2996b5-c7c9-4670-807b-83a01e30c46e", "metadata": {"aucs": [0.7205030193227951, 0.49503991145308857, 0.451660575379945], "final_y": [0.0, 2.256544066955098e-09, 2.7596597508969434e-07]}, "mutation_prompt": null}
{"id": "d59a289c-656e-4ca9-9eaa-a64b8cb6ea27", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, self.budget // 3), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.2 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance initial sampling strategy by ensuring diversity and improve dynamic scaling for better convergence.", "configspace": "", "generation": 6, "fitness": 0.5256006363502707, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.526 with standard deviation 0.055. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8f2996b5-c7c9-4670-807b-83a01e30c46e", "metadata": {"aucs": [0.6030512012521221, 0.4902276650751436, 0.4835230427235465], "final_y": [3.9252670752396975e-08, 2.4045552823675397e-08, 3.729849732786209e-08]}, "mutation_prompt": null}
{"id": "fe55bfb2-c45b-4fc6-965d-ab78922c19f5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(10, self.budget // 2), self.budget - 1)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance initial exploration by increasing initial sample size while ensuring robust convergence with adaptive bounds.", "configspace": "", "generation": 6, "fitness": 0.5147410217455531, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.515 with standard deviation 0.099. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8f2996b5-c7c9-4670-807b-83a01e30c46e", "metadata": {"aucs": [0.6536941016790268, 0.46170606617169385, 0.4288228973859387], "final_y": [0.0, 7.441493797803646e-08, 3.472262064478007e-07]}, "mutation_prompt": null}
{"id": "6c0aa1ab-6c93-4d9f-86f5-7653dd61d0b8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, self.budget // 3), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[np.argsort(initial_values)[num_samples // 2]]  # Changed to median\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve local search initialization by using median instead of min to select the best initial guess.", "configspace": "", "generation": 6, "fitness": 0.5331989258207946, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.533 with standard deviation 0.092. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8f2996b5-c7c9-4670-807b-83a01e30c46e", "metadata": {"aucs": [0.6630688061433125, 0.47648744731683257, 0.46004052400223894], "final_y": [0.0, 3.0115612063344305e-08, 1.0678382865869882e-07]}, "mutation_prompt": null}
{"id": "89e8e890-e05b-4599-9fe9-5ea116cf598c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B with adaptive learning rate\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals, 'learning_rate': 'adaptive'}  # Added adaptive learning rate\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate adaptive learning rate in L-BFGS-B to enhance convergence efficiency.", "configspace": "", "generation": 7, "fitness": 0.5530293638292946, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.553 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eecaf422-12eb-4333-ab31-631bd6a24629", "metadata": {"aucs": [0.5762432578247262, 0.55282447773092, 0.5300203559322376], "final_y": [1.4527248692894594e-08, 3.0115612063344305e-08, 1.0678382865869882e-07]}, "mutation_prompt": null}
{"id": "b7e030d8-0692-43c8-a02c-91d1c651e3e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce adaptive exploration by incrementally increasing the initial sample size based on budget utilization.", "configspace": "", "generation": 7, "fitness": 0.613470835680311, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.613 with standard deviation 0.089. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eecaf422-12eb-4333-ab31-631bd6a24629", "metadata": {"aucs": [0.737826060635704, 0.5678283577610493, 0.5347580886441798], "final_y": [0.0, 9.64636196119778e-08, 1.1440877852802082e-07]}, "mutation_prompt": null}
{"id": "4ca781af-1101-4876-9ee1-91f601b35131", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Adjust num_samples based on current evaluations\n            num_samples = min(max(7, int((self.budget - self.evals) * 0.1)), self.budget - self.evals)\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploration-exploitation balance by dynamically adapting the initial sample size based on the convergence status.", "configspace": "", "generation": 7, "fitness": 0.5530293638292946, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.553 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eecaf422-12eb-4333-ab31-631bd6a24629", "metadata": {"aucs": [0.5762432578247262, 0.55282447773092, 0.5300203559322376], "final_y": [1.4527248692894594e-08, 3.0115612063344305e-08, 1.0678382865869882e-07]}, "mutation_prompt": null}
{"id": "85b3cbf5-5da3-43e0-ac0a-10072fb369ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.3)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Adjust the initial sampling strategy to balance exploration and exploitation by reducing the sampling budget if initial guesses are consistent.", "configspace": "", "generation": 7, "fitness": 0.5279483732950745, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.528 with standard deviation 0.034. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eecaf422-12eb-4333-ab31-631bd6a24629", "metadata": {"aucs": [0.569966073153061, 0.5261725782777644, 0.4877064684543979], "final_y": [4.1990789732774574e-07, 7.910557293415384e-08, 4.447405631241221e-07]}, "mutation_prompt": null}
{"id": "434538d9-003d-4750-b36e-9de6709d43c4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Implement dynamic convergence scaling to improve the adaptive local refinement step.", "configspace": "", "generation": 7, "fitness": 0.5530293638292946, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.553 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eecaf422-12eb-4333-ab31-631bd6a24629", "metadata": {"aucs": [0.5762432578247262, 0.55282447773092, 0.5300203559322376], "final_y": [1.4527248692894594e-08, 3.0115612063344305e-08, 1.0678382865869882e-07]}, "mutation_prompt": null}
{"id": "b1732628-1555-4731-aea5-1f2bdac4ba3a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Refine convergence by adjusting the strategy to utilize past exploration data for better local optimization initiation.", "configspace": "", "generation": 8, "fitness": 0.5530293636258191, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.553 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b7e030d8-0692-43c8-a02c-91d1c651e3e5", "metadata": {"aucs": [0.5762432578247262, 0.55282447773092, 0.5300203553218112], "final_y": [1.4527248692894594e-08, 3.0115612063344305e-08, 1.0678383236351046e-07]}, "mutation_prompt": null}
{"id": "3032fce3-e2ee-486f-8cb3-d6f940541128", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance conclusive search by fine-tuning dynamic scaling factor for tighter bound adjustments.", "configspace": "", "generation": 8, "fitness": 0.5295166602097375, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.530 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b7e030d8-0692-43c8-a02c-91d1c651e3e5", "metadata": {"aucs": [0.5394183247812662, 0.5120885731004482, 0.5370430827474982], "final_y": [9.590958341991309e-08, 2.995029006097858e-07, 2.8492895436997243e-08]}, "mutation_prompt": null}
{"id": "a6c716ec-e337-48ae-a107-925b97f86e07", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds using Nelder-Mead\n            result = minimize(\n                func,\n                result.x,\n                method='Nelder-Mead',  # Changed from 'L-BFGS-B' to 'Nelder-Mead'\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce dynamic adjustment of the local search method from L-BFGS-B to Nelder-Mead based on the remaining budget for potentially better convergence towards the end.", "configspace": "", "generation": 8, "fitness": 0.5530293636258191, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.553 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b7e030d8-0692-43c8-a02c-91d1c651e3e5", "metadata": {"aucs": [0.5762432578247262, 0.55282447773092, 0.5300203553218112], "final_y": [1.4527248692894594e-08, 3.0115612063344305e-08, 1.0678383236351046e-07]}, "mutation_prompt": null}
{"id": "51b383ee-e61d-41d8-a383-a00dda668f42", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals, 'ftol': 1e-8}  # Changed tolerance for convergence\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.10 * (1 - self.evals / self.budget))  # Changed scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals, 'ftol': 1e-8}  # Changed tolerance for convergence\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local optimization by refining convergence parameters and adaptive bounds adjustments.", "configspace": "", "generation": 8, "fitness": 0.5530293636258191, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.553 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b7e030d8-0692-43c8-a02c-91d1c651e3e5", "metadata": {"aucs": [0.5762432578247262, 0.55282447773092, 0.5300203553218112], "final_y": [1.4527248692894594e-08, 3.0115612063344305e-08, 1.0678383236351046e-07]}, "mutation_prompt": null}
{"id": "59a897d7-2373-434f-968f-8f8475649989", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Add Gaussian noise to the current best position for better local exploration\n            noisy_guess = result.x + np.random.normal(0, scale_factor * (bounds[:, 1] - bounds[:, 0]), self.dim)\n            noisy_guess = np.clip(noisy_guess, bounds[:, 0], bounds[:, 1])  # Ensure within bounds\n            \n            # Another local search with adjusted bounds\n            result = minimize(\n                func,\n                noisy_guess,  # Use the noisy initial guess\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local exploration by using dynamic initial sample size and introducing Gaussian noise in the local search phase. ", "configspace": "", "generation": 8, "fitness": 0.5530293636258191, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.553 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b7e030d8-0692-43c8-a02c-91d1c651e3e5", "metadata": {"aucs": [0.5762432578247262, 0.55282447773092, 0.5300203553218112], "final_y": [1.4527248692894594e-08, 3.0115612063344305e-08, 1.0678383236351046e-07]}, "mutation_prompt": null}
{"id": "6a41a934-0c38-4809-b6f9-42b1a64a338d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim)) + np.random.normal(0, 0.01, (num_samples, self.dim))  # Changed line\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance convergence by employing an adaptive sampling strategy for initial exploration.", "configspace": "", "generation": 9, "fitness": 0.5681677851598991, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.568 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1732628-1555-4731-aea5-1f2bdac4ba3a", "metadata": {"aucs": [0.5833315810568458, 0.5678283577610493, 0.5533434166618019], "final_y": [3.4390496926948367e-08, 9.64636196119778e-08, 2.11993288652067e-08]}, "mutation_prompt": null}
{"id": "a6964410-7818-4ffa-a2fc-caa9da752fae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            improvement_rate = np.abs(initial_values[best_idx] - result.fun) / initial_values[best_idx]  # New line\n            scale_factor = max(0.05, 0.15 * improvement_rate)  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance convergence by incorporating adaptive learning rate adjustments based on the rate of improvement in function values.  ", "configspace": "", "generation": 9, "fitness": 0.5376770580320676, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.538 with standard deviation 0.003. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1732628-1555-4731-aea5-1f2bdac4ba3a", "metadata": {"aucs": [0.5423529663363503, 0.5353221145523293, 0.5353560932075233], "final_y": [3.566386458670221e-08, 6.675779147674142e-08, 1.4494310304451048e-07]}, "mutation_prompt": null}
{"id": "51c17e2d-0fb1-46de-94c9-3f5362bbdd4e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for dynamically setting initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4 * (1 + 0.1 * np.log10(self.dim)))), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improved convergence by dynamically adjusting sample density during initial exploration phase.", "configspace": "", "generation": 9, "fitness": 0.5286873808452784, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.529 with standard deviation 0.005. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1732628-1555-4731-aea5-1f2bdac4ba3a", "metadata": {"aucs": [0.5223286318075936, 0.5306300558534911, 0.5331034548747505], "final_y": [5.351639552929597e-08, 2.0040563963467127e-07, 4.925518724495819e-08]}, "mutation_prompt": null}
{"id": "1685051f-e8d6-43d5-a71f-75ce79d397b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for adaptive initial sample size\n        num_samples = min(max(7, int(self.budget * 0.3)), self.budget // 2)  # Changed initial sample size calculation\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate and budget\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor calculation\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhanced convergence by introducing adaptive sampling size based on remaining budget and refined scaling factor calculation.", "configspace": "", "generation": 9, "fitness": 0.5639262300047178, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.564 with standard deviation 0.036. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1732628-1555-4731-aea5-1f2bdac4ba3a", "metadata": {"aucs": [0.6141963448290282, 0.5405392624376267, 0.5370430827474982], "final_y": [9.590958341991309e-08, 3.1951848940394656e-09, 2.8492895436997243e-08]}, "mutation_prompt": null}
{"id": "d5dffdd2-158d-420c-a2e2-fc7bc39322c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim))\n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals, 'gtol': 1e-8}  # Modified for adaptive precision\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Random restart strategy based on remaining budget\n            if self.evals < self.budget * 0.75:  # New line for strategic restart mechanism\n                best_guess = np.random.uniform(bounds[:, 0], bounds[:, 1], self.dim)  # New line for restart\n                result = minimize(\n                    func,\n                    best_guess,\n                    method='L-BFGS-B',\n                    bounds=narrowed_bounds,\n                    options={'maxfun': self.budget - self.evals, 'gtol': 1e-8}  # Adaptive precision maintained\n                )\n            else:\n                result = minimize(\n                    func,\n                    result.x,\n                    method='L-BFGS-B',\n                    bounds=narrowed_bounds,\n                    options={'maxfun': self.budget - self.evals, 'gtol': 1e-8}  # Ensuring precision control\n                )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local search efficiency by incorporating adaptive precision control and strategic restart mechanism for better convergence.", "configspace": "", "generation": 9, "fitness": 0.5561698737618371, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.556 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b1732628-1555-4731-aea5-1f2bdac4ba3a", "metadata": {"aucs": [0.5815468675331528, 0.55282447773092, 0.5341382760214386], "final_y": [0.0, 3.0115612063344305e-08, 7.523395896360328e-08]}, "mutation_prompt": null}
{"id": "41ad0a13-862b-4fc8-b365-2626b52e7ffb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve adaptive sampling by incorporating Sobol sequences for better coverage of the parameter space.", "configspace": "", "generation": 10, "fitness": 0.562772770221614, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.563 with standard deviation 0.047. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6a41a934-0c38-4809-b6f9-42b1a64a338d", "metadata": {"aucs": [0.6286802980706998, 0.5393691333278948, 0.5202688792662474], "final_y": [1.0569769457678796e-07, 1.370261610837915e-07, 1.606067865245868e-07]}, "mutation_prompt": null}
{"id": "4b05e6b4-595b-4998-bf0c-d03e6d2cf0a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim)) + np.random.normal(0, 0.02, (num_samples, self.dim))  # Changed line\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.20 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Optimize convergence by tweaking sampling randomness and scaling dynamically.", "configspace": "", "generation": 10, "fitness": 0.537450359657799, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.537 with standard deviation 0.003. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6a41a934-0c38-4809-b6f9-42b1a64a338d", "metadata": {"aucs": [0.5408112864987297, 0.5334491563114785, 0.5380906361631886], "final_y": [2.8965677267793694e-08, 1.2061465594835846e-07, 6.964761720307656e-08]}, "mutation_prompt": null}
{"id": "f0701ca0-380e-4d66-a99c-2c987c99dadf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim)) + np.random.normal(0, 0.01, (num_samples, self.dim))\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Adjusted line here to use the previous best parameters instead of best_guess\n            result = minimize(\n                func,\n                result.x,  # Changed line\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve the adaptive bound adjustment phase by incorporating feedback from convergence trends.", "configspace": "", "generation": 10, "fitness": 0.5511412431113881, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.551 with standard deviation 0.050. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6a41a934-0c38-4809-b6f9-42b1a64a338d", "metadata": {"aucs": [0.6215440517329545, 0.507644116838774, 0.5242355607624358], "final_y": [1.0819676281033038e-07, 3.767892570607108e-07, 2.4330720652491887e-07]}, "mutation_prompt": null}
{"id": "f9f208f4-912d-4e8d-be15-ac1f8979d979", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim)) + np.random.normal(0, 0.005, (num_samples, self.dim))  # Changed line\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve convergence by refining the adaptive sampling noise for more precise initial exploration.", "configspace": "", "generation": 10, "fitness": 0.5283728959132512, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.528 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6a41a934-0c38-4809-b6f9-42b1a64a338d", "metadata": {"aucs": [0.5254806751456111, 0.5393691333278948, 0.5202688792662474], "final_y": [1.1031006299723288e-07, 1.370261610837915e-07, 1.606067865245868e-07]}, "mutation_prompt": null}
{"id": "93403439-9dcf-49be-82dc-90c4b01d7786", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        initial_samples = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_samples, self.dim)) + np.random.normal(0, 0.01, (num_samples, self.dim))  # Changed line\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x from the previous iteration for further refinement\n            best_guess = result.x  # Changed line\n            \n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve convergence by dynamically adjusting the local optimizer strategy based on early sampling feedback.", "configspace": "", "generation": 10, "fitness": 0.5264131618514116, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.526 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6a41a934-0c38-4809-b6f9-42b1a64a338d", "metadata": {"aucs": [0.5196014729600924, 0.5393691333278948, 0.5202688792662474], "final_y": [1.4535018857552274e-07, 1.370261610837915e-07, 1.606067865245868e-07]}, "mutation_prompt": null}
{"id": "e5b83cdf-27cd-4168-a4b8-15d257929863", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol, Halton\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Halton(d=self.dim, scramble=True)  # Changed Sobol to Halton\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance initial sampling by incorporating a Halton sequence for broader initial coverage and diversity.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'Halton' object has no attribute 'random_base2'\").", "error": "AttributeError(\"'Halton' object has no attribute 'random_base2'\")", "parent_id": "41ad0a13-862b-4fc8-b365-2626b52e7ffb", "metadata": {}, "mutation_prompt": null}
{"id": "4d536516-b1db-4a40-a4d5-31fb568e23af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B with momentum\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals, 'gtol': 1e-5}  # Added gradient tolerance\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals, 'gtol': 1e-5}  # Added gradient tolerance\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local exploration by incorporating momentum in the optimization process to exploit the smooth cost function landscape.", "configspace": "", "generation": 11, "fitness": 0.6354882914930219, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.635 with standard deviation 0.035. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "41ad0a13-862b-4fc8-b365-2626b52e7ffb", "metadata": {"aucs": [0.611728357197093, 0.6093485371784155, 0.6853879801035575], "final_y": [2.0014314215145746e-07, 7.401791119617665e-08, 1.2981321917849635e-09]}, "mutation_prompt": null}
{"id": "055a451a-3721-47b6-bfaf-090a1d78eaf4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted line for increased initial sample size\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Refined dynamic scaling factor based on convergence rate\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))  # Changed dynamic scaling factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                result.x,  # Changed variable to use the latest result for continued refinement\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate dynamic local search strategy based on convergence rate to enhance the precision of refined optimization.", "configspace": "", "generation": 11, "fitness": 0.6230776521283556, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.623 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "41ad0a13-862b-4fc8-b365-2626b52e7ffb", "metadata": {"aucs": [0.6091330552786953, 0.6238928492547089, 0.6362070518516626], "final_y": [2.3486947290871193e-07, 1.4028119463923223e-07, 7.311756330142725e-08]}, "mutation_prompt": null}
{"id": "9e4252e6-74aa-4b18-9354-7ba8580978fb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using TNC\n        result = minimize(\n            func,\n            best_guess,\n            method='TNC',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='TNC',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local optimization by switching to the 'TNC' method for potentially better handling of bounds.", "configspace": "", "generation": 11, "fitness": 0.6632186653511107, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.663 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "41ad0a13-862b-4fc8-b365-2626b52e7ffb", "metadata": {"aucs": [0.6869243836084118, 0.6058869879436417, 0.6968446245012787], "final_y": [2.1086176263413324e-08, 1.4685980540556712e-07, 1.104807604106881e-08]}, "mutation_prompt": null}
{"id": "22237f5b-6ba8-4273-8cef-47b7044170a0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass ImprovedMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Sobol sequence for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best sample and refine using trust-region method\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n\n        # Use trust-region method for local optimization\n        result = minimize(\n            func,\n            best_guess,\n            method='trust-constr',\n            bounds=local_bounds,\n            options={'maxiter': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        # Step 3: Progressive tightening of bounds\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n\n            result = minimize(\n                func,\n                result.x,\n                method='trust-constr',\n                bounds=narrowed_bounds,\n                options={'maxiter': self.budget - self.evals}\n            )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "ImprovedMetaheuristicOptimizer", "description": "Enhance the optimizer by combining Sobol sequence-based sampling with a progressive tightening of bounds using trust-region methods to efficiently navigate and exploit the smooth landscape.", "configspace": "", "generation": 11, "fitness": 0.5489730746360818, "feedback": "The algorithm ImprovedMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.549 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "41ad0a13-862b-4fc8-b365-2626b52e7ffb", "metadata": {"aucs": [0.5354745315808974, 0.5237355650855176, 0.5877091272418301], "final_y": [1.5383828080597706e-07, 1.6556570020195895e-07, 2.0395289037533615e-07]}, "mutation_prompt": null}
{"id": "52b455ee-f6c6-428d-8fa4-c628591252c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using SLSQP\n        result = minimize(\n            func,\n            best_guess,\n            method='SLSQP',\n            bounds=local_bounds,\n            options={'maxiter': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='SLSQP',\n                bounds=narrowed_bounds,\n                options={'maxiter': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local optimization by switching to the 'SLSQP' method for improved handling of constraints and potential for better convergence.", "configspace": "", "generation": 12, "fitness": 0.6267291489700217, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.627 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9e4252e6-74aa-4b18-9354-7ba8580978fb", "metadata": {"aucs": [0.6150444794140437, 0.6508861500901326, 0.6142568174058889], "final_y": [2.1716460651799156e-07, 7.330368513413771e-08, 1.4135476207289046e-07]}, "mutation_prompt": null}
{"id": "aca8916f-9573-4a9d-97c2-366e83d55bbf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate the L-BFGS-B method for enhanced local optimization, leveraging its superior handling of bounds and convergence properties.", "configspace": "", "generation": 12, "fitness": 0.6400556217188288, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.640 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9e4252e6-74aa-4b18-9354-7ba8580978fb", "metadata": {"aucs": [0.6020311903788291, 0.640203972622763, 0.6779317021548943], "final_y": [2.2993399325653862e-07, 5.06526380874909e-08, 2.6330926035815232e-08]}, "mutation_prompt": null}
{"id": "8fa231f7-90f5-4ed6-902d-b7d18ae367d7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Combine Sobol sampling with a gradient-free optimizer 'L-BFGS-B' and adaptively adjust bounds for enhanced local exploration.", "configspace": "", "generation": 12, "fitness": 0.6468646800961638, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.647 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9e4252e6-74aa-4b18-9354-7ba8580978fb", "metadata": {"aucs": [0.6646862997625584, 0.6241973213674431, 0.6517104191584899], "final_y": [6.543339397774625e-08, 1.5426854966384523e-07, 7.544665720664565e-08]}, "mutation_prompt": null}
{"id": "892449bd-7868-4b0f-abca-dc17a32bc399", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve local search by switching to the 'L-BFGS-B' method for potentially better handling of both bounds and smooth landscapes.", "configspace": "", "generation": 12, "fitness": 0.6304901746393914, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.630 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9e4252e6-74aa-4b18-9354-7ba8580978fb", "metadata": {"aucs": [0.6215439982680524, 0.6329600580111798, 0.6369664676389419], "final_y": [9.424854388629923e-08, 2.0961535364524057e-08, 1.8436996360625187e-07]}, "mutation_prompt": null}
{"id": "65893b6d-3469-4b2a-8c62-212ae8a5f0cb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(10, int(self.budget * 0.4)), self.budget // 2)  # Changed the minimum number of samples from 7 to 10\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using TNC\n        result = minimize(\n            func,\n            best_guess,\n            method='TNC',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.15 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='TNC',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve initial sampling by increasing the number of samples for better exploration.", "configspace": "", "generation": 12, "fitness": 0.6410033075304061, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.641 with standard deviation 0.029. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9e4252e6-74aa-4b18-9354-7ba8580978fb", "metadata": {"aucs": [0.6052887048229194, 0.6763319184719154, 0.6413892992963837], "final_y": [5.529252955939731e-08, 1.5464645711566282e-08, 8.96252255907023e-08]}, "mutation_prompt": null}
{"id": "6c4ab399-781a-41c7-8590-cc8d1e9cc7fb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        second_best_idx = np.argmin([v for i, v in enumerate(initial_values) if i != best_idx])\n        best_guess = (initial_samples[best_idx] + initial_samples[second_best_idx]) / 2  # Hybrid of top two samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance initial guess refinement by utilizing a hybrid approach combining best initial samples to increase robustness.", "configspace": "", "generation": 13, "fitness": 0.6452214452534414, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.645 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8fa231f7-90f5-4ed6-902d-b7d18ae367d7", "metadata": {"aucs": [0.657298529917558, 0.6278803189212565, 0.6504854869215095], "final_y": [2.239062086303723e-08, 1.2376186304310222e-07, 4.832763623658422e-08]}, "mutation_prompt": null}
{"id": "c8ce403a-cd77-4d0f-9608-5e76118b9c33", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.2 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate a dynamic scaling factor in the adaptive bound adjustment to enhance local exploration efficiency.", "configspace": "", "generation": 13, "fitness": 0.6290512703117083, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.629 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8fa231f7-90f5-4ed6-902d-b7d18ae367d7", "metadata": {"aucs": [0.6142015942236423, 0.6286031582523739, 0.6443490584591085], "final_y": [2.5509810836672786e-07, 2.04127280453941e-07, 9.78783570927129e-08]}, "mutation_prompt": null}
{"id": "8ffa40a6-2248-46d7-868e-a24700ac5aef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(10, int(self.budget * 0.5)), self.budget // 2)\n        \n        # Step 1: Dynamic adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        # Step 3: Adaptive restart based on convergence progress\n        while self.evals < self.budget:\n            if result.success:\n                result = minimize(\n                    func,\n                    result.x,\n                    method='L-BFGS-B',\n                    bounds=local_bounds,\n                    options={'maxfun': self.budget - self.evals}\n                )\n            else:\n                scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))\n                narrowed_bounds = [\n                    (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                    for i, (lb, ub) in enumerate(bounds)\n                ]\n                result = minimize(\n                    func,\n                    best_guess,\n                    method='L-BFGS-B',\n                    bounds=narrowed_bounds,\n                    options={'maxfun': self.budget - self.evals}\n                )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate Sobol sampling with L-BFGS-B, introducing dynamic scaling of sampling density and adaptive restart based on convergence speed.", "configspace": "", "generation": 13, "fitness": 0.6339664392106902, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.634 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8fa231f7-90f5-4ed6-902d-b7d18ae367d7", "metadata": {"aucs": [0.6201953750285352, 0.6513327869979171, 0.6303711556056184], "final_y": [2.962562043255392e-08, 2.3405919040517022e-08, 7.825052682769483e-08]}, "mutation_prompt": null}
{"id": "41243721-b113-4eca-8252-0d5f823a0740", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(10, int(self.budget * 0.3)), self.budget // 2)  # Adjusted sample size\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random(num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        while self.evals < self.budget:\n            if self.evals < 0.8 * self.budget:\n                scale_factor = max(0.05, 0.2 * (1 - self.evals / self.budget))  # Increased scale\n            else:\n                scale_factor = 0.05\n\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate Sobol sampling and 'L-BFGS-B' with multi-start and adaptive refinement for enhanced solution quality.", "configspace": "", "generation": 13, "fitness": 0.6370911665475534, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.637 with standard deviation 0.006. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8fa231f7-90f5-4ed6-902d-b7d18ae367d7", "metadata": {"aucs": [0.6321050080683042, 0.6460580935837235, 0.6331103979906326], "final_y": [6.0541990735356415e-09, 7.17689676946217e-08, 9.569712589939126e-08]}, "mutation_prompt": null}
{"id": "06f8f657-f85e-4c55-a591-fbdb6de7c96c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.3)), self.budget // 2)  # Adjusted sampling rate\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        best_guess = initial_samples[best_idx]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Combine Sobol sampling with 'L-BFGS-B' optimization, utilizing flexible Sobol sample size and adaptive bounds for efficient local exploration.", "configspace": "", "generation": 13, "fitness": 0.6445876747896666, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.645 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8fa231f7-90f5-4ed6-902d-b7d18ae367d7", "metadata": {"aucs": [0.6713241678736597, 0.6302254110034279, 0.6322134454919122], "final_y": [4.2888353194833906e-08, 1.1203878912287445e-07, 6.747002920684753e-08]}, "mutation_prompt": null}
{"id": "b7c206c2-a53f-44e3-8226-2ee8e59b35cb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        second_best_idx = np.argmin([v for i, v in enumerate(initial_values) if i != best_idx])\n        best_guess = (initial_samples[best_idx] + initial_samples[second_best_idx]) / 2  # Hybrid of top two samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce an adaptive Sobol sequence sampling to improve exploration efficiency.", "configspace": "", "generation": 14, "fitness": 0.6253723927669133, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.625 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6c4ab399-781a-41c7-8590-cc8d1e9cc7fb", "metadata": {"aucs": [0.6265989869452131, 0.607740102748862, 0.6417780886066646], "final_y": [8.056863417641896e-08, 1.0317209509038027e-07, 4.416966340423502e-08]}, "mutation_prompt": null}
{"id": "c18e6020-0d2a-4504-ab74-6bf32469485b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  # Hybrid of top three samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Utilize a more informed hybrid initial guess by averaging the top three samples for enhanced robustness.", "configspace": "", "generation": 14, "fitness": 0.6591337019539153, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.659 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6c4ab399-781a-41c7-8590-cc8d1e9cc7fb", "metadata": {"aucs": [0.6832569495439121, 0.6487378846858183, 0.6454062716320156], "final_y": [1.0120027796790243e-08, 2.104074822004302e-08, 3.084229685179558e-08]}, "mutation_prompt": null}
{"id": "fbb36041-f38f-4240-b3e9-bd6954a35cf1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        second_best_idx = np.argmin([v for i, v in enumerate(initial_values) if i != best_idx])\n        best_guess = (initial_samples[best_idx] + initial_samples[second_best_idx]) / 2  # Hybrid of top two samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        # Golden-section refinement step\n        result.x = result.x + 0.618 * (best_guess - result.x)\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Incorporate a golden-section refinement step to enhance the best guess precision.", "configspace": "", "generation": 14, "fitness": 0.634490891941064, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.634 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6c4ab399-781a-41c7-8590-cc8d1e9cc7fb", "metadata": {"aucs": [0.6423114756973736, 0.6216478939531972, 0.6395133061726213], "final_y": [2.6130054391870152e-08, 1.6232268444878528e-07, 2.8529431078936624e-08]}, "mutation_prompt": null}
{"id": "6cb4ee3b-3c00-447d-a1ab-01f105c9f182", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_idx = np.argmin(initial_values)\n        second_best_idx = np.argmin([v for i, v in enumerate(initial_values) if i != best_idx])\n        best_guess = (0.7 * initial_samples[best_idx] + 0.3 * initial_samples[second_best_idx])  # Weighted hybrid of top two samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve solution precision by refining the hybrid initial guess using a weighted average of the top samples.", "configspace": "", "generation": 14, "fitness": 0.6359435500559895, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.636 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6c4ab399-781a-41c7-8590-cc8d1e9cc7fb", "metadata": {"aucs": [0.6146095136117682, 0.6462918542499961, 0.6469292823062041], "final_y": [1.412928549500485e-07, 3.9018068887672385e-08, 1.3598438185176965e-07]}, "mutation_prompt": null}
{"id": "40f9b75d-d737-4027-8a01-a0af2339da54", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Rank the initial samples\n        sorted_indices = np.argsort(initial_values)\n        \n        # Modified line: Weighted average for the top two guesses\n        best_guess = 0.7 * initial_samples[sorted_indices[0]] + 0.3 * initial_samples[sorted_indices[1]]\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce a rank-based weighting for initial samples to improve the robustness of the initial guess.", "configspace": "", "generation": 14, "fitness": 0.6316509484340592, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.632 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6c4ab399-781a-41c7-8590-cc8d1e9cc7fb", "metadata": {"aucs": [0.637296641158082, 0.6409558604737982, 0.6167003436702974], "final_y": [7.527673692044851e-08, 5.205461439328425e-08, 9.790805360694326e-08]}, "mutation_prompt": null}
{"id": "62fd2b8a-1c93-432b-887f-1b76cc76276a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        # Adjusted number of samples based on remaining budget\n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  # Hybrid of top three samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Introduce a dynamic adjustment of the initial sampling size based on the remaining budget to enhance exploration.", "configspace": "", "generation": 15, "fitness": 0.6378863153674752, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.638 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c18e6020-0d2a-4504-ab74-6bf32469485b", "metadata": {"aucs": [0.6614683573848135, 0.6364338400603158, 0.6157567486572965], "final_y": [2.2622920890689285e-09, 5.6524508237858816e-08, 1.9441298637777832e-07]}, "mutation_prompt": null}
{"id": "0459397f-7953-4f36-ab9f-8fbecb38ac51", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  # Hybrid of top three samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Refine bounds adjustment using adaptive increments to enhance local search efficiency.", "configspace": "", "generation": 15, "fitness": 0.6496518221791001, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.650 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c18e6020-0d2a-4504-ab74-6bf32469485b", "metadata": {"aucs": [0.6742752240506587, 0.6387638080804103, 0.6359164344062314], "final_y": [5.880861549166835e-09, 7.921134676322136e-08, 1.0899862665481783e-07]}, "mutation_prompt": null}
{"id": "87877adc-c8bf-49a2-8cde-031a4715293f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  # Hybrid of top three samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            if self.evals % (self.budget // 4) == 0:  # Restart mechanism at regular intervals\n                initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n                initial_values = [func(x) for x in initial_samples]\n                best_indices = np.argsort(initial_values)[:3]\n                best_guess = np.mean(initial_samples[best_indices], axis=0)\n            \n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Implement a restart mechanism to diversify search after local convergence.", "configspace": "", "generation": 15, "fitness": 0.6253304613513212, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.625 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c18e6020-0d2a-4504-ab74-6bf32469485b", "metadata": {"aucs": [0.6404861621298237, 0.627496860575459, 0.6080083613486811], "final_y": [1.0605188363216977e-07, 1.1663925437667671e-07, 2.775208903319651e-07]}, "mutation_prompt": null}
{"id": "b81734c2-6425-4fab-aa80-86aeac0f3624", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol, LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)  # Changed from Sobol to LatinHypercube\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  # Hybrid of top three samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Refine initial sampling with Latin Hypercube Sampling (LHS) for better initial coverage.", "configspace": "", "generation": 15, "fitness": 0.6581862727283011, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.658 with standard deviation 0.021. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c18e6020-0d2a-4504-ab74-6bf32469485b", "metadata": {"aucs": [0.6390328178488192, 0.6875841129565847, 0.6479418873794994], "final_y": [1.1408446321948544e-07, 1.6777711326813128e-08, 8.010066916294317e-08]}, "mutation_prompt": null}
{"id": "bf34f264-0302-4979-bece-692430b05f4f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  # Hybrid of top three samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        # Adding coordinate descent step for parameter refinement\n        for i in range(self.dim):\n            coord_bounds = [(lb, ub) if j == i else (result.x[j], result.x[j]) for j, (lb, ub) in enumerate(bounds)]\n            result = minimize(func, result.x, method='L-BFGS-B', bounds=coord_bounds, options={'maxfun': self.budget - self.evals})\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate an additional step of coordinate descent to refine the result further by optimizing one parameter at a time.", "configspace": "", "generation": 15, "fitness": 0.6528883195225433, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.653 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c18e6020-0d2a-4504-ab74-6bf32469485b", "metadata": {"aucs": [0.63574596293114, 0.6679462549122337, 0.6549727407242563], "final_y": [5.936163527684595e-08, 2.993007373808214e-08, 5.0473799133764275e-08]}, "mutation_prompt": null}
{"id": "78fcfadb-af3b-490a-8100-cb459fdd0f63", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\nfrom skopt import gp_minimize\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim) \n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with Bayesian optimization\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  \n        \n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Replace local optimizer with Gaussian Process-based Bayesian optimization\n        result = gp_minimize(\n            func=lambda x: func(np.array(x)),\n            dimensions=[(lb, ub) for lb, ub in bounds],\n            n_calls=self.budget - self.evals,\n            x0=best_guess.tolist(),\n            random_state=42\n        )\n        \n        self.evals += len(result.func_vals)\n        \n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            result = gp_minimize(\n                func=lambda x: func(np.array(x)),\n                dimensions=narrowed_bounds,\n                n_calls=self.budget - self.evals,\n                x0=result.x.tolist(),\n                random_state=42\n            )\n            \n            self.evals += len(result.func_vals)\n        \n        return np.array(result.x)", "name": "NovelMetaheuristicOptimizer", "description": "Enhance sampling and local exploration with Bayesian optimization for improved convergence.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'skopt'\").", "error": "ModuleNotFoundError(\"No module named 'skopt'\")", "parent_id": "b81734c2-6425-4fab-aa80-86aeac0f3624", "metadata": {}, "mutation_prompt": null}
{"id": "8a241d3c-dae4-4377-8568-75d68759eade", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MultiStartSequentialOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Calculate number of random starts and iterations per local optimization\n        num_starts = min(max(5, int(self.budget * 0.3)), self.budget // 4)\n        local_iters = self.budget // num_starts\n\n        best_solution = None\n        best_value = float('inf')\n\n        for _ in range(num_starts):\n            # Randomly sample an initial guess within the bounds\n            initial_guess = np.random.rand(self.dim) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n            \n            # Local optimization using L-BFGS-B\n            result = minimize(\n                func,\n                initial_guess,\n                method='L-BFGS-B',\n                bounds=local_bounds,\n                options={'maxfun': min(local_iters, self.budget - self.evals)}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n            \n            # Check and update the best solution found so far\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n            # Stop if the budget is exhausted\n            if self.evals >= self.budget:\n                break\n        \n        return best_solution", "name": "MultiStartSequentialOptimizer", "description": "Employs a multi-start approach with Random Sampling and Sequential Local Optimization to effectively explore and exploit the solution space.", "configspace": "", "generation": 16, "fitness": 0.17191680041120272, "feedback": "The algorithm MultiStartSequentialOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.172 with standard deviation 0.028. And the mean value of best solutions found was 0.398 (0. is the best) with standard deviation 0.125.", "error": "", "parent_id": "b81734c2-6425-4fab-aa80-86aeac0f3624", "metadata": {"aucs": [0.13724693444255798, 0.20563141817534003, 0.17287204861571015], "final_y": [0.5698937508208256, 0.2756700992174251, 0.34975714544532344]}, "mutation_prompt": null}
{"id": "aa6e7e5b-01bb-428b-b951-40b1943ee9d5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol, LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)  # Changed from Sobol to LatinHypercube\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  # Hybrid of top three samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhanced initial sampling by using Gaussian sampling to improve convergence.", "configspace": "", "generation": 16, "fitness": 0.5402193449648804, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.540 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b81734c2-6425-4fab-aa80-86aeac0f3624", "metadata": {"aucs": [0.5607249998644357, 0.5366065349121594, 0.5233265001180463], "final_y": [7.146586397771133e-08, 9.917436973680138e-08, 1.2969802084036817e-08]}, "mutation_prompt": null}
{"id": "a8ac345f-9f09-439a-a5c5-a2689f05614b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol, LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)  # Changed from Sobol to LatinHypercube\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:2]  # Changed from top three to top two samples\n        best_guess = np.mean(initial_samples[best_indices], axis=0)  # Hybrid of top two samples\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Adjusted factor\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use best_guess instead of result.x to further leverage initial sampling data\n            result = minimize(\n                func,\n                best_guess,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Utilize the top two initial samples for a more reliable starting point in local optimization.", "configspace": "", "generation": 16, "fitness": 0.5424797965462825, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.542 with standard deviation 0.006. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b81734c2-6425-4fab-aa80-86aeac0f3624", "metadata": {"aucs": [0.5514097998562861, 0.5396530542059355, 0.5363765355766261], "final_y": [4.7919527126056125e-08, 3.328342190955596e-08, 1.7867751002329187e-08]}, "mutation_prompt": null}
{"id": "37413c2d-ef02-492a-82a7-f667aad2e738", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.05 * (1 - self.evals / self.budget))  # Adjusted factor by halving\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploitation by iteratively refining the best guess using a shrinking neighborhood.", "configspace": "", "generation": 16, "fitness": 0.5477104958612143, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.548 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b81734c2-6425-4fab-aa80-86aeac0f3624", "metadata": {"aucs": [0.5045623566659527, 0.5563605195568972, 0.5822086113607929], "final_y": [1.6646313838771652e-07, 4.420435247351267e-08, 1.4050989765258827e-08]}, "mutation_prompt": null}
{"id": "11bbe0df-bf06-4bfd-9da9-0a0b1ffdde68", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(10, int(self.budget * 0.4)), self.budget // 2)  # Increased minimum samples\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.05 * (1 - self.evals / self.budget))  # Adjusted factor by halving\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve solution accuracy by expanding the initial sampling phase for enhanced exploration.", "configspace": "", "generation": 17, "fitness": 0.5307081994076138, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.531 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37413c2d-ef02-492a-82a7-f667aad2e738", "metadata": {"aucs": [0.5283899460453085, 0.551985327827368, 0.511749324350165], "final_y": [1.412334936257627e-07, 3.694804771571624e-08, 2.1672082088301563e-07]}, "mutation_prompt": null}
{"id": "90b0e2f8-f2e0-40f9-bc5d-a75e64596ee0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:2]  # Changed from 3 to 2 to refine strategy\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.05 * (1 - self.evals / self.budget))  # Adjusted factor by halving\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance search by refining adaptive bounds using top two initial guesses as starting points for optimization.", "configspace": "", "generation": 17, "fitness": 0.5277168800847397, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.528 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37413c2d-ef02-492a-82a7-f667aad2e738", "metadata": {"aucs": [0.5470265374872423, 0.5283977696571966, 0.5077263331097803], "final_y": [4.3543763972411846e-08, 3.009361300882154e-07, 1.8662812762532086e-07]}, "mutation_prompt": null}
{"id": "e6b39dfb-72ff-4d73-b195-c33f82ac6450", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.05 * (1 - self.evals / self.budget))  # Adjusted factor by halving\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n\n            # Adjust sampling based on remaining budget\n            if self.evals < self.budget - num_samples:  # Added dynamic adjustment condition\n                num_samples = min(max(3, int((self.budget - self.evals) * 0.2)), self.budget - self.evals)\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploitation by refining the best guess using shrinking neighborhood and dynamically adjusting sampling based on remaining budget.", "configspace": "", "generation": 17, "fitness": 0.5109564687446783, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.511 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37413c2d-ef02-492a-82a7-f667aad2e738", "metadata": {"aucs": [0.5263853871627037, 0.5059215733594673, 0.5005624457118638], "final_y": [9.663310751128024e-08, 3.3461846599833813e-07, 4.0343216458961886e-07]}, "mutation_prompt": null}
{"id": "61602270-a795-4131-84aa-11d0f72fae85", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploitation by dynamically adjusting the scaling factor based on iteration progress and success rate.", "configspace": "", "generation": 17, "fitness": 0.5346498102542819, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.535 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37413c2d-ef02-492a-82a7-f667aad2e738", "metadata": {"aucs": [0.5135536870411204, 0.5194047671494075, 0.5709909765723178], "final_y": [1.6865017211238614e-07, 2.408678442627574e-07, 7.167117847330592e-09]}, "mutation_prompt": null}
{"id": "8dae41c4-c81a-4139-ae63-3206c3781f73", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(10, int(self.budget * 0.5)), self.budget // 2)  # Changed from 7 to 10 and budget * 0.4 to 0.5\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.05 * (1 - self.evals / self.budget))  # Adjusted factor by halving\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve exploration by increasing initial sample size for better initial guesses.", "configspace": "", "generation": 17, "fitness": 0.5251114388424424, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.525 with standard deviation 0.029. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37413c2d-ef02-492a-82a7-f667aad2e738", "metadata": {"aucs": [0.5211913460290221, 0.4916809351329484, 0.5624620353653567], "final_y": [3.3639609185514256e-09, 4.258290535856151e-07, 7.700504997644034e-08]}, "mutation_prompt": null}
{"id": "6d609b94-1333-44e0-a0a9-eadd584639f5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.median(initial_samples[best_indices], axis=0)  # Changed from mean to median\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploitation by fine-tuning the initial guess selection process using median-based aggregation for robustness.", "configspace": "", "generation": 18, "fitness": 0.5484517947187086, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.548 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "61602270-a795-4131-84aa-11d0f72fae85", "metadata": {"aucs": [0.5268249488089873, 0.5495483911459265, 0.5689820442012121], "final_y": [1.1248460865936659e-07, 8.339241317824593e-08, 2.26373635864303e-08]}, "mutation_prompt": null}
{"id": "2a3f2d02-a700-4dc5-93cc-3144be4edb50", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:5]  # Change from 3 to 5\n        best_guess = np.median(initial_samples[best_indices], axis=0)  # Change from mean to median\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Optimize balance of exploration and exploitation by tweaking initial guess selection and local optimizer strategy.", "configspace": "", "generation": 18, "fitness": 0.5104134541237797, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.510 with standard deviation 0.023. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "61602270-a795-4131-84aa-11d0f72fae85", "metadata": {"aucs": [0.4945780434089112, 0.493997747029433, 0.542664571932995], "final_y": [4.224929518841431e-07, 3.435520341829663e-07, 7.08426106983769e-08]}, "mutation_prompt": null}
{"id": "96f58662-5a2a-4fc6-a8c1-8dd76ccf83c4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n        self.memory = []  # Added memory to store top-performing solutions\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Store best performing samples in memory\n        self.memory.extend(initial_samples[best_indices])\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Incorporate a memory mechanism to store and revisit the top-performing solutions for enhanced exploitation.", "configspace": "", "generation": 18, "fitness": 0.5245429929191862, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.525 with standard deviation 0.006. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "61602270-a795-4131-84aa-11d0f72fae85", "metadata": {"aucs": [0.5283395789647383, 0.5289527985962375, 0.5163366011965829], "final_y": [2.4404417874283606e-07, 1.4458725160615518e-07, 2.3252655526601647e-07]}, "mutation_prompt": null}
{"id": "414a72c5-bb54-490c-bf76-8e57934073a5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb)), min(ub, result.x[i] + scale_factor * (ub - lb)))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n\n            # Increased local sampling density around the best solution (new line added)\n            if self.evals + 10 <= self.budget:\n                local_samples = np.clip(result.x + scale_factor * np.random.randn(10, self.dim), func.bounds.lb, func.bounds.ub)\n                local_values = [func(x) for x in local_samples]\n                self.evals += 10\n\n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Improve exploitation by increasing local sampling density around the best solution after each refinement.", "configspace": "", "generation": 18, "fitness": 0.5355904969037185, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.536 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "61602270-a795-4131-84aa-11d0f72fae85", "metadata": {"aucs": [0.5459264072025671, 0.5335147693893616, 0.5273303141192269], "final_y": [1.2876604939586365e-07, 1.6197405449238505e-07, 1.2816352607284842e-07]}, "mutation_prompt": null}
{"id": "795f73b6-5ab9-4b5f-b3b7-f097b888d4bf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploration by incorporating simulated annealing during adaptive bound adjustments for dynamic local refinement.", "configspace": "", "generation": 18, "fitness": 0.5665000306647163, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.567 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "61602270-a795-4131-84aa-11d0f72fae85", "metadata": {"aucs": [0.5834568380218074, 0.5404094469223021, 0.5756338070500395], "final_y": [1.000535608023534e-08, 9.091826977498078e-08, 3.4773015002061636e-08]}, "mutation_prompt": null}
{"id": "8e6e6c06-c460-43f4-9cd7-06a87362dcc9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='Nelder-Mead',  # Changed from L-BFGS-B to Nelder-Mead\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local refinement by incorporating dual-phase optimization using both L-BFGS-B and Nelder-Mead.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"Sobol.random_base2() got an unexpected keyword argument 'n'\").", "error": "TypeError(\"Sobol.random_base2() got an unexpected keyword argument 'n'\")", "parent_id": "795f73b6-5ab9-4b5f-b3b7-f097b888d4bf", "metadata": {}, "mutation_prompt": null}
{"id": "02bf1b4e-c25d-446f-91c1-f3c9cb50d8b4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        # Changed from LatinHypercube to Sobol for better low-discrepancy sampling\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(n=int(np.log2(num_samples))) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploitation by optimizing the initial sampling strategy with a Sobol sequence for improved convergence robustness.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"Sobol.random_base2() got an unexpected keyword argument 'n'\").", "error": "TypeError(\"Sobol.random_base2() got an unexpected keyword argument 'n'\")", "parent_id": "795f73b6-5ab9-4b5f-b3b7-f097b888d4bf", "metadata": {}, "mutation_prompt": null}
{"id": "5885d008-e9c5-4a98-b3ea-f45c9d55b204", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(10, int(self.budget * 0.5)), self.budget // 2)  # Increased samples for better initial coverage\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guesses and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:5]  # Using top 5 initial samples\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.03, 0.08 * (1 - self.evals / self.budget))  # Adjusted scale factor for nuanced refinement\n            temperature = 1.0 - (self.evals / self.budget)\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            result = minimize(\n                func,\n                result.x,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance initial sampling and leverage multi-start local searches with adaptive bound scaling for improved convergence.", "configspace": "", "generation": 19, "fitness": 0.5241311590779199, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.524 with standard deviation 0.049. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "795f73b6-5ab9-4b5f-b3b7-f097b888d4bf", "metadata": {"aucs": [0.4633798441733685, 0.5839224285959328, 0.5250912044644581], "final_y": [8.409974729133623e-08, 2.067697713748868e-08, 9.241815689637748e-08]}, "mutation_prompt": null}
{"id": "f998bb64-d37c-4c69-a35a-ce232247fc1b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed from best_guess to result.x\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance exploration by introducing a diverse initial sample distribution using Latin Hypercube Sampling with optimized stratification.", "configspace": "", "generation": 19, "fitness": 0.5357732152286094, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.536 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "795f73b6-5ab9-4b5f-b3b7-f097b888d4bf", "metadata": {"aucs": [0.518024315578772, 0.518155107489036, 0.5711402226180203], "final_y": [1.6273305687143914e-07, 1.7964076907534628e-07, 2.2157025557672264e-08]}, "mutation_prompt": null}
{"id": "21d5b41a-bb04-463b-8e3b-e3b500224f03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                initial_samples[np.argmin(initial_values)],  # Changed from result.x to a better initial guess\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate multi-start local search to enhance convergence reliability.", "configspace": "", "generation": 19, "fitness": 0.5427062471769983, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.543 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "795f73b6-5ab9-4b5f-b3b7-f097b888d4bf", "metadata": {"aucs": [0.5306799941867206, 0.5440115850510767, 0.5534271622931977], "final_y": [7.946520891115436e-08, 8.990329307172709e-08, 7.806483588443724e-08]}, "mutation_prompt": null}
{"id": "62146549-df86-45d6-9fdf-0f56798cba2e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            # Change: Adjusted scale_factor to a more aggressive approach\n            scale_factor = max(0.03, 0.08 * (1 - self.evals / self.budget))  \n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                initial_samples[np.argmin(initial_values)],  # Changed from result.x to a better initial guess\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Integrate multi-start local search with refined bound adjustment strategy for enhanced convergence reliability.", "configspace": "", "generation": 20, "fitness": 0.5524112077655924, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.552 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "21d5b41a-bb04-463b-8e3b-e3b500224f03", "metadata": {"aucs": [0.5676677401553252, 0.5493067200967776, 0.5402591630446743], "final_y": [2.4126630997345813e-09, 1.7882353273465637e-08, 1.7075076375019943e-08]}, "mutation_prompt": null}
{"id": "eb1483e9-997a-4df6-b39f-3ffa658a26f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.1, 0.15 * (1 - self.evals / self.budget))  # Adjusted scaling factor\n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                initial_samples[np.argmin(initial_values)],  # Changed from result.x to a better initial guess\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Implement a minor adjustment to the adaptive bound scaling to exploit the search space more effectively.", "configspace": "", "generation": 20, "fitness": 0.5460111256176633, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.546 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "21d5b41a-bb04-463b-8e3b-e3b500224f03", "metadata": {"aucs": [0.5526470986488281, 0.5532492877329531, 0.5321369904712088], "final_y": [2.497057766480442e-08, 6.770852577027864e-09, 4.512178869481954e-08]}, "mutation_prompt": null}
{"id": "9442fcf8-372e-492e-9b0d-dc4fd81d9895", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.median(initial_samples[best_indices], axis=0)  # Changed from mean to median\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                initial_samples[np.argmin(initial_values)],  # Changed from result.x to a better initial guess\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance convergence reliability by dynamically adjusting the initial choice for the local optimizer.", "configspace": "", "generation": 20, "fitness": 0.5284543638165987, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.528 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "21d5b41a-bb04-463b-8e3b-e3b500224f03", "metadata": {"aucs": [0.5017922816045116, 0.5603463540814276, 0.523224455763857], "final_y": [1.6463590667747124e-07, 3.2551020546267056e-09, 1.3901560443026518e-07]}, "mutation_prompt": null}
{"id": "2713b338-30aa-4d31-8538-bb738047ed4b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guesses and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        self.evals += result.nfev\n\n        # Step 3: Use simulated annealing-like adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))\n            temperature = 1.0 - (self.evals / self.budget)\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            perturbation = temperature * np.random.uniform(-1, 1, self.dim) * (bounds[:, 1] - bounds[:, 0]) * scale_factor\n            perturbed_initial = np.clip(result.x + perturbation, func.bounds.lb, func.bounds.ub)\n            \n            result = minimize(\n                func,\n                perturbed_initial,\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': self.budget - self.evals}\n            )\n            \n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Hybrid Adaptive Local Search combining Latin Hypercube Sampling with Simulated Annealing for improved convergence in smooth, low-dimensional spaces.", "configspace": "", "generation": 20, "fitness": 0.5246712793207173, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.525 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "21d5b41a-bb04-463b-8e3b-e3b500224f03", "metadata": {"aucs": [0.49503218157539974, 0.5254690026469631, 0.5535126537397892], "final_y": [1.6025738115854075e-07, 1.477059349510639e-07, 4.361940502716306e-08]}, "mutation_prompt": null}
{"id": "bbfa2adf-180a-4dd5-8812-e4a0ca72f8c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import LatinHypercube\n\nclass NovelMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evals = 0\n\n    def __call__(self, func):\n        bounds = np.array(list(zip(func.bounds.lb, func.bounds.ub)))\n        \n        num_samples = min(max(7, int(self.budget * 0.4)), self.budget // 2)\n        \n        # Step 1: Adaptive uniform sampling for initial exploration\n        sampler = LatinHypercube(d=self.dim)\n        initial_samples = sampler.random(n=num_samples) * (bounds[:, 1] - bounds[:, 0]) + bounds[:, 0]\n        \n        initial_values = [func(x) for x in initial_samples]\n        self.evals += num_samples\n        \n        # Step 2: Select the best initial guess and refine with local optimizer\n        best_indices = np.argsort(initial_values)[:3]\n        best_guess = np.mean(initial_samples[best_indices], axis=0)\n        \n        # Define the bounds again for the local optimizer\n        local_bounds = [(lb, ub) for lb, ub in zip(func.bounds.lb, func.bounds.ub)]\n        \n        # Local optimizer using L-BFGS-B\n        result = minimize(\n            func,\n            best_guess,\n            method='L-BFGS-B',\n            bounds=local_bounds,\n            options={'maxfun': self.budget - self.evals}\n        )\n        \n        # Update the number of evaluations used by the optimizer\n        self.evals += result.nfev\n\n        # If evaluations are not yet exhausted, perform adaptive bound adjustment\n        while self.evals < self.budget:\n            scale_factor = max(0.05, 0.1 * (1 - self.evals / self.budget))  # Reduce factor incrementally\n            temperature = 1.0 - (self.evals / self.budget)  # Simulated annealing temperature\n            narrowed_bounds = [\n                (max(lb, result.x[i] - scale_factor * (ub - lb) * temperature), \n                 min(ub, result.x[i] + scale_factor * (ub - lb) * temperature))\n                for i, (lb, ub) in enumerate(bounds)\n            ]\n            \n            # Use result.x instead of best_guess to further leverage refined data\n            result = minimize(\n                func,\n                result.x,  # Changed to leverage refined results\n                method='L-BFGS-B',\n                bounds=narrowed_bounds,\n                options={'maxfun': int((self.budget - self.evals) * (0.9 + 0.1 * temperature))}  # Adjust evaluation allocation\n            )\n            \n            # Update the number of evaluations\n            self.evals += result.nfev\n        \n        return result.x", "name": "NovelMetaheuristicOptimizer", "description": "Enhance local refinement with adaptive step size scaling for improved convergence.", "configspace": "", "generation": 20, "fitness": 0.53457647771961, "feedback": "The algorithm NovelMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.535 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "21d5b41a-bb04-463b-8e3b-e3b500224f03", "metadata": {"aucs": [0.5401855098790211, 0.5388606883005184, 0.5246832349792907], "final_y": [4.058333010967163e-08, 7.344020399720214e-08, 1.1190812736717556e-07]}, "mutation_prompt": null}
