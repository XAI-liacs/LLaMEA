{"id": "1f0c9eeb-53e9-4687-9d9d-fec01f2cba1f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HGLIO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        mid_point = (lb + ub) / 2\n        init_pop = np.random.uniform(lb, ub, (self.dim, self.dim))\n        opp_pop = mid_point + (mid_point - init_pop)\n        combined_pop = np.vstack((init_pop, opp_pop))\n        return np.clip(combined_pop, lb, ub)\n\n    def differential_evolution(self, population, func, bounds, F=0.5, CR=0.9):\n        next_gen = population.copy()\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = optimize.Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.quasi_oppositional_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "HGLIO", "description": "Hybrid Global-Local Interference Optimizer (HGLIO) combining quasi-oppositional initialization with DE and BFGS for enhanced exploration and exploitation tailored for periodic solutions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 55, in __call__\n  File \"<string>\", line 35, in local_optimization\nNameError: name 'optimize' is not defined. Did you mean: 'minimize'?\n.", "error": "NameError(\"name 'optimize' is not defined\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 55, in __call__\n  File \"<string>\", line 35, in local_optimization\nNameError: name 'optimize' is not defined. Did you mean: 'minimize'?\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "0a1d10de-63f1-4a52-8139-7db46721c869", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HGLIO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        mid_point = (lb + ub) / 2\n        init_pop = np.random.uniform(lb, ub, (self.dim, self.dim))\n        opp_pop = mid_point + (mid_point - init_pop)\n        combined_pop = np.vstack((init_pop, opp_pop))\n        return np.clip(combined_pop, lb, ub)\n\n    def differential_evolution(self, population, func, bounds, F=0.5, CR=0.9):\n        next_gen = population.copy()\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = 0.5 + 0.3 * (self.evaluations / self.budget)\n            mutant = a + F_adaptive * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = minimize.Bounds(func.bounds.lb, func.bounds.ub) # Corrected typo\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.quasi_oppositional_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "HGLIO", "description": "Enhanced HGLIO using adaptive DE parameters and improved local optimization with constrained bounds.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'function' object has no attribute 'Bounds'\").", "error": "AttributeError(\"'function' object has no attribute 'Bounds'\")", "parent_id": "1f0c9eeb-53e9-4687-9d9d-fec01f2cba1f", "metadata": {}, "mutation_prompt": null}
{"id": "e9fefcf3-08f7-4fd1-b9a3-b493eb93910c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EHGLIO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        mid_point = (lb + ub) / 2\n        init_pop = np.random.uniform(lb, ub, (self.dim, self.dim))\n        opp_pop = mid_point + (mid_point - init_pop)\n        combined_pop = np.vstack((init_pop, opp_pop))\n        return np.clip(combined_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, CR=0.9):\n        next_gen = population.copy()\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_adaptive = F * np.random.rand()  # Adaptive mutation factor\n            mutant = a + F_adaptive * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = minimize.Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.quasi_oppositional_init(bounds)\n        best_solution = None\n        best_value = np.inf\n\n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n\n        return best_solution", "name": "EHGLIO", "description": "Enhanced Hybrid Global-Local Interference Optimizer (EHGLIO) integrates quasi-oppositional initialization with DE, incorporates adaptive parameter strategies, and fixes local optimization import error for refined exploration and exploitation in periodic optimization tasks.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'function' object has no attribute 'Bounds'\").", "error": "AttributeError(\"'function' object has no attribute 'Bounds'\")", "parent_id": "1f0c9eeb-53e9-4687-9d9d-fec01f2cba1f", "metadata": {}, "mutation_prompt": null}
{"id": "10a19fe0-4bb1-4184-8bde-7a2a19989064", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HGLIO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        mid_point = (lb + ub) / 2\n        init_pop = np.random.uniform(lb, ub, (self.dim, self.dim))\n        opp_pop = mid_point + (mid_point - init_pop)\n        combined_pop = np.vstack((init_pop, opp_pop))\n        return np.clip(combined_pop, lb, ub)\n\n    def differential_evolution(self, population, func, bounds, F=0.5, CR=0.9):\n        next_gen = population.copy()\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = minimize.Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            raise RuntimeError(\"Evaluation budget exceeded\")\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.quasi_oppositional_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "HGLIO", "description": "Improved Hybrid Global-Local Interference Optimizer (HGLIO) with better error handling and refined local optimization step to enhance convergence for periodic solutions.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'function' object has no attribute 'Bounds'\").", "error": "AttributeError(\"'function' object has no attribute 'Bounds'\")", "parent_id": "1f0c9eeb-53e9-4687-9d9d-fec01f2cba1f", "metadata": {}, "mutation_prompt": null}
{"id": "f3b13ee7-fd6e-418b-955c-0893b2a93260", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HGLIO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def quasi_oppositional_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        mid_point = (lb + ub) / 2\n        init_pop = np.random.uniform(lb, ub, (self.dim, self.dim))\n        opp_pop = mid_point + (mid_point - init_pop)\n        combined_pop = np.vstack((init_pop, opp_pop))\n        return np.clip(combined_pop, lb, ub)\n\n    def differential_evolution(self, population, func, bounds, F=0.5, CR=0.9):\n        next_gen = population.copy()\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = minimize.Bounds(func.bounds.lb, func.bounds.ub)  # Adjusted line\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.quasi_oppositional_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "HGLIO", "description": "Refined Hybrid Global-Local Interference Optimizer (HGLIO) with a bug fix in local optimization to ensure correct function calls and improved performance.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'function' object has no attribute 'Bounds'\").", "error": "AttributeError(\"'function' object has no attribute 'Bounds'\")", "parent_id": "1f0c9eeb-53e9-4687-9d9d-fec01f2cba1f", "metadata": {}, "mutation_prompt": null}
{"id": "6227756c-1e76-4c3b-805a-665faea94b2c", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Adaptive Periodic Wave Optimizer (APWO) leveraging periodic perturbations and adaptive crossover in DE to enhance global exploration and exploitation.", "configspace": "", "generation": 1, "fitness": 0.9376006806684914, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.938 with standard deviation 0.044. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "1f0c9eeb-53e9-4687-9d9d-fec01f2cba1f", "metadata": {"aucs": [0.9497188371676394, 0.8781338551977607, 0.9849493496400743], "final_y": [0.16485941310503605, 0.1818789597726469, 0.16485604498124884]}, "mutation_prompt": null}
{"id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c + c - a)  # Enhanced mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved Adaptive Periodic Wave Optimizer (APWO) with enhanced differential mutation strategy for better exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.9927120700855352, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6227756c-1e76-4c3b-805a-665faea94b2c", "metadata": {"aucs": [0.9955642282052632, 0.9953622893355909, 0.9872096927157514], "final_y": [0.16485603008441918, 0.16485591992346038, 0.16485600542750778]}, "mutation_prompt": null}
{"id": "a339bd01-63e8-42fb-92f7-e3fe9e7ea872", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass PSE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_and_random_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_part = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim // 2)])\n        random_part = np.random.uniform(lb, ub, (self.dim // 2, self.dim))\n        return np.clip(np.vstack((periodic_part, random_part)), lb, ub)\n\n    def symbiotic_evolution(self, population, func, bounds, F=0.5):\n        next_gen = population.copy()\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            partner = population[np.random.choice(indices)]\n            symbiotic_vector = F * (partner - population[i])\n            trial = population[i] + symbiotic_vector\n            trial = np.clip(trial, bounds.lb, bounds.ub)\n            \n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n        \n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_and_random_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.symbiotic_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "PSE", "description": "Periodic Symbiotic Evolution (PSE) combining mutualistic symbiosis-inspired interactions with periodic solution embedding to enhance convergence in complex optimization landscapes.", "configspace": "", "generation": 2, "fitness": 0.9765943924322623, "feedback": "The algorithm PSE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6227756c-1e76-4c3b-805a-665faea94b2c", "metadata": {"aucs": [0.9540893093461577, 0.9805933611819747, 0.9951005067686548], "final_y": [0.16485693812608138, 0.16485603088183431, 0.16485601164472907]}, "mutation_prompt": null}
{"id": "a480dcef-7193-4cbb-a629-8b01e213d36b", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, initial_F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        F = initial_F  # Add dynamic scaling factor\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n                F = min(1.0, F + 0.05)  # Increase F if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n                F = max(0.1, F - 0.05)  # Decrease F if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic scaling factor to improve mutation step size adaptively.", "configspace": "", "generation": 2, "fitness": 0.95273935678942, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6227756c-1e76-4c3b-805a-665faea94b2c", "metadata": {"aucs": [0.9489208954659457, 0.924369031421618, 0.9849281434806963], "final_y": [0.16485941310503605, 0.16485651617415087, 0.16485668047782742]}, "mutation_prompt": null}
{"id": "34c911aa-501c-47de-96dd-6c5cff50dcc2", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass EAPWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        adaptive_F = F\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + adaptive_F * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05)  # Fine-tune CR increment\n                adaptive_F = min(1.0, adaptive_F + 0.02)  # Increase F if successful\n            else:\n                CR = max(0.1, CR - 0.05)  # Fine-tune CR decrement\n                adaptive_F = max(0.1, adaptive_F - 0.02)  # Decrease F if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds, options={'maxiter': 10})  # Limit iterations for efficiency\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "EAPWO", "description": "Enhanced Adaptive Periodic Wave Optimizer (EAPWO) introduces layer-wise periodicity and adaptive learning rates to better explore and exploit in DE, coupled with gradient-based local refinement for improved performance on black box optimization.", "configspace": "", "generation": 2, "fitness": 0.9718865464549865, "feedback": "The algorithm EAPWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.009. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6227756c-1e76-4c3b-805a-665faea94b2c", "metadata": {"aucs": [0.959271324523776, 0.9797036861471168, 0.9766846286940668], "final_y": [0.16573099311836614, 0.16485598526643142, 0.1648558529875036]}, "mutation_prompt": null}
{"id": "35e611cd-4740-40f2-85b2-620a43ed31e7", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass EAPWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def dynamic_differential_evolution(self, population, func, bounds, F_base=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F = F_base * (1 + np.random.uniform(-0.1, 0.1))  # Introduce dynamic mutation factor\n            mutant = a + F * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def periodic_pattern_reinforcement(self, population):\n        # Reinforce periodic patterns by averaging alternate layers\n        for i in range(0, self.dim, 2):\n            population[:, i] = np.mean(population[:, i])\n        return population\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.dynamic_differential_evolution(population, func, bounds)\n            population = self.periodic_pattern_reinforcement(population)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "EAPWO", "description": "Enhanced Adaptive Periodic Wave Optimizer (EAPWO), introducing a dynamic mutation strategy and periodic pattern reinforcement to further improve global search and convergence speed.", "configspace": "", "generation": 2, "fitness": 0.9776615221627513, "feedback": "The algorithm EAPWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6227756c-1e76-4c3b-805a-665faea94b2c", "metadata": {"aucs": [0.9765964690949112, 0.9797036861471168, 0.9766844112462261], "final_y": [0.1648561729589254, 0.16485598526643142, 0.16485599392116035]}, "mutation_prompt": null}
{"id": "7c6b9427-5c4f-43f5-9214-dd054f814030", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c + c - a)  # Enhanced mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05)  # Slightly modified CR adjustment\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with improved adaptive CR adjustment for better convergence efficiency.", "configspace": "", "generation": 3, "fitness": 0.9925177275443392, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9955635796180535, 0.9947800611320984, 0.9872095418828659], "final_y": [0.1648560494047765, 0.16485591992346038, 0.1648561477559295]}, "mutation_prompt": null}
{"id": "ecf829ea-f176-45e9-955b-ea803cc76973", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F = np.random.uniform(0.4, 0.9)  # Dynamic adjustment of F\n            mutant = a + F * (b - c + c - a)  # Enhanced mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "The Improved Adaptive Periodic Wave Optimizer (APWO) adopts a refined mutation strategy by adjusting the scaling factor F dynamically to enhance adaptability across different optimization landscapes.", "configspace": "", "generation": 3, "fitness": 0.9859087788459372, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9923145900341394, 0.9843210563362733, 0.9810906901673989], "final_y": [0.16485597721223733, 0.1648564957517321, 0.16485623899302748]}, "mutation_prompt": null}
{"id": "a2bf5053-745f-4a97-bcd8-bed30e6033b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F = 0.5 + (0.5 * (self.budget - self.evaluations) / self.budget)  # Adaptive scaling factor adjustment\n            mutant = a + F * (b - c + c - a)  # Enhanced mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Refined Improved Adaptive Periodic Wave Optimizer (APWO) with adaptive scaling factor adjustment for enhanced convergence speed.", "configspace": "", "generation": 3, "fitness": 0.9550884201888111, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.039. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.8998534703144176, 0.984321745698991, 0.9810900445530246], "final_y": [0.16485941310503605, 0.16485585017589233, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "ca4bb44e-1a22-4a9d-b863-575534b60091", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass EAPWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        adaptive_F = F\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + adaptive_F * (b - c)  # Self-adaptive mutation\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05)  # Fine-tuned increase\n                adaptive_F = min(1.0, adaptive_F + 0.05)\n            else:\n                CR = max(0.1, CR - 0.05)  # Fine-tuned decrease\n                adaptive_F = max(0.1, adaptive_F - 0.05)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "EAPWO", "description": "Enhanced Adaptive Periodic Wave Optimizer (EAPWO) incorporating a self-adaptive differential mutation and crossover strategy for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.9404631264761113, "feedback": "The algorithm EAPWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.940 with standard deviation 0.040. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9488156872438668, 0.8876455487037709, 0.9849281434806963], "final_y": [0.1648577651277533, 0.1818786560804545, 0.16485668047782742]}, "mutation_prompt": null}
{"id": "d5e56286-ae16-40f0-8e55-56c4a08918e7", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F = np.clip(np.random.normal(F, 0.1), 0, 1)  # Intelligent scaling factor adjustment\n            mutant = a + F * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced Adaptive Differential Evolution with intelligent scaling factor adjustment to balance exploration and exploitation.", "configspace": "", "generation": 3, "fitness": 0.9846319660373407, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.985 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9884844795359131, 0.9843210563362733, 0.9810903622398357], "final_y": [0.16485657523595876, 0.1648564957517321, 0.1648564519409319]}, "mutation_prompt": null}
{"id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic scaling factor adjustment for improved convergence.", "configspace": "", "generation": 4, "fitness": 0.9931473630932239, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9955631371887539, 0.9953630995151452, 0.9885158525757727], "final_y": [0.1648559120893145, 0.16485598738003204, 0.1648558775563066]}, "mutation_prompt": null}
{"id": "0bf001fd-24cf-436d-ad64-7b82fb681395", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        diversity = np.std(population, axis=0).mean()  # Measure population diversity\n        F_adaptive = max(0.3, min(0.9, diversity))  # Adaptive scaling factor\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F_adaptive * (b - c + c - a)  # Enhanced mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive scaling factor that dynamically adjusts based on population diversity for improved exploration.", "configspace": "", "generation": 4, "fitness": 0.9768172472469691, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9479647316974089, 0.993971571030531, 0.9885154390129671], "final_y": [0.16485941310503605, 0.16485598907232235, 0.1648560853408746]}, "mutation_prompt": null}
{"id": "ea31fe6c-852e-4f71-8768-ea28f70de79c", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.6, initial_CR=0.9):  # Adjusted mutation factor F\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            mutant = a + F * (b - c + c - a)  # Enhanced mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced Adaptive Periodic Wave Optimizer (APWO) with improved local search initialization and refined mutation strategy.", "configspace": "", "generation": 4, "fitness": 0.9918421857424072, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9929124486582183, 0.9939715960437264, 0.9886425125252768], "final_y": [0.16485625672550563, 0.16485607449024509, 0.16485599110482207]}, "mutation_prompt": null}
{"id": "678a0b63-11ea-4186-b8a9-f95e2fecc9b4", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F + np.random.uniform(-0.1, 0.1)  # Dynamic mutation factor\n            mutant = a + F_dynamic * (b - c + c - a)  # Enhanced mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Increase CR if successful\n            else:\n                CR = max(0.1, CR - 0.1)  # Decrease CR if not successful\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by integrating a dynamic mutation factor for improved convergence in challenging landscapes.", "configspace": "", "generation": 4, "fitness": 0.9930424584877423, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9966406003419324, 0.993971534179026, 0.9885152409422682], "final_y": [0.16485600566921732, 0.16485615110949714, 0.16485613103075292]}, "mutation_prompt": null}
{"id": "cb2761ed-be77-4d97-b28c-0d92896ffe2e", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Dynamic scaling factor\n            mutant = a + F_dynamic * (b - c + c - a)  # Adjusted mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by incorporating a dynamic scaling factor for improved balance between exploration and exploitation.", "configspace": "", "generation": 4, "fitness": 0.9926838071836666, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "03b5110b-397b-46b9-a2af-dcaee2f78cd6", "metadata": {"aucs": [0.9955642739880034, 0.9939715821768561, 0.9885155653861403], "final_y": [0.1648559563525972, 0.16485610278008345, 0.16485598345091246]}, "mutation_prompt": null}
{"id": "e7873242-a420-43b5-a3a8-c6cfe83f735f", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[:] = True  # Change to ensure full crossover when no points are selected\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved convergence by enhancing crossover operation in adaptive differential evolution.", "configspace": "", "generation": 5, "fitness": 0.9720103819572873, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.033. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955631371887539, 0.9953630995151452, 0.9251049091679627], "final_y": [0.1648559120893145, 0.16485598738003204, 0.1648567220271986]}, "mutation_prompt": null}
{"id": "32dc51bd-d445-4aac-b7da-adc43b95b443", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.population_size = max(5, dim * 2)  # Adaptive population size based on dimension\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.population_size)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO with adaptive population size and enhanced periodicity for better convergence.", "configspace": "", "generation": 5, "fitness": 0.9920576240265585, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9919428795691708, 0.9943404856840485, 0.9898895068264563], "final_y": [0.16485660188365503, 0.16485616587257834, 0.1648559395468231]}, "mutation_prompt": null}
{"id": "faa0cc1a-2150-4854-87a7-7b3647863934", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_factor = np.sin(np.linspace(0, 2 * np.pi, self.dim))  # Adaptive periodicity\n        periodic_pop = np.array([(base_solution + periodic_factor * i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        best_index = np.argmin([self.evaluate(func, ind) for ind in population])\n        best_individual = population[best_index]\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (0.5 + np.abs(np.sin(2 * np.pi * self.evaluations / self.budget)))  # Adaptive scaling factor\n            mutant = a + F_dynamic * (b - c)\n            mutant = np.clip(mutant + best_individual * 0.1, bounds.lb, bounds.ub)  # Add elitist pressure\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive periodicity and elitist strategy for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.9868748075978576, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.987 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.976393905678318, 0.9943409453943873, 0.9898895717208674], "final_y": [0.1648565311343595, 0.16485605299359785, 0.1648559103381697]}, "mutation_prompt": null}
{"id": "b484fe6a-d5c5-4397-b579-948f7e1020e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        best_idx = np.argmin([self.evaluate(func, ind) for ind in population])\n        best_individual = population[best_idx]\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            if i != best_idx:\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                    next_gen[i] = trial\n                    CR = min(1.0, CR + 0.1)\n                else:\n                    CR = max(0.1, CR - 0.1)\n        next_gen[best_idx] = best_individual  # Introduce elitism\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Introduced periodicity control and elitism to the Enhanced APWO algorithm for improved convergence and solution quality.", "configspace": "", "generation": 5, "fitness": 0.9927894134379526, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9941380149768655, 0.9943409827710198, 0.9898892425659723], "final_y": [0.1648562459751972, 0.16485608690886655, 0.16485596195474195]}, "mutation_prompt": null}
{"id": "895701ad-f252-42ed-8f37-9908a423898e", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def quasi_oppositional_init(self, bounds):  # Added quasi-oppositional initialization strategy\n        periodic_pop = self.periodic_init(bounds)\n        opp_pop = bounds.ub - (periodic_pop - bounds.lb)\n        combined_pop = np.vstack((periodic_pop, opp_pop))\n        return combined_pop[np.random.choice(combined_pop.shape[0], self.dim, replace=False)]\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  \n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            CR = 0.9 * (1 - self.evaluations / self.budget) + 0.1 * CR  # Updated dynamic CR adjustment\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.quasi_oppositional_init(bounds)  # Use the new initialization strategy\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO using quasi-oppositional initialization and dynamic crossover rate adjustments.", "configspace": "", "generation": 5, "fitness": 0.9833378228271709, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.983 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9657840126650518, 0.9943400387928076, 0.9898894170236533], "final_y": [0.1648561287188286, 0.1648560899666084, 0.1648559490353847]}, "mutation_prompt": null}
{"id": "88f740c9-3b8e-47f5-98ba-b89c8b7f7f6c", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / (2 * self.dim)) % (ub - lb) + lb for i in range(self.dim)])  # Changed line\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.2)  # Adjusted CR increment\n            else:\n                CR = max(0.1, CR - 0.05)  # Adjusted CR decrement\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive CR and strategic periodicity amplification for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.9712991571995726, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.971 with standard deviation 0.028. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9313308863733776, 0.9953631438697917, 0.9872034413555487], "final_y": [0.1818784900957786, 0.16485586964494003, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "a9b8ee86-7ae2-4b4a-9764-c26b25fcb3ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.15)  # Adjusted CR increment for faster adaptation\n            else:\n                CR = max(0.15, CR - 0.1)  # Adjusted CR decrement for better exploration\n        return next_gen\n\n    def local_optimization(self, func, x):\n        velocity = np.zeros_like(x)  # Added momentum-based local search initialization\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x + 0.1 * velocity, method='L-BFGS-B', bounds=bounds)  # Adjust local search with momentum\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO by adding a momentum-based local search and refining adaptive CR bounds for better exploration.", "configspace": "", "generation": 6, "fitness": 0.9927102398568876, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955641183599533, 0.9953631598551609, 0.9872034413555487], "final_y": [0.16485584240913298, 0.16485583414956928, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "3ebd48af-26a0-42ec-ba08-d01553600ad6", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget) \n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        if self.evaluations > self.budget / 2:  # Decrease population size dynamically\n            next_gen = next_gen[:len(next_gen) // 2]\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic scaling factor adjustment and adaptive population size for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.9927099555911982, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955631581452242, 0.99536305664951, 0.9872036519788606], "final_y": [0.16485621786093696, 0.16485598738003204, 0.1648559363564881]}, "mutation_prompt": null}
{"id": "cc0dddf0-447a-4639-b6dc-823854f73087", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive mutation strategy for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.9927099444726166, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955632320370237, 0.99536305664951, 0.987203544731316], "final_y": [0.16485592866079968, 0.16485598738003204, 0.16485592216288347]}, "mutation_prompt": null}
{"id": "b79eca22-ba85-4d0c-a00d-6a1ea1ebc7f6", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if np.random.rand() < 0.1:  # Allow occasional full crossover for exploration\n                cross_points = np.ones(self.dim, dtype=bool)\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with periodic enhancement and improved crossover strategy for better convergence and exploration.", "configspace": "", "generation": 6, "fitness": 0.9857737019527951, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9759070996527878, 0.994210564850049, 0.9872034413555487], "final_y": [0.16485667020340267, 0.16485603409774718, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "02c9f583-b5fe-4659-88fb-db402d092aee", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  \n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if np.all(np.abs(trial - population[i]) > 1e-5):  # Added line for diversity preservation\n                if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                    next_gen[i] = trial\n                    CR = min(1.0, CR + 0.1)\n                else:\n                    CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO by integrating a diversity-preserving mechanism for enhanced exploration.", "configspace": "", "generation": 7, "fitness": 0.9856002778533949, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.986 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9739070349348853, 0.9933507189986064, 0.989543079626693], "final_y": [0.164856763516133, 0.1648560464714084, 0.1648560451716754]}, "mutation_prompt": null}
{"id": "7678f29e-a426-4eae-9881-87169c19a669", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        initial_guess = x + 0.1 * (np.random.rand(self.dim) - 0.5)  # Adjusted initialization\n        result = minimize(func, initial_guess, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO with dynamic mutation scaling and enhanced local search initialization for better convergence.", "configspace": "", "generation": 7, "fitness": 0.977225220342418, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.020. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9487818535257954, 0.9933507189986064, 0.9895430885028524], "final_y": [0.16485836185284608, 0.1648560464714084, 0.16485603834366702]}, "mutation_prompt": null}
{"id": "61f45e82-9895-4199-97a6-da74fc193453", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds, options={'maxiter': 50})  # Adjusted max iterations\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO with refined local optimization strategy for enhanced convergence.", "configspace": "", "generation": 7, "fitness": 0.9928188247717037, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.995562514270429, 0.993350743009913, 0.989543217034769], "final_y": [0.16485632784098903, 0.16485602769944419, 0.16485596480360276]}, "mutation_prompt": null}
{"id": "3cb2d6f7-293b-47c4-8405-e71a56188b8f", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            # Changed mutation strategy to multi-strategy\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            if np.random.rand() < 0.5:\n                mutant = a + F_dynamic * (b - c)\n            else:\n                mutant = a + F_dynamic * (b - population[i]) + F_dynamic * (c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        # Enhanced local optimization by adding random restarts\n        best_result = x\n        best_value = self.evaluate(func, x)\n        for _ in range(2):\n            result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n            value = self.evaluate(func, result.x)\n            if value < best_value:\n                best_value = value\n                best_result = result.x\n            x = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        return best_result\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO algorithm with multi-strategy mutation and improved local exploitation for better convergence and solution quality.", "configspace": "", "generation": 7, "fitness": 0.9770745894644817, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.020. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9483299697681457, 0.9933507189986064, 0.989543079626693], "final_y": [0.16485959612492984, 0.1648560464714084, 0.1648560451716754]}, "mutation_prompt": null}
{"id": "97ece6a9-a4a8-438e-8b72-865b9a918c57", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            opposition = bounds.lb + bounds.ub - trial  # Added opposition-based learning\n            opposition = np.clip(opposition, bounds.lb, bounds.ub)  # Ensure within bounds\n            if self.evaluate(func, opposition) < self.evaluate(func, trial):  # Check opposition solution\n                trial = opposition\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic opposition-based learning for improved exploration and convergence.", "configspace": "", "generation": 7, "fitness": 0.9923228779253526, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9940748022632923, 0.993350743009913, 0.9895430885028524], "final_y": [0.16485591783828968, 0.16485602769944419, 0.16485603834366702]}, "mutation_prompt": null}
{"id": "4d9b44ff-2ca7-4c18-bd85-30704a2e89ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a) + np.random.normal(0, 0.1, size=self.dim)  # Enhanced trial mutation\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Augmented APWO with enhanced trial mutation for improved exploration.", "configspace": "", "generation": 8, "fitness": 0.9924253719880727, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9964735522486706, 0.9954006334545481, 0.9854019302609992], "final_y": [0.16485611519895027, 0.16485605181122953, 0.16485621797546213]}, "mutation_prompt": null}
{"id": "bd4170d4-be88-4e3b-b3db-33454f23156d", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * np.sqrt(1 - (self.evaluations / self.budget)**2)  # Non-linear scaling factor\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Introduced non-linear scaling factor to enhance exploration-exploitation balance and convergence rate.", "configspace": "", "generation": 8, "fitness": 0.9503887990884001, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.950 with standard deviation 0.046. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955638076880637, 0.8867093060617988, 0.9688932835153377], "final_y": [0.16485616310985618, 0.18188008685393486, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "d996763f-3720-4555-baad-0605bcf1e88d", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR * (1 - self.evaluations / self.budget)  # Modified crossover rate initialization\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive crossover rate initialization and improved mutation strategy.", "configspace": "", "generation": 8, "fitness": 0.9351867558565855, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.935 with standard deviation 0.035. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.94995767799262, 0.8867093060617988, 0.9688932835153377], "final_y": [0.16485941310503605, 0.18188008685393486, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "a5512975-7bf4-4f9f-ad03-8b76c7a4306d", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        best_individual = population[np.argmin([self.evaluate(func, ind) for ind in population])]\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        next_gen[np.argmax([self.evaluate(func, ind) for ind in next_gen])] = best_individual\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO by enhancing mutation strategy diversity and adding elitism to maintain best solutions.", "configspace": "", "generation": 8, "fitness": 0.9496409228966236, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.950 with standard deviation 0.046. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9933201791127347, 0.8867093060617988, 0.9688932835153377], "final_y": [0.16485648881643256, 0.18188008685393486, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "797dfba1-7b2e-4ce9-b3f2-060a33e06f00", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  \n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial = self.enforce_periodicity(trial, bounds)  # New\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05)  # Adjusted\n            else:\n                CR = max(0.05, CR - 0.05)  # Adjusted\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def enforce_periodicity(self, solution, bounds):  # New function\n        period = (bounds.ub - bounds.lb) / 2\n        return np.mod(solution - bounds.lb, period) + bounds.lb\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with hybrid genetic-memetic strategies and a focus on periodicity for improved convergence in multilayer optimization problems.", "configspace": "", "generation": 8, "fitness": 0.9363189806517305, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.936 with standard deviation 0.036. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.953354352378055, 0.8867093060617988, 0.9688932835153377], "final_y": [0.16485941310503605, 0.18188008685393486, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "f52918b0-21e3-479d-bc6c-1838d3cb7586", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            # Improved mutation strategy by altering the mutant vector calculation\n            mutant = b + F_dynamic * (c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05)  # Adjusted CR increment step\n            else:\n                CR = max(0.1, CR - 0.05)  # Adjusted CR decrement step\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with improved mutation strategy and dynamic CR adaptation for robust convergence.", "configspace": "", "generation": 9, "fitness": 0.9392782461603071, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.939 with standard deviation 0.045. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.8781719190896276, 0.9539813852087157, 0.985681434182578], "final_y": [0.1648566005626193, 0.16485643260763494, 0.16485625136085258]}, "mutation_prompt": null}
{"id": "b5d07602-013e-401f-b28e-2c0d12fc21b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                if self.evaluations < self.budget // 2:  # Encouraging periodicity in early stages\n                    periodic_mutant = np.mean([population[j] for j in [a, b, c]], axis=0)\n                    trial = np.where(cross_points, periodic_mutant, population[i])\n                    if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                        next_gen[i] = trial\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic scaling factor adjustment and adaptive periodicity encouragement for improved convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('arrays used as indices must be of integer (or boolean) type').", "error": "IndexError('arrays used as indices must be of integer (or boolean) type')", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {}, "mutation_prompt": null}
{"id": "d9aa6b37-61fb-4e06-9dca-1508b679e7f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for i, individual in enumerate(population):  # Line modified for tracking index\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n                # Added line: adjust population[i] based on periodicity\n                population[i] = 0.5 * (individual + population[i])\n\n        return best_solution", "name": "APWO", "description": "Improved APWO by integrating adaptive periodicity adjustment for enhanced convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('arrays used as indices must be of integer (or boolean) type').", "error": "IndexError('arrays used as indices must be of integer (or boolean) type')", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {}, "mutation_prompt": null}
{"id": "08496c9d-d580-4f51-86f4-58eb6e093877", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen[:int((1 - self.evaluations / self.budget) * len(population))]\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO with adaptive population size for enhanced exploration and convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('arrays used as indices must be of integer (or boolean) type').", "error": "IndexError('arrays used as indices must be of integer (or boolean) type')", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {}, "mutation_prompt": null}
{"id": "b7d721a2-8524-4850-af0e-243f091e4977", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + np.random.uniform(-0.5, 0.5, self.dim))\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO by adjusting mutation strategy for enhanced diversity and stability.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('arrays used as indices must be of integer (or boolean) type').", "error": "IndexError('arrays used as indices must be of integer (or boolean) type')", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {}, "mutation_prompt": null}
{"id": "d3eadbda-35df-4304-a112-b42bf1ca1f47", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        periodic_pop += np.sin(np.linspace(0, 2*np.pi, self.dim))  # Added adaptive periodicity\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive periodic initialization for improved exploration and convergence.", "configspace": "", "generation": 10, "fitness": 0.9820729800229776, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9954754121364815, 0.9952450853106578, 0.9554984426217936], "final_y": [0.1648561847012332, 0.1648559571320355, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "06cbbb44-4a71-43c4-af9a-d86732506d32", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - np.abs(np.sin(np.pi * self.evaluations / self.budget)))  # Line change 1\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved adaptive differential evolution with dynamic F adaptation and periodic initialization refinement.", "configspace": "", "generation": 10, "fitness": 0.9927098534429494, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955625568780135, 0.9953630995151452, 0.9872039039356896], "final_y": [0.16485697532723664, 0.16485598738003204, 0.16485597613148284]}, "mutation_prompt": null}
{"id": "2241e38e-a848-4327-b1dc-794ee12a603c", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Line modified for dynamic CR adjustment\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved convergence by dynamically adjusting crossover probability in adaptive differential evolution.", "configspace": "", "generation": 10, "fitness": 0.9927111551880409, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955669246934288, 0.9953630995151452, 0.9872034413555487], "final_y": [0.16485586201504365, 0.16485598738003204, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "ba879e38-ac70-4bd5-84eb-ab1a6d0a6894", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.initial_population_size = 10 + dim  # Adaptive population size\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        population_size = self.initial_population_size\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(population_size)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def multi_periodic_optimization(self, population, func, bounds):\n        multi_pop = []\n        for base in population:\n            shift = np.random.uniform(-0.1, 0.1, self.dim)  # Slight shift to encourage multi-periodicity\n            multi_pop.extend([(base + shift * j) % (bounds.ub - bounds.lb) + bounds.lb for j in range(3)])\n        return np.clip(multi_pop, bounds.lb, bounds.ub)\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            if self.evaluations < self.budget / 2:  # Use multi-periodicity for first half \n                population = self.multi_periodic_optimization(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO with adaptive population size and multi-periodicity for enhanced exploration and exploitation.", "configspace": "", "generation": 10, "fitness": 0.9550481892755895, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.051. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.8825779292367134, 0.9953630995151452, 0.9872035390749102], "final_y": [0.20044546885751713, 0.16485598738003204, 0.1648560750781688]}, "mutation_prompt": null}
{"id": "e283ee9b-9a69-4bf0-8a5b-dd2d3558ab1c", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR * (1 - 0.1))  # Modified to use dynamic adjustment\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhance the APWO by using a dynamic crossover probability adjustment for better exploration and exploitation trade-off.", "configspace": "", "generation": 10, "fitness": 0.9882078793803571, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.005. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9820570972703775, 0.9953630995151452, 0.9872034413555487], "final_y": [0.16485599409808305, 0.16485598738003204, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "4fb7386d-1533-4bb9-be18-46ec08771bad", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05 + 0.05 * (1 - self.evaluations / self.budget))  # Adjusted CR adaptation\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced dynamic adaptation of crossover rate to balance exploration and exploitation.", "configspace": "", "generation": 11, "fitness": 0.992405984901548, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955631371887539, 0.9953631142060173, 0.9862917033098725], "final_y": [0.1648559120893145, 0.16485597086452086, 0.16485668047782742]}, "mutation_prompt": null}
{"id": "63039db2-1da0-47f2-892e-bafe33bd713e", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * ((1 - self.evaluations / self.budget) ** 2)  # Enhanced dynamic scaling factor\n            mutant = a + F_dynamic * (b - c + np.random.uniform(-0.1, 0.1, self.dim))  # Introduced diversity preservation\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO by enhancing dynamic scaling factor and introducing diversity preservation.", "configspace": "", "generation": 11, "fitness": 0.9750585162061828, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.021. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9454033794776471, 0.9934803291649654, 0.9862918399759357], "final_y": [0.16485941310503605, 0.16485626370377127, 0.1648562876091848]}, "mutation_prompt": null}
{"id": "a3d433b5-b287-42d7-ab0f-bbf14df24188", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass HPOL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_opposition_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        center = (ub + lb) / 2\n        radius = (ub - lb) / 2\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        opp_solution = center + (center - base_solution)\n        population = np.vstack([base_solution, opp_solution])\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(np.vstack([population, periodic_pop]), lb, ub)\n\n    def adaptive_mutation(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + np.random.uniform(-0.5, 0.5, self.dim) * (c - a))\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_opposition_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_mutation(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "HPOL", "description": "Hybrid Periodic Opposition-Based Learning with Adaptive Mutation to enhance global search diversity while maintaining convergence efficiency.", "configspace": "", "generation": 11, "fitness": 0.9629115294996294, "feedback": "The algorithm HPOL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.963 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9392778564328761, 0.9684402844904099, 0.981016447575602], "final_y": [0.16485647066111775, 0.1648612531625614, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "d67dfb9d-2c69-4b7e-aa66-2b07c8da6359", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - np.sin(self.evaluations / self.budget * np.pi / 2))  # Non-linear dynamic scaling\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n            if np.random.rand() < 0.1:  # Diversity preservation\n                next_gen[i] = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Introduced a non-linear dynamic scaling factor and included a diversity preservation mechanism for enhanced exploration and convergence.", "configspace": "", "generation": 11, "fitness": 0.991779092918213, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955652240713528, 0.9934801415018614, 0.9862919131814247], "final_y": [0.1648558732389932, 0.16485619644672844, 0.16485644666131405]}, "mutation_prompt": null}
{"id": "671d791f-24b6-4c79-981b-1c60caebeebf", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR + 0.1 * np.sin(2 * np.pi * self.evaluations / self.budget)\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive crossover probability to stabilize convergence.", "configspace": "", "generation": 11, "fitness": 0.9917794041638531, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955643281111485, 0.9934812555212793, 0.9862926288591315], "final_y": [0.16485602074152605, 0.1648559291036018, 0.16485605476006449]}, "mutation_prompt": null}
{"id": "e31e85f6-8b13-4997-8772-8e6442a601cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR + 0.1 * np.sin(2 * np.pi * self.evaluations / self.budget)  # Added adaptive CR adjustment\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive crossover rate adjustment for improved exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.9927092629614455, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955609868633751, 0.9953631166613993, 0.987203685359562], "final_y": [0.16485789073288692, 0.16485598738003204, 0.1648559775132279]}, "mutation_prompt": null}
{"id": "2f73618c-6922-48e7-89b1-9bf95bd0a016", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (0.5 * (b - c) + 0.5 * (c - a))  # Adjusted mutation scheme\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with an improved mutation scheme using a weighted sum of differences to promote diversity.", "configspace": "", "generation": 12, "fitness": 0.9815899749094306, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.982 with standard deviation 0.014. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9622030984032516, 0.9953631166613993, 0.9872037096636407], "final_y": [0.16485661281875197, 0.16485598738003204, 0.16485589343925944]}, "mutation_prompt": null}
{"id": "8d9ca0ac-f2ec-4435-a04e-1abdfc2447b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            d = population[np.random.choice(indices, 1, replace=False)][0]  # Changed line: Additional random vector for mutation diversity\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + d - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO by modifying the mutation strategy to include a blend of strategies for enhanced diversity.", "configspace": "", "generation": 12, "fitness": 0.9837530282777341, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.984 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9686920127351311, 0.9953631166613993, 0.987203955436672], "final_y": [0.16485632269544015, 0.16485598738003204, 0.16485589383749344]}, "mutation_prompt": null}
{"id": "2bfc9830-56e9-47bc-97ce-812b1b8c1086", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05)  # Adjusted CR increment\n            else:\n                CR = max(0.1, CR - 0.05)  # Adjusted CR decrement\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Incorporate adaptive crossover probability scaling in APWO to enhance solution refinement.", "configspace": "", "generation": 12, "fitness": 0.9924884582812875, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955644400586995, 0.9953631166613993, 0.9865378181237638], "final_y": [0.16485589926321054, 0.16485598738003204, 0.16485614422634232]}, "mutation_prompt": null}
{"id": "0c379095-e8ce-4e65-ba55-2a2c918376d9", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * np.exp(-self.evaluations / self.budget)  # Changed from linear to exponential decay\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with exponential decay for adaptive scaling factor to improve search efficiency.", "configspace": "", "generation": 12, "fitness": 0.9925428578656219, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9959693784737733, 0.9953631166613993, 0.986296078461693], "final_y": [0.16485610015103214, 0.16485598738003204, 0.1648560712850743]}, "mutation_prompt": null}
{"id": "45dbd88b-ccbf-4b14-8027-465a95a4da90", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                if np.random.rand() < 0.5:  # Adjusted local search frequency\n                    individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Refined APWO with adaptive local search frequency for better solution exploitation.", "configspace": "", "generation": 13, "fitness": 0.9927100159364901, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955634990211568, 0.9953631074327648, 0.9872034413555487], "final_y": [0.1648559580693023, 0.16485595346899806, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "3dfdbcc2-412a-4db1-b877-9844c9d7186f", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds, options={\"maxiter\": 50})  # Change made here\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic scaling factor adjustment and improved local search strategy for enhanced convergence.", "configspace": "", "generation": 13, "fitness": 0.9927101670409386, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955639602521221, 0.9953630995151452, 0.9872034413555487], "final_y": [0.16485600452891247, 0.16485598738003204, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "25c0da1d-74da-4f4b-af63-e12bcb92d839", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        next_gen = self.enforce_periodicity(next_gen, bounds)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def enforce_periodicity(self, population, bounds):\n        for i, individual in enumerate(population):\n            individual = np.mod(individual - bounds.lb, bounds.ub - bounds.lb) + bounds.lb\n        return population\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with periodicity-enforcing selection and adaptive local search for higher solution quality.", "configspace": "", "generation": 13, "fitness": 0.992710228864292, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955641457221821, 0.9953630995151452, 0.9872034413555487], "final_y": [0.16485609985079308, 0.16485598738003204, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "551ac13d-e85b-478f-a4bb-7e552b3827c5", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c)  # Adjusted mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Introduced fine-tuned exploration by adjusting the mutation strategy in the adaptive differential evolution.", "configspace": "", "generation": 13, "fitness": 0.9775086176239144, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.978 with standard deviation 0.020. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9499592079221966, 0.9953632035939978, 0.9872034413555487], "final_y": [0.16485658850617702, 0.1648558379293632, 0.16485620303429105]}, "mutation_prompt": null}
{"id": "8ca6a22c-176e-4f64-b613-c80364991520", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget) \n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)  # Adjusted CR update for exploration (line change 1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        diversity = np.std(population, axis=0).mean()  # Calculate population diversity (line change 2)\n        CR = min(1.0, CR + 0.1 * diversity)  # Adjust CR based on diversity (line change 3)\n        F_dynamic *= (1 + 0.1 * diversity)  # Adjust F_dynamic based on diversity (line change 4)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic CR based on population diversity for better exploration-exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.9927101440700442, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955635422562585, 0.9953632425104365, 0.9872036474434376], "final_y": [0.16485589015711466, 0.16485588699322007, 0.16485600310799353]}, "mutation_prompt": null}
{"id": "703e9f82-8c98-463e-8513-65f60d305c77", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass EnhancedAPWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, best, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                if self.evaluate(func, trial) < self.evaluate(func, best):\n                    best = trial\n                CR = max(0.1, CR - 0.1)\n        return next_gen, best\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population, best_solution = self.adaptive_differential_evolution(population, best_solution, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "EnhancedAPWO", "description": "Introduce periodic elitism into adaptive differential evolution to enhance convergence by preserving superior periodic solutions.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. brag_mirror (iid=1 dim=10)>, None').", "error": "TypeError('__call__(): incompatible function arguments. The following argument types are supported:\\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\\n\\nInvoked with: <RealSingleObjectiveProblem 1121. brag_mirror (iid=1 dim=10)>, None')", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {}, "mutation_prompt": null}
{"id": "edbd807a-4928-45e5-bbcc-c7f7d62eac15", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a + (np.mean(population, axis=0) - a))  # Adaptive mutation\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic scaling and adaptive mutation strategy for improved convergence.", "configspace": "", "generation": 14, "fitness": 0.9616575939803015, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.962 with standard deviation 0.043. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9882692755438466, 0.9957536153996981, 0.9009498909973596], "final_y": [0.1648565822454735, 0.16485631219987384, 0.16650241568119295]}, "mutation_prompt": null}
{"id": "88561c0a-f093-47e1-afe0-71393fbb812f", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            adaptive_factor = 0.5 * (1 + np.sin(2 * np.pi * self.evaluations / self.budget))  # Adaptive mutation strategy\n            mutant = a + adaptive_factor * (b - c)  # Replaced mutation strategy with adaptive factor\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic scaling and adaptive mutation strategies for efficient convergence.", "configspace": "", "generation": 14, "fitness": 0.9164645705503723, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.916 with standard deviation 0.022. And the mean value of best solutions found was 0.175 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9475702153990999, 0.9008736052546578, 0.9009498909973596], "final_y": [0.16485941310503605, 0.19369397586314774, 0.16650241568119295]}, "mutation_prompt": null}
{"id": "1e8b5b27-7fa1-490c-aa92-41a395901161", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c)  # Changed mutation strategy\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds, options={'maxiter': 5})  # Improved local optimization\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Introduced a novel mutation strategy and improved the local optimization to enhance convergence precision.", "configspace": "", "generation": 14, "fitness": 0.8993109496666668, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.899 with standard deviation 0.002. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.8961093527479828, 0.9008736052546578, 0.9009498909973596], "final_y": [0.16965386233127622, 0.19369397586314774, 0.16650241568119295]}, "mutation_prompt": null}
{"id": "82ec8cfa-a494-488b-a745-9a8315a7345b", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='TNC', bounds=bounds)  # Changed method from 'L-BFGS-B' to 'TNC'\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with a refined local optimization strategy for better convergence.", "configspace": "", "generation": 14, "fitness": 0.8944501220997676, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.894 with standard deviation 0.009. And the mean value of best solutions found was 0.186 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.8815268700472854, 0.9008736052546578, 0.9009498909973596], "final_y": [0.1976950480400006, 0.19369397586314774, 0.16650241568119295]}, "mutation_prompt": null}
{"id": "31ec5ef5-2ad5-4eed-aa94-113437046429", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            # Periodicity-promoting mutation strategy\n            mutated_a = a + F_dynamic * (b - c)\n            periodic_effect = np.sin(2 * np.pi * (a - b))\n            mutant = mutated_a + 0.1 * periodic_effect\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1)\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic scaling factor adjustment and periodicity-promoting mutation strategy for improved convergence.", "configspace": "", "generation": 15, "fitness": 0.9751480041682128, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.019. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9492437333306167, 0.984917510343636, 0.9912827688303855], "final_y": [0.16485642294439662, 0.16485582135956178, 0.16485638554158788]}, "mutation_prompt": null}
{"id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO by introducing a dynamic crossover rate and periodic mutation strategies for enhanced exploration and exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.9938785793806263, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.995564326749431, 0.9947885830193298, 0.9912828283731179], "final_y": [0.16485588805360463, 0.16485603279602257, 0.16485628979577738]}, "mutation_prompt": null}
{"id": "056f047c-7e0b-4000-855d-f1cb263305a0", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        step_size = (ub - lb) / np.random.randint(2, self.dim + 1)\n        periodic_pop = np.array([(base_solution + i * step_size) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_value = self.evaluate(func, trial)\n            if trial_value < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05)  # Adjusted crossover rate increment\n            else:\n                CR = max(0.1, CR - 0.05)  # Adjusted crossover rate decrement\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "An enhanced APWO with variable periodic step size and adaptive crossover rate for improved exploration and exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.9722285436905036, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.972 with standard deviation 0.025. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9885712032231986, 0.9368316815799097, 0.9912827462684028], "final_y": [0.16485599120733363, 0.18187821279180572, 0.16485643385801574]}, "mutation_prompt": null}
{"id": "739b7fa4-345e-4b35-bc15-ab3254a3cbd5", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            CR_dynamic = CR * (1 - self.evaluations / self.budget)  # Added dynamic crossover rate adjustment\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR_dynamic + 0.1)  # Adjusted to CR_dynamic\n            else:\n                CR = max(0.1, CR_dynamic - 0.1)  # Adjusted to CR_dynamic\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with adaptive crossover rate adjustment for improved exploration and exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.9745590858655939, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.975 with standard deviation 0.027. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.9955627730552238, 0.9368316815799097, 0.9912828029616483], "final_y": [0.1648563858300236, 0.18187821279180572, 0.16485636612694488]}, "mutation_prompt": null}
{"id": "c27cb18e-01ef-4c02-a3d1-45eb47ced731", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            D = np.random.choice([0.8, 1.0, 1.2])  # Added multi-strategy evolution by using different scaling factors\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + D * F_dynamic * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = np.clip(CR + np.random.uniform(-0.05, 0.05), 0.1, 1.0)  # Adaptive CR adjustment\n            else:\n                CR = max(0.1, CR - 0.1)\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                global_best = min(population, key=lambda ind: self.evaluate(func, ind))\n                local_exploit_weight = self.evaluations / self.budget\n                individual = self.local_optimization(func, local_exploit_weight * individual + (1 - local_exploit_weight) * global_best)  # Adaptive exploitation\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with multi-strategy evolution and adaptive exploitation for improved convergence and diversity.", "configspace": "", "generation": 15, "fitness": 0.9553788966707772, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.955 with standard deviation 0.025. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3e9aafb2-2a37-4f55-9a8e-62ce6954d292", "metadata": {"aucs": [0.93802222980372, 0.9368316815799097, 0.9912827786287021], "final_y": [0.18187879701398324, 0.18187821279180572, 0.16485639450134426]}, "mutation_prompt": null}
{"id": "47efb660-9aec-409e-aa64-5a1e36a5e25b", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n            best_solution = self.local_optimization(func, best_solution)  # Apply to the best solution\n        \n        return best_solution", "name": "APWO", "description": "Enhanced local optimization strategy by selectively applying gradient-based refinement to the best solution found in each generation to improve convergence speed and solution quality.", "configspace": "", "generation": 16, "fitness": 0.9796624560767825, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.980 with standard deviation 0.013. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9955643172681762, 0.9642471152117553, 0.9791759357504161], "final_y": [0.16485588805360463, 0.16818394047432028, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "237b7b06-4fe3-403f-9a54-582bb3364930", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget) * (0.5 + 0.5 * self.evaluations / self.budget)  # Adjusted dynamic scaling factor\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Introduced a dynamic mutation factor in the adaptive differential evolution to enhance exploration in early stages and exploitation in later stages.", "configspace": "", "generation": 16, "fitness": 0.9644402513180873, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.008. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9633326437976775, 0.9549350013098268, 0.9750531088467574], "final_y": [0.16485637286071386, 0.16485603062230303, 0.16485611162117975]}, "mutation_prompt": null}
{"id": "369e5afd-8dad-4266-9925-10266b78c7d1", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a) + 0.01 * np.sin(2 * np.pi * self.evaluations / self.budget)  # Added periodic mutation perturbation\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds, options={'maxiter': 5})  # Limited iterations for quicker convergence\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by implementing an adaptive periodic mutation strategy and improved local search integration for superior global and local optimization synergy.", "configspace": "", "generation": 16, "fitness": 0.9703800316383178, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.970 with standard deviation 0.011. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9770195208769544, 0.9549350013098268, 0.9791855727281723], "final_y": [0.16606733416068842, 0.16485603062230303, 0.16485588962774256]}, "mutation_prompt": null}
{"id": "0d9e58fa-ad7d-4811-9d02-389a21e734be", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a + np.sin(2 * np.pi * i / self.dim))  # Added sinusoidal term\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by refining mutation strategy to better explore near-optimal periodic solutions.", "configspace": "", "generation": 16, "fitness": 0.9765586656193531, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.977 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9955650597978163, 0.9549350013098268, 0.9791759357504161], "final_y": [0.16485610985214416, 0.16485603062230303, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "4d5178fb-6135-4d79-94da-baa2def6b99e", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget) * (np.sin(np.pi * self.evaluations / self.budget)) # Modified line\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved adaptive mutation strategy by scaling the mutation factor with a sinusoidal function to enhance exploration near convergence.", "configspace": "", "generation": 16, "fitness": 0.9661176755810171, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.966 with standard deviation 0.010. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9642420896828089, 0.9549350013098268, 0.9791759357504161], "final_y": [0.1648564540491767, 0.16485603062230303, 0.16485690289284805]}, "mutation_prompt": null}
{"id": "899ad687-3b65-4cbf-b082-0120ea86d4df", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_freq = np.random.randint(1, 5)  # Adjusted periodic frequency dynamically\n        periodic_pop = np.array([(base_solution + i * periodic_freq * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim // periodic_freq)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            selected = np.random.choice(indices, np.random.randint(2, 4), replace=False)  # Stochastic selection of indices\n            a, b, c = population[selected]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by introducing adaptive periodic frequency and stochastic selection strategies for improved exploration-exploitation balance.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {}, "mutation_prompt": null}
{"id": "b59f8c30-6197-4d8c-ba3c-28de070b9da6", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        chaotic_sequence = np.sin(np.linspace(0, np.pi, population.shape[0]))  # Added chaotic sequence\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget) * chaotic_sequence[i]  # Adjusted dynamic scaling factor\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced the mutation strategy by incorporating a chaotic sequence to improve exploration capabilities.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {}, "mutation_prompt": null}
{"id": "a50235ea-99d1-438e-81d3-32d4b4c5fac6", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        periodic_bias = np.mean(x.reshape(-1, 2), axis=1).repeat(2)  # Added bias towards periodicity\n        x = 0.9 * x + 0.1 * periodic_bias\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by incorporating a periodicity bias during local optimization, aligning solutions closer to known optimal structures.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('not enough values to unpack (expected 3, got 2)').", "error": "ValueError('not enough values to unpack (expected 3, got 2)')", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {}, "mutation_prompt": null}
{"id": "eabf924a-8e03-46cb-ac16-8f8b7a71df90", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / (2 * self.budget))  # Adjusted dynamic scaling factor\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by introducing dynamic scaling of the mutation factor F for improved exploration and local search capabilities.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {}, "mutation_prompt": null}
{"id": "50873a96-c099-4309-bfb5-e7540bb82007", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - (self.evaluations / self.budget)**0.5)  # Modified dynamic scaling factor\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(lambda x: func(np.mod(x - func.bounds.lb, func.bounds.ub - func.bounds.lb) + func.bounds.lb), x, method='L-BFGS-B', bounds=bounds)  # Added periodicity awareness\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by incorporating periodicity awareness in local optimization and dynamic mutation strategies for better convergence.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"Cannot take a larger sample than population when 'replace=False'\").", "error": "ValueError(\"Cannot take a larger sample than population when 'replace=False'\")", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {}, "mutation_prompt": null}
{"id": "e04abf8b-06c0-422e-b7f7-0ab1c80978d4", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            diversity = np.std(population, axis=0).mean()\n            F_dynamic = F * (1 - self.evaluations / self.budget) * (1 + 0.1 * diversity)  # Adjusted dynamic scaling factor\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by introducing a dynamic mutation factor adjustment based on population diversity for improved convergence.", "configspace": "", "generation": 18, "fitness": 0.946895765761627, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.947 with standard deviation 0.029. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9135597447849306, 0.9421774044064402, 0.9849501480935104], "final_y": [0.18188119011264237, 0.16485714568792187, 0.16485610557571884]}, "mutation_prompt": null}
{"id": "b91be161-2265-46cc-9acc-fe252c5cbedf", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c)  # Modified mutation strategy for better periodicity\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds, options={'ftol': 1e-9})  # Improved local search precision\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by refining mutation strategy for periodic structures and improving local search effectiveness.", "configspace": "", "generation": 18, "fitness": 0.9517290952179168, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.952 with standard deviation 0.020. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9500626557521145, 0.9283706713543328, 0.9767539585473032], "final_y": [0.164858549200545, 0.16485607986143613, 0.16485598287569592]}, "mutation_prompt": null}
{"id": "11b068e6-0212-4990-8fe2-5c2b467c19cb", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget) * np.sin(self.evaluations)  # Modified scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        if self.evaluations > self.budget * 0.5:  # Added dynamic trigger for local optimization\n            result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n            return result.x\n        return x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with strategic periodic scaling factor adjustments and dynamic local search trigger for improved convergence.", "configspace": "", "generation": 18, "fitness": 0.9636308235546721, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.964 with standard deviation 0.038. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9101943721404645, 0.9934853651023697, 0.9872127334211818], "final_y": [0.1818786560752479, 0.16485654190564059, 0.1648559234758602]}, "mutation_prompt": null}
{"id": "2a7a2022-aa95-455c-8d14-5796cde5022d", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - 0.9 * self.evaluations / self.budget)  # Slightly adjusted dynamic scaling\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by adjusting the mutation scaling factor for better adaptability throughout the optimization process.", "configspace": "", "generation": 18, "fitness": 0.9920654114412691, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9955653200886695, 0.9934194570795084, 0.9872114571556295], "final_y": [0.1648559832199502, 0.16485615042832058, 0.16485609256897538]}, "mutation_prompt": null}
{"id": "b166a253-4a82-4f4f-b1f3-e100143472df", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x + np.random.normal(0, 0.01, self.dim), method='L-BFGS-B', bounds=bounds)  # Added diversity\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by introducing a diversity preservation mechanism in local optimization to avoid premature convergence.", "configspace": "", "generation": 18, "fitness": 0.9907413992704669, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9915725185330493, 0.9934391852106503, 0.9872124940677012], "final_y": [0.1648560921901664, 0.16485628113851492, 0.1648562065723307]}, "mutation_prompt": null}
{"id": "f4c3c328-cc41-463c-b8db-03c98760aa2d", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / (2 * self.budget))  # Adjust F scaling\n            mutant = a + F_dynamic * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.05 * (self.evaluations / self.budget))  # Adjust CR increment\n            else:\n                CR = max(0.1, CR - 0.05 * (1 - self.evaluations / self.budget))  # Adjust CR decay\n        return next_gen\n\n    def segment_local_optimization(self, func, x):\n        segments = np.array_split(x, 2)  # Split into two segments\n        optimized_segments = [self.local_optimization(func, seg) for seg in segments]\n        return np.concatenate(optimized_segments)\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.segment_local_optimization(func, individual)  # Use segment optimization\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with self-adaptive mutation scaling and segment-based local optimization for improved black-box function performance.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The number of bounds is not compatible with the length of `x0`.').", "error": "ValueError('The number of bounds is not compatible with the length of `x0`.')", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {}, "mutation_prompt": null}
{"id": "43a8f5f7-b92b-4789-a7ab-2899da3095f6", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            W_dynamic = 0.5 + 0.5 * (self.evaluations / self.budget)  # Introduce dynamic weight factor\n            mutant = a + F_dynamic * (b - c + c - a) * W_dynamic\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (1 - (self.evaluations / self.budget)))  # Feedback-based CR adjustment\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by introducing adaptive weight factor in DE and a feedback mechanism for dynamic CR adjustment to boost convergence.", "configspace": "", "generation": 19, "fitness": 0.978793738844546, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.012. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9622041787249587, 0.9892616461168963, 0.9849153916917832], "final_y": [0.16485674696870678, 0.16485597864031487, 0.16485627673535885]}, "mutation_prompt": null}
{"id": "9218be36-a60b-4d3a-9c94-62e71e398bf2", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - (self.evaluations / self.budget) ** 2)  # Changed dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhance APWO by incorporating a self-adaptive scaling factor to dynamically balance exploration and exploitation.", "configspace": "", "generation": 19, "fitness": 0.9920838577536006, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9955639681623774, 0.993478215251307, 0.9872093898471179], "final_y": [0.16485588184268896, 0.16485699857073566, 0.16485619815557018]}, "mutation_prompt": null}
{"id": "bd11086b-9f6a-4ef4-8365-d87d1f565ee8", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        elite = population[np.argmin([self.evaluate(func, ind) for ind in population])]\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            adaptive_mutation = np.random.uniform(0, F_dynamic)\n            trial = np.where(np.random.rand(self.dim) < adaptive_mutation, trial, elite)  # Adaptive mutation scaling\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n\n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n\n        return best_solution", "name": "APWO", "description": "Enhanced APWO by incorporating elitism and adaptive mutation scaling for improved convergence.", "configspace": "", "generation": 19, "fitness": 0.9926799837630084, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.993 with standard deviation 0.001. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9928980481489309, 0.9936745150969504, 0.991467388043144], "final_y": [0.16485644195415516, 0.16485629702290938, 0.1648560955651992]}, "mutation_prompt": null}
{"id": "c5122f31-7a82-447e-b6b6-c954f4cfdcfd", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget)) \n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def dynamic_population_size(self, current_size):\n        max_size = 2 * self.dim\n        min_size = int(0.5 * self.dim)\n        return max(min_size, current_size - int(self.evaluations / self.budget * (current_size - min_size)))\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population_size = self.dim\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            population_size = self.dynamic_population_size(population_size)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO with dynamic population size and feedback-driven periodic mutation for improved convergence.", "configspace": "", "generation": 19, "fitness": 0.9935685066526944, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.994 with standard deviation 0.002. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9955634350466523, 0.9936746810818093, 0.9914674038296214], "final_y": [0.16485605083864563, 0.164856009012866, 0.16485605235807566]}, "mutation_prompt": null}
{"id": "e35de1dd-18a7-4a75-8406-5fca5e7b1a2a", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        best_idx = np.argmin([self.evaluate(func, ind) for ind in population])  # Identify best solution in the population\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        population[best_idx] = next_gen[best_idx]  # Reinforce best solution into next generation\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by integrating adaptive learning from best solutions to refine crossover rate and mutation strategies.", "configspace": "", "generation": 20, "fitness": 0.9914702760034894, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.991 with standard deviation 0.003. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9940007957863064, 0.9932238290014692, 0.9871862032226922], "final_y": [0.16485587928605328, 0.16485668617974747, 0.16485619008939123]}, "mutation_prompt": null}
{"id": "779977fa-63b7-413b-acee-32760755e7c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        return np.array([(lb + (ub - lb) * (i / self.dim) ** 2) % (ub - lb) for i in range(self.dim)])  # Enhanced periodic init\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)  # Added dynamic scaling factor adjustment\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Improved APWO by enhancing the periodic_init function to better exploit periodic structures using geometric progression.", "configspace": "", "generation": 20, "fitness": 0.9793795432713241, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.979 with standard deviation 0.016. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9577285112038787, 0.9932239153874013, 0.9871862032226922], "final_y": [0.16485721446801205, 0.16485652629574188, 0.16485619008939123]}, "mutation_prompt": null}
{"id": "c2e5c362-66a9-4c8d-9c0f-e5b14fa6f8af", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.15 * (self.evaluations / self.budget))  # Increase adjustment rate\n            else:\n                CR = max(0.05, CR - 0.1 * (1 - self.evaluations / self.budget))  # Slightly lower minimum CR\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO through adaptive periodic mutation and dynamic population scaling to improve balance between exploration and exploitation.", "configspace": "", "generation": 20, "fitness": 0.9919914529911975, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.992 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.995564326749431, 0.9932238290014692, 0.9871862032226922], "final_y": [0.16485588805360463, 0.16485668617974747, 0.16485619008939123]}, "mutation_prompt": null}
{"id": "ec7576c9-a7f4-4cf2-a500-05554591a0bc", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            # Change made here: Adjusted F_dynamic based on fitness difference\n            F_dynamic = F * (abs(self.evaluate(func, b) - self.evaluate(func, c)) / (1 + abs(self.evaluate(func, a) - self.evaluate(func, b)))) \n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < self.evaluate(func, population[i]):\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))  # Dynamic CR based on evaluations\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))  # Adjusted CR decay\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        \n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n        \n        return best_solution", "name": "APWO", "description": "Enhanced APWO by introducing a dynamic adjustment of the F factor based on the difference in fitness values, improving the balance between exploration and exploitation.", "configspace": "", "generation": 20, "fitness": 0.9811973025351591, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.981 with standard deviation 0.013. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9631818753813158, 0.9932238290014692, 0.9871862032226922], "final_y": [0.16485748795265442, 0.16485668617974747, 0.16485619008939123]}, "mutation_prompt": null}
{"id": "3762e43e-e0fa-434c-9260-9583bbdb1e31", "solution": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass APWO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def periodic_init(self, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        base_solution = np.random.uniform(lb, ub, self.dim)\n        periodic_pop = np.array([(base_solution + i * (ub - lb) / self.dim) % (ub - lb) + lb for i in range(self.dim)])\n        return np.clip(periodic_pop, lb, ub)\n\n    def adaptive_differential_evolution(self, population, func, bounds, F=0.5, initial_CR=0.9):\n        next_gen = population.copy()\n        CR = initial_CR\n        fitness = np.array([self.evaluate(func, ind) for ind in population])\n        for i in range(population.shape[0]):\n            indices = list(range(population.shape[0]))\n            indices.remove(i)\n            a, b, c = population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = F * (1 - self.evaluations / self.budget)\n            mutant = a + F_dynamic * (b - c + c - a)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR * (fitness[i] / (np.max(fitness) + 1e-5))\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            if self.evaluate(func, trial) < fitness[i]:\n                next_gen[i] = trial\n                CR = min(1.0, CR + 0.1 * (self.evaluations / self.budget))\n            else:\n                CR = max(0.1, CR - 0.1 * (1 - self.evaluations / self.budget))\n        return next_gen\n\n    def local_optimization(self, func, x):\n        bounds = Bounds(func.bounds.lb, func.bounds.ub)\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds)\n        return result.x\n\n    def evaluate(self, func, x):\n        if self.evaluations < self.budget:\n            self.evaluations += 1\n            return func(x)\n        else:\n            return np.inf\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = self.periodic_init(bounds)\n        best_solution = None\n        best_value = np.inf\n        while self.evaluations < self.budget:\n            population = self.adaptive_differential_evolution(population, func, bounds)\n            for individual in population:\n                individual = self.local_optimization(func, individual)\n                value = self.evaluate(func, individual)\n                if value < best_value:\n                    best_value = value\n                    best_solution = individual\n            # Dynamically adjust population size based on budget and evaluations\n            if self.evaluations < 0.5 * self.budget:\n                population = population[:self.dim]\n        \n        return best_solution", "name": "APWO", "description": "Enhanced Adaptive Periodic Wave Optimization (EAPWO) with dynamic population size and fitness-weighted crossover for improved convergence and exploration-exploitation balance.", "configspace": "", "generation": 20, "fitness": 0.9883969441861954, "feedback": "The algorithm APWO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.988 with standard deviation 0.004. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f96e1149-a322-481f-baa7-57478e3b1cf2", "metadata": {"aucs": [0.9847808003344246, 0.9932238290014692, 0.9871862032226922], "final_y": [0.16485617793812046, 0.16485668617974747, 0.16485619008939123]}, "mutation_prompt": null}
