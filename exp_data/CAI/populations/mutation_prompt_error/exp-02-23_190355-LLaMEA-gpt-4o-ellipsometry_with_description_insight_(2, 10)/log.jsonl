{"id": "12b8299c-dbea-41c7-8e93-54296a44393d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AMSLS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_initial_guesses = min(5, self.budget // 10)  # Limit the number of initial guesses\n        remaining_budget = self.budget\n        \n        # Generate diverse initial guesses across the search space\n        initial_guesses = [\n            np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n            for _ in range(num_initial_guesses)\n        ]\n        \n        best_solution = None\n        best_value = float('inf')\n\n        # Iteratively optimize from different starting points\n        for init_guess in initial_guesses:\n            if remaining_budget <= 0:\n                break\n            \n            bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n            \n            result = minimize(\n                func,\n                init_guess,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={'maxfun': min(remaining_budget, 10)}  # Restrict function evaluations per run\n            )\n            \n            remaining_budget -= result.nfev\n            \n            # Update best known solution\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n                \n            # Adapt search bounds based on current best solution\n            for i in range(self.dim):\n                bounds[i] = (\n                    max(bounds[i][0], best_solution[i] - (func.bounds.ub[i] - func.bounds.lb[i]) * 0.1),\n                    min(bounds[i][1], best_solution[i] + (func.bounds.ub[i] - func.bounds.lb[i]) * 0.1)\n                )\n        \n        return best_solution", "name": "AMSLS", "description": "Adaptive Multi-Seed Local Search (AMSLS) combines multiple initial guesses with adaptive adjustment of search bounds to efficiently explore and exploit smooth, low-dimensional cost landscapes.", "configspace": "", "generation": 0, "fitness": 0.17291923331331618, "feedback": "The algorithm AMSLS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.173 with standard deviation 0.065. And the mean value of best solutions found was 1.672 (0. is the best) with standard deviation 1.414.", "error": "", "parent_id": null, "metadata": {"aucs": [0.10726339946902097, 0.14998304950365382, 0.2615112509672738], "final_y": [3.5284128128213066, 1.3877665143298128, 0.10008605865830604]}, "mutation_prompt": null}
{"id": "7f893020-8287-4dde-ae1d-a67d1d2983c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "\"Adaptive Nelder-Mead with Dynamic Bound Adjustment\" - Combines the simplicity and robustness of the Nelder-Mead method with dynamic boundary adjustments to efficiently explore and exploit low-dimensional, smooth optimization landscapes within constrained budgets.", "configspace": "", "generation": 0, "fitness": 0.46003446437174006, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.460 with standard deviation 0.292. And the mean value of best solutions found was 4.632 (0. is the best) with standard deviation 6.550.", "error": "", "parent_id": null, "metadata": {"aucs": [0.663991250780877, 0.6690170220716609, 0.047095120262682366], "final_y": [8.539535005181962e-06, 8.639768878198026e-06, 13.894791323191058]}, "mutation_prompt": null}
{"id": "d62d485b-8042-4843-9e75-a8d17ac1b4b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        sobol = Sobol(d=self.dim)\n        initial_guess = sobol.random_base2(n=1).flatten() * (bounds[1] - bounds[0]) + bounds[0]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "\"Enhanced Adaptive Nelder-Mead with Dynamic Bound Adjustment\" - Further improves efficiency by refining initial guesses using Sobol sequences for better coverage in low-dimensional, smooth optimization landscapes.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"Sobol.random_base2() got an unexpected keyword argument 'n'\").", "error": "TypeError(\"Sobol.random_base2() got an unexpected keyword argument 'n'\")", "parent_id": "7f893020-8287-4dde-ae1d-a67d1d2983c2", "metadata": {}, "mutation_prompt": null}
{"id": "5f49f39b-7509-41f1-ab76-81ae87af5389", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass AMSLS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_initial_guesses = min(5, self.budget // 10)  # Limit the number of initial guesses\n        remaining_budget = self.budget\n        \n        # Generate diverse initial guesses across the search space using Sobol sequences\n        sobol = Sobol(d=self.dim, scramble=True)\n        initial_guesses = sobol.random_base2(m=int(np.log2(num_initial_guesses)))\n        initial_guesses = initial_guesses * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        \n        best_solution = None\n        best_value = float('inf')\n\n        # Iteratively optimize from different starting points\n        for init_guess in initial_guesses:\n            if remaining_budget <= 0:\n                break\n            \n            bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n            \n            result = minimize(\n                func,\n                init_guess,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={'maxfun': min(remaining_budget, 10)}  # Restrict function evaluations per run\n            )\n            \n            remaining_budget -= result.nfev\n            \n            # Update best known solution\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n                \n            # Adapt search bounds based on current best solution\n            for i in range(self.dim):\n                bounds[i] = (\n                    max(bounds[i][0], best_solution[i] - (func.bounds.ub[i] - func.bounds.lb[i]) * 0.1),\n                    min(bounds[i][1], best_solution[i] + (func.bounds.ub[i] - func.bounds.lb[i]) * 0.1)\n                )\n        \n        return best_solution", "name": "AMSLS", "description": "\"Enhanced AMSLS with Adaptive Initial Guess Selection\" - Introduces a strategic refinement by selecting initial guesses based on Sobol sequences for better coverage, enhancing the exploration of low-dimensional, smooth optimization landscapes.", "configspace": "", "generation": 1, "fitness": 0.16073362642533184, "feedback": "The algorithm AMSLS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.161 with standard deviation 0.027. And the mean value of best solutions found was 1.347 (0. is the best) with standard deviation 0.906.", "error": "", "parent_id": "12b8299c-dbea-41c7-8e93-54296a44393d", "metadata": {"aucs": [0.1228823713623689, 0.17605270044240573, 0.1832658074712209], "final_y": [2.6276660418432884, 0.740913593338188, 0.6710843605325911]}, "mutation_prompt": null}
{"id": "0813f32a-be55-4b8f-a316-fac9c3f3f39e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EAMSLS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_initial_guesses = min(5, self.budget // 10)\n        remaining_budget = self.budget\n        \n        initial_guesses = [\n            np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n            for _ in range(num_initial_guesses)\n        ]\n        \n        best_solution = None\n        best_value = float('inf')\n        adaptive_rates = np.linspace(0.05, 0.2, num_initial_guesses)  # Adaptive learning rates\n\n        for idx, init_guess in enumerate(initial_guesses):\n            if remaining_budget <= 0:\n                break\n            \n            bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n            \n            result = minimize(\n                func,\n                init_guess,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={'maxfun': min(remaining_budget, 10)}\n            )\n            \n            remaining_budget -= result.nfev\n            \n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n                \n            learning_rate = adaptive_rates[idx]\n            for i in range(self.dim):\n                bounds[i] = (\n                    max(bounds[i][0], best_solution[i] - (func.bounds.ub[i] - func.bounds.lb[i]) * learning_rate),\n                    min(bounds[i][1], best_solution[i] + (func.bounds.ub[i] - func.bounds.lb[i]) * learning_rate)\n                )\n        \n        return best_solution", "name": "EAMSLS", "description": "\"Enhanced Adaptive Multi-Seed Local Search (EAMSLS)\" - Incorporates a dynamic balance between global exploration and local exploitation with adaptive learning rates to improve solution quality in smooth optimization landscapes.", "configspace": "", "generation": 1, "fitness": 0.19621906475440246, "feedback": "The algorithm EAMSLS got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.196 with standard deviation 0.056. And the mean value of best solutions found was 0.852 (0. is the best) with standard deviation 0.692.", "error": "", "parent_id": "12b8299c-dbea-41c7-8e93-54296a44393d", "metadata": {"aucs": [0.13732808975528665, 0.2716687144258134, 0.17966039008210732], "final_y": [1.7632030240566634, 0.08822077315359479, 0.7030932596836139]}, "mutation_prompt": null}
{"id": "f7e8f372-1aa9-4682-8e03-3834d088e2f0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            # Early stopping if the solution has converged\n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with Early Stopping - Refines the original Adaptive Nelder-Mead by incorporating an early stopping mechanism to prevent unnecessary evaluations, making it more efficient under constrained budgets.", "configspace": "", "generation": 1, "fitness": 0.6804008088945412, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.680 with standard deviation 0.029. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7f893020-8287-4dde-ae1d-a67d1d2983c2", "metadata": {"aucs": [0.7205483227182916, 0.66849802687303, 0.6521560770923018], "final_y": [2.6472235638344027e-06, 8.660000705592675e-06, 6.043329734663786e-06]}, "mutation_prompt": null}
{"id": "985e0404-7e2e-477d-8bf0-cbd80b96118e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        # Enhanced initial sampling for better exploration\n        for _ in range(5):  # Sample 5 random initial points\n            random_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n            if func(random_guess) < func(initial_guess):\n                initial_guess = random_guess\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "\"Improved Adaptive Nelder-Mead with Enhanced Initial Sampling\" - Introduces enhanced initial sampling to the existing algorithm to improve exploration in smooth, low-dimensional optimization landscapes while maintaining efficient convergence.", "configspace": "", "generation": 1, "fitness": 0.6736667270585487, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.674 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7f893020-8287-4dde-ae1d-a67d1d2983c2", "metadata": {"aucs": [0.6578297274466685, 0.6790380587479827, 0.6841323949809947], "final_y": [9.757827567504784e-06, 6.763804682693809e-06, 3.4386828468746077e-06]}, "mutation_prompt": null}
{"id": "351c088f-1599-451b-a5d4-d66f9c884d15", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "\"Enhanced Adaptive Nelder-Mead with Initial Population Seeding\" - Augments the Nelder-Mead method with multiple initial guesses for broader exploration and refined dynamic boundary adjustments for efficient convergence in smooth, low-dimensional spaces.", "configspace": "", "generation": 1, "fitness": 0.7387643348287402, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.739 with standard deviation 0.132. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7f893020-8287-4dde-ae1d-a67d1d2983c2", "metadata": {"aucs": [0.92491781466402, 0.6608463105684483, 0.6305288792537523], "final_y": [0.0, 9.932932703264351e-06, 1.3058105939042023e-05]}, "mutation_prompt": null}
{"id": "ec728d55-f656-4875-9603-6015b99704ac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicBoundaryGradient:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        remaining_budget = self.budget\n        num_initial_guesses = max(2, self.budget // 15)\n        \n        # Generate initial guesses across the search space\n        initial_guesses = [\n            np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n            for _ in range(num_initial_guesses)\n        ]\n        \n        best_solution = None\n        best_value = float('inf')\n\n        # Iteratively optimize from different starting points\n        for init_guess in initial_guesses:\n            if remaining_budget <= 0:\n                break\n            \n            bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n            \n            result = minimize(\n                func,\n                init_guess,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={'maxfun': min(remaining_budget, 10)}  # Limit evaluations per run\n            )\n            \n            remaining_budget -= result.nfev\n            \n            # Update the best known solution\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n            # Dynamically adjust bounds based on the gradient information\n            gradient = result.jac if result.jac is not None else np.zeros(self.dim)\n            for i in range(self.dim):\n                step_size = (func.bounds.ub[i] - func.bounds.lb[i]) * 0.05\n                bounds[i] = (\n                    max(bounds[i][0], best_solution[i] - step_size * np.sign(gradient[i])),\n                    min(bounds[i][1], best_solution[i] + step_size * np.sign(gradient[i]))\n                )\n        \n        return best_solution", "name": "DynamicBoundaryGradient", "description": "\"Dynamic Boundary Exploration with Gradient-Informed Adjustments\" - Uses adaptive boundary refinement and gradient-based cues to efficiently navigate low-dimensional, smooth optimization landscapes within a constrained evaluation budget.", "configspace": "", "generation": 1, "fitness": 0.46797894093797315, "feedback": "The algorithm DynamicBoundaryGradient got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.468 with standard deviation 0.367. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.149.", "error": "", "parent_id": "12b8299c-dbea-41c7-8e93-54296a44393d", "metadata": {"aucs": [0.9863292415641783, 0.20384503221709127, 0.21376254903265013], "final_y": [0.0, 0.35050645456136903, 0.08567043224404693]}, "mutation_prompt": null}
{"id": "cb491e24-79a0-4a62-b114-f1bcac5b8f36", "solution": "import numpy as np\n\nclass StochasticGradientDescentWithAdaptiveSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.learning_rate = 0.1\n    \n    def __call__(self, func):\n        remaining_budget = self.budget\n        \n        # Initial random guess within bounds\n        current_solution = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        best_solution = current_solution.copy()\n        best_value = func(current_solution)\n        remaining_budget -= 1\n        \n        # Initialize learning rate and sampling range\n        sampling_range = np.array([func.bounds.ub[i] - func.bounds.lb[i] for i in range(self.dim)])\n        \n        while remaining_budget > 0:\n            # Generate a new candidate solution using SGD with adaptive sampling\n            gradient = self.estimate_gradient(func, current_solution, remaining_budget)\n            new_solution = current_solution - self.learning_rate * gradient\n            \n            # Clamp solution within bounds\n            new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate and update if we found a better solution\n            new_value = func(new_solution)\n            remaining_budget -= 1\n            \n            if new_value < best_value:\n                best_value = new_value\n                best_solution = new_solution.copy()\n                # Increase learning rate for faster convergence\n                self.learning_rate *= 1.1\n            else:\n                # Reduce learning rate to refine search\n                self.learning_rate *= 0.9\n            \n            current_solution = new_solution\n        \n        return best_solution\n    \n    def estimate_gradient(self, func, solution, remaining_budget):\n        # Estimate gradient using finite differences\n        gradient = np.zeros(self.dim)\n        epsilon = 1e-5\n        \n        for i in range(self.dim):\n            if remaining_budget <= 0:\n                break\n            \n            direction = np.zeros(self.dim)\n            direction[i] = epsilon\n            \n            plus_epsilon = np.clip(solution + direction, func.bounds.lb, func.bounds.ub)\n            minus_epsilon = np.clip(solution - direction, func.bounds.lb, func.bounds.ub)\n            \n            gradient[i] = (func(plus_epsilon) - func(minus_epsilon)) / (2 * epsilon)\n            remaining_budget -= 2\n        \n        return gradient", "name": "StochasticGradientDescentWithAdaptiveSampling", "description": "\"Stochastic Gradient Descent with Adaptive Sampling\" employs stochastic gradient descent (SGD) with dynamic learning rate adjustments and adaptive sampling to efficiently navigate smooth, low-dimensional parameter spaces within a constrained evaluation budget.", "configspace": "", "generation": 1, "fitness": 0.41817726432558605, "feedback": "The algorithm StochasticGradientDescentWithAdaptiveSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.418 with standard deviation 0.423. And the mean value of best solutions found was 11.184 (0. is the best) with standard deviation 15.752.", "error": "", "parent_id": "12b8299c-dbea-41c7-8e93-54296a44393d", "metadata": {"aucs": [1.0, 0.24553645096184207, 0.00899534201491603], "final_y": [0.0, 0.09118914878991302, 33.460396823398135]}, "mutation_prompt": null}
{"id": "3409eb55-96d1-4440-9b43-9740c831aa33", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        shrink_factor = 0.2 if result.success else 0.1  # Shrink more if successful\n        new_bounds = np.array([np.maximum(center - shrink_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + shrink_factor * (bounds[1] - bounds[0]), bounds[1])])\n\n        # Restart optimization with updated bounds if budget allows\n        while self.func_evaluations < self.budget:\n            try:\n                result = minimize(func, result.x, method='Nelder-Mead', \n                                  bounds=new_bounds.T, callback=callback, \n                                  options={'maxiter': min(self.budget - self.func_evaluations, 100), 'disp': False})\n                # Further contraction on success\n                if result.success:\n                    new_bounds = np.array([np.maximum(result.x - 0.2 * (new_bounds[1] - new_bounds[0]), new_bounds[0]),\n                                           np.minimum(result.x + 0.2 * (new_bounds[1] - new_bounds[0]), new_bounds[1])])\n            except StopIteration:\n                break\n\n        return result.x", "name": "EnhancedAdaptiveNelderMead", "description": "\"Enhanced Adaptive Nelder-Mead with Successive Contraction\" - Integrates Nelder-Mead with success-driven boundary contraction and adaptive restart, improving convergence on smooth, low-dimensional problems with constrained evaluations.", "configspace": "", "generation": 1, "fitness": 0.4525191393532558, "feedback": "The algorithm EnhancedAdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.453 with standard deviation 0.292. And the mean value of best solutions found was 5.544 (0. is the best) with standard deviation 7.841.", "error": "", "parent_id": "7f893020-8287-4dde-ae1d-a67d1d2983c2", "metadata": {"aucs": [0.6547418251839388, 0.6630388452906384, 0.03977674758519023], "final_y": [9.369055816644617e-06, 7.638134347280376e-06, 16.63246152879587]}, "mutation_prompt": null}
{"id": "45925c83-959b-4644-9985-3b789fe413c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveRandomPerturbation:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n\n        # Initial random guess within bounds\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n\n        # Track function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Define a random perturbation function\n        def random_perturbation(x, scale=0.05):\n            return x + np.random.uniform(-scale, scale, size=x.shape)\n\n        # Perform initial optimization using Nelder-Mead\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead',\n                              bounds=bounds.T, callback=callback,\n                              options={'maxiter': self.budget, 'disp': False})\n        except StopIteration:\n            pass\n\n        # Iteratively refine the solution with random perturbations\n        current_solution = result.x\n        while self.func_evaluations < self.budget:\n            perturbed_solution = random_perturbation(current_solution)\n            try:\n                result = minimize(func, perturbed_solution, method='Nelder-Mead',\n                                  bounds=bounds.T, callback=callback,\n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                break\n            current_solution = result.x\n\n            # Update bounds to focus on the neighborhood of the current solution\n            new_bounds = np.array([np.maximum(current_solution - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                                   np.minimum(current_solution + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n            bounds = new_bounds\n\n        return current_solution", "name": "AdaptiveRandomPerturbation", "description": "\"Adaptive Random Perturbation Optimization\" - Integrates random perturbations into local search methods for enhanced exploration, refining solutions iteratively within adaptive bounds to efficiently utilize constrained budgets in smooth, low-dimensional landscapes.", "configspace": "", "generation": 1, "fitness": 0.4792638772579778, "feedback": "The algorithm AdaptiveRandomPerturbation got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.479 with standard deviation 0.288. And the mean value of best solutions found was 2.605 (0. is the best) with standard deviation 3.684.", "error": "", "parent_id": "7f893020-8287-4dde-ae1d-a67d1d2983c2", "metadata": {"aucs": [0.6875739750325838, 0.6778082433136219, 0.07240941342772789], "final_y": [5.268533771939585e-06, 6.285816505527295e-06, 7.815508577326766]}, "mutation_prompt": null}
{"id": "fdef576d-72be-4689-bcac-54e0a9bd6630", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            # Early stopping if the solution has converged\n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, result.x, method='BFGS',  # Changed method to 'BFGS'\n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with Gradient-Based Refinement - Enhances convergence by integrating a gradient-based refinement step post-initial optimization for smoother progress within constraints.", "configspace": "", "generation": 2, "fitness": 0.66066217603155, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.661 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7e8f372-1aa9-4682-8e03-3834d088e2f0", "metadata": {"aucs": [0.6711838754667862, 0.6642101843081456, 0.6465924683197183], "final_y": [6.493532740155241e-06, 8.354409094298063e-06, 6.371191927008548e-06]}, "mutation_prompt": null}
{"id": "6b77df60-0fde-4418-b83b-683c4e5b3f10", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Prioritize initial guesses based on a simple evaluation\n        initial_guesses.sort(key=lambda x: func(x))\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Introduced an adaptive weighting mechanism for initial guesses, prioritizing the most promising ones.", "configspace": "", "generation": 2, "fitness": 0.6778059884276536, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.678 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "351c088f-1599-451b-a5d4-d66f9c884d15", "metadata": {"aucs": [0.6694443083138149, 0.6994426404694674, 0.6645310164996785], "final_y": [8.594852959171815e-06, 4.268388777224933e-06, 4.29030986877186e-06]}, "mutation_prompt": null}
{"id": "1664fd52-664b-474c-9399-20c6016a17fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            # Early stopping if the solution has converged\n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, initial_guess=result.x+0.05*(bounds[1]-bounds[0]), method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Adaptive Nelder-Mead with Early Stopping and Enhanced Initial Guess Refinement.", "configspace": "", "generation": 2, "fitness": 0.6607751785596454, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.661 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7e8f372-1aa9-4682-8e03-3834d088e2f0", "metadata": {"aucs": [0.6613209520670935, 0.6684805026482239, 0.6525240809636188], "final_y": [9.919233112963999e-06, 6.976755022825154e-06, 7.730698640395509e-06]}, "mutation_prompt": null}
{"id": "a44383cc-edb0-4ed2-8172-f980cab4fee5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False, 'adaptive': True})\n            \n            # Early stopping if the solution has converged\n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "\"Enhanced Adaptive Nelder-Mead with Early Stopping and Dynamic Step Sizes\" - Optimizes convergence by dynamically adjusting step sizes based on progress, enhancing exploration within budget constraints.", "configspace": "", "generation": 2, "fitness": 0.7248656304812638, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.725 with standard deviation 0.089. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7e8f372-1aa9-4682-8e03-3834d088e2f0", "metadata": {"aucs": [0.8510233193468393, 0.6665468351618233, 0.6570267369351287], "final_y": [9.561118079846883e-08, 7.294652268275452e-06, 3.7084870051284923e-06]}, "mutation_prompt": null}
{"id": "354fb57a-c603-472b-9f6a-722558b0f00e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Integrates a weighted random sampling restart strategy into the Adaptive Nelder-Mead for enhanced exploration dynamics and convergence efficiency.", "configspace": "", "generation": 2, "fitness": 0.7822716100534173, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.154. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7e8f372-1aa9-4682-8e03-3834d088e2f0", "metadata": {"aucs": [0.9998461201971002, 0.689926825287572, 0.6570418846755794], "final_y": [0.0, 5.513996087461023e-06, 5.940377140612902e-06]}, "mutation_prompt": null}
{"id": "e455e2a1-f7c2-4e93-8b0b-8c6389c863ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n\n        # Calculate an approximate gradient direction using finite differences\n        def approximate_gradient(x):\n            eps = 1e-8\n            grad = np.zeros_like(x)\n            for i in range(self.dim):\n                x_eps = x.copy()\n                x_eps[i] += eps\n                grad[i] = (func(x_eps) - func(x)) / eps\n            return grad\n\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            # Modify the initial_guess by incorporating a small step towards the gradient\n            gradient_direction = approximate_gradient(initial_guess)\n            initial_guess = initial_guess - 0.05 * gradient_direction  # Small step towards gradient\n\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with Gradient-Inspired Directional Bias - Introduces a directional bias inspired by gradient estimates to guide the search towards promising regions, improving convergence efficiency within budget constraints.", "configspace": "", "generation": 2, "fitness": 0.6559462265930465, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.656 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7e8f372-1aa9-4682-8e03-3834d088e2f0", "metadata": {"aucs": [0.6521215254888559, 0.6727241854107087, 0.6429929688795748], "final_y": [9.522576206483968e-06, 6.750778606641008e-06, 6.8215670427745936e-06]}, "mutation_prompt": null}
{"id": "73fd0def-46d0-403a-acf3-bc2836d34e37", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        # Change: Use dynamic initial guess based on midpoint of bounds\n        initial_guess = (bounds[0] + bounds[1]) / 2\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            # Early stopping if the solution has converged\n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with Dynamic Initial Guesses - Introduces a dynamic strategy for generating initial guesses based on previous evaluations to further improve convergence efficiency in constrained budgets.", "configspace": "", "generation": 2, "fitness": 0.7302865826622575, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.730 with standard deviation 0.103. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7e8f372-1aa9-4682-8e03-3834d088e2f0", "metadata": {"aucs": [0.875314482746986, 0.6662637698218857, 0.6492814954179007], "final_y": [5.2721875151566326e-08, 8.046261172287966e-06, 6.021402633777913e-06]}, "mutation_prompt": null}
{"id": "2e19c7dd-f889-4582-a023-07a5c091ccde", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds and initial guesses\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        # Perform optimization with dynamic boundary adjustments\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=bounds.T, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            # Early stopping if the solution has converged\n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        # Adjust bounds based on the found solution for potential further refinement\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        # Added adaptive learning rate and perturbation strategy\n        perturbation = np.random.uniform(-0.05, 0.05, size=self.dim)\n        initial_guess = center + perturbation\n\n        # Re-run optimization with updated bounds if budget allows\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              bounds=new_bounds.T, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Incorporate adaptive learning rate and perturbation strategy for improved convergence in smooth, low-dimensional landscapes.", "configspace": "", "generation": 2, "fitness": 0.5022213557881243, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.502 with standard deviation 0.279. And the mean value of best solutions found was 1.195 (0. is the best) with standard deviation 1.690.", "error": "", "parent_id": "f7e8f372-1aa9-4682-8e03-3834d088e2f0", "metadata": {"aucs": [0.10843874227992634, 0.6827709352873801, 0.7154543897970664], "final_y": [3.585191637723231, 2.0703915306570105e-06, 2.4735982659957045e-06]}, "mutation_prompt": null}
{"id": "e91997e1-7af2-4cfb-b485-9866d5353d58", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Adjusting 0.1 to 0.15 for wider refinement\n                    new_bounds = np.array([np.maximum(center - 0.15 * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + 0.15 * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with Initial Population Seeding and Strategy Modification for Better Exploration", "configspace": "", "generation": 2, "fitness": 0.7468783229847679, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.747 with standard deviation 0.126. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "351c088f-1599-451b-a5d4-d66f9c884d15", "metadata": {"aucs": [0.92491781466402, 0.6727241854107087, 0.6429929688795748], "final_y": [0.0, 6.750778606641008e-06, 6.8215670427745936e-06]}, "mutation_prompt": null}
{"id": "21050e2a-ec32-4e65-aa18-a31dec5858e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Increase number of initial guesses to enhance initial exploration\n        num_initial_guesses = 5  # Changed from 3 to 5\n        \n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n            # Strategic restart if local stagnation is detected\n            if not result.success and self.func_evaluations < self.budget:\n                initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)  # New initial guess for restart\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                except StopIteration:\n                    pass\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "\"Adaptive Nelder-Mead with Strategic Restart and Enhanced Initial Exploration\" - Introduces a strategic restart mechanism when local stagnation is detected and enhances initial exploration by increasing the number of initial guesses, optimizing convergence efficiency in smooth, constrained landscapes.", "configspace": "", "generation": 2, "fitness": 0.7468783229847679, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.747 with standard deviation 0.126. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "351c088f-1599-451b-a5d4-d66f9c884d15", "metadata": {"aucs": [0.92491781466402, 0.6727241854107087, 0.6429929688795748], "final_y": [0.0, 6.750778606641008e-06, 6.8215670427745936e-06]}, "mutation_prompt": null}
{"id": "65056fd1-4efa-41f5-88d0-dd4505b85fdf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.2 * (bounds[1] - bounds[0]), bounds[0]),  # Adjusted boundary scaling factor\n                               np.minimum(center + 0.2 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            dynamic_weights = np.linspace(0.05, 1.0, self.dim)  # Adjusted weight scaling\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * dynamic_weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces dynamic boundary adjustment and adaptive weight scaling to improve exploration and convergence in the Adaptive Nelder-Mead algorithm.", "configspace": "", "generation": 3, "fitness": 0.712578360310542, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.713 with standard deviation 0.095. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "354fb57a-c603-472b-9f6a-722558b0f00e", "metadata": {"aucs": [0.8456789243014431, 0.6608463105684472, 0.6312098460617358], "final_y": [1.1407053661853625e-07, 9.932932703264351e-06, 1.3058105939042023e-05]}, "mutation_prompt": null}
{"id": "0dd624eb-7e1f-42f8-90dc-0aa8141b6c4f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        step_size = 0.05 * (self.budget - self.func_evaluations) / self.budget\n        new_bounds = np.array([np.maximum(center - step_size * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + step_size * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Integrates a dynamic adaptive step size into the Adaptive Nelder-Mead to improve exploration and convergence efficiency.", "configspace": "", "generation": 3, "fitness": 0.712578360310542, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.713 with standard deviation 0.095. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "354fb57a-c603-472b-9f6a-722558b0f00e", "metadata": {"aucs": [0.8456789243014431, 0.6608463105684472, 0.6312098460617358], "final_y": [1.1407053661853625e-07, 9.932932703264351e-06, 1.3058105939042023e-05]}, "mutation_prompt": null}
{"id": "1cafb20c-1501-4ba0-9479-ffcfe9a6b21f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.5, 1.5, self.dim)  # Adjusted dynamic weighting range\n            new_guess = np.random.uniform(new_bounds[0], new_bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Augments Adaptive Nelder-Mead with dynamic weighting in the restart strategy to enhance convergence precision.", "configspace": "", "generation": 3, "fitness": 0.748719833342613, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.749 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "354fb57a-c603-472b-9f6a-722558b0f00e", "metadata": {"aucs": [0.8291599016149029, 0.7626383981993304, 0.6543612002136061], "final_y": [1.4645215981939534e-07, 1.8533312367560653e-07, 6.724513462499461e-06]}, "mutation_prompt": null}
{"id": "752aa97f-88f6-4ba6-8e76-6742f954e329", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=1e-5, callback=callback,  # Adjusted tolerance\n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic tolerance adjustment for improved convergence efficiency.", "configspace": "", "generation": 3, "fitness": 0.7152850786400734, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.715 with standard deviation 0.039. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "354fb57a-c603-472b-9f6a-722558b0f00e", "metadata": {"aucs": [0.7561777436770141, 0.727646208742433, 0.6620312835007736], "final_y": [1.0437024902650396e-06, 1.1611643919825492e-06, 5.232059190644538e-06]}, "mutation_prompt": null}
{"id": "510531c8-f0db-46e6-b6b4-0435dd2a7b33", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Augmented the Adaptive Nelder-Mead by introducing a dynamic adaptive tolerance adjustment based on the function evaluations for enhanced precision.", "configspace": "", "generation": 3, "fitness": 0.752922013677097, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.753 with standard deviation 0.086. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "354fb57a-c603-472b-9f6a-722558b0f00e", "metadata": {"aucs": [0.8690885487880846, 0.727646208742433, 0.6620312835007736], "final_y": [6.415752132168867e-08, 1.1611643919825492e-06, 5.232059190644538e-06]}, "mutation_prompt": null}
{"id": "d1e7d271-8c6f-4c45-bb9b-159416328a81", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 5  # Increased from 3 to 5 for better exploration\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Adjusting 0.1 to 0.15 for wider refinement\n                    new_bounds = np.array([np.maximum(center - 0.15 * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + 0.15 * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "AdaptiveNelderMead with increased initial guesses for improved exploration and convergence.", "configspace": "", "generation": 3, "fitness": 0.7389913237647344, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.739 with standard deviation 0.132. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e91997e1-7af2-4cfb-b485-9866d5353d58", "metadata": {"aucs": [0.92491781466402, 0.6608463105684472, 0.6312098460617358], "final_y": [0.0, 9.932932703264351e-06, 1.3058105939042023e-05]}, "mutation_prompt": null}
{"id": "fd4245b2-782d-41e7-84d8-df95b7d8cd3d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Introduce a dynamic adjustment factor for bounds refinement\n                    adjust_factor = 0.15 * (1 + np.random.uniform(-0.05, 0.05))\n                    new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Introduce a dynamic adjustment factor for the bounds refinement step to enhance exploitation while maintaining broad exploration.", "configspace": "", "generation": 3, "fitness": 0.7642677036693857, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.090. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e91997e1-7af2-4cfb-b485-9866d5353d58", "metadata": {"aucs": [0.8758035125952205, 0.7626383981993304, 0.6543612002136061], "final_y": [0.0, 1.8533312367560653e-07, 6.724513462499461e-06]}, "mutation_prompt": null}
{"id": "60235e03-33ae-4296-8488-94d0770f5e0d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Adjusting 0.1 to 0.15 for wider refinement\n                    new_bounds = np.array([np.maximum(center - 0.15 * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + 0.15 * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n            # Additional refinement with Powell's method using remaining budget\n            if result and self.func_evaluations < self.budget:\n                try:\n                    result = minimize(func, result.x, method='Powell', bounds=bounds.T,\n                                      options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                except StopIteration:\n                    pass\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Incorporates a final local search step with Powell's method if budget allows, to refine the solution.", "configspace": "", "generation": 3, "fitness": 0.6898148196958808, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.690 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e91997e1-7af2-4cfb-b485-9866d5353d58", "metadata": {"aucs": [0.6797669668444359, 0.727646208742433, 0.6620312835007736], "final_y": [5.912903395812199e-06, 1.1611643919825492e-06, 5.232059190644538e-06]}, "mutation_prompt": null}
{"id": "89f6a13d-055b-4006-aea0-c9e3478f291e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 4  # Increased from 3 to 4 for improved exploration\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Adjusting 0.1 to 0.15 for wider refinement\n                    new_bounds = np.array([np.maximum(center - 0.15 * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + 0.15 * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Improve exploration by increasing the number of initial guesses to enhance diversity.", "configspace": "", "generation": 3, "fitness": 0.6272724250766896, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.627 with standard deviation 0.006. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e91997e1-7af2-4cfb-b485-9866d5353d58", "metadata": {"aucs": [0.6288267260707718, 0.61946884758585, 0.6335217015734467], "final_y": [1.3913156111734375e-05, 1.7034743788974242e-05, 1.8239704871684237e-05]}, "mutation_prompt": null}
{"id": "199e5cc5-d3c0-49ec-9f26-61622b0d4ee5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Dynamic adjustment based on success rate\n                    contraction_factor = 0.1 if result.success else 0.05\n                    new_bounds = np.array([np.maximum(center - contraction_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + contraction_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Integrate dynamic parameter tuning and adaptive boundary contraction in Enhanced Adaptive Nelder-Mead for improved convergence precision.", "configspace": "", "generation": 3, "fitness": 0.6478973541836891, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.648 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e91997e1-7af2-4cfb-b485-9866d5353d58", "metadata": {"aucs": [0.6522315402874364, 0.6646781916181382, 0.6267823306454928], "final_y": [1.0503639940680971e-05, 1.943037981913999e-07, 1.2686459091313368e-05]}, "mutation_prompt": null}
{"id": "c2e9b842-fbed-4392-ba52-3b5dc46fa248", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-5 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduce a varied dynamic tolerance adjustment to different search phases for better precision and convergence. ", "configspace": "", "generation": 4, "fitness": 0.6949322923893968, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.695 with standard deviation 0.050. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "510531c8-f0db-46e6-b6b4-0435dd2a7b33", "metadata": {"aucs": [0.7641942772505025, 0.6745556093199933, 0.6460469905976949], "final_y": [6.629659200810466e-07, 6.469527862575546e-06, 7.691906158796932e-06]}, "mutation_prompt": null}
{"id": "b0fe7e63-b729-45ee-8421-856a5fcb2854", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        weights = np.linspace(0.5, 1.5, self.dim)  # Adjusted weight range for initial guess\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + 0.5 * (self.func_evaluations / self.budget))  # Adjusted tolerance calculation\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights  # Consistent weighted sampling\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhanced exploration by varying initial guess with weighted sampling and adjusted dynamic tolerance for balance between exploration and exploitation.", "configspace": "", "generation": 4, "fitness": 0.7022100977082543, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.702 with standard deviation 0.076. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "510531c8-f0db-46e6-b6b4-0435dd2a7b33", "metadata": {"aucs": [0.8083608958273512, 0.6662533899457375, 0.6320160073516743], "final_y": [1.7839407180553974e-07, 7.905519746227032e-06, 1.3673193901840472e-05]}, "mutation_prompt": null}
{"id": "a0bb233a-1d31-455e-a684-b807e955c1d1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)) * (1 - (self.func_evaluations / self.budget))  # Changed line\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduce a weighted dynamic tolerance factor that decreases with the budget progress to enhance adaptability and precision.", "configspace": "", "generation": 4, "fitness": 0.7132974099918393, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.713 with standard deviation 0.076. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "510531c8-f0db-46e6-b6b4-0435dd2a7b33", "metadata": {"aucs": [0.820179287587793, 0.67034281962794, 0.6493701227597849], "final_y": [1.878765173840638e-07, 8.100709381567321e-06, 7.462673700720998e-06]}, "mutation_prompt": null}
{"id": "29d5c3ec-b137-478b-8365-a5d81cb4b8f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.9, 1.1)  # Dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduced a dynamic scaling factor to the initial guess to enhance exploration capabilities.", "configspace": "", "generation": 4, "fitness": 0.7244328980588457, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.724 with standard deviation 0.093. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "510531c8-f0db-46e6-b6b4-0435dd2a7b33", "metadata": {"aucs": [0.8547209295478231, 0.6774175093912207, 0.6411602552374933], "final_y": [7.336494323580635e-08, 7.220439011749304e-06, 8.525738615560748e-06]}, "mutation_prompt": null}
{"id": "13f66315-d622-4a3e-a94f-1ed36a1e538f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Evaluate initial guesses and sort by performance\n        evaluated_guesses = [(guess, func(guess)) for guess in initial_guesses]\n        evaluated_guesses.sort(key=lambda x: x[1])\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            # Use the best initial guess based on initial evaluations\n            for initial_guess, _ in evaluated_guesses:\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Introduce a dynamic adjustment factor for bounds refinement\n                    adjust_factor = 0.15 * (1 + np.random.uniform(-0.05, 0.05))\n                    new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Introduce adaptive exploration via an elite selection mechanism for initial guesses to refocus the search on promising areas.", "configspace": "", "generation": 4, "fitness": 0.7255607060330673, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.726 with standard deviation 0.091. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fd4245b2-782d-41e7-84d8-df95b7d8cd3d", "metadata": {"aucs": [0.8518098552745845, 0.6854855566869127, 0.6393867061377048], "final_y": [7.493516652769346e-08, 5.80686783639557e-06, 9.439150892159853e-06]}, "mutation_prompt": null}
{"id": "83ba89a5-ca9c-41e1-a24e-567a58629b90", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        sobol = Sobol(d=self.dim)\n        initial_guess = sobol.random_base2(m=1)[0] * (bounds[1] - bounds[0]) + bounds[0]\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhanced initial guess sampling by integrating a Sobol sequence for better exploration coverage.", "configspace": "", "generation": 4, "fitness": 0.7096093937449796, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.710 with standard deviation 0.071. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "510531c8-f0db-46e6-b6b4-0435dd2a7b33", "metadata": {"aucs": [0.8104407489145542, 0.6605184649668399, 0.6578689673535443], "final_y": [1.8509052681934016e-07, 9.788248256440077e-06, 6.970234262744708e-06]}, "mutation_prompt": null}
{"id": "d53aa0cc-ce80-4384-b071-cccab15c1660", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.linspace(0.8, 1.2, self.dim)  # Changed line\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduce a dynamic scaling factor for initial guess generation to improve convergence and exploration balance.", "configspace": "", "generation": 4, "fitness": 0.700135442596412, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.700 with standard deviation 0.074. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "510531c8-f0db-46e6-b6b4-0435dd2a7b33", "metadata": {"aucs": [0.8045816413723799, 0.6494195221915626, 0.6464051642252939], "final_y": [1.8211323124804296e-07, 1.332414110869523e-05, 7.600356703811685e-06]}, "mutation_prompt": null}
{"id": "93c02d3f-1d4c-4823-8b70-3b5dba88eee3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Introduce a dynamic adjustment factor for bounds refinement\n                    progress_ratio = self.func_evaluations / self.budget\n                    adjust_factor = 0.1 * ((1 + progress_ratio) + np.random.uniform(-0.05, 0.05))\n                    new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Improve search efficiency by adaptively scaling the adjustment factor based on function evaluation progress.", "configspace": "", "generation": 4, "fitness": 0.6644223710990569, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.664 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fd4245b2-782d-41e7-84d8-df95b7d8cd3d", "metadata": {"aucs": [0.6922857905463117, 0.6518439121245244, 0.6491374106263346], "final_y": [4.915344725737659e-06, 7.192650330294403e-06, 6.057191286330898e-06]}, "mutation_prompt": null}
{"id": "8d10c3eb-c3f4-4697-81bb-b2f2b1565ee0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                # Apply a perturbation factor on the initial guesses\n                perturb_factor = 0.1 * (1 + np.random.uniform(-0.05, 0.05))\n                initial_perturbation = np.random.uniform(-perturb_factor, perturb_factor, self.dim)\n                initial_guess += initial_perturbation\n\n                # Perform optimization with dynamic boundary adjustments\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Refine bounds with dynamic adjustment factor\n                    adjust_factor = 0.12 * (1 + np.random.uniform(-0.05, 0.05))\n                    new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Introduce an adaptive perturbation factor during initial exploration to enhance coverage and refinement potential.", "configspace": "", "generation": 4, "fitness": 0.6718557838773324, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.672 with standard deviation 0.019. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fd4245b2-782d-41e7-84d8-df95b7d8cd3d", "metadata": {"aucs": [0.6958584443761926, 0.67034281962794, 0.6493660876278645], "final_y": [4.73315613700614e-06, 8.100709381567321e-06, 7.462673700720998e-06]}, "mutation_prompt": null}
{"id": "2962b345-73ce-4567-b659-1829fd6bc2a4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses with added stochastic perturbations for enhanced diversity\n        num_initial_guesses = 3\n        perturbation_scale = 0.05  # New perturbation scale\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) + \n                           np.random.normal(0, perturbation_scale, self.dim)  # Added perturbation\n                           for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        \n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            for initial_guess in initial_guesses:\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    adjust_factor = 0.15 * (1 + np.random.uniform(-0.05, 0.05))\n                    new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Introduce stochastic location perturbation for initial guesses to increase exploration diversity.", "configspace": "", "generation": 4, "fitness": 0.6627241742955228, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.663 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "fd4245b2-782d-41e7-84d8-df95b7d8cd3d", "metadata": {"aucs": [0.6729469367320142, 0.6469446962354033, 0.6682808899191508], "final_y": [2.9166555360850044e-06, 4.9788303949834065e-06, 6.023833324178192e-06]}, "mutation_prompt": null}
{"id": "2ec0012e-9828-4bc2-893f-f7e257b7115c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted scaling factor range\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 0.9, self.dim)  # Adjusted weight range\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhance exploration by adjusting scaling factor range and introducing weighted random initialization.", "configspace": "", "generation": 5, "fitness": 0.7203621548268072, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.720 with standard deviation 0.089. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "29d5c3ec-b137-478b-8365-a5d81cb4b8f8", "metadata": {"aucs": [0.8452138800218969, 0.6474184741645522, 0.6684541102939727], "final_y": [8.544391296052584e-08, 1.2836973986629791e-05, 7.783781755675553e-06]}, "mutation_prompt": null}
{"id": "034a99ad-f287-4b2d-a4bf-f6f702d87064", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.9, 1.1)  # Dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.05 * (bounds[1] - bounds[0]), bounds[0]),  # Adjusted from 0.1\n                               np.minimum(center + 0.05 * (bounds[1] - bounds[0]), bounds[1])])  # Adjusted from 0.1\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Integrate adaptive boundary refinement to enhance exploitation efficiency in promising regions.", "configspace": "", "generation": 5, "fitness": 0.7203621548268072, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.720 with standard deviation 0.089. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "29d5c3ec-b137-478b-8365-a5d81cb4b8f8", "metadata": {"aucs": [0.8452138800218969, 0.6474184741645522, 0.6684541102939727], "final_y": [8.544391296052584e-08, 1.2836973986629791e-05, 7.783781755675553e-06]}, "mutation_prompt": null}
{"id": "dbd796fd-2c12-4817-b304-d6f5fd2c5a07", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Evaluate initial guesses and sort by performance\n        evaluated_guesses = [(guess, func(guess)) for guess in initial_guesses]\n        evaluated_guesses.sort(key=lambda x: x[1])\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            # Use the median initial guess based on initial evaluations\n            initial_guess = evaluated_guesses[len(evaluated_guesses) // 2][0]\n            try:\n                result = minimize(func, initial_guess, method='Nelder-Mead', \n                                  bounds=bounds.T, callback=callback, \n                                  options={'maxiter': self.budget, 'disp': False})\n            except StopIteration:\n                pass\n\n            # Adjust bounds based on the found solution for potential further refinement\n            if result.success:\n                center = result.x\n                bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                # Introduce a dynamic adjustment factor for bounds refinement\n                adjust_factor = 0.15 * (1 + np.random.uniform(-0.05, 0.05))\n                new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                       np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                \n                # Re-run optimization with updated bounds if budget allows\n                try:\n                    result = minimize(func, result.x, method='Nelder-Mead', \n                                      bounds=new_bounds.T, callback=callback, \n                                      options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                except StopIteration:\n                    pass\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Refine the exploration strategy by utilizing median-based initial guess selection for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.6532479311912498, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.653 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "13f66315-d622-4a3e-a94f-1ed36a1e538f", "metadata": {"aucs": [0.6438712091152243, 0.6474184741645522, 0.6684541102939727], "final_y": [1.41406317735703e-05, 1.2836973986629791e-05, 7.783781755675553e-06]}, "mutation_prompt": null}
{"id": "77eb9194-f011-4d66-8845-f2dd85f59829", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = 0.15  # Added local sampling for bounds adjustment\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Hybridizes dynamic scaling and local sampling to enhance exploration and convergence precision.", "configspace": "", "generation": 5, "fitness": 0.7308380649037857, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.731 with standard deviation 0.097. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "29d5c3ec-b137-478b-8365-a5d81cb4b8f8", "metadata": {"aucs": [0.8670671708637838, 0.6502248595045959, 0.6752221643429772], "final_y": [6.371458326454967e-08, 9.869784542781504e-06, 6.2921376293054675e-06]}, "mutation_prompt": null}
{"id": "162746e4-d1a9-46f5-934a-3fbfb2077d9e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.9, 1.1)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhanced local search efficiency by introducing an adaptive learning rate for dynamic scaling factor adjustment.", "configspace": "", "generation": 5, "fitness": 0.6532479311912498, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.653 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "29d5c3ec-b137-478b-8365-a5d81cb4b8f8", "metadata": {"aucs": [0.6438712091152243, 0.6474184741645522, 0.6684541102939727], "final_y": [1.41406317735703e-05, 1.2836973986629791e-05, 7.783781755675553e-06]}, "mutation_prompt": null}
{"id": "3d4c83d9-3da2-4523-b569-f162a53c38b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.9, 1.1)  # Dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x, convergence=0):  # Modified line\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduced a dynamic scaling factor to the initial guess to enhance exploration capabilities and adjusted the callback mechanism to be more efficient.", "configspace": "", "generation": 5, "fitness": 0.7192801153994095, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.719 with standard deviation 0.087. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "29d5c3ec-b137-478b-8365-a5d81cb4b8f8", "metadata": {"aucs": [0.842351451683379, 0.6590353411753859, 0.6564535533394638], "final_y": [9.64732371306254e-08, 8.991616654016494e-06, 1.0118149714019531e-05]}, "mutation_prompt": null}
{"id": "59344423-b0d5-4bc1-95e3-22a734f37e80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Evaluate initial guesses and sort by performance\n        evaluated_guesses = [(guess, func(guess)) for guess in initial_guesses]\n        evaluated_guesses.sort(key=lambda x: x[1])\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            # Use the best initial guess based on initial evaluations\n            for initial_guess, _ in evaluated_guesses:\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Introduce a dynamic adjustment factor with an adaptive cooling schedule for bounds refinement\n                    adjust_factor = 0.15 * (1 + np.random.uniform(-0.05, 0.05)) * (1 - self.func_evaluations / self.budget)\n                    new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Enhance AdaptiveNelderMead by incorporating an adaptive cooling schedule in the adjustment factor for dynamic bounds refinement.", "configspace": "", "generation": 5, "fitness": 0.471440385559019, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.471 with standard deviation 0.306. And the mean value of best solutions found was 5.495 (0. is the best) with standard deviation 7.772.", "error": "", "parent_id": "13f66315-d622-4a3e-a94f-1ed36a1e538f", "metadata": {"aucs": [0.6933438523390929, 0.03933910151291631, 0.6816382028250478], "final_y": [3.9327830471255174e-06, 16.486016487484385, 5.232059190644538e-06]}, "mutation_prompt": null}
{"id": "a1f87ae8-2bfe-4e34-9a46-6bf017d1b638", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 5  # Adjusted for more diverse initial sampling\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Evaluate initial guesses and sort by performance\n        evaluated_guesses = [(guess, func(guess)) for guess in initial_guesses]\n        evaluated_guesses.sort(key=lambda x: x[1])\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            # Use the top two initial guesses based on initial evaluations\n            for initial_guess, _ in evaluated_guesses[:2]:  # Limited to top two for refined exploration\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Introduce a dynamic adjustment factor for bounds refinement\n                    adjust_factor = 0.1 * (1 + np.random.uniform(-0.05, 0.05))  # Slight reduction for precision\n                    new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Refine exploration by introducing a dynamic elite pool and adaptive boundaries to enhance convergence efficiency.", "configspace": "", "generation": 5, "fitness": 0.65477894642343, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.655 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "13f66315-d622-4a3e-a94f-1ed36a1e538f", "metadata": {"aucs": [0.6683757263494496, 0.6504826570563642, 0.6454784558644762], "final_y": [7.607649523615132e-06, 1.1629698113873267e-05, 1.2686459091313368e-05]}, "mutation_prompt": null}
{"id": "8e44b024-5a20-457a-a608-21cdeddd12b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Evaluate initial guesses and sort by performance\n        evaluated_guesses = [(guess, func(guess)) for guess in initial_guesses]\n        evaluated_guesses.sort(key=lambda x: x[1])\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            attempt = 0  # Counter for restart attempts\n            while self.func_evaluations < self.budget and attempt < 3:  # Limit number of restarts\n                for initial_guess, _ in evaluated_guesses:\n                    try:\n                        result = minimize(func, initial_guess, method='Nelder-Mead', \n                                          bounds=bounds.T, callback=callback, \n                                          options={'maxiter': self.budget, 'disp': False})\n                    except StopIteration:\n                        break\n\n                    # Adjust bounds based on the found solution for potential further refinement\n                    if result.success:\n                        center = result.x\n                        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                        # Introduce a dynamic adjustment factor for bounds refinement\n                        adjust_factor = 0.15 * (1 + np.random.uniform(-0.05, 0.05))\n                        new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                               np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n\n                        # Re-run optimization with updated bounds if budget allows\n                        try:\n                            result = minimize(func, result.x, method='Nelder-Mead', \n                                              bounds=new_bounds.T, callback=callback, \n                                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                        except StopIteration:\n                            break\n                        \n                        # Restart with new guesses if converged locally\n                        if result.success:\n                            # Reinitialize guesses if stuck in local optima\n                            evaluated_guesses = [(np.random.uniform(bounds[0], bounds[1], self.dim), float('inf')) for _ in range(num_initial_guesses)]\n                            attempt += 1\n\n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Implement adaptive restart strategy by reinitializing with new random guesses when local convergence is detected.", "configspace": "", "generation": 5, "fitness": 0.6856755373646869, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.686 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "13f66315-d622-4a3e-a94f-1ed36a1e538f", "metadata": {"aucs": [0.6779359434843819, 0.697452465784631, 0.6816382028250478], "final_y": [5.912903395812199e-06, 4.268388777224933e-06, 5.232059190644538e-06]}, "mutation_prompt": null}
{"id": "abee955a-89d0-4ff3-9037-60dd23d8f5b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize bounds \n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        \n        # Multiple initial guesses to enhance initial exploration\n        num_initial_guesses = 3\n        initial_guesses = [np.random.uniform(bounds[0], bounds[1], self.dim) for _ in range(num_initial_guesses)]\n        \n        # Evaluate initial guesses and sort by performance\n        evaluated_guesses = [(guess, func(guess)) for guess in initial_guesses]\n        evaluated_guesses.sort(key=lambda x: x[1])\n        \n        # Introduce perturbed elite solutions for initial guesses\n        best_guess = evaluated_guesses[0][0]\n        perturbed_guesses = [best_guess + np.random.uniform(-0.05, 0.05, self.dim) for _ in range(num_initial_guesses)]\n        \n        # Define the optimization callback to count function evaluations\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            result = None\n            # Use the best initial guess based on initial evaluations\n            for initial_guess, _ in zip(perturbed_guesses, evaluated_guesses):\n                try:\n                    result = minimize(func, initial_guess, method='Nelder-Mead', \n                                      bounds=bounds.T, callback=callback, \n                                      options={'maxiter': self.budget, 'disp': False})\n                except StopIteration:\n                    break\n\n                # Adjust bounds based on the found solution for potential further refinement\n                if result.success:\n                    center = result.x\n                    bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n                    # Introduce a dynamic adjustment factor for bounds refinement\n                    adjust_factor = 0.15 * (1 + np.random.uniform(-0.05, 0.05))\n                    new_bounds = np.array([np.maximum(center - adjust_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                           np.minimum(center + adjust_factor * (bounds[1] - bounds[0]), bounds[1])])\n                    \n                    # Re-run optimization with updated bounds if budget allows\n                    try:\n                        result = minimize(func, result.x, method='Nelder-Mead', \n                                          bounds=new_bounds.T, callback=callback, \n                                          options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                    except StopIteration:\n                        break\n            \n        except StopIteration:\n            pass\n\n        return result.x if result else initial_guesses[0]  # Return initial if no result was obtained", "name": "AdaptiveNelderMead", "description": "Enhance adaptive exploration by diversifying initial guesses with perturbed elite solutions.", "configspace": "", "generation": 5, "fitness": 0.6682518186993764, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.668 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "13f66315-d622-4a3e-a94f-1ed36a1e538f", "metadata": {"aucs": [0.6794717736884412, 0.660000284393317, 0.6652833980163713], "final_y": [5.8427336308503205e-06, 9.500311332906286e-06, 7.2106943418879395e-06]}, "mutation_prompt": null}
{"id": "219f9637-71e6-4275-ab73-1c377ae730dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        \n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            if self.func_evaluations < self.budget * 0.5:  # Added dynamic restart strategy\n                result = minimize(func, result.x, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.3, 0.8, self.dim)  # Adjusted weight range\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Integrates adaptive dynamic restart strategy and customized reflection to optimize exploration and convergence.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'AdaptiveNelderMead' object has no attribute 'func_evaluations'\").", "error": "AttributeError(\"'AdaptiveNelderMead' object has no attribute 'func_evaluations'\")", "parent_id": "2ec0012e-9828-4bc2-893f-f7e257b7115c", "metadata": {}, "mutation_prompt": null}
{"id": "4eb4b9d2-9828-4f4f-8f5f-dc401ab91ad8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15) + (self.func_evaluations / self.budget) * 0.1  # Adaptive scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 0.9, self.dim)  # Adjusted weight range\n            new_guess = np.random.uniform(new_bounds[0], new_bounds[1], self.dim) * weights  # Adjusted to use new_bounds\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduce adaptive scaling factor based on iterations and enhance initial guess with strategic boundary adjustments.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'AdaptiveNelderMead' object has no attribute 'func_evaluations'\").", "error": "AttributeError(\"'AdaptiveNelderMead' object has no attribute 'func_evaluations'\")", "parent_id": "2ec0012e-9828-4bc2-893f-f7e257b7115c", "metadata": {}, "mutation_prompt": null}
{"id": "14712468-a1e7-4e14-9754-603e5cf59eb9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (np.sqrt(self.func_evaluations) / self.budget))  # Changed tolerance adaptation\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = 0.10  # Reduced local sampling factor for tighter bounds\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Integrate an adaptive convergence threshold and dynamic local refinement to enhance precision within budget constraints.", "configspace": "", "generation": 6, "fitness": 0.31052617740813593, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.311 with standard deviation 0.389. And the mean value of best solutions found was 12.030 (0. is the best) with standard deviation 8.507.", "error": "", "parent_id": "77eb9194-f011-4d66-8845-f2dd85f59829", "metadata": {"aucs": [0.8600008403527917, 0.03564262778760552, 0.03593506408401059], "final_y": [6.688181723069891e-08, 18.04562371212558, 18.04562371212558]}, "mutation_prompt": null}
{"id": "36fb0821-51ca-4701-8c94-6fb3a4486c72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted scaling factor range\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        initial_guess += np.random.uniform(-0.02, 0.02, self.dim)  # Introduce random perturbation\n\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 0.9, self.dim)  # Adjusted weight range\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhance exploration and convergence by introducing random perturbations to initial guess.", "configspace": "", "generation": 6, "fitness": 0.8150756534051536, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.005. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ec0012e-9828-4bc2-893f-f7e257b7115c", "metadata": {"aucs": [0.8209800959402883, 0.8086450071586931, 0.8156018571164794], "final_y": [1.8921026901918223e-07, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "4e2696cb-f154-4e92-a128-be70f96e6322", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.8, 1.2)  # Adjusted dynamic scaling factor range\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = 0.2  # Adjusted local sampling factor for more exploration\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Added stochastic perturbation\n        adjusted_center = center + perturbation\n\n        try:\n            result = minimize(func, adjusted_center, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.2, 1.0, self.dim)  # Adjusted weights for final random guess\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Integrate stochastic perturbations into dynamic scaling and local sampling for enhanced exploration and convergence.", "configspace": "", "generation": 6, "fitness": 0.29552618915450224, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.296 with standard deviation 0.396. And the mean value of best solutions found was 18.911 (0. is the best) with standard deviation 13.372.", "error": "", "parent_id": "77eb9194-f011-4d66-8845-f2dd85f59829", "metadata": {"aucs": [0.8557208091052586, 0.015358061670032175, 0.015499696688215936], "final_y": [8.501509387355421e-08, 28.367105276157496, 28.367105276157496]}, "mutation_prompt": null}
{"id": "286f9917-3e1d-445c-9011-6c20e82cf2d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted scaling factor range\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Added adaptive perturbation\n            result = minimize(func, result.x + perturbation, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 0.9, self.dim)  # Adjusted weight range\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Incorporate adaptive perturbation to improve convergence speed and exploration balance.", "configspace": "", "generation": 6, "fitness": 0.2967104081694903, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.297 with standard deviation 0.405. And the mean value of best solutions found was 21.045 (0. is the best) with standard deviation 14.881.", "error": "", "parent_id": "2ec0012e-9828-4bc2-893f-f7e257b7115c", "metadata": {"aucs": [0.8688868695664516, 0.01057454309440864, 0.010669811847610622], "final_y": [6.812998115325779e-08, 31.56698242455676, 31.56698242455676]}, "mutation_prompt": null}
{"id": "94d12565-ea5c-4aaa-99f6-54075bf22313", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        self.success_count = 0  # New success count tracker\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                self.success_count += 1  # Increment success count if successful\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = 0.15  # Added local sampling for bounds adjustment\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces adaptive scaling factor adjustment based on success rate to improve convergence precision.", "configspace": "", "generation": 6, "fitness": 0.8301048512224037, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "77eb9194-f011-4d66-8845-f2dd85f59829", "metadata": {"aucs": [0.8231115400293811, 0.8449065412573435, 0.8222964723804864], "final_y": [1.3988302479745017e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "1945463c-c103-4604-884b-9494917b634a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by introducing adaptive scaling in local sampling step.", "configspace": "", "generation": 6, "fitness": 0.8240605995661466, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "77eb9194-f011-4d66-8845-f2dd85f59829", "metadata": {"aucs": [0.8479349344232674, 0.8086450071586931, 0.8156018571164794], "final_y": [9.423243348009828e-08, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "3312e38a-f697-4daf-b04f-556067cc91b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.linspace(0.85, 1.15, self.budget)  # Introduced time-varying scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor[0]\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = 0.15  # Added local sampling for bounds adjustment\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by introducing time-varying scaling factor for initial guesses.", "configspace": "", "generation": 6, "fitness": 0.07890736947572563, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.079 with standard deviation 0.096. And the mean value of best solutions found was 20.944 (0. is the best) with standard deviation 14.608.", "error": "", "parent_id": "77eb9194-f011-4d66-8845-f2dd85f59829", "metadata": {"aucs": [0.21446006790809902, 0.009451636973151434, 0.012810403545926441], "final_y": [0.3276013310005213, 32.3895286871302, 30.116039548677826]}, "mutation_prompt": null}
{"id": "a132bac8-f61e-4c7c-958d-0d6caafab16b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ImprovedNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        self.learning_rate = 0.1\n\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            if self.func_evaluations % 10 == 0:  # Adjust learning rate every 10 evaluations\n                self.learning_rate *= 0.95\n        \n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'adaptive': True, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        bounds = np.clip(bounds, func.bounds.lb, func.bounds.ub)\n        new_bounds = np.array([np.maximum(center - 0.1 * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + 0.1 * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'adaptive': True, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 0.9, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'adaptive': True, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "ImprovedNelderMead", "description": "Incorporate adaptive learning rate based on feedback from function evaluations to enhance convergence speed and precision.", "configspace": "", "generation": 6, "fitness": 0.5901239859268911, "feedback": "The algorithm ImprovedNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.590 with standard deviation 0.364. And the mean value of best solutions found was 2.468 (0. is the best) with standard deviation 3.490.", "error": "", "parent_id": "2ec0012e-9828-4bc2-893f-f7e257b7115c", "metadata": {"aucs": [0.8632641833366377, 0.8313189724886378, 0.07578880195539772], "final_y": [5.3730884990318036e-08, 9.567428609131606e-08, 7.403839118408355]}, "mutation_prompt": null}
{"id": "cf74c0e8-9c61-4541-a2ff-13da4ff58a38", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 + (1e-3 - 1e-6) * (self.func_evaluations / self.budget)  # Adjusted dynamic tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces adaptive dynamic tolerance based on remaining budget for improved convergence precision.", "configspace": "", "generation": 7, "fitness": 0.30593113418616896, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.306 with standard deviation 0.417. And the mean value of best solutions found was 20.835 (0. is the best) with standard deviation 14.762.", "error": "", "parent_id": "1945463c-c103-4604-884b-9494917b634a", "metadata": {"aucs": [0.8955313620394291, 0.009451636973151434, 0.012810403545926441], "final_y": [3.027384587477321e-08, 32.3895286871302, 30.116039548677826]}, "mutation_prompt": null}
{"id": "1d9e9df4-17b1-40be-9514-a39b9f868103", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.budget - self.func_evaluations) / self.budget)  # Adjusted dynamic tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by introducing adaptive scaling and dynamic tolerance adjustment based on remaining budget.", "configspace": "", "generation": 7, "fitness": 0.8266064673297141, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1945463c-c103-4604-884b-9494917b634a", "metadata": {"aucs": [0.8126163883513122, 0.8449065412573435, 0.8222964723804864], "final_y": [1.8777729205467746e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "405d5fa6-ff05-4e3a-a030-4e4ed327c391", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        self.success_count = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n\n            if result.success:\n                self.success_count += 1\n                return result.x\n                \n        except StopIteration:\n            pass\n        \n        center = result.x\n        local_sampling_factor = 0.15 \n        new_center = center + 0.05 * (bounds[1] - bounds[0])  \n        new_bounds = np.array([np.maximum(new_center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(new_center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces adaptive search space re-centering based on convergence rate to enhance precision.", "configspace": "", "generation": 7, "fitness": 0.8038015742926827, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "94d12565-ea5c-4aaa-99f6-54075bf22313", "metadata": {"aucs": [0.789352910255466, 0.8289021809529531, 0.7931496316696293], "final_y": [2.3730396189463817e-07, 1.5246438632067654e-07, 2.588720419445304e-07]}, "mutation_prompt": null}
{"id": "9f8f829b-cf80-47a1-9c86-402eccdb573c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0] + 0.1 * (bounds[1] - bounds[0]),  # Modified initial guess distribution\n                                          bounds[1] - 0.1 * (bounds[1] - bounds[0]), self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by introducing adaptive scaling in local sampling step and better initial guess distribution.", "configspace": "", "generation": 7, "fitness": 0.8310678053186967, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1945463c-c103-4604-884b-9494917b634a", "metadata": {"aucs": [0.8260004023182606, 0.8449065412573435, 0.8222964723804864], "final_y": [1.3577808104691926e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "d45e3a07-f682-4530-8f51-2289b19ce871", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by introducing adaptive scaling in local sampling step with dynamic initial scaling.", "configspace": "", "generation": 7, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1945463c-c103-4604-884b-9494917b634a", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "dd5e5b8d-c86c-4f7e-9bfd-5a4336028cbb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2) \n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        # Introduce gradient-based perturbations\n        gradient_perturbation = np.random.normal(0, 0.01, self.dim)  \n        perturbed_center = center + gradient_perturbation\n        perturbed_center = np.clip(perturbed_center, bounds[0], bounds[1])\n\n        try:\n            result = minimize(func, perturbed_center, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces gradient-based perturbations to refine convergence precision and efficiency.", "configspace": "", "generation": 7, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1945463c-c103-4604-884b-9494917b634a", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "4475eb17-e9cd-42c8-a28b-2e3a66d4c9ae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget) * np.random.uniform(0.9, 1.1))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2) * np.random.uniform(0.9, 1.1)\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces stochastic perturbation in the dynamic tolerance and sampling factor to enhance exploration and exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.8241726136893764, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1945463c-c103-4604-884b-9494917b634a", "metadata": {"aucs": [0.8482709767929567, 0.8086450071586931, 0.8156018571164794], "final_y": [8.161377635860433e-08, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "1747e67a-7a9b-4f51-97c3-e15584d2a528", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        self.success_count = 0\n        \n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                self.success_count += 1\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = 0.1  # Slightly decreased local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6 * (1 + self.success_count / (self.func_evaluations + 1)),  # Adjusted tolerance\n                              callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6 * (1 + self.success_count / (self.func_evaluations + 1)),  # Adjusted tolerance\n                                  callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "AdaptiveNelderMead introduces dynamic scaling factor and learning rate adjustment based on success rate to enhance convergence speed and precision.", "configspace": "", "generation": 7, "fitness": 0.8241726136893764, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "94d12565-ea5c-4aaa-99f6-54075bf22313", "metadata": {"aucs": [0.8482709767929567, 0.8086450071586931, 0.8156018571164794], "final_y": [8.161377635860433e-08, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "8fa4e9b3-70f9-4266-bd5e-082630266b38", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            scaling_factor = np.random.uniform(0.85, 1.15) * (1 + (self.budget - self.func_evaluations) / self.budget)  # Dynamic scaling based on remaining budget\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights * scaling_factor  # Apply dynamic scaling\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces dynamic scaling based on the ratio of remaining budget to improve convergence precision.", "configspace": "", "generation": 7, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1945463c-c103-4604-884b-9494917b634a", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "421dac57-01be-4354-9a3b-c39ec2335bfd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        self.success_count = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            # Changed line: dynamic_tol calculation refined for improved convergence\n            dynamic_tol = 1e-6 * (1 + 0.1 * (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                self.success_count += 1\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = 0.15\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces adaptive scaling factor adjustment based on success rate to improve convergence precision by refining the dynamic tolerance calculation.", "configspace": "", "generation": 7, "fitness": 0.5972975040479768, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.597 with standard deviation 0.342. And the mean value of best solutions found was 1.063 (0. is the best) with standard deviation 1.504.", "error": "", "parent_id": "94d12565-ea5c-4aaa-99f6-54075bf22313", "metadata": {"aucs": [0.8210433991393093, 0.857295428468899, 0.11355368453572201], "final_y": [1.1417958699011266e-07, 8.475471343045072e-08, 3.190400567669599]}, "mutation_prompt": null}
{"id": "a8f96b69-7c5d-4b6a-aca4-88676c51a2b5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0] + 0.1 * (bounds[1] - bounds[0]),  # Modified initial guess distribution\n                                          bounds[1] - 0.1 * (bounds[1] - bounds[0]), self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            scaling_factor = np.random.uniform(0.9, 1.1)  # Mid-optimization scaling adjustment\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Incorporates a mid-optimization dynamic adjustment to the scaling factor for enhanced convergence in smooth landscapes.", "configspace": "", "generation": 8, "fitness": 0.8127979608795982, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9f8f829b-cf80-47a1-9c86-402eccdb573c", "metadata": {"aucs": [0.8112333797235273, 0.822685081891551, 0.8044754210237162], "final_y": [2.3469417410890723e-07, 1.6580498752573241e-07, 2.4577039348555296e-07]}, "mutation_prompt": null}
{"id": "e7a3131a-4451-45c4-a2b2-f6d8d44aafe5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, center, method='Nelder-Mead',  # Changed from result.x to center\n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by introducing dynamic scaling and local sampling improvements with a refined starting point.", "configspace": "", "generation": 8, "fitness": 0.8083314877794859, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d45e3a07-f682-4530-8f51-2289b19ce871", "metadata": {"aucs": [0.789352910255466, 0.8309207616444737, 0.8047207914385178], "final_y": [2.3730396189463817e-07, 1.6024786863345178e-07, 2.0831744691353745e-07]}, "mutation_prompt": null}
{"id": "55146504-d548-4477-9dd6-76e226121875", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))  # Added dynamic_tol for second minimize\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduced a dynamic tol factor for the second minimize call to potentially enhance convergence precision.", "configspace": "", "generation": 8, "fitness": 0.8127979608795982, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d45e3a07-f682-4530-8f51-2289b19ce871", "metadata": {"aucs": [0.8112333797235273, 0.822685081891551, 0.8044754210237162], "final_y": [2.3469417410890723e-07, 1.6580498752573241e-07, 2.4577039348555296e-07]}, "mutation_prompt": null}
{"id": "06bf56b8-2b18-4cd7-94b9-5a3c6e12e4ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.8, 1.2)  # Adjusted dynamic scaling factor range\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + np.exp(self.func_evaluations / self.budget))  # Modified dynamic tolerance calculation\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.15, 0.25)  # Modified adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces a dynamic adaptive scaling strategy by varying the initial guess and local sampling factors based on iteration progress.", "configspace": "", "generation": 8, "fitness": 0.808645439477712, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d45e3a07-f682-4530-8f51-2289b19ce871", "metadata": {"aucs": [0.798775815517869, 0.822685081891551, 0.8044754210237162], "final_y": [2.8009886215067406e-07, 1.6580498752573241e-07, 2.4577039348555296e-07]}, "mutation_prompt": null}
{"id": "0656868a-50b5-410d-9bde-e6d4bc565680", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim) * np.random.uniform(0.9, 1.1)  # Adaptive weight adjustment\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n            if not result.success:  # Stochastic restart\n                new_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n                try:\n                    result = minimize(func, new_guess, method='Nelder-Mead', \n                                      tol=1e-6, callback=callback, \n                                      options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                except StopIteration:\n                    pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces stochastic restarting and adaptive weight adjustments to improve convergence and solution diversity.", "configspace": "", "generation": 8, "fitness": 0.8288059363914035, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d45e3a07-f682-4530-8f51-2289b19ce871", "metadata": {"aucs": [0.8152135690791962, 0.8226189960457313, 0.8485852440492829], "final_y": [1.7586309408220226e-07, 1.4274939108465477e-07, 8.24096925619194e-08]}, "mutation_prompt": null}
{"id": "97ca4579-30ed-4c57-b326-52848a1bf138", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            if np.random.rand() < 0.1:  # Randomly re-initialize with a small probability\n                initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = initial_guess * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces a random re-initialization phase to enhance exploration and escape local minima.", "configspace": "", "generation": 8, "fitness": 0.8348684355996858, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d45e3a07-f682-4530-8f51-2289b19ce871", "metadata": {"aucs": [0.8334010667040431, 0.8226189960457313, 0.8485852440492829], "final_y": [1.1257198573194978e-07, 1.4274939108465477e-07, 8.24096925619194e-08]}, "mutation_prompt": null}
{"id": "4d3379f6-182a-4ea9-9f82-0291c9d733ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Improves convergence by introducing an adaptive tolerance adjustment based on progress rate, enhancing precision.", "configspace": "", "generation": 8, "fitness": 0.8324015474710915, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d45e3a07-f682-4530-8f51-2289b19ce871", "metadata": {"aucs": [0.8260004023182606, 0.8226189960457313, 0.8485852440492829], "final_y": [1.3577808104691926e-07, 1.4274939108465477e-07, 8.24096925619194e-08]}, "mutation_prompt": null}
{"id": "631302cd-4add-4217-b68a-e3fe7676b935", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + np.log1p(self.func_evaluations / self.budget))  # Improved dynamic tolerance calculation\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by introducing adaptive scaling in local sampling step with dynamic initial scaling and improved dynamic tolerance calculation.", "configspace": "", "generation": 8, "fitness": 0.8268739432113078, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d45e3a07-f682-4530-8f51-2289b19ce871", "metadata": {"aucs": [0.8094175895389091, 0.8226189960457313, 0.8485852440492829], "final_y": [2.1072043273561537e-07, 1.4274939108465477e-07, 8.24096925619194e-08]}, "mutation_prompt": null}
{"id": "0b9c544e-5ac3-40a6-83e7-4794d220d4d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0] + 0.1 * (bounds[1] - bounds[0]),\n                                          bounds[1] - 0.1 * (bounds[1] - bounds[0]), self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n            # Adjust step size dynamically\n            nonlocal dynamic_step_size\n            dynamic_step_size = 1e-6 * (1 + 0.5 * (self.func_evaluations / self.budget))\n\n        dynamic_step_size = 1e-6\n        try:\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_step_size, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.05, 0.15)  # Reduced range for more precise local search\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=dynamic_step_size, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=dynamic_step_size, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Improves convergence by introducing a dynamic step-size adjustment based on evaluation progress and adaptive local resampling.", "configspace": "", "generation": 8, "fitness": 0.8324015474710915, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9f8f829b-cf80-47a1-9c86-402eccdb573c", "metadata": {"aucs": [0.8260004023182606, 0.8226189960457313, 0.8485852440492829], "final_y": [1.3577808104691926e-07, 1.4274939108465477e-07, 8.24096925619194e-08]}, "mutation_prompt": null}
{"id": "b70bdea4-c1ba-44d2-b9f5-8c6bf020f4fa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0] + 0.1 * (bounds[1] - bounds[0]),  # Modified initial guess distribution\n                                          bounds[1] - 0.1 * (bounds[1] - bounds[0]), self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)) * np.log(2 + self.func_evaluations)  # Modified dynamic tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            perturbation = np.random.uniform(-0.05, 0.05, self.dim)  # Added weighted random perturbation\n            new_guess += perturbation\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces a weighted random perturbation to enhance exploration around the solution and dynamic tolerance scaling based on convergence speed.", "configspace": "", "generation": 8, "fitness": 0.08022908284423831, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.080 with standard deviation 0.095. And the mean value of best solutions found was 20.187 (0. is the best) with standard deviation 14.042.", "error": "", "parent_id": "9f8f829b-cf80-47a1-9c86-402eccdb573c", "metadata": {"aucs": [0.2151939009113123, 0.012700088632850104, 0.012793258988552547], "final_y": [0.3276013310005236, 30.116039548677826, 30.116039548677826]}, "mutation_prompt": null}
{"id": "f15936a7-9b88-4ec7-8f4f-5332403e1f63", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        # Modified line:\n        local_sampling_factor = np.random.uniform(0.05, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces an adaptive scaling factor based on function evaluation count to enhance convergence rate.", "configspace": "", "generation": 9, "fitness": 0.30593113418616896, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.306 with standard deviation 0.417. And the mean value of best solutions found was 20.835 (0. is the best) with standard deviation 14.762.", "error": "", "parent_id": "4d3379f6-182a-4ea9-9f82-0291c9d733ff", "metadata": {"aucs": [0.8955313620394291, 0.009451636973151434, 0.012810403545926441], "final_y": [3.027384587477321e-08, 32.3895286871302, 30.116039548677826]}, "mutation_prompt": null}
{"id": "eb38470e-e573-409c-b85d-be649c4d362c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        if self.func_evaluations < self.budget // 2:  # Periodic reseeding\n            initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances exploration by introducing periodic random reseeding of initial guesses during the optimization process.", "configspace": "", "generation": 9, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4d3379f6-182a-4ea9-9f82-0291c9d733ff", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "437dd079-a514-439a-8a90-2ff8735a26a7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhancing the algorithm by incorporating a dynamic scaling factor update based on optimization progress, improving adaptiveness and precision.", "configspace": "", "generation": 9, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4d3379f6-182a-4ea9-9f82-0291c9d733ff", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "0771dac4-9d44-4bae-8bf9-da5093528fc7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.9, 1.1)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances robustness by adjusting initial guess scaling, improving exploration and convergence.", "configspace": "", "generation": 9, "fitness": 0.7985572994042668, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.023. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4d3379f6-182a-4ea9-9f82-0291c9d733ff", "metadata": {"aucs": [0.773620085590218, 0.8289021809529531, 0.7931496316696293], "final_y": [2.9007768660518967e-07, 1.5246438632067654e-07, 2.588720419445304e-07]}, "mutation_prompt": null}
{"id": "6df86372-201b-4685-84e7-467ec2206f0c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        # Changed line to dynamically adjust the local_sampling_factor\n        local_sampling_factor = np.random.uniform(0.1, 0.2) * (1 + self.func_evaluations / self.budget)\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Integrates a dynamic exploration boost by adjusting the local sampling factor based on budget utilization to improve search efficiency.", "configspace": "", "generation": 9, "fitness": 0.8386806336845264, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4d3379f6-182a-4ea9-9f82-0291c9d733ff", "metadata": {"aucs": [0.8488388874157493, 0.8449065412573435, 0.8222964723804864], "final_y": [7.412187497081724e-08, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "be506913-0214-4b13-ae9c-49ac02059fc6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.9, 1.2)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.3)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            if np.random.rand() < 0.1:  # Randomly re-initialize with a small probability\n                initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = initial_guess * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances exploration and convergence by incorporating a dynamic scaling factor and adaptive local search strategy with random perturbations.", "configspace": "", "generation": 9, "fitness": 0.8181597100278791, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97ca4579-30ed-4c57-b326-52848a1bf138", "metadata": {"aucs": [0.8302322658084648, 0.8086450071586931, 0.8156018571164794], "final_y": [1.2629598465007635e-07, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "303aa9ca-31d2-4837-810d-301c46cfda7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n\n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n\n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by introducing a dynamic initialization based on the progress rate, improving exploration and exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4d3379f6-182a-4ea9-9f82-0291c9d733ff", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "85ee07e4-bdb5-4cd4-8f3a-232bc44114fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2) * (1 + np.random.rand())  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            if np.random.rand() < 0.1:  # Randomly re-initialize with a small probability\n                initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = initial_guess * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces dynamic scaling of local sampling factor for enhanced adaptability during exploration and exploitation phases.", "configspace": "", "generation": 9, "fitness": 0.8181597100278791, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97ca4579-30ed-4c57-b326-52848a1bf138", "metadata": {"aucs": [0.8302322658084648, 0.8086450071586931, 0.8156018571164794], "final_y": [1.2629598465007635e-07, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "4949dd17-5188-4b24-b1fd-da04620a9473", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget))\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            dynamic_reinit_prob = 0.1 * (1 + (self.func_evaluations / self.budget))  # Dynamically adjusted re-init probability\n            if np.random.rand() < dynamic_reinit_prob:  # Randomly re-initialize with a dynamically adjusted probability\n                initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = initial_guess * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Incorporate a dynamically adjusted exploration probability to improve exploration without compromising convergence.", "configspace": "", "generation": 9, "fitness": 0.8241726136893764, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97ca4579-30ed-4c57-b326-52848a1bf138", "metadata": {"aucs": [0.8482709767929567, 0.8086450071586931, 0.8156018571164794], "final_y": [8.161377635860433e-08, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "9810b07f-6bdb-40a6-9449-33be2b1aab7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances initial guess by utilizing dynamic scaling based on budget, improving convergence speed.", "configspace": "", "generation": 9, "fitness": 0.8241726136893764, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4d3379f6-182a-4ea9-9f82-0291c9d733ff", "metadata": {"aucs": [0.8482709767929567, 0.8086450071586931, 0.8156018571164794], "final_y": [8.161377635860433e-08, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "3572920f-3e56-4610-b7db-f22791338d7f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        if self.func_evaluations < self.budget // 2:\n            initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        mutation_strength = 0.05  # Added adaptive mutation strength\n        local_sampling_factor = np.random.uniform(0.1, mutation_strength)  # Updated adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Integrates adaptive mutation with periodic reseeding to enhance exploration and convergence accuracy.", "configspace": "", "generation": 10, "fitness": 0.5666312957261003, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.567 with standard deviation 0.387. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "eb38470e-e573-409c-b85d-be649c4d362c", "metadata": {"aucs": [0.8573710451896017, 0.8221074496871502, 0.020415392301548763], "final_y": [6.714330897545795e-08, 1.3097918032020838e-07, 25.44556613682321]}, "mutation_prompt": null}
{"id": "0c2d5486-2abf-4daa-8025-5c1a7243a1ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.9, 1.1)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        # Changed line to dynamically adjust the local_sampling_factor\n        local_sampling_factor = np.random.uniform(0.1, 0.2) * (1 + self.func_evaluations / self.budget)\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances convergence by adaptive scaling of initial guesses and dynamic tolerance adjustments based on function evaluation progress.", "configspace": "", "generation": 10, "fitness": 0.8374722978865755, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6df86372-201b-4685-84e7-467ec2206f0c", "metadata": {"aucs": [0.8452138800218969, 0.8449065412573435, 0.8222964723804864], "final_y": [8.544391296052584e-08, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "8add676d-c241-4e75-8c2c-fc82555404a6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.8, 1.2)  # Modified dynamic scaling factor range\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**1.5)  # Adjusted adaptive tolerance calculation\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        if self.func_evaluations < self.budget // 3:  # More frequent reseeding\n            initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        local_sampling_factor = np.random.uniform(0.1, 0.3)  # Expanded adaptive local sampling factor range\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces dynamic re-scaling of the initial guess and adaptive reseeding based on convergence rate to enhance exploration and exploit smooth landscapes.", "configspace": "", "generation": 10, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eb38470e-e573-409c-b85d-be649c4d362c", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "20d0806e-0bbc-46ef-b1c5-a1501cfac826", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.8, 1.2)  # Extended dynamic scaling range\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**1.5)  # Modified adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.05, 0.15)  # Adjusted local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances exploration with adaptive multi-start strategies and dynamic scaling to improve convergence efficiency.", "configspace": "", "generation": 10, "fitness": 0.8361859203935579, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eb38470e-e573-409c-b85d-be649c4d362c", "metadata": {"aucs": [0.8413547475428436, 0.8449065412573435, 0.8222964723804864], "final_y": [9.410989618616976e-08, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "d4bc3398-dbbc-42db-a21c-9923db96e04e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        if self.func_evaluations < self.budget // 2:\n            initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n\n        local_sampling_factor = np.random.uniform(0.1, 0.2)\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.5, self.dim)  # Change to improve initial guess variance\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces an enhanced reseeding strategy with improved initial guess variance to boost exploration and convergence.", "configspace": "", "generation": 10, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eb38470e-e573-409c-b85d-be649c4d362c", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "01546cad-8425-4b54-a0ef-63d0a4608a5e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2) * (1 + self.func_evaluations / self.budget)\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        # Random restart mechanism if stagnation detected\n        if not result.success and self.func_evaluations < self.budget:\n            initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n            try:\n                result = minimize(func, initial_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Incorporates a random restart mechanism activated upon stagnation, improving exploration without exceeding the budget.", "configspace": "", "generation": 10, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6df86372-201b-4685-84e7-467ec2206f0c", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "04b4402c-46f6-4060-ba23-7552eb9c17ef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        if self.func_evaluations < self.budget // 2:  # Periodic reseeding\n            initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim) + (0.05 * (self.func_evaluations / self.budget))  # Adjustment for line change\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Introduces a budget-based adaptive scaling factor to improve convergence efficiency by fine-tuning the exploration and exploitation balance.", "configspace": "", "generation": 10, "fitness": 0.8241726136893764, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eb38470e-e573-409c-b85d-be649c4d362c", "metadata": {"aucs": [0.8482709767929567, 0.8086450071586931, 0.8156018571164794], "final_y": [8.161377635860433e-08, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "77fda2c5-7370-461c-9899-1560083e33ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        local_sampling_factor = np.random.uniform(0.1, 0.2) * (1 + self.func_evaluations / self.budget)\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        # Added small perturbation to the best solution to encourage exploration\n        best_solution_perturbation = result.x + np.random.uniform(-0.01, 0.01, self.dim)\n\n        try:\n            result = minimize(func, best_solution_perturbation, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances exploration by making small-scale random perturbations to the best solution found during optimization, encouraging further exploration near promising regions.", "configspace": "", "generation": 10, "fitness": 0.8241726136893764, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6df86372-201b-4685-84e7-467ec2206f0c", "metadata": {"aucs": [0.8482709767929567, 0.8086450071586931, 0.8156018571164794], "final_y": [8.161377635860433e-08, 1.9651439255209579e-07, 1.4962493484901396e-07]}, "mutation_prompt": null}
{"id": "e4b0dda1-2206-4d0f-af90-a58a603374d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        scaling_factor = np.random.uniform(0.85, 1.15)  # Adjusted dynamic scaling factor\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        self.func_evaluations = 0\n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)  # Slight change here for adaptive tolerance\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                return result.x\n                \n        except StopIteration:\n            pass\n\n        center = result.x\n        reseed_condition = self.func_evaluations < (self.budget * 0.4)  # Dynamically adjusted reseeding condition\n        if reseed_condition:  \n            initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n        \n        local_sampling_factor = np.random.uniform(0.1, 0.2)  # Adaptive local sampling factor\n        new_bounds = np.array([np.maximum(center - local_sampling_factor * (bounds[1] - bounds[0]), bounds[0]),\n                               np.minimum(center + local_sampling_factor * (bounds[1] - bounds[0]), bounds[1])])\n        \n        try:\n            result = minimize(func, result.x, method='Nelder-Mead', \n                              tol=1e-6, callback=callback, \n                              options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n        except StopIteration:\n            pass\n\n        if self.func_evaluations < self.budget:\n            weights = np.linspace(0.1, 1.0, self.dim)\n            new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * weights\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n            except StopIteration:\n                pass\n\n        return result.x", "name": "AdaptiveNelderMead", "description": "Enhances exploration by dynamically scaling reseed frequency based on remaining budget percentage.", "configspace": "", "generation": 10, "fitness": 0.8274721942390088, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eb38470e-e573-409c-b85d-be649c4d362c", "metadata": {"aucs": [0.8152135690791962, 0.8449065412573435, 0.8222964723804864], "final_y": [1.7586309408220226e-07, 8.166329506666448e-08, 1.4801871423523473e-07]}, "mutation_prompt": null}
{"id": "6f389382-9829-4942-b568-199301a0bd0d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMeadDynamicControl:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        self.func_evaluations = 0\n        initial_guess = np.random.uniform(bounds[0], bounds[1], self.dim)\n        \n        def callback(x):\n            self.func_evaluations += 1\n            if self.func_evaluations >= self.budget:\n                raise StopIteration(\"Budget exhausted\")\n\n        try:\n            dynamic_tol = 1e-6 * (1 + (self.func_evaluations / self.budget)**2)\n            result = minimize(func, initial_guess, method='Nelder-Mead', \n                              tol=dynamic_tol, callback=callback, \n                              options={'maxiter': self.budget, 'disp': False})\n            \n            if result.success:\n                center = result.x\n\n        except StopIteration:\n            return result.x\n\n        while self.func_evaluations < self.budget:\n            exploration_phase = self.func_evaluations < self.budget // 2\n            if exploration_phase:\n                scaling_factor = np.random.uniform(0.9, 1.1)\n                new_guess = np.random.uniform(bounds[0], bounds[1], self.dim) * scaling_factor\n            else:\n                adjustment_factor = max(0.1, 1 - (self.func_evaluations / self.budget))\n                new_bounds = np.array([np.maximum(center - adjustment_factor * (bounds[1] - bounds[0]), bounds[0]),\n                                       np.minimum(center + adjustment_factor * (bounds[1] - bounds[0]), bounds[1])])\n                new_guess = np.random.uniform(new_bounds[0], new_bounds[1], self.dim)\n\n            try:\n                result = minimize(func, new_guess, method='Nelder-Mead', \n                                  tol=1e-6, callback=callback, \n                                  options={'maxiter': self.budget - self.func_evaluations, 'disp': False})\n                \n                if result.success:\n                    center = result.x\n\n            except StopIteration:\n                break\n\n        return result.x", "name": "AdaptiveNelderMeadDynamicControl", "description": "Introduces a dynamic adaptive control mechanism that adjusts exploration and exploitation phases based on real-time evaluation of performance to optimize search efficiency.", "configspace": "", "generation": 10, "fitness": 0.8157089542927708, "feedback": "The algorithm AdaptiveNelderMeadDynamicControl got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "eb38470e-e573-409c-b85d-be649c4d362c", "metadata": {"aucs": [0.8026518400588758, 0.8114579678751392, 0.8330170549442973], "final_y": [1.578588183163937e-07, 1.2299427031525147e-07, 1.0204936248994274e-07]}, "mutation_prompt": null}
