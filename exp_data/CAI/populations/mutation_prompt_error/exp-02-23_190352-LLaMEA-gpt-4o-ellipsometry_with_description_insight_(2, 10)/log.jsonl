{"id": "6cc83c70-bb6b-4790-80be-b4a4896ed9b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Uniform Sampling\n        num_samples = min(self.budget // 2, 10)  # Use up to half the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "A hybrid metaheuristic combining uniform sampling for initial exploration and BFGS for fast local exploitation in smooth, low-dimensional spaces.", "configspace": "", "generation": 0, "fitness": 0.8351343042225309, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.004. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8317427898182468, 0.8412959524433626, 0.8323641704059831], "final_y": [4.3475664938268855e-08, 3.813250633368821e-08, 4.987344299990449e-08]}, "mutation_prompt": null}
{"id": "9176a31d-5ea1-4220-ba71-7bd3b00ebd78", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveBoundaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds_lower = func.bounds.lb\n        bounds_upper = func.bounds.ub\n        \n        # Create a uniform random sample for the initial guess within bounds\n        x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n        \n        def bounded_func(x):\n            # Clip the parameters to keep them within bounds\n            x_clipped = np.clip(x, bounds_lower, bounds_upper)\n            return func(x_clipped)\n\n        # Define the optimization options\n        options = {\n            'maxiter': self.budget,  # Budget for function evaluations\n            'adaptive': True         # Adaptive simplex size\n        }\n\n        # Perform optimization using the Nelder-Mead algorithm\n        result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n\n        # Use adaptive boundary shrinking based on the result\n        if result.success:\n            for _ in range(self.budget - result.nfev):\n                # Shrink bounds towards the current best solution\n                bounds_lower = np.maximum(bounds_lower, result.x - (bounds_upper - bounds_lower) * 0.1)\n                bounds_upper = np.minimum(bounds_upper, result.x + (bounds_upper - bounds_lower) * 0.1)\n                x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n                result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n                if result.nfev >= self.budget:\n                    break\n        \n        return result.x\n\n# Example usage:\n# optimizer = AdaptiveBoundaryOptimizer(budget=100, dim=2)\n# best_solution = optimizer(your_black_box_function)", "name": "AdaptiveBoundaryOptimizer", "description": "A hybrid metaheuristic that blends the Nelder-Mead simplex method for local exploration with adaptive boundary shrinking to iteratively refine solutions in smooth, low-dimensional spaces.", "configspace": "", "generation": 0, "fitness": 0.25908990057609516, "feedback": "The algorithm AdaptiveBoundaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.259 with standard deviation 0.283. And the mean value of best solutions found was 12.187 (0. is the best) with standard deviation 15.105.", "error": "", "parent_id": null, "metadata": {"aucs": [0.6545284743745123, 0.11378913871246465, 0.008952088641308475], "final_y": [4.554932691073197e-06, 3.0869964190094255, 33.473385603903445]}, "mutation_prompt": null}
{"id": "459ad711-52cc-4720-a99c-d25371a68584", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Uniform Sampling\n        num_samples = min(self.budget // 2, 15)  # Increased sample size from 10 to 15\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Enhanced HybridOptimizer by increasing the sample size for initial exploration to improve starting points.", "configspace": "", "generation": 1, "fitness": 0.8090126917794812, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6cc83c70-bb6b-4790-80be-b4a4896ed9b6", "metadata": {"aucs": [0.8080940686397544, 0.8014242035113398, 0.8175198031873494], "final_y": [7.932592894745415e-08, 1.4249337519115955e-07, 7.282230866379556e-08]}, "mutation_prompt": null}
{"id": "7514bcba-6494-4186-9974-1c83222bb4ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Uniform Sampling\n        num_samples = min(self.budget // 3, 15)  # Use up to a third of the budget, increased to 15 samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n\n        # Use a weighted average of top samples as initial guess\n        sorted_indices = np.argsort(sample_evals)\n        top_samples = samples[sorted_indices[:3]]\n        best_sample = np.average(top_samples, axis=0, weights=[0.6, 0.3, 0.1])\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Enhanced hybrid metaheuristic with increased initial sampling and weighted average initialization for improved local optimization.", "configspace": "", "generation": 1, "fitness": 0.8188247691267265, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.819 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6cc83c70-bb6b-4790-80be-b4a4896ed9b6", "metadata": {"aucs": [0.806993400081146, 0.8616419191486239, 0.7878389881504096], "final_y": [1.0451555703503817e-07, 2.0196823595908337e-08, 1.2955639778705305e-07]}, "mutation_prompt": null}
{"id": "a69fea64-7a77-4977-b7e0-083f2e43caf8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Sampling\n        num_samples = max(5, self.budget // 3)  # Adjusted to use up to a third of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step size\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Enhancing HybridOptimizer by incorporating dynamic sampling and adaptive step size to improve convergence in smooth landscapes.", "configspace": "", "generation": 1, "fitness": 0.864749039973432, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.086. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6cc83c70-bb6b-4790-80be-b4a4896ed9b6", "metadata": {"aucs": [0.9862586772714715, 0.7995835754290337, 0.8084048672197912], "final_y": [0.0, 1.9007618641913948e-07, 8.5345903100586e-08]}, "mutation_prompt": null}
{"id": "de9351ac-d11e-4e5f-b91f-4edc68d72822", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass SobolCGOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Sobol Sequence Sampling for initial exploration\n        num_samples = min(self.budget // 2, 10)  # Use up to half the budget\n        sampler = qmc.Sobol(d=self.dim, scramble=True)\n        sobol_samples = sampler.random_base2(m=int(np.ceil(np.log2(num_samples))))\n        sobol_samples = lb + (ub - lb) * sobol_samples\n        sample_evals = np.array([func(sample) for sample in sobol_samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = sobol_samples[best_index]\n        \n        # Use remaining budget for CG optimization\n        remaining_budget = self.budget - len(sobol_samples)\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using Conjugate Gradient (CG)\n        result = minimize(limited_func, best_sample, method='CG', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black-box function\n# optimizer = SobolCGOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "SobolCGOptimizer", "description": "A novel two-phase optimizer leveraging Sobol sequence sampling for thorough initial exploration and Conjugate Gradient (CG) for precise local refinement in smooth, low-dimensional spaces.", "configspace": "", "generation": 1, "fitness": 0.6509448649732414, "feedback": "The algorithm SobolCGOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.651 with standard deviation 0.052. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "6cc83c70-bb6b-4790-80be-b4a4896ed9b6", "metadata": {"aucs": [0.6387520164713689, 0.5943670298729387, 0.7197155485754168], "final_y": [2.2533183348906017e-06, 1.398025946592736e-05, 3.8547804774533694e-07]}, "mutation_prompt": null}
{"id": "4ffe752d-d225-4b66-baf4-084964800d8d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedBoundaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds_lower = func.bounds.lb\n        bounds_upper = func.bounds.ub\n\n        def bounded_func(x):\n            x_clipped = np.clip(x, bounds_lower, bounds_upper)\n            return func(x_clipped)\n\n        def dynamic_bounds_shrink(bounds_lower, bounds_upper, current_best):\n            center = current_best\n            span = (bounds_upper - bounds_lower) * 0.1\n            new_lower = np.maximum(bounds_lower, center - span)\n            new_upper = np.minimum(bounds_upper, center + span)\n            return new_lower, new_upper\n\n        num_restarts = 5\n        evals_per_restart = self.budget // num_restarts\n        best_solution = None\n        best_value = np.inf\n\n        for _ in range(num_restarts):\n            x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n            options = {'maxiter': evals_per_restart, 'adaptive': True}\n            result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n            bounds_lower, bounds_upper = dynamic_bounds_shrink(bounds_lower, bounds_upper, result.x)\n\n        return best_solution\n\n# Example usage:\n# optimizer = EnhancedBoundaryOptimizer(budget=100, dim=2)\n# best_solution = optimizer(your_black_box_function)", "name": "EnhancedBoundaryOptimizer", "description": "The EnhancedBoundaryOptimizer combines Nelder-Mead with dynamic boundary adjustment and restart strategies to improve convergence in smooth, low-dimensional spaces by efficiently exploring and exploiting the parameter space.", "configspace": "", "generation": 1, "fitness": 0.5684333795200719, "feedback": "The algorithm EnhancedBoundaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.568 with standard deviation 0.411. And the mean value of best solutions found was 9.456 (0. is the best) with standard deviation 13.372.", "error": "", "parent_id": "9176a31d-5ea1-4220-ba71-7bd3b00ebd78", "metadata": {"aucs": [1.0, 0.6899268252875703, 0.01537331327264524], "final_y": [0.0, 5.513996087461023e-06, 28.367105276157496]}, "mutation_prompt": null}
{"id": "51a6fe94-a992-4dd6-8b00-c8173990a97c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveBoundaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds_lower = func.bounds.lb\n        bounds_upper = func.bounds.ub\n        \n        # Create a uniform random sample for the initial guess within bounds\n        x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n        \n        def bounded_func(x):\n            # Clip the parameters to keep them within bounds\n            x_clipped = np.clip(x, bounds_lower, bounds_upper)\n            return func(x_clipped)\n\n        # Define the optimization options\n        options = {\n            'maxiter': self.budget,  # Budget for function evaluations\n            'adaptive': True         # Adaptive simplex size\n        }\n\n        # Perform optimization using the Nelder-Mead algorithm\n        result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n\n        # Use adaptive boundary shrinking based on the result\n        if result.success:\n            shrink_factor = 0.05  # Smaller shrink factor for finer refinement\n            for _ in range(self.budget - result.nfev):\n                # Shrink bounds towards the current best solution\n                bounds_lower = np.maximum(bounds_lower, result.x - (bounds_upper - bounds_lower) * shrink_factor)\n                bounds_upper = np.minimum(bounds_upper, result.x + (bounds_upper - bounds_lower) * shrink_factor)\n                x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n                result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n                if result.nfev >= self.budget:\n                    break\n        \n        return result.x\n\n# Example usage:\n# optimizer = AdaptiveBoundaryOptimizer(budget=100, dim=2)\n# best_solution = optimizer(your_black_box_function)", "name": "AdaptiveBoundaryOptimizer", "description": "An enhanced metaheuristic integrating Nelder-Mead with dynamic boundary adjustment, leveraging convergence feedback to refine search areas in smooth, low-dimensional spaces.", "configspace": "", "generation": 1, "fitness": 0.4669925740214374, "feedback": "The algorithm AdaptiveBoundaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.467 with standard deviation 0.299. And the mean value of best solutions found was 5.081 (0. is the best) with standard deviation 7.186.", "error": "", "parent_id": "9176a31d-5ea1-4220-ba71-7bd3b00ebd78", "metadata": {"aucs": [0.6708584430515259, 0.6858497389361178, 0.04426954007666861], "final_y": [7.439827056821939e-06, 5.835467321123771e-06, 15.243286924192692]}, "mutation_prompt": null}
{"id": "19dade67-66a7-4a3f-a5b2-59020cecaf71", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveBoundaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds_lower = func.bounds.lb\n        bounds_upper = func.bounds.ub\n        \n        # Create a uniform random sample for the initial guess within bounds\n        x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n        \n        def bounded_func(x):\n            # Clip the parameters to keep them within bounds\n            x_clipped = np.clip(x, bounds_lower, bounds_upper)\n            return func(x_clipped)\n\n        # Define the optimization options\n        options = {\n            'maxiter': self.budget,  # Budget for function evaluations\n            'adaptive': True         # Adaptive simplex size\n        }\n\n        # Perform optimization using the Nelder-Mead algorithm\n        result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n\n        # Use adaptive boundary shrinking based on the result\n        if result.success:\n            for _ in range(self.budget - result.nfev):\n                # Shrink bounds towards the current best solution more frequently\n                bounds_lower = np.maximum(bounds_lower, result.x - (bounds_upper - bounds_lower) * 0.05)\n                bounds_upper = np.minimum(bounds_upper, result.x + (bounds_upper - bounds_lower) * 0.05)\n                x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n                result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n                if result.nfev >= self.budget:\n                    break\n        \n        return result.x\n\n# Example usage:\n# optimizer = AdaptiveBoundaryOptimizer(budget=100, dim=2)\n# best_solution = optimizer(your_black_box_function)", "name": "AdaptiveBoundaryOptimizer", "description": "A hybrid metaheuristic that blends the Nelder-Mead simplex method with more frequent adaptive boundary shrinking to iteratively refine solutions in smooth, low-dimensional spaces.", "configspace": "", "generation": 1, "fitness": 0.22692779631169535, "feedback": "The algorithm AdaptiveBoundaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.227 with standard deviation 0.000. And the mean value of best solutions found was 0.257 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9176a31d-5ea1-4220-ba71-7bd3b00ebd78", "metadata": {"aucs": [0.22681911477425032, 0.2269901383404045, 0.22697413582043124], "final_y": [0.25695398550425613, 0.25695398550425647, 0.2569539855042568]}, "mutation_prompt": null}
{"id": "47b80956-6d2b-4545-b88b-afd7127ea811", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedAdaptiveBoundaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds_lower = np.array(func.bounds.lb, dtype=float)\n        bounds_upper = np.array(func.bounds.ub, dtype=float)\n        best_solution = None\n        best_value = float('inf')\n        evaluations_used = 0\n        \n        while evaluations_used < self.budget:\n            # Create a uniform random sample for the initial guess within bounds\n            x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n            \n            def bounded_func(x):\n                x_clipped = np.clip(x, bounds_lower, bounds_upper)\n                return func(x_clipped)\n\n            options = {\n                'maxiter': min(self.budget - evaluations_used, 100),  # Limit iterations per Nelder-Mead run\n                'adaptive': True\n            }\n\n            result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n            evaluations_used += result.nfev\n\n            if result.success and result.fun < best_value:\n                best_solution = result.x\n                best_value = result.fun\n                # Shrink bounds towards the current best solution\n                bounds_lower = np.maximum(bounds_lower, best_solution - (bounds_upper - bounds_lower) * 0.1)\n                bounds_upper = np.minimum(bounds_upper, best_solution + (bounds_upper - bounds_lower) * 0.1)\n\n        return best_solution\n\n# Example usage:\n# optimizer = EnhancedAdaptiveBoundaryOptimizer(budget=100, dim=2)\n# best_solution = optimizer(your_black_box_function)", "name": "EnhancedAdaptiveBoundaryOptimizer", "description": "A hybrid metaheuristic that leverages a combination of random restart strategy and Nelder-Mead simplex method with adaptive boundary shrinking to iteratively refine solutions in smooth, low-dimensional spaces, improving convergence by exploring diverse regions of the parameter space.", "configspace": "", "generation": 1, "fitness": 0.4574260236751379, "feedback": "The algorithm EnhancedAdaptiveBoundaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.457 with standard deviation 0.309. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "9176a31d-5ea1-4220-ba71-7bd3b00ebd78", "metadata": {"aucs": [0.6665921045546491, 0.6844534072257463, 0.021232559245018212], "final_y": [8.04617114740424e-06, 5.380122432303615e-06, 25.44556613682321]}, "mutation_prompt": null}
{"id": "38f6bc69-9087-4549-bc7b-f37e70bd0d4a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveBoundaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds_lower = func.bounds.lb\n        bounds_upper = func.bounds.ub\n        \n        # Create a uniform random sample for the initial guess within bounds\n        x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim)\n        \n        def bounded_func(x):\n            # Clip the parameters to keep them within bounds\n            x_clipped = np.clip(x, bounds_lower, bounds_upper)\n            return func(x_clipped)\n\n        # Define the optimization options\n        options = {\n            'maxiter': self.budget,  # Budget for function evaluations\n            'adaptive': True         # Adaptive simplex size\n        }\n\n        # Perform optimization using the Nelder-Mead algorithm\n        result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n\n        # Use adaptive boundary shrinking based on the result\n        if result.success:\n            for _ in range(self.budget - result.nfev):\n                # Shrink bounds towards the current best solution\n                bounds_lower = np.maximum(bounds_lower, result.x - (bounds_upper - bounds_lower) * 0.1)\n                bounds_upper = np.minimum(bounds_upper, result.x + (bounds_upper - bounds_lower) * 0.1)\n                # Introduce stochastic perturbation to x0\n                x0 = np.random.uniform(bounds_lower, bounds_upper, self.dim) + np.random.normal(0, 0.01, self.dim)\n                result = minimize(bounded_func, x0, method='Nelder-Mead', options=options)\n                if result.nfev >= self.budget:\n                    break\n        \n        return result.x", "name": "AdaptiveBoundaryOptimizer", "description": "A hybrid metaheuristic that blends the Nelder-Mead simplex method with adaptive boundary shrinking and stochastic perturbation to iteratively refine solutions in smooth, low-dimensional spaces.", "configspace": "", "generation": 1, "fitness": 0.46349502698795125, "feedback": "The algorithm AdaptiveBoundaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.463 with standard deviation 0.306. And the mean value of best solutions found was 6.915 (0. is the best) with standard deviation 9.779.", "error": "", "parent_id": "9176a31d-5ea1-4220-ba71-7bd3b00ebd78", "metadata": {"aucs": [0.6821184314779793, 0.6779521155634689, 0.030414533922405673], "final_y": [5.323453295177585e-06, 6.286001733863111e-06, 20.744636172174793]}, "mutation_prompt": null}
{"id": "b5f6e445-3cfa-4154-bb8a-8abe008461d1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import differential_evolution\n\nclass DE_LBFGSB_Optimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        \n        # Function to clip parameters within bounds\n        def bounded_func(x):\n            x_clipped = np.clip(x, func.bounds.lb, func.bounds.ub)\n            return func(x_clipped)\n        \n        # Calculate budget split between DE and L-BFGS-B\n        de_budget = int(0.7 * self.budget)\n        lbfgs_budget = self.budget - de_budget\n\n        # Differential Evolution for global search\n        de_result = differential_evolution(\n            bounded_func, bounds, \n            maxiter=de_budget // (self.dim + 1), \n            polish=False, disp=False\n        )\n\n        # Use L-BFGS-B for local refinement\n        options = {'maxiter': lbfgs_budget}\n        lbfgs_result = minimize(\n            bounded_func, de_result.x, method='L-BFGS-B', bounds=bounds, options=options\n        )\n\n        return lbfgs_result.x\n\n# Example usage:\n# optimizer = DE_LBFGSB_Optimizer(budget=100, dim=2)\n# best_solution = optimizer(your_black_box_function)", "name": "DE_LBFGSB_Optimizer", "description": "A two-phase metaheuristic that begins with global exploration using Differential Evolution followed by local refinement with L-BFGS-B for efficient fine-tuning in smooth, low-dimensional spaces.", "configspace": "", "generation": 1, "fitness": 0.3657242493087039, "feedback": "The algorithm DE_LBFGSB_Optimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.366 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9176a31d-5ea1-4220-ba71-7bd3b00ebd78", "metadata": {"aucs": [0.36956293726825074, 0.3840573762921202, 0.3435524343657407], "final_y": [1.253460970453792e-05, 1.3170755206109706e-05, 0.00012055302892633855]}, "mutation_prompt": null}
{"id": "3d83125f-39e6-43f0-afa3-2da99cbe5e1b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Sampling\n        num_samples = max(5, self.budget // 3)  # Adjusted to use up to a third of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Introduce confidence-based adaptive sampling\n        best_five_indices = np.argsort(sample_evals)[:5]  # Get indices of best five samples\n        refined_samples = samples[best_five_indices] + np.random.normal(0, 0.1, refined_samples.shape)  # Refine best samples\n        refined_evals = np.array([func(refined_sample) for refined_sample in refined_samples])\n        \n        # Find the best initial sample after refinement\n        best_index = np.argmin(refined_evals)\n        best_sample = refined_samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples - len(refined_samples)\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step size\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Introduce a confidence-based adaptive sampling to refine initial sampling and enhance local optimization convergence.", "configspace": "", "generation": 2, "fitness": 0.44070875661743186, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.441 with standard deviation 0.397. And the mean value of best solutions found was 1.220 (0. is the best) with standard deviation 1.468.", "error": "", "parent_id": "a69fea64-7a77-4977-b7e0-083f2e43caf8", "metadata": {"aucs": [0.9990689392237291, 0.11268204438485763, 0.21037528624370883], "final_y": [0.0, 3.2851543761096953, 0.37415900720927164]}, "mutation_prompt": null}
{"id": "f8a07bbf-a338-4343-ae22-b0b9dc991139", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Sampling\n        num_samples = max(5, self.budget // 3)  # Adjusted to use up to a third of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample from top-performing samples\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[np.argsort(sample_evals)[:max(1, num_samples // 10)]]  # Changed line: select top 10% samples\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step size\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Modifying sample selection to focus on top-performing samples for better initial guesses in local search.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'x0' must only have one dimension.\").", "error": "ValueError(\"'x0' must only have one dimension.\")", "parent_id": "a69fea64-7a77-4977-b7e0-083f2e43caf8", "metadata": {}, "mutation_prompt": null}
{"id": "8ab17190-2c8f-431b-86ab-e98784a7c0af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Adaptive Sampling\n        initial_samples = min(max(self.budget // 5, 5), 20)  # Adaptive range based on budget\n        samples = np.random.uniform(lb, ub, (initial_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Use a weighted average of top samples as initial guess\n        sorted_indices = np.argsort(sample_evals)\n        top_samples = samples[sorted_indices[:3]]\n        best_sample = np.average(top_samples, axis=0, weights=[0.5, 0.3, 0.2])\n        \n        # Step 2: Dynamic Local Refinement\n        def dynamic_bounds_adjustment(x):\n            # Expand search around current best estimate\n            epsilon = 0.05 * (ub - lb)\n            new_lb = np.maximum(lb, x - epsilon)\n            new_ub = np.minimum(ub, x + epsilon)\n            return list(zip(new_lb, new_ub))\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - initial_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Local Optimization using BFGS with dynamic bounds\n        bounds = dynamic_bounds_adjustment(best_sample)\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=bounds)\n        \n        return result.x\n\n# Example usage with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Optimized hybrid metaheuristic incorporating adaptive sampling density and dynamic local refinement for improved convergence and precision.", "configspace": "", "generation": 2, "fitness": 0.3417546179679933, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.342 with standard deviation 0.337. And the mean value of best solutions found was 2.777 (0. is the best) with standard deviation 2.249.", "error": "", "parent_id": "7514bcba-6494-4186-9974-1c83222bb4ee", "metadata": {"aucs": [0.11819270002070303, 0.08897742740994208, 0.8180937264733347], "final_y": [2.823000713301309, 5.509019581375764, 1.0621590739779888e-07]}, "mutation_prompt": null}
{"id": "c95da39f-26dc-49ea-91ea-5b93d9e004dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveGradientOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Adaptive Sampling\n        num_samples = min(self.budget // 4, 20)  # Use up to a fourth of the budget, increased to 20 samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Adaptive selection based on performance\n        top_indices = np.argsort(sample_evals)[:5]\n        top_samples = samples[top_indices]\n        \n        # Calculate centroid for initial guess\n        initial_guess = np.mean(top_samples, axis=0)\n        \n        # Calculate the variance and adjust step sizes\n        variances = np.var(top_samples, axis=0)\n        step_sizes = np.maximum(variances, np.full(self.dim, 1e-3))\n        \n        # Use remaining budget for local optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step sizes\n        result = minimize(limited_func, initial_guess, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'eps': step_sizes})\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = AdaptiveGradientOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "AdaptiveGradientOptimizer", "description": "Adaptive Sampling and Gradient-Driven Optimization that leverages initial adaptive sampling for diverse solution discovery and gradient-based refinement for final optimization.", "configspace": "", "generation": 2, "fitness": 0.4231872402532728, "feedback": "The algorithm AdaptiveGradientOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.423 with standard deviation 0.409. And the mean value of best solutions found was 1.812 (0. is the best) with standard deviation 1.990.", "error": "", "parent_id": "7514bcba-6494-4186-9974-1c83222bb4ee", "metadata": {"aucs": [1.0, 0.17205735945309497, 0.09750436130672335], "final_y": [0.0, 0.8528062516658197, 4.581941273936946]}, "mutation_prompt": null}
{"id": "7a0f0528-d70f-4221-a4a2-9b8202fd5963", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Uniform Sampling based on budget\n        num_samples = max(3, int(self.budget * 0.2))  # 20% of the budget for initial sampling\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Adaptive initialization using top samples' weighted mean\n        sorted_indices = np.argsort(sample_evals)\n        top_samples = samples[sorted_indices[:5]]\n        weights = np.linspace(1, 0.2, num=5)\n        best_sample = np.average(top_samples, axis=0, weights=weights)\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with improved initialization\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "HybridOptimizer", "description": "Enhanced Hybrid Optimizer with dynamic sampling size based on budget and adaptive initialization to boost convergence in smooth parameter spaces.", "configspace": "", "generation": 2, "fitness": 0.34192058929249186, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.342 with standard deviation 0.268. And the mean value of best solutions found was 0.891 (0. is the best) with standard deviation 0.631.", "error": "", "parent_id": "7514bcba-6494-4186-9974-1c83222bb4ee", "metadata": {"aucs": [0.7213808675371967, 0.15074255957982663, 0.15363834076045235], "final_y": [2.7493635567202716e-08, 1.3770420979326723, 1.2956773710954694]}, "mutation_prompt": null}
{"id": "22d47423-604a-4374-b3d1-d499fc118f11", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Sampling\n        num_samples = max(5, self.budget // 3)  # Adjusted to use up to a third of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample based on median evaluation\n        best_index = np.argsort(sample_evals)[len(sample_evals) // 2]\n        best_sample = samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step size\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Refined HybridOptimizer by improving dynamic sampling and modifying initial sample selection for enhanced convergence.", "configspace": "", "generation": 2, "fitness": 0.28567086634319144, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.286 with standard deviation 0.196. And the mean value of best solutions found was 1.390 (0. is the best) with standard deviation 1.582.", "error": "", "parent_id": "a69fea64-7a77-4977-b7e0-083f2e43caf8", "metadata": {"aucs": [0.5589912956468516, 0.1901005790220196, 0.10792072436070321], "final_y": [1.466620902785148e-07, 0.5650731921160221, 3.603663684946181]}, "mutation_prompt": null}
{"id": "32777d13-150a-4fc3-b5c0-4b91be627597", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Sampling\n        num_samples = max(5, self.budget // 4)  # Adjusted to use up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step size\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'maxiter': max(2, remaining_budget // 2)})\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Modify dynamic sampling and integrate partial budget allocation for improved exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.25625116230725664, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.256 with standard deviation 0.232. And the mean value of best solutions found was 3.579 (0. is the best) with standard deviation 2.850.", "error": "", "parent_id": "a69fea64-7a77-4977-b7e0-083f2e43caf8", "metadata": {"aucs": [0.5844846626334719, 0.1058657539296538, 0.07840307035864424], "final_y": [7.445348824195178e-08, 3.7619715560780533, 6.975031593855547]}, "mutation_prompt": null}
{"id": "1908ce70-ada7-4516-8c9f-10e8f7d1ede0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Sampling\n        num_samples = int((self.budget ** 0.618) // 1)  # Adjusted using the golden ratio\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step size\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)), options={'gtol': 1e-8})\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Improve HybridOptimizer by fine-tuning sampling strategy with a golden ratio for sample size and adjusting termination criteria for better exploitation.", "configspace": "", "generation": 2, "fitness": 0.28252386307384203, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.283 with standard deviation 0.187. And the mean value of best solutions found was 1.124 (0. is the best) with standard deviation 1.166.", "error": "", "parent_id": "a69fea64-7a77-4977-b7e0-083f2e43caf8", "metadata": {"aucs": [0.5439142621076123, 0.18328128321743753, 0.12037604389647627], "final_y": [2.853198847518128e-07, 0.6395628492275063, 2.7314075652956094]}, "mutation_prompt": null}
{"id": "59c467f9-30fa-43af-adc6-cf4be2530da5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Sampling with Latin Hypercube\n        num_samples = max(5, self.budget // 3)  # Adjusted to use up to a third of the budget\n        sampler = qmc.LatinHypercube(d=self.dim)\n        samples = qmc.scale(sampler.random(num_samples), lb, ub)\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step size\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Enhanced sampling strategy using Latin Hypercube Sampling and adaptive constraints for improved convergence.", "configspace": "", "generation": 2, "fitness": 0.29681797586162223, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.297 with standard deviation 0.217. And the mean value of best solutions found was 1.656 (0. is the best) with standard deviation 1.989.", "error": "", "parent_id": "a69fea64-7a77-4977-b7e0-083f2e43caf8", "metadata": {"aucs": [0.5985149448383078, 0.09849903551539263, 0.1934399472311663], "final_y": [6.028179519090793e-08, 4.453895465006839, 0.514770245440405]}, "mutation_prompt": null}
{"id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 12)  # Allocate up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 1.0 / (1.0 + norm(gradient))\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = AdaptiveGradientSampling(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "AdaptiveGradientSampling", "description": "Adaptive Gradient Sampling (AGS) algorithm that combines adaptive sampling with gradient estimation to enhance convergence in smooth, low-dimensional landscapes.", "configspace": "", "generation": 2, "fitness": 0.7627572302607275, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.763 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7514bcba-6494-4186-9974-1c83222bb4ee", "metadata": {"aucs": [0.7768352732016863, 0.7653995533751564, 0.7460368642053397], "final_y": [9.455305757157926e-08, 2.164920858345039e-07, 3.4259105960336203e-07]}, "mutation_prompt": null}
{"id": "e974f6e3-6cef-477e-8872-8aa61b3b408b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 12)  # Allocate up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = AdaptiveGradientSampling(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling (EAGS) with dynamic step size adjustment and improved sampling strategy for better convergence.", "configspace": "", "generation": 3, "fitness": 0.8612524209508781, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.087. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "metadata": {"aucs": [0.9832078083905578, 0.8138121556184011, 0.7867372988436752], "final_y": [0.0, 3.9858363739202224e-08, 9.515947979685155e-08]}, "mutation_prompt": null}
{"id": "b6bade5c-1d39-40ef-a780-e3a0b6414c29", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Decrease epsilon for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 1.0 / (1.0 + norm(gradient))\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with dynamic epsilon and improved initial sampling strategy for faster convergence.", "configspace": "", "generation": 3, "fitness": 0.808486466461077, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.021. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "metadata": {"aucs": [0.8171658336598413, 0.8286174493213898, 0.779676116402], "final_y": [6.568932329637255e-08, 2.359725309438917e-08, 1.793945815717744e-07]}, "mutation_prompt": null}
{"id": "70fc02e2-eb23-45b8-98ed-3563a4b03950", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 12)  # Allocate up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 1.2 / (1.0 + norm(gradient))  # Changed from 1.0 to 1.2\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n                step_size *= 1.1  # Increase step size if improvement\n\n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = AdaptiveGradientSampling(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "AdaptiveGradientSampling", "description": "Enhance Adaptive Gradient Sampling by incorporating a dynamic step size adjustment based on sample performance to refine convergence.", "configspace": "", "generation": 3, "fitness": 0.771096462932984, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "metadata": {"aucs": [0.7631037110481802, 0.7711063404791086, 0.7790793372716629], "final_y": [1.7081105471986992e-07, 2.0047313620819735e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "1bf14864-d4cc-4eb1-bc14-e4880b1ba799", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 12)  # Allocate up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Modified step size\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = AdaptiveGradientSampling(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling (EAGS) algorithm incorporating adaptive step size adjustments for optimized convergence in smooth, low-dimensional landscapes. ", "configspace": "", "generation": 3, "fitness": 0.7858814480906858, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "metadata": {"aucs": [0.807458666521286, 0.7711063404791086, 0.7790793372716629], "final_y": [6.452121204664196e-08, 2.0047313620819735e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "a5269f26-1a72-49fc-a33e-ac66881d77fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 12)  # Allocate up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Line changed\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Adaptively adjust the gradient estimation step size for improved convergence in low-dimensional optimization.", "configspace": "", "generation": 3, "fitness": 0.8042786747726494, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "metadata": {"aucs": [0.812386139867501, 0.8203879469872577, 0.7800619374631895], "final_y": [4.981276450418174e-08, 3.588285052343643e-08, 1.3829755742128705e-07]}, "mutation_prompt": null}
{"id": "5ca89a3e-d1d6-46de-bf0a-c45ea5a85033", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 15)  # Adjusted sample size\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS with an additional starting point\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = AdaptiveGradientSampling(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling (EAGS) refines sampling with adaptive step sizes and incorporates curvature information for improved local search.", "configspace": "", "generation": 3, "fitness": 0.797421943602699, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "metadata": {"aucs": [0.7759213952290768, 0.8198816759674732, 0.7964627596115468], "final_y": [8.796994303709699e-08, 3.2249834100466645e-08, 8.320995577009276e-08]}, "mutation_prompt": null}
{"id": "093e2813-0002-4783-acfc-af10bc51bcd9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 12)  # Allocate up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 1.0 / (1.0 + 10 * norm(gradient))  # Changed line: Adjusted step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhance convergence by using a dynamic step size based on the gradient's magnitude during adaptive sampling.", "configspace": "", "generation": 3, "fitness": 0.7893189271707968, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.049. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "metadata": {"aucs": [0.74687979497075, 0.7637714466210328, 0.8573055399206078], "final_y": [2.332484529668708e-07, 1.5352857458867965e-07, 1.0883605485989754e-08]}, "mutation_prompt": null}
{"id": "29e84fe3-3e53-4c13-a6b3-9fff85e1e046", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 12)  # Allocate up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Reduced step size for finer adjustments\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with dynamic step size adjustment for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.764158537617066, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.052. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f98e13db-9992-4914-82c2-1b5856cfcac9", "metadata": {"aucs": [0.8258325587436263, 0.6992519605773566, 0.767391093530215], "final_y": [3.2670030759919305e-08, 1.6268987465696556e-07, 1.962875886646122e-07]}, "mutation_prompt": null}
{"id": "0a497a97-45c6-4f91-bf2c-a0b75c008040", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Dynamic Sampling\n        num_samples = max(5, self.budget // 3)  # Adjusted to use up to a third of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Introduce confidence-based adaptive sampling with momentum\n        best_five_indices = np.argsort(sample_evals)[:5]  # Get indices of best five samples\n        momentum = 0.9\n        refined_samples = samples[best_five_indices] + momentum * np.random.normal(0, 0.1, samples[best_five_indices].shape)  # Refine best samples\n        refined_evals = np.array([func(refined_sample) for refined_sample in refined_samples])\n        \n        # Find the best initial sample after refinement\n        best_index = np.argmin(refined_evals)\n        best_sample = refined_samples[best_index]\n        \n        # Use remaining budget for BFGS optimization\n        remaining_budget = self.budget - num_samples - len(refined_samples)\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        # Step 2: Local Optimization using BFGS with adaptive step size\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = HybridOptimizer(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "HybridOptimizer", "description": "Add a momentum term to the confidence-based adaptive sampling for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.6083246409820223, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.608 with standard deviation 0.044. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3d83125f-39e6-43f0-afa3-2da99cbe5e1b", "metadata": {"aucs": [0.5888632492547407, 0.6686244326082724, 0.5674862410830539], "final_y": [6.541039014250065e-08, 1.3652979748751763e-10, 1.3161885406647425e-07]}, "mutation_prompt": null}
{"id": "a595bb16-81a6-4880-8d69-409832e43bce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = max(5, self.budget // 3)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_five_indices = np.argsort(sample_evals)[:5]\n        # Refine using covariance-based sampling\n        best_samples = samples[best_five_indices]\n        cov_matrix = np.cov(best_samples, rowvar=False)  # Calculate covariance\n        refined_samples = best_samples + np.random.multivariate_normal(np.zeros(self.dim), cov_matrix, size=5)\n        refined_evals = np.array([func(refined_sample) for refined_sample in refined_samples])\n        \n        best_index = np.argmin(refined_evals)\n        best_sample = refined_samples[best_index]\n        \n        remaining_budget = self.budget - num_samples - len(refined_samples)\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "HybridOptimizer", "description": "Enhance the refinement step with covariance-based adaptive sampling to better explore promising regions.", "configspace": "", "generation": 3, "fitness": 0.589056970952997, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.589 with standard deviation 0.036. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3d83125f-39e6-43f0-afa3-2da99cbe5e1b", "metadata": {"aucs": [0.5401961925864204, 0.6264135314953285, 0.600561188777242], "final_y": [4.177872992331025e-07, 5.629079122949006e-09, 4.314389690966634e-08]}, "mutation_prompt": null}
{"id": "b3b9f292-97df-4e93-b2b4-827f4c0a8141", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-7):  # Decrease epsilon further for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for subtler updates\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with refined gradient estimation and step size for improved performance.", "configspace": "", "generation": 4, "fitness": 0.8634101812819667, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.092. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b6bade5c-1d39-40ef-a780-e3a0b6414c29", "metadata": {"aucs": [0.9867947878803308, 0.7648007756850669, 0.8386349802805024], "final_y": [0.0, 1.5779432273764986e-07, 2.745601625934043e-08]}, "mutation_prompt": null}
{"id": "2ea8cd8e-8d75-41ed-a015-57337077dd60", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Decrease epsilon for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjust step size for improved convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined Adaptive Gradient Sampling with dynamic step size adjustment for improved convergence.", "configspace": "", "generation": 4, "fitness": 0.8630617876885373, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.089. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b6bade5c-1d39-40ef-a780-e3a0b6414c29", "metadata": {"aucs": [0.9886360641576972, 0.8138120702227012, 0.7867372286852132], "final_y": [0.0, 3.9858363739202224e-08, 9.515947979685155e-08]}, "mutation_prompt": null}
{"id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined Adaptive Gradient Sampling with enhanced dynamic sampling and step size adjustment for improved exploration and convergence.", "configspace": "", "generation": 4, "fitness": 0.8709183064067414, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.083. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e974f6e3-6cef-477e-8872-8aa61b3b408b", "metadata": {"aucs": [0.9877367833800705, 0.8122976575674524, 0.8127204782727013], "final_y": [0.0, 5.0147401074065723e-08, 3.584096814014314e-08]}, "mutation_prompt": null}
{"id": "d14ec789-9fed-436d-a678-5402b7f41ac4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Decrease epsilon for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for finer adjustments\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introducing dynamic step-size reduction based on previous iteration results for improved convergence.", "configspace": "", "generation": 4, "fitness": 0.7574237258069653, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.757 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b6bade5c-1d39-40ef-a780-e3a0b6414c29", "metadata": {"aucs": [0.7631036308190166, 0.7711063220175501, 0.738061224584329], "final_y": [1.7081105471986992e-07, 2.0047313620819735e-07, 3.594446699404703e-07]}, "mutation_prompt": null}
{"id": "61cdc8cc-b499-4c69-8588-835e6cfeec8c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 4, 12)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(2): # Reduced iterations for increased exploration\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.3 / (1.0 + norm(gradient)) # Adjusted step size\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                dynamic_radius = 0.2 * (1.0 - eval_count / remaining_budget)\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(np.clip(sample + dynamic_radius, lb, ub)) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Gradient Sampling with Dynamic Sampling Radius and Improved Local Search for Efficient Convergence.", "configspace": "", "generation": 4, "fitness": 0.7745671790492148, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.003. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e974f6e3-6cef-477e-8872-8aa61b3b408b", "metadata": {"aucs": [0.7735159900112527, 0.7711063220175501, 0.7790792251188416], "final_y": [1.651174960897638e-07, 2.0047313620819735e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "901399b3-c996-4df1-a1c1-c1e51eafd4cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-7):  # Adjusted epsilon for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 1.5 / (1.0 + norm(gradient))  # Modified step size for better convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refine Adaptive Gradient Sampling by employing a curved search technique to enhance local exploitation of the cost function landscape.", "configspace": "", "generation": 4, "fitness": 0.7635812985350761, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b6bade5c-1d39-40ef-a780-e3a0b6414c29", "metadata": {"aucs": [0.767415428950761, 0.7702776776820021, 0.7530507889724654], "final_y": [1.7286656253812353e-07, 2.0825774069694135e-07, 1.8453944289960645e-07]}, "mutation_prompt": null}
{"id": "70d14939-5487-4737-8b28-a7db3c77cb26", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 4, 12)  # Allocate up to a quarter of the budget\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample, epsilon=1e-6)  # Adaptive epsilon\n            step_size = np.random.uniform(0.3, 0.7) / (1.0 + norm(gradient))  # Dynamic step size\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x\n\n# Usage example with a hypothetical black box function\n# optimizer = AdaptiveGradientSampling(budget=50, dim=2)\n# best_params = optimizer(black_box_func)", "name": "AdaptiveGradientSampling", "description": "Refinement of Enhanced Adaptive Gradient Sampling with dynamic initialization of step size and adaptive epsilon for improved convergence.", "configspace": "", "generation": 4, "fitness": 0.7707666158544173, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.022. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e974f6e3-6cef-477e-8872-8aa61b3b408b", "metadata": {"aucs": [0.7975291780637765, 0.7711063220175501, 0.7436643474819251], "final_y": [6.452121204664196e-08, 2.0047313620819735e-07, 2.778246871095399e-07]}, "mutation_prompt": null}
{"id": "69099e7f-631c-477d-8eaf-486e4f9d4c81", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Decrease epsilon for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for better control\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined Adaptive Gradient Sampling with improved gradient step adjustment for enhanced convergence.", "configspace": "", "generation": 4, "fitness": 0.8336294724853337, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.034. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b6bade5c-1d39-40ef-a780-e3a0b6414c29", "metadata": {"aucs": [0.7869620135464142, 0.8682067308775893, 0.8457196730319974], "final_y": [9.385659986836134e-08, 9.684961576139747e-09, 1.8529939649802667e-08]}, "mutation_prompt": null}
{"id": "a5663c0d-feac-4b84-a5d9-90267ca20183", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Decrease epsilon for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon) + np.random.normal(0, 1e-5)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 1.0 / (1.0 + norm(gradient))\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Added stochastic perturbation to enhance local exploration capabilities in the gradient estimation step.", "configspace": "", "generation": 4, "fitness": 0.7769352382056266, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b6bade5c-1d39-40ef-a780-e3a0b6414c29", "metadata": {"aucs": [0.7631036308190166, 0.7886228586790214, 0.7790792251188416], "final_y": [1.7081105471986992e-07, 1.0035584862657546e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "b7337dd9-aa96-4055-ae10-efbdae791da1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=5e-7):  # Decrease epsilon for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.8 / (1.0 + norm(gradient))  # Refine step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improved Adaptive Gradient Sampling with enhanced gradient estimation and dynamic sample refinement.", "configspace": "", "generation": 4, "fitness": 0.8179927706380136, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.045. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b6bade5c-1d39-40ef-a780-e3a0b6414c29", "metadata": {"aucs": [0.8726843204114211, 0.8178977414223986, 0.7633962500802208], "final_y": [1.2774288646494246e-08, 4.5026820884984886e-08, 2.1067118011970456e-07]}, "mutation_prompt": null}
{"id": "4e22ba87-075f-4772-8d03-be1aed595d42", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (5, self.dim))  # Increase sampling frequency\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined Adaptive Gradient Sampling with enhanced convergence through state-based dynamic sampling and optimized step size adjustment.", "configspace": "", "generation": 5, "fitness": 0.7486030130284641, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.749 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "metadata": {"aucs": [0.7389754414733678, 0.7630205125107596, 0.743813085101265], "final_y": [1.7795744995588255e-07, 2.164920858345039e-07, 3.4259105960336203e-07]}, "mutation_prompt": null}
{"id": "c26f4051-e9e6-4df8-8c0b-5560ec0b76e3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-8):  # Decrease epsilon further for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.4 / (1.0 + norm(gradient))  # Adjusted step size for subtler updates\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined Adaptive Gradient Sampling with enhanced convergence by optimizing gradient estimation and final sampling adjustments.", "configspace": "", "generation": 5, "fitness": 0.77563628835107, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b3b9f292-97df-4e93-b2b4-827f4c0a8141", "metadata": {"aucs": [0.7631036300896664, 0.777627498551265, 0.7861777364122785], "final_y": [1.7081105471986992e-07, 2.0047313620819735e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "78dff8b3-398d-4681-9dae-e4ccf32b11d4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Optimized Adaptive Gradient Sampling with enhanced initial sampling strategy for better exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.7642325728389258, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "metadata": {"aucs": [0.7288924835532338, 0.777627498551265, 0.7861777364122785], "final_y": [2.858864956237314e-07, 2.0047313620819735e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "4667a992-c02f-49c5-82ae-092b1c3f507e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined Adaptive Gradient Sampling with increased adaptive sampling iterations for enhanced exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.7600867757615224, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.760 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "metadata": {"aucs": [0.7432538950638733, 0.7769997438368232, 0.7600066883838704], "final_y": [1.7286656253812353e-07, 2.0825774069694135e-07, 1.8453944289960645e-07]}, "mutation_prompt": null}
{"id": "28601ab0-ea54-4a9c-8864-db2c6c7afea4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling\n        num_samples = min(self.budget // 3, 15)  # Allocate a third of the budget and increase samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-8):  # Decrease epsilon further for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for subtler updates\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improved Adaptive Gradient Sampling with refined gradient estimation by further reducing epsilon for finer precision.", "configspace": "", "generation": 5, "fitness": 0.77563628835107, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b3b9f292-97df-4e93-b2b4-827f4c0a8141", "metadata": {"aucs": [0.7631036300896664, 0.777627498551265, 0.7861777364122785], "final_y": [1.7081105471986992e-07, 2.0047313620819735e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "46ba69cf-879d-435a-ae6c-78844a301d26", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2.5, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Slightly adjusted initial sampling strategy for better exploration efficiency.", "configspace": "", "generation": 5, "fitness": 0.7758700005901656, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "metadata": {"aucs": [0.7638047668069534, 0.777627498551265, 0.7861777364122785], "final_y": [1.5257630313668485e-07, 2.0047313620819735e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "d99d1920-cc63-4c54-b3ac-58bb191ee99b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with refined initial sampling for improved exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.8111878224034698, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "metadata": {"aucs": [0.8202939047191906, 0.79727413776873, 0.8159954247224886], "final_y": [5.246411624071939e-08, 6.124384430515526e-08, 4.196430542211363e-08]}, "mutation_prompt": null}
{"id": "5d2fde58-3d87-4fc3-b660-9646c8d46886", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)  # Changed from central to centralized difference\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced gradient estimation with centralized difference for improved accuracy in AdaptiveGradientSampling.", "configspace": "", "generation": 5, "fitness": 0.7703199239824311, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.770 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "metadata": {"aucs": [0.7534998666169301, 0.7633159215011279, 0.7941439838292351], "final_y": [1.2192347632400355e-07, 1.5175986684377308e-07, 1.234803027021929e-07]}, "mutation_prompt": null}
{"id": "5e4511fa-a7eb-4a39-8183-019ea9e9d0f6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 3, 15)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(4):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.9 / (1.0 + norm(gradient))  # Changed step size adjustment\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample) * 0.95  # Modifying evaluation for exploration\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improved Adaptive Gradient Sampling with dynamic step size and enhanced local exploration for superior convergence.", "configspace": "", "generation": 5, "fitness": 0.7689320143292888, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "metadata": {"aucs": [0.7875626641203403, 0.7614194577281015, 0.7578139211394249], "final_y": [6.561667539449164e-08, 1.7348585247764197e-07, 2.630794963401785e-07]}, "mutation_prompt": null}
{"id": "b0fd1b62-8305-457f-933f-d38d9cefac68", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with an increased initial sample allocation for improved exploration.", "configspace": "", "generation": 5, "fitness": 0.7780148289111644, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.029. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ab5eeeeb-3877-4157-9433-3ec033177c26", "metadata": {"aucs": [0.7666059019380007, 0.749223868381456, 0.8182147164140362], "final_y": [1.0515812406629133e-07, 2.370623030372712e-07, 5.743230887782771e-08]}, "mutation_prompt": null}
{"id": "71c829e1-9901-450a-854c-521cb42f96a4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined step-size calculation for enhanced convergence stability and efficiency.", "configspace": "", "generation": 6, "fitness": 0.8573334498166197, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.093. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d99d1920-cc63-4c54-b3ac-58bb191ee99b", "metadata": {"aucs": [0.9735629990209421, 0.7455745865658905, 0.8528627638630267], "final_y": [0.0, 2.164920858345039e-07, 4.519556081064603e-10]}, "mutation_prompt": null}
{"id": "de38050b-fa99-49ed-ade9-1d33628ff2cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Adaptive Gradient Sampling refined by enhanced gradient step adjustment for faster convergence.", "configspace": "", "generation": 6, "fitness": 0.8142926967473377, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.117. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b0fd1b62-8305-457f-933f-d38d9cefac68", "metadata": {"aucs": [0.9800976638409884, 0.7352745877459868, 0.7275058386550383], "final_y": [0.0, 1.5081174043809296e-07, 3.4259105960336203e-07]}, "mutation_prompt": null}
{"id": "bb107b95-825f-42c7-bc75-128de5492d60", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                grad[i] = (limited_func(x1) - limited_func(x)) / epsilon\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            adaptive_step_size = 0.5 / (1.0 + norm(gradient))  # Adaptive step size\n            new_sample = best_sample - adaptive_step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with optimized gradient estimation and dynamic step sizing for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.7474606312133253, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.747 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d99d1920-cc63-4c54-b3ac-58bb191ee99b", "metadata": {"aucs": [0.7412507278895407, 0.7301620746570896, 0.7709690910933452], "final_y": [2.714460770998835e-07, 2.8032349880188783e-07, 1.1759294994027355e-07]}, "mutation_prompt": null}
{"id": "da6a02d9-694f-4c6f-92fc-696fb4a347f6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 1.0 / (1.0 + norm(gradient))  # Updated step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improved Adaptive Gradient Sampling with optimized step size calculation for enhanced convergence.", "configspace": "", "generation": 6, "fitness": 0.7373907881587094, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.737 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b0fd1b62-8305-457f-933f-d38d9cefac68", "metadata": {"aucs": [0.7390919392551997, 0.7455745865658905, 0.7275058386550383], "final_y": [1.7795744995588255e-07, 2.164920858345039e-07, 3.4259105960336203e-07]}, "mutation_prompt": null}
{"id": "efb5f28b-3c4e-42d8-969c-4752eab5007a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (4, self.dim))  # Changed from 3 to 4, dynamic increase\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Incremental Sampling Optimization enhances adaptive gradient sampling by dynamically adjusting sampling frequency for better convergence.", "configspace": "", "generation": 6, "fitness": 0.7424335911432453, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.742 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d99d1920-cc63-4c54-b3ac-58bb191ee99b", "metadata": {"aucs": [0.7542203482088072, 0.7455745865658905, 0.7275058386550383], "final_y": [1.082860715885513e-07, 2.164920858345039e-07, 3.4259105960336203e-07]}, "mutation_prompt": null}
{"id": "a44dc265-3496-4797-a47c-dc87006cad6b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for better convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (5, self.dim))  # Increased sample size for better exploration\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with a refined adaptive sampling strategy and dynamic step size adjustments for better convergence.", "configspace": "", "generation": 6, "fitness": 0.7623118443945378, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.762 with standard deviation 0.005. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b0fd1b62-8305-457f-933f-d38d9cefac68", "metadata": {"aucs": [0.7564227213202129, 0.7616868657156824, 0.7688259461477178], "final_y": [1.5257630313668485e-07, 2.0047313620819735e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "213d39ce-10ca-4861-bf9a-e8ab3bb7d9fb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Optimized Adaptive Sampling with recalibrated step size for enhanced convergence velocity.", "configspace": "", "generation": 6, "fitness": 0.768199613941588, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.768 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b0fd1b62-8305-457f-933f-d38d9cefac68", "metadata": {"aucs": [0.7569425662027401, 0.778830329474306, 0.7688259461477178], "final_y": [1.3489125299279056e-07, 1.0035584862657546e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "7301df8f-ed8a-46d2-af17-5cd834897a2b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (epsilon)  # Reduced difference calculation\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + np.sqrt(norm(gradient)))  # Adjusted step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with refined gradient estimation and dynamic step size adjustment for superior convergence.", "configspace": "", "generation": 6, "fitness": 0.7739026397988725, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d99d1920-cc63-4c54-b3ac-58bb191ee99b", "metadata": {"aucs": [0.7475872540672179, 0.797309908377106, 0.7768107569522937], "final_y": [1.7198828971741915e-07, 3.64348016262714e-08, 1.234273310031432e-07]}, "mutation_prompt": null}
{"id": "05946ab6-b66a-47d8-8d26-db0c42f1e6ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Changed epsilon to a dynamic value\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introducing a dynamic epsilon strategy for gradient estimation to improve convergence accuracy.", "configspace": "", "generation": 6, "fitness": 0.80911638714624, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d99d1920-cc63-4c54-b3ac-58bb191ee99b", "metadata": {"aucs": [0.8294723385166073, 0.7878859335193517, 0.8099908894027611], "final_y": [2.6278853803778478e-08, 6.928947253832735e-08, 4.8324774507663934e-08]}, "mutation_prompt": null}
{"id": "c34f6493-f5f1-48d8-8022-c9d72f41de37", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (5, self.dim))  # Adjusted sample size for exploration\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined Adaptive Gradient Sampling with improved step size and dynamic adaptation for enhanced convergence.  ", "configspace": "", "generation": 6, "fitness": 0.7801341101785638, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b0fd1b62-8305-457f-933f-d38d9cefac68", "metadata": {"aucs": [0.7603645894719848, 0.7799109070019064, 0.8001268340618002], "final_y": [1.0515812406629133e-07, 8.457375861661305e-08, 5.743230887782771e-08]}, "mutation_prompt": null}
{"id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced sampling strategy through increased initial sample allocation for improved initial search space exploration.", "configspace": "", "generation": 7, "fitness": 0.840044817030494, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.103. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "71c829e1-9901-450a-854c-521cb42f96a4", "metadata": {"aucs": [0.9862586772714715, 0.767506693943772, 0.7663690798762385], "final_y": [0.0, 1.0035584862657546e-07, 1.0035584862657546e-07]}, "mutation_prompt": null}
{"id": "37675c4d-478c-4e62-9996-ed3d589d2763", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 25)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhance sampling strategy by increasing initial samples for better exploration coverage.", "configspace": "", "generation": 7, "fitness": 0.8400750300101762, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.104. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "71c829e1-9901-450a-854c-521cb42f96a4", "metadata": {"aucs": [0.9857474608067893, 0.7493647966142432, 0.7851128326094963], "final_y": [0.0, 2.0825774069694135e-07, 6.045386610470593e-08]}, "mutation_prompt": null}
{"id": "faf8ca1f-af29-48a1-bacd-b13948f60562", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Reduced step size for gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Fine-tuning gradient estimation by reducing step size for improved accuracy.", "configspace": "", "generation": 7, "fitness": 0.8342518027521352, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.834 with standard deviation 0.108. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "71c829e1-9901-450a-854c-521cb42f96a4", "metadata": {"aucs": [0.9862586772714715, 0.7508182401580525, 0.7656784908268818], "final_y": [0.0, 2.0047313620819735e-07, 7.87655777868057e-08]}, "mutation_prompt": null}
{"id": "9fc38f77-4942-4e51-a2c9-c9caa15e397c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon * 2  # Dynamically adapting perturbation size\n                x2[i] -= epsilon * 2  # Dynamically adapting perturbation size\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced gradient step adjustment by dynamically adapting perturbation size to improve convergence accuracy.", "configspace": "", "generation": 7, "fitness": 0.7277946701333775, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.728 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de38050b-fa99-49ed-ade9-1d33628ff2cf", "metadata": {"aucs": [0.7333171929345004, 0.7336796008781405, 0.7163872165874918], "final_y": [2.653511479892357e-07, 2.164920858345039e-07, 3.4259105960336203e-07]}, "mutation_prompt": null}
{"id": "895a94ac-50bd-4269-9012-ce44d32532a3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 15)  # Adjusted line: More initial samples\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (0.5 + norm(gradient))  # Adjusted line: Ensure stable step size\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with improved initial sample allocation for better convergence.", "configspace": "", "generation": 7, "fitness": 0.761498505187913, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.761 with standard deviation 0.004. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de38050b-fa99-49ed-ade9-1d33628ff2cf", "metadata": {"aucs": [0.7599937163002244, 0.767506693943772, 0.7569951053197423], "final_y": [1.6440825242499083e-07, 1.0035584862657546e-07, 1.4977560079892651e-07]}, "mutation_prompt": null}
{"id": "d7e72371-c80f-4b35-8ee5-32270e2b9117", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.3 / (1.0 + norm(gradient))  # Adjusted step size for enhanced convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced Adaptive Gradient Sampling with refined step-size adjustment for improved convergence speed and accuracy.", "configspace": "", "generation": 7, "fitness": 0.796854265078648, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.038. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de38050b-fa99-49ed-ade9-1d33628ff2cf", "metadata": {"aucs": [0.8038562470509838, 0.7473507461954163, 0.8393558019895435], "final_y": [5.090609842233525e-08, 1.2016936704157376e-07, 4.519556081064603e-10]}, "mutation_prompt": null}
{"id": "df1cb4af-8a93-4e0e-a030-18c34b8a22e3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.4 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (4, self.dim))  # Increased dynamic samples\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced convergence with dynamic step size adjustment and refined sampling strategy.", "configspace": "", "generation": 7, "fitness": 0.7965341646570522, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de38050b-fa99-49ed-ade9-1d33628ff2cf", "metadata": {"aucs": [0.8097099361572309, 0.8077617872268752, 0.7721307705870506], "final_y": [2.7112428548063916e-08, 5.223947049560212e-08, 7.116708356953401e-08]}, "mutation_prompt": null}
{"id": "f8f05d18-4eff-4b7e-8fea-1af6e78eb7de", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 15)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n                    \n            # Re-evaluate the best sample to enhance robustness\n            best_sample = new_sample if limited_func(new_sample) < limited_func(best_sample) else best_sample\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improve adaptive sampling by re-evaluating the best solution periodically for increased robustness.", "configspace": "", "generation": 7, "fitness": 0.7593204979196431, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.759 with standard deviation 0.036. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de38050b-fa99-49ed-ade9-1d33628ff2cf", "metadata": {"aucs": [0.7854936646303476, 0.783805921320459, 0.7086619078081227], "final_y": [5.235801800839856e-08, 3.64348016262714e-08, 3.7751264089431125e-07]}, "mutation_prompt": null}
{"id": "41657bb3-8020-4d3d-a168-0a76710e2f7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(5):  # Reduced adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.6 / (1.0 + 0.5 * norm(gradient))  # Adjusted step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced adaptive gradient sampling with dynamic step size and iteration adjustments.", "configspace": "", "generation": 7, "fitness": 0.7503556088747555, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.750 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "71c829e1-9901-450a-854c-521cb42f96a4", "metadata": {"aucs": [0.7638476614902784, 0.7552721556642411, 0.7319470094697469], "final_y": [1.3595385462629981e-07, 1.024724492257352e-07, 2.1745968939492792e-07]}, "mutation_prompt": null}
{"id": "4bba0032-b391-42aa-97e6-d22f361168ae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation (Change 1)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(5):  # Increased adaptive iterations (Change 2)\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.3 / (1.0 + norm(gradient))  # Adjusted step size for faster convergence (Change 3)\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced initial sampling and adaptive gradient update strategy for improved convergence.", "configspace": "", "generation": 7, "fitness": 0.7729621628903868, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "de38050b-fa99-49ed-ade9-1d33628ff2cf", "metadata": {"aucs": [0.770355166143043, 0.7937520416964416, 0.754779280831676], "final_y": [7.060324866560823e-08, 5.68066966286902e-08, 1.3044163463439877e-07]}, "mutation_prompt": null}
{"id": "0ba29b87-b46c-41e9-b65e-e71400fdbf3a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 2, 25)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.8 / (1.0 + 0.3 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (4, self.dim))  # Increased sample size\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introduced dynamic sampling and adaptive gradient scaling to better exploit the smooth landscape.", "configspace": "", "generation": 8, "fitness": 0.8485196619717655, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.099. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37675c4d-478c-4e62-9996-ed3d589d2763", "metadata": {"aucs": [0.9810091195648313, 0.744944112541878, 0.8196057538085872], "final_y": [0.0, 7.083850361192473e-08, 1.5626085659873823e-09]}, "mutation_prompt": null}
{"id": "af78fbcc-769c-4b46-98e4-4cf885a28962", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + norm(x))):  # Adaptive epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introduce adaptive epsilon in gradient estimation to improve precision and robustness in convergence.", "configspace": "", "generation": 8, "fitness": 0.8355955793972026, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.113. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "metadata": {"aucs": [0.9810091195648313, 0.7069815410021385, 0.8187960776246382], "final_y": [0.0, 1.5081174043809296e-07, 1.5626085659873823e-09]}, "mutation_prompt": null}
{"id": "4099d681-4223-48c2-a8c8-9d004b7df623", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 30)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = min(0.7 / (1.0 + 0.5 * norm(gradient)), 0.1)  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (5, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improved adaptive gradient sampling by expanding local search and dynamic learning rate adjustment.", "configspace": "", "generation": 8, "fitness": 0.814276217114427, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.115. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "37675c4d-478c-4e62-9996-ed3d589d2763", "metadata": {"aucs": [0.9771775373075378, 0.7343901469229063, 0.7312609671128374], "final_y": [0.0, 1.121236310213639e-07, 2.513750928561522e-07]}, "mutation_prompt": null}
{"id": "7630b505-7eff-48b6-a445-c577ed8ad940", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 25)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            adaptive_epsilon = max(epsilon * 0.5, 1e-6)  # Dynamic epsilon adjustment\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(3):  # Reduced adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Optimized adaptive strategy with dynamic sample allocation and enhanced gradient estimation for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.7299158938080531, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.730 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "metadata": {"aucs": [0.7206698144324367, 0.733558495770166, 0.7355193712215565], "final_y": [2.898818097091489e-07, 1.1762620358523274e-07, 1.2560350965530835e-07]}, "mutation_prompt": null}
{"id": "9bd23e61-55b9-4813-8994-3c1116a37e4c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon * (1 + np.abs(x[i]))  # Changed to adaptive epsilon\n                x2[i] -= epsilon * (1 + np.abs(x[i]))\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introduce adaptive gradient estimation with dynamically adjusted epsilon for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.752321864522976, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.752 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "metadata": {"aucs": [0.7523156332687644, 0.7733889931873261, 0.7312609671128374], "final_y": [2.325480062149474e-07, 4.59061838927756e-08, 2.513750928561522e-07]}, "mutation_prompt": null}
{"id": "6ca10485-f89e-463c-aff7-91cdeeafe6db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Changed epsilon for finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "AdaptiveGradientSampling with improved gradient estimation and dynamic sample adjustment.", "configspace": "", "generation": 8, "fitness": 0.7625969528530062, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.763 with standard deviation 0.040. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "metadata": {"aucs": [0.7356072004274332, 0.7329062879071028, 0.8192773702244825], "final_y": [2.653511479892357e-07, 1.1024263255781084e-07, 1.5626085659873823e-09]}, "mutation_prompt": null}
{"id": "eb8bae27-e032-4530-9e3a-b24439d82c10", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Finer gradient estimation\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + 0.5 * norm(gradient))  # Adjusted step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced convergence strategy by fine-tuning the gradient estimation and local search parameters.", "configspace": "", "generation": 8, "fitness": 0.7808608308349703, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.037. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "metadata": {"aucs": [0.7887911056280872, 0.7324202441224549, 0.8213711427543688], "final_y": [5.934334601503659e-08, 1.2016936704157376e-07, 4.519556081064603e-10]}, "mutation_prompt": null}
{"id": "99ca7f1f-7fe9-4ab5-934f-8252737e333f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.3 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refine adaptive sampling strategy by adjusting step size formula for improved convergence speed.", "configspace": "", "generation": 8, "fitness": 0.7429449926263224, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.743 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "metadata": {"aucs": [0.7496554883741537, 0.7611127221735752, 0.718066767331238], "final_y": [8.92850451632848e-08, 6.509651318254288e-08, 3.1100145412409763e-07]}, "mutation_prompt": null}
{"id": "6d9fd49f-2f4e-494e-b7eb-cfe6c2c372dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.85 / (1.0 + 0.5 * norm(gradient))  # Enhanced step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced adaptive step size strategy to improve convergence.", "configspace": "", "generation": 8, "fitness": 0.7819962938612361, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.039. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "metadata": {"aucs": [0.7652799037471802, 0.7448906437632674, 0.8358183340732609], "final_y": [7.290788347961648e-08, 7.083850361192473e-08, 3.1270620121274835e-09]}, "mutation_prompt": null}
{"id": "cea08a07-1900-472d-9091-76b196172dea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Reduced epsilon for better sensitivity\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improved gradient estimation accuracy by reducing epsilon for better sensitivity in parameter space.", "configspace": "", "generation": 8, "fitness": 0.7728092398560062, "feedback": "The algorithm AdaptiveGradientSampling got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.773 with standard deviation 0.035. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ba124ca-9f0e-4c3d-8a65-e4824469e97c", "metadata": {"aucs": [0.8213957552886113, 0.7434626883185118, 0.7535692759608956], "final_y": [3.62030418762678e-08, 1.138724703375935e-07, 9.568575106641078e-08]}, "mutation_prompt": null}
{"id": "f2959c8a-cbb2-461d-81b7-ac6d15cebea0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 3, 25)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + norm(x))): \n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introduced dynamic step size adjustment and additional local search phases to enhance convergence precision and robustness.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "af78fbcc-769c-4b46-98e4-4cf885a28962", "metadata": {}, "mutation_prompt": null}
{"id": "a6bd659f-fd72-4146-abbb-eec9f0651591", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6 * (1 + norm(x))):  # Reduced epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < new_eval:  # Changed from func(best_sample)\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced gradient estimation precision with adaptive epsilon and improved local search selection criterion.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "af78fbcc-769c-4b46-98e4-4cf885a28962", "metadata": {}, "mutation_prompt": null}
{"id": "2a1bac2d-f056-4296-8bdd-3f235c2acf70", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + norm(x))):  # Adaptive epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        damping_factor = 0.85  # Introduced damping factor\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * damping_factor * gradient  # Applied damping factor\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introduce adaptive step size damping in gradient descent to enhance convergence stability and accuracy.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "af78fbcc-769c-4b46-98e4-4cf885a28962", "metadata": {}, "mutation_prompt": null}
{"id": "af71585a-f2e6-41df-a51c-f186b69f6726", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 2, 25)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6):  # Adjusted epsilon value for better precision\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.8 / (1.0 + 0.3 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (4, self.dim))  # Increased sample size\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improved the gradient estimation precision by adjusting epsilon in the estimate_gradient function.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "0ba29b87-b46c-41e9-b65e-e71400fdbf3a", "metadata": {}, "mutation_prompt": null}
{"id": "896766e9-ed73-4965-96b7-e2dfba26542c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 2, 25)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.9 / (1.0 + 0.25 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (4, self.dim))  # Increased sample size\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced the convergence strategy by adjusting the gradient step size for more precise exploitation in smooth landscapes.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "0ba29b87-b46c-41e9-b65e-e71400fdbf3a", "metadata": {}, "mutation_prompt": null}
{"id": "ee17fd1f-3cc5-4b85-83e3-8a9127f669ab", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + norm(x))):  # Adaptive epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.3 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introduced adaptive step size scaling based on evaluation progress to refine convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "af78fbcc-769c-4b46-98e4-4cf885a28962", "metadata": {}, "mutation_prompt": null}
{"id": "800a0e9b-9e7a-491b-aad0-aa4d5d0419c3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + norm(x))):  # Adaptive epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + 0.3 * norm(gradient))  # Adjusted step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced gradient estimation with adaptive learning rate for improved convergence in black-box optimization.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "af78fbcc-769c-4b46-98e4-4cf885a28962", "metadata": {}, "mutation_prompt": null}
{"id": "6d21f967-24cf-4475-8275-56c8ac856ead", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 2, 25)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            dynamic_epsilon = epsilon * (1.0 + 0.1 * norm(x))  # Dynamic epsilon scaling added\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += dynamic_epsilon\n                x2[i] -= dynamic_epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * dynamic_epsilon)\n            return grad\n        \n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.8 / (1.0 + 0.3 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (4, self.dim))  # Increased sample size\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introduced dynamic epsilon scaling to enhance precision in gradient estimation for improved convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "0ba29b87-b46c-41e9-b65e-e71400fdbf3a", "metadata": {}, "mutation_prompt": null}
{"id": "d33c714d-dce7-4c1c-b9da-eed1692957d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 3, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + np.sqrt(norm(x)))):  # Adaptive epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.3 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhance the adaptive gradient sampling strategy by refining the gradient estimation step and introducing a dynamic budget allocation for improved convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "af78fbcc-769c-4b46-98e4-4cf885a28962", "metadata": {}, "mutation_prompt": null}
{"id": "64d686c1-472d-4ad5-9894-eb7e4ae65004", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 2, 25)\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5):\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(5):\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.2 * norm(gradient))  # Changed step-size coefficients\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            \n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 4:  # More frequent local exploration\n                samples = np.random.uniform(lb, ub, (5, self.dim))  # Modified sample size\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Incorporate adaptive step-size tuning and selective local search to enhance exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "0ba29b87-b46c-41e9-b65e-e71400fdbf3a", "metadata": {}, "mutation_prompt": null}
{"id": "7ffae388-e4f9-40d8-9899-1b274b905b89", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 3, 25)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + norm(x))): \n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < limited_func(best_sample):  # Fixed line\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < limited_func(best_sample):  # Fixed line\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced convergence by fixing a bug in the gradient estimation and refining local search criterion.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "f2959c8a-cbb2-461d-81b7-ac6d15cebea0", "metadata": {}, "mutation_prompt": null}
{"id": "2fcea4d0-c26b-473e-b8e4-11b6e9569ff3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6 * (1 + norm(x))):  # Reduced epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < limited_func(best_sample):  # Changed this line for correction\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < new_eval:  # Changed from func(best_sample)\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced adaptive sampling logic by modifying the gradient estimation with more accurate central differencing.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "a6bd659f-fd72-4146-abbb-eec9f0651591", "metadata": {}, "mutation_prompt": null}
{"id": "e70ae6c5-f510-424d-8af7-f58f46f8a80b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6 * (1 + norm(x))):  # Reduced epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < new_eval:  # Changed from func(best_sample)\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Introduced error handling for function call to prevent premature termination when budget is exceeded.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "a6bd659f-fd72-4146-abbb-eec9f0651591", "metadata": {}, "mutation_prompt": null}
{"id": "16650513-8480-4783-bccc-e7fb99e3eae5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6 * (1 + norm(x))):  # Reduced epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < new_eval:  # Changed from func(best_sample)\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Refined handling of limited_func calls to avoid exceeding the budget during adaptive sampling.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "a6bd659f-fd72-4146-abbb-eec9f0651591", "metadata": {}, "mutation_prompt": null}
{"id": "69b32a32-c492-43d0-9990-1ef3b8fee565", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6 * max(1, norm(x))):  # Reduced epsilon and fixed NameError\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < new_eval:  # Changed from func(best_sample)\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Improved gradient estimation precision through minor refinement in epsilon calculation and fixed NameError.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "a6bd659f-fd72-4146-abbb-eec9f0651591", "metadata": {}, "mutation_prompt": null}
{"id": "8d2194be-2de3-42f0-923b-9a9a7b84d81a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6 * (1 + norm(x))):  # Reduced epsilon\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + 0.1 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < limited_func(best_sample):  # Fixed function reference\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < new_eval:\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced gradient robustness via refined step size scaling and precise gradient estimation.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "a6bd659f-fd72-4146-abbb-eec9f0651591", "metadata": {}, "mutation_prompt": null}
{"id": "2482bfa9-4e24-48e6-9f1f-b720d5a37b96", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Extract bounds\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        # Step 1: Initial Sampling with expanded exploration\n        num_samples = min(self.budget // 2, 20)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        # Find the best initial sample\n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        # Step 2: Gradient Estimation and Adaptive Sampling\n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-6 * (1 + norm(np.array(x)))):  # Properly initialized x\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        # Adaptive sampling loop with gradient guidance\n        for _ in range(4):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + 0.5 * norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            # Dynamic adjustment of sampling based on current progress\n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < new_eval:  # Changed from func(best_sample)\n                    best_sample = samples[best_idx]\n        \n        # Step 3: Final Local Optimization using BFGS\n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced gradient estimation with proper initialization to prevent undefined variable errors.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "a6bd659f-fd72-4146-abbb-eec9f0651591", "metadata": {}, "mutation_prompt": null}
{"id": "bdc5e427-a2ac-494c-988f-43ae2d51701e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 3, 25)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + norm(x))): \n            grad = np.zeros_like(x)  # Changed from np.zeros(self.dim) to np.zeros_like(x)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.7 / (1.0 + norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced computation stability by correcting gradient estimation initialization.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "f2959c8a-cbb2-461d-81b7-ac6d15cebea0", "metadata": {}, "mutation_prompt": null}
{"id": "85cd25a3-d022-41b9-a008-4e35c7eaec99", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 3, 25)  # Adjusted initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=1e-5 * (1 + norm(x))): \n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n\n        for _ in range(5):  # Increased adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.5 / (1.0 + norm(gradient))  # Refined step size calculation\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (3, self.dim))\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced convergence by refining step size adaptation and resolving gradient estimation error.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "f2959c8a-cbb2-461d-81b7-ac6d15cebea0", "metadata": {}, "mutation_prompt": null}
{"id": "5b1b01b7-2176-41e4-8f8e-27e7848e7314", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import norm\n\nclass AdaptiveGradientSampling:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        lb = np.array(func.bounds.lb)\n        ub = np.array(func.bounds.ub)\n        \n        num_samples = min(self.budget // 3, 30)  # Increased initial sample allocation\n        samples = np.random.uniform(lb, ub, (num_samples, self.dim))\n        sample_evals = np.array([func(sample) for sample in samples])\n        \n        best_index = np.argmin(sample_evals)\n        best_sample = samples[best_index]\n        \n        remaining_budget = self.budget - num_samples\n        eval_count = 0\n        \n        def limited_func(x):\n            nonlocal eval_count\n            if eval_count >= remaining_budget:\n                raise ValueError(\"Budget exceeded\")\n            eval_count += 1\n            return func(x)\n        \n        def estimate_gradient(x, epsilon=None):\n            if epsilon is None:\n                epsilon = 1e-5 * (1 + norm(x))  # Made epsilon adaptive based on x\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x1, x2 = np.array(x), np.array(x)\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                grad[i] = (limited_func(x1) - limited_func(x2)) / (2 * epsilon)\n            return grad\n        \n        for _ in range(5):  # Retained adaptive iterations\n            gradient = estimate_gradient(best_sample)\n            step_size = 0.8 / (1.0 + norm(gradient))  # Slightly adjusted step size\n            new_sample = best_sample - step_size * gradient\n            new_sample = np.clip(new_sample, lb, ub)\n            new_eval = limited_func(new_sample)\n            if new_eval < func(best_sample):\n                best_sample = new_sample\n            \n            if eval_count < remaining_budget // 2:\n                samples = np.random.uniform(lb, ub, (4, self.dim))  # Adjusted sample density\n                sample_evals = np.array([limited_func(sample) for sample in samples])\n                best_idx = np.argmin(sample_evals)\n                if sample_evals[best_idx] < func(best_sample):\n                    best_sample = samples[best_idx]\n        \n        result = minimize(limited_func, best_sample, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n        \n        return result.x", "name": "AdaptiveGradientSampling", "description": "Enhanced convergence through adaptive sampling density and refined gradient estimation sensitivity.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'x' is not defined\").", "error": "NameError(\"name 'x' is not defined\")", "parent_id": "f2959c8a-cbb2-461d-81b7-ac6d15cebea0", "metadata": {}, "mutation_prompt": null}
