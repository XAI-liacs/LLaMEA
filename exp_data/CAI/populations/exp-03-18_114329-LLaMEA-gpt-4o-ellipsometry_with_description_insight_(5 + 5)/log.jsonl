{"id": "e8e059b8-74bb-4387-837d-6fa5f2a3e00e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=func.bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "An adaptive local search metaheuristic that starts with random sampling, employs BFGS for efficient local optimization, and iteratively refines bounds based on intermediate solutions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 42, in __call__\n  File \"<string>\", line 34, in bfgs_optimization\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nTypeError: zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds\n.", "error": "TypeError('zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 42, in __call__\n  File \"<string>\", line 34, in bfgs_optimization\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 670, in minimize\n    bounds = standardize_bounds(bounds, x0, 'new')\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 1058, in standardize_bounds\n    lb, ub = old_bound_to_new(bounds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_constraints.py\", line 433, in old_bound_to_new\n    lb, ub = zip(*bounds)\nTypeError: zip() argument after * must be an iterable, not ioh.iohcpp.RealBounds\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "7ab5f14f-a9be-4462-a80b-8c10de3cd582", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        # Initial uniform sampling in parameter space\n        samples = self.initial_sampling(func)\n        best_sample = min(samples, key=lambda x: x[1])\n        \n        # Adaptive boundary refinement\n        bounds = self.adaptive_bounds(func.bounds, best_sample[0])\n\n        # Local optimization using BFGS\n        result = minimize(fun=func, x0=best_sample[0], bounds=bounds, method='L-BFGS-B', options={'maxfun': self.budget - self.evaluations})\n        return result.x, result.fun\n\n    def initial_sampling(self, func):\n        num_initial_samples = min(10, self.budget // 2)\n        lower_bounds = func.bounds.lb\n        upper_bounds = func.bounds.ub\n        samples = []\n        \n        for _ in range(num_initial_samples):\n            sample = np.random.uniform(lower_bounds, upper_bounds, self.dim)\n            value = func(sample)\n            self.evaluations += 1\n            samples.append((sample, value))\n            if self.evaluations >= self.budget:\n                break\n\n        return samples\n\n    def adaptive_bounds(self, bounds, best_sample):\n        lb, ub = bounds.lb, bounds.ub\n        adaptive_lb = np.maximum(lb, best_sample - (ub - lb) * 0.1)\n        adaptive_ub = np.minimum(ub, best_sample + (ub - lb) * 0.1)\n        return list(zip(adaptive_lb, adaptive_ub))", "name": "HybridOptimizer", "description": "An adaptive local-global hybrid optimization algorithm that uses initial uniform sampling followed by dynamic boundary refinement with a local optimizer for efficient convergence on smooth landscapes.", "configspace": "", "generation": 0, "fitness": 0.14022241052548645, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.140 with standard deviation 0.031. And the mean value of best solutions found was 2.277 (0. is the best) with standard deviation 1.701.", "error": "", "parent_id": null, "metadata": {"aucs": [0.15930723476370456, 0.09658011216631268, 0.1647798846464421], "final_y": [1.1433795043051533, 4.681519189034669, 1.0063226291522367]}, "mutation_prompt": null}
{"id": "7e5bd7e3-2581-4f9c-baf4-90a74e6bdeb5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveBFGS:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initialize counters and results\n        remaining_budget = self.budget\n        best_solution = None\n        best_value = float('inf')\n\n        # Uniformly sample the initial guesses in the parameter space\n        num_initial_samples = min(5, self.budget // 10)\n        initial_samples = [np.random.uniform(func.bounds.lb, func.bounds.ub) \n                           for _ in range(num_initial_samples)]\n        \n        # Define the objective function wrapper to track budget\n        def objective_wrapper(x):\n            nonlocal remaining_budget\n            if remaining_budget <= 0:\n                raise RuntimeError(\"Budget exceeded\")\n            remaining_budget -= 1\n            return func(x)\n\n        # Run local optimization from each initial guess\n        for initial_sample in initial_samples:\n            result = minimize(\n                objective_wrapper,\n                initial_sample,\n                method='L-BFGS-B',\n                bounds=list(zip(func.bounds.lb, func.bounds.ub))\n            )\n            \n            # Update best solution if found\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n            \n            # Break if budget is near exhausted\n            if remaining_budget < num_initial_samples:\n                break\n\n        return best_solution", "name": "AdaptiveBFGS", "description": "Adaptive Boundary Constrained BFGS with Uniform Initial Sampling for Fast Convergence in Low-Dimensional Smooth Landscapes", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 30, in __call__\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 738, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py\", line 441, in _minimize_lbfgsb\n    f, g = func_and_grad(x)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 345, in fun_and_grad\n    self._update_grad()\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 307, in _update_grad\n    self.g = self._wrapped_grad(self.x, f0=self.f)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 48, in wrapped1\n    return approx_derivative(\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_numdiff.py\", line 523, in approx_derivative\n    return _dense_difference(fun_wrapped, x0, f0, h,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_numdiff.py\", line 596, in _dense_difference\n    df = fun(x1) - f0\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_numdiff.py\", line 474, in fun_wrapped\n    f = np.atleast_1d(fun(x, *args, **kwargs))\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 21, in wrapped\n    fx = fun(np.copy(x), *args)\n  File \"<string>\", line 24, in objective_wrapper\nRuntimeError: Budget exceeded\n.", "error": "RuntimeError('Budget exceeded')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 30, in __call__\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_minimize.py\", line 738, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py\", line 441, in _minimize_lbfgsb\n    f, g = func_and_grad(x)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 345, in fun_and_grad\n    self._update_grad()\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 307, in _update_grad\n    self.g = self._wrapped_grad(self.x, f0=self.f)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 48, in wrapped1\n    return approx_derivative(\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_numdiff.py\", line 523, in approx_derivative\n    return _dense_difference(fun_wrapped, x0, f0, h,\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_numdiff.py\", line 596, in _dense_difference\n    df = fun(x1) - f0\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_numdiff.py\", line 474, in fun_wrapped\n    f = np.atleast_1d(fun(x, *args, **kwargs))\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py\", line 21, in wrapped\n    fx = fun(np.copy(x), *args)\n  File \"<string>\", line 24, in objective_wrapper\nRuntimeError: Budget exceeded\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "a2467198-4959-4f80-b05b-3b248c2aed21", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Uniform random sampling to generate initial guesses.\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        # Generate initial guesses within the bounds\n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            # Local optimization using BFGS\n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            # Update best solution found\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n\n            # Adjust bounds and constraints iteratively\n            if evaluations < self.budget:\n                new_bounds = [(max(lb[i], res.x[i] - 0.1 * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + 0.1 * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):  # Retry with slightly adjusted bounds\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "A hybrid optimization algorithm combining global random sampling for diverse initial guesses and local BFGS optimization for fast convergence in smooth landscapes.", "configspace": "", "generation": 0, "fitness": 0.7983280060144985, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8125505433387815, 0.7795544283317957, 0.8028790463729181], "final_y": [1.1808147624100802e-07, 1.1219069173832981e-07, 3.500418516416499e-08]}, "mutation_prompt": null}
{"id": "b05c4f67-331a-440c-a23d-5e99a82d4847", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ABEE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        remaining_budget = self.budget\n        \n        # Initialize with uniform random sampling\n        best_solution = None\n        best_value = np.inf\n        \n        for _ in range(remaining_budget // 4):\n            x0 = np.random.uniform(lb, ub)\n            value = func(x0)\n            remaining_budget -= 1\n            \n            if value < best_value:\n                best_value = value\n                best_solution = x0\n\n            if remaining_budget <= 0:\n                break\n        \n        # Local optimization using successful initial guesses\n        def bounded_minimize(x0):\n            nonlocal remaining_budget\n            result = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            remaining_budget -= result.nfev\n            return result.x, result.fun, remaining_budget\n        \n        if best_solution is not None:\n            best_solution, best_value, remaining_budget = bounded_minimize(best_solution)\n        \n        # Iteratively refine bounds and re-optimize\n        while remaining_budget > 0:\n            # Shrink bounds based on the last best solution\n            new_lb = np.maximum(lb, best_solution - (ub - lb) * 0.1)\n            new_ub = np.minimum(ub, best_solution + (ub - lb) * 0.1)\n            \n            x0 = np.random.uniform(new_lb, new_ub)\n            local_solution, local_value = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(new_lb, new_ub))).x, func(x0)\n            remaining_budget -= 1\n            \n            if local_value < best_value:\n                best_value = local_value\n                best_solution = local_solution\n                \n            if remaining_budget <= 0:\n                break\n            \n        return best_solution", "name": "ABEE", "description": "Adaptive Bounded Exploration and Exploitation (ABEE) algorithm blends global search with adaptive local refinement to efficiently converge to optimal solutions within bounded parameter spaces.", "configspace": "", "generation": 0, "fitness": 0.6264152529997068, "feedback": "The algorithm ABEE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.626 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.614844007492616, 0.6307598923268602, 0.6336418591796441], "final_y": [1.0995470067644762e-07, 1.2810359537865436e-07, 5.889131187330439e-08]}, "mutation_prompt": null}
{"id": "b3764099-d06a-4ea7-9fe2-e0ce33159571", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        # Initial uniform sampling in parameter space\n        samples = self.initial_sampling(func)\n        best_sample = min(samples, key=lambda x: x[1])\n        \n        # Adaptive boundary refinement\n        bounds = self.adaptive_bounds(func.bounds, best_sample[0])\n\n        # Local optimization using BFGS\n        result = minimize(fun=func, x0=best_sample[0], bounds=bounds, method='L-BFGS-B', options={'maxfun': self.budget - self.evaluations})\n        return result.x, result.fun\n\n    def initial_sampling(self, func):\n        num_initial_samples = min(15, self.budget // 3)  # Adjusted sample size based on budget\n        lower_bounds = func.bounds.lb\n        upper_bounds = func.bounds.ub\n        samples = []\n        \n        for _ in range(num_initial_samples):\n            sample = np.random.uniform(lower_bounds, upper_bounds, self.dim)\n            value = func(sample)\n            self.evaluations += 1\n            samples.append((sample, value))\n            if self.evaluations >= self.budget:\n                break\n\n        return samples\n\n    def adaptive_bounds(self, bounds, best_sample):\n        lb, ub = bounds.lb, bounds.ub\n        adaptive_lb = np.maximum(lb, best_sample - (ub - lb) * 0.1)\n        adaptive_ub = np.minimum(ub, best_sample + (ub - lb) * 0.1)\n        return list(zip(adaptive_lb, adaptive_ub))", "name": "HybridOptimizer", "description": "Enhanced HybridOptimizer with dynamic sample size adjustment based on budget for improved convergence.", "configspace": "", "generation": 1, "fitness": 0.18045004254766203, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.180 with standard deviation 0.069. And the mean value of best solutions found was 2.239 (0. is the best) with standard deviation 2.837.", "error": "", "parent_id": "7ab5f14f-a9be-4462-a80b-8c10de3cd582", "metadata": {"aucs": [0.08289965423750856, 0.22931035615214923, 0.22914011725332828], "final_y": [6.251319200439111, 0.232722904105426, 0.232722904105426]}, "mutation_prompt": null}
{"id": "05bd7c26-77db-4ec4-a588-947564159a04", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        # Initial uniform sampling in parameter space\n        samples = self.initial_sampling(func)\n        best_sample = min(samples, key=lambda x: x[1])\n        \n        # Adaptive boundary refinement\n        bounds = self.adaptive_bounds(func.bounds, best_sample[0])\n\n        # Local optimization using BFGS with dynamic step size reduction\n        result = minimize(fun=func, x0=best_sample[0], bounds=bounds, method='L-BFGS-B', options={'maxfun': self.budget - self.evaluations, 'ftol': 1e-9})\n        return result.x, result.fun\n\n    def initial_sampling(self, func):\n        num_initial_samples = min(10, self.budget // 2)\n        lower_bounds = func.bounds.lb\n        upper_bounds = func.bounds.ub\n        samples = []\n        \n        for _ in range(num_initial_samples):\n            sample = np.random.uniform(lower_bounds, upper_bounds, self.dim)\n            value = func(sample)\n            self.evaluations += 1\n            samples.append((sample, value))\n            if self.evaluations >= self.budget:\n                break\n\n        return samples\n\n    def adaptive_bounds(self, bounds, best_sample):\n        lb, ub = bounds.lb, bounds.ub\n        adaptive_lb = np.maximum(lb, best_sample - (ub - lb) * 0.1)\n        adaptive_ub = np.minimum(ub, best_sample + (ub - lb) * 0.1)\n        return list(zip(adaptive_lb, adaptive_ub))", "name": "HybridOptimizer", "description": "Enhanced HybridOptimizer with dynamic reduction of local optimizer step size for improved convergence accuracy.", "configspace": "", "generation": 1, "fitness": 0.15073657865222714, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.151 with standard deviation 0.034. And the mean value of best solutions found was 1.710 (0. is the best) with standard deviation 0.904.", "error": "", "parent_id": "7ab5f14f-a9be-4462-a80b-8c10de3cd582", "metadata": {"aucs": [0.198307286662601, 0.13100957835570692, 0.12289287093837353], "final_y": [0.4524213426795858, 2.138952023108762, 2.539021453771765]}, "mutation_prompt": null}
{"id": "95f714a2-614d-4244-a93b-9dd4a10ac72d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Uniform random sampling to generate initial guesses.\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        # Generate initial guesses within the bounds\n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            # Local optimization using BFGS\n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            # Update best solution found\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n\n            # Adjust bounds and constraints iteratively\n            if evaluations < self.budget:\n                factor = 0.2  # Changed from 0.1 to 0.2 for deeper exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):  # Retry with slightly adjusted bounds\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Dynamic Bound Adjustment for Fast Convergence.", "configspace": "", "generation": 1, "fitness": 0.823612742310572, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a2467198-4959-4f80-b05b-3b248c2aed21", "metadata": {"aucs": [0.8659652440458416, 0.8105542569452544, 0.7943187259406201], "final_y": [0.0, 9.901441631642744e-08, 8.143018959401186e-08]}, "mutation_prompt": null}
{"id": "623b8737-4685-4dca-80d4-64b2ff7e5df1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Uniform random sampling to generate more initial guesses for robustness.\n        num_initial_guesses = min(20, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        # Generate initial guesses within the bounds\n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            # Local optimization using BFGS\n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            # Update best solution found\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n\n            # Adjust bounds and constraints iteratively\n            if evaluations < self.budget:\n                new_bounds = [(max(lb[i], res.x[i] - 0.1 * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + 0.1 * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):  # Retry with slightly adjusted bounds\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced HybridOptimization with improved initial sampling strategy for better convergence in smooth parameter spaces.", "configspace": "", "generation": 1, "fitness": 0.8094791505623307, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a2467198-4959-4f80-b05b-3b248c2aed21", "metadata": {"aucs": [0.8445559656129592, 0.7833026853999062, 0.8005788006741267], "final_y": [0.0, 1.1931728370589841e-07, 4.7424236176700844e-08]}, "mutation_prompt": null}
{"id": "316c4371-3567-4a09-8a11-a4784497d592", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Corrected Bounds Handling to Prevent TypeErrors.", "configspace": "", "generation": 1, "fitness": 0.8516253897164908, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.102. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "e8e059b8-74bb-4387-837d-6fa5f2a3e00e", "metadata": {"aucs": [0.9955172457207834, 0.7801979675967982, 0.7791609558318907], "final_y": [0.0, 1.096084083878065e-08, 4.8325529786531765e-09]}, "mutation_prompt": null}
{"id": "4b9e3c5f-4f98-4aa1-97ee-1871b97c208c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ABEE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        remaining_budget = self.budget\n        \n        # Initialize with uniform random sampling\n        best_solution = None\n        best_value = np.inf\n        \n        for _ in range(remaining_budget // 4):\n            x0 = np.random.uniform(lb, ub)\n            value = func(x0)\n            remaining_budget -= 1\n            \n            if value < best_value:\n                best_value = value\n                best_solution = x0\n\n            if remaining_budget <= 0:\n                break\n        \n        # Local optimization using successful initial guesses\n        def bounded_minimize(x0):\n            nonlocal remaining_budget\n            result = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            remaining_budget -= result.nfev\n            return result.x, result.fun, remaining_budget\n        \n        if best_solution is not None:\n            best_solution, best_value, remaining_budget = bounded_minimize(best_solution)\n        \n        # Iteratively refine bounds and re-optimize\n        while remaining_budget > 0:\n            # Shrink bounds based on the last best solution\n            new_lb = np.maximum(lb, best_solution - (ub - lb) * 0.2)  # Increased bound adjustment factor\n            new_ub = np.minimum(ub, best_solution + (ub - lb) * 0.2)  # Increased bound adjustment factor\n            \n            x0 = np.random.uniform(new_lb, new_ub)\n            local_solution, local_value = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(new_lb, new_ub))).x, func(x0)\n            remaining_budget -= 1\n            \n            if local_value < best_value:\n                best_value = local_value\n                best_solution = local_solution\n                \n            if remaining_budget <= 0:\n                break\n            \n        return best_solution", "name": "ABEE", "description": "Enhanced ABEE with adaptive sampling range adjustment to improve exploration efficiency.", "configspace": "", "generation": 2, "fitness": 0.8082477360946251, "feedback": "The algorithm ABEE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.126. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b05c4f67-331a-440c-a23d-5e99a82d4847", "metadata": {"aucs": [0.9857656665238889, 0.724929780886623, 0.7140477608733635], "final_y": [0.0, 3.715904322455421e-10, 2.4586876497799555e-09]}, "mutation_prompt": null}
{"id": "d4ae0f4e-3944-4857-8b74-760eac13946b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 5)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with dynamic initial sampling size based on optimization progress for improved convergence.", "configspace": "", "generation": 2, "fitness": 0.80298259323467, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.139. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.9990689392237291, 0.7120222139096609, 0.6978566265706199], "final_y": [0.0, 2.2590957699094864e-08, 8.36507887818505e-09]}, "mutation_prompt": null}
{"id": "8dd1b1dc-d4bf-4cae-b122-1ca4c97310f6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.3  # Changed from 0.2 to 0.3 to enhance exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Refined Hybrid Optimization with Adaptive Restart Mechanism for Enhanced Convergence.", "configspace": "", "generation": 2, "fitness": 0.8402771458578476, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "95f714a2-614d-4244-a93b-9dd4a10ac72d", "metadata": {"aucs": [0.8619271014186344, 0.8294521680774543, 0.8294521680774543], "final_y": [2.2242347081261207e-08, 2.6570457452620614e-08, 2.6570457452620614e-08]}, "mutation_prompt": null}
{"id": "dc11ba76-b708-429f-a179-ef8593b3b0fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Use Sobol sequence for initial sampling to enhance diversity.\n        num_initial_guesses = min(20, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        # Generate initial guesses within the bounds using Sobol sequence\n        sobol = qmc.Sobol(d=self.dim, scramble=False)\n        initial_guesses = qmc.scale(sobol.random(num_initial_guesses), lb, ub)\n        evaluations = 0\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            # Local optimization using BFGS\n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            # Update best solution found\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n\n            # Adjust bounds and constraints iteratively\n            if evaluations < self.budget:\n                new_bounds = [(max(lb[i], res.x[i] - 0.1 * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + 0.1 * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):  # Retry with slightly adjusted bounds\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "The algorithm enhances initial sampling diversity by incorporating Sobol sequence sampling to cover the parameter space more uniformly.", "configspace": "", "generation": 2, "fitness": 0.8004826243991924, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.000. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "623b8737-4685-4dca-80d4-64b2ff7e5df1", "metadata": {"aucs": [0.8004826243991924, 0.8004826243991924, 0.8004826243991924], "final_y": [6.355467513296881e-08, 6.355467513296881e-08, 6.355467513296881e-08]}, "mutation_prompt": null}
{"id": "c9cf47e9-b0c1-495c-aa99-33a77ddf6b1e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Uniform random sampling to generate more initial guesses for robustness.\n        num_initial_guesses = min(20, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        # Generate initial guesses within the bounds\n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            # Local optimization using BFGS\n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            # Update best solution found\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n\n            # Adjust bounds and constraints iteratively\n            if evaluations < self.budget:\n                new_bounds = [(max(lb[i], res.x[i] - 0.05 * (ub[i] - lb[i])),  # Changed from 0.1 to 0.05\n                               min(ub[i], res.x[i] + 0.05 * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):  # Retry with slightly adjusted bounds\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced initial sampling strategy with refined bounds adjustment for improved convergence in smooth landscapes.", "configspace": "", "generation": 2, "fitness": 0.7788453675082184, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.779 with standard deviation 0.000. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "623b8737-4685-4dca-80d4-64b2ff7e5df1", "metadata": {"aucs": [0.7788453675082184, 0.7788453675082184, 0.7788453675082184], "final_y": [1.7017752243087207e-07, 1.7017752243087207e-07, 1.7017752243087207e-07]}, "mutation_prompt": null}
{"id": "d6669874-414e-4d17-a189-086e29651f13", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6  # Line added for early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                # Early stopping if improvement is below threshold\n                if best_value < early_stop_threshold:  # Changed line\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.35  # Changed from 0.3 to 0.35 to enhance exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Optimized Hybrid Optimization with Early Stopping and Enhanced Sampling for Fast Convergence.", "configspace": "", "generation": 3, "fitness": 0.8657729798899386, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8dd1b1dc-d4bf-4cae-b122-1ca4c97310f6", "metadata": {"aucs": [0.8814576916902557, 0.8657763883098907, 0.8500848596696695], "final_y": [1.4819192387255872e-08, 1.555749536610201e-08, 3.315673871224911e-08]}, "mutation_prompt": null}
{"id": "274070d8-563f-4d5b-9569-44ace4d8d392", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ABEE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        remaining_budget = self.budget\n        \n        # Initialize with uniform random sampling\n        best_solution = None\n        best_value = np.inf\n        \n        for _ in range(remaining_budget // 4):\n            x0 = np.random.uniform(lb, ub)\n            value = func(x0)\n            remaining_budget -= 1\n            \n            if value < best_value:\n                best_value = value\n                best_solution = x0\n\n            if remaining_budget <= 0:\n                break\n        \n        # Local optimization using successful initial guesses\n        def bounded_minimize(x0):\n            nonlocal remaining_budget\n            result = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            remaining_budget -= result.nfev\n            return result.x, result.fun, remaining_budget\n        \n        if best_solution is not None:\n            best_solution, best_value, remaining_budget = bounded_minimize(best_solution)\n        \n        # Iteratively refine bounds and re-optimize\n        while remaining_budget > 0:\n            # Shrink bounds based on the last best solution\n            new_lb = np.maximum(lb, best_solution - (ub - lb) * 0.1)  # Adjusted bound adjustment factor\n            new_ub = np.minimum(ub, best_solution + (ub - lb) * 0.1)  # Adjusted bound adjustment factor\n            \n            x0 = np.random.uniform(new_lb, new_ub)\n            local_solution, local_value = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(new_lb, new_ub))).x, func(x0)\n            remaining_budget -= 1\n            \n            if local_value < best_value:\n                best_value = local_value\n                best_solution = local_solution\n                \n            if remaining_budget <= 0:\n                break\n            \n        return best_solution", "name": "ABEE", "description": "The algorithm ABEE is refined by optimizing initial guess selection, enhancing convergence efficiency.", "configspace": "", "generation": 3, "fitness": 0.6884892877399514, "feedback": "The algorithm ABEE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.688 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b9e3c5f-4f98-4aa1-97ee-1871b97c208c", "metadata": {"aucs": [0.6801287296118874, 0.6632231222368156, 0.7221160113711513], "final_y": [5.695967225182432e-08, 2.1298915120088828e-08, 2.4586876497799555e-09]}, "mutation_prompt": null}
{"id": "d612a13c-5190-46eb-b3b6-044a3c82d4c7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(8, self.budget // 8)  # Adjusted number of initial points\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.12 * (ub - lb))  # Adjusted bound refinement factor\n            new_ub = np.minimum(ub, best_solution + 0.12 * (ub - lb))  # Adjusted bound refinement factor\n            lb, ub = new_lb, new_ub\n            \n            # Strategic restart: weighted sampling towards best solution\n            restart_point = np.random.uniform(lb, ub, size=self.dim) * 0.9 + best_solution * 0.1\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Strategic Restart Points and Improved Initial Sampling.", "configspace": "", "generation": 3, "fitness": 0.814773110330895, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.127. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.9943767868584872, 0.729862342488578, 0.7200802016456196], "final_y": [0.0, 6.061603293290003e-08, 8.16759762819262e-08]}, "mutation_prompt": null}
{"id": "29857486-8826-48d6-999a-e9b599f4ddec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ABEE:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        remaining_budget = self.budget\n        \n        # Initialize with uniform random sampling\n        best_solution = None\n        best_value = np.inf\n        \n        for _ in range(remaining_budget // 4):\n            x0 = np.random.uniform(lb, ub)\n            value = func(x0)\n            remaining_budget -= 1\n            \n            if value < best_value:\n                best_value = value\n                best_solution = x0\n\n            if remaining_budget <= 0:\n                break\n        \n        # Local optimization using successful initial guesses\n        def bounded_minimize(x0):\n            nonlocal remaining_budget\n            result = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(lb, ub)))\n            remaining_budget -= result.nfev\n            return result.x, result.fun, remaining_budget\n        \n        if best_solution is not None:\n            best_solution, best_value, remaining_budget = bounded_minimize(best_solution)\n        \n        # Iteratively refine bounds and re-optimize\n        while remaining_budget > 0:\n            # Shrink bounds based on the last best solution\n            new_lb = np.maximum(lb, best_solution - (ub - lb) * 0.1)  # Reduced bound adjustment factor\n            new_ub = np.minimum(ub, best_solution + (ub - lb) * 0.1)  # Reduced bound adjustment factor\n            \n            x0 = np.random.uniform(new_lb, new_ub)\n            local_solution, local_value = minimize(func, x0, method='L-BFGS-B', bounds=list(zip(new_lb, new_ub))).x, func(x0)\n            remaining_budget -= 1\n            \n            if local_value < best_value:\n                best_value = local_value\n                best_solution = local_solution\n                \n            if remaining_budget <= 0:\n                break\n            \n        return best_solution", "name": "ABEE", "description": "Enhanced ABEE with a strategic reduction of the bound adjustment factor to improve local exploitation efficiency.", "configspace": "", "generation": 3, "fitness": 0.660900125193823, "feedback": "The algorithm ABEE got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.661 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b9e3c5f-4f98-4aa1-97ee-1871b97c208c", "metadata": {"aucs": [0.6612159345850812, 0.6383820557031312, 0.6831023852932567], "final_y": [5.148537894325949e-09, 3.8285157905576095e-08, 3.504584141260517e-09]}, "mutation_prompt": null}
{"id": "37dbec9e-9669-42a2-8dc2-ecd7e6e804e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        # Uniform random sampling to generate more initial guesses for robustness.\n        num_initial_guesses = min(30, self.budget // 2)  # Increased number of initial guesses\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        # Generate initial guesses within the bounds\n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            # Local optimization using BFGS\n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            # Update best solution found\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n\n            # Adjust bounds and constraints iteratively\n            if evaluations < self.budget:\n                new_bounds = [(max(lb[i], res.x[i] - 0.1 * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + 0.1 * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):  # Retry with slightly adjusted bounds\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Refinement of initial sampling and adaptive restart for improved convergence.", "configspace": "", "generation": 3, "fitness": 0.8100302341212592, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "623b8737-4685-4dca-80d4-64b2ff7e5df1", "metadata": {"aucs": [0.845734262550532, 0.783777639139119, 0.8005788006741267], "final_y": [0.0, 1.1931728370589841e-07, 4.7424236176700844e-08]}, "mutation_prompt": null}
{"id": "979f8ecd-20bc-4815-8c68-fa0cac69ab2d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-5  # Adjusted for quicker early stopping\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='BFGS', bounds=zip(lb, ub))  # Switched method for potential improvement\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.3  # Adjusted back to original for more balanced exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                step_adjustment = 0.15  # Added an adjustment factor for adaptive steps\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x + step_adjustment * (np.random.rand(self.dim) - 0.5),  # Adjust initial point\n                                   method='BFGS', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Optimized Hybrid Optimization with Enhanced Sampling and Dynamic Adaptive Steps for Swift Convergence.", "configspace": "", "generation": 4, "fitness": 0.846834729066746, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8259307996674063, 0.8930468477252496, 0.8215265398075822], "final_y": [1.4322417009922276e-07, 1.2976952355306787e-08, 4.163179477783078e-08]}, "mutation_prompt": null}
{"id": "725988e0-fb4d-4617-a397-bd35db6c47ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Changed line for improved early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.4  # Changed line for refined exploration factor\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Improved Hybrid Optimization with Enhanced Early Stopping and Refined Exploration Factor for Fast Convergence.", "configspace": "", "generation": 4, "fitness": 0.8503890793037464, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8365938503784073, 0.8930468477252496, 0.8215265398075822], "final_y": [6.785416154341331e-08, 1.2976952355306787e-08, 4.163179477783078e-08]}, "mutation_prompt": null}
{"id": "34cee8c5-ead7-4b1e-b3af-ef2c5c987ac2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased points\n        num_initial_points = max(10, self.budget // 10)  # Change line: increased initial points\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Improved Initial Sampling Density for Better Convergence.", "configspace": "", "generation": 4, "fitness": 0.7595684888645566, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.760 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.7267380119189635, 0.8007814676461745, 0.751185987028532], "final_y": [1.2287771809541814e-07, 2.5328109178009002e-08, 3.388467768681558e-08]}, "mutation_prompt": null}
{"id": "3fef5fe2-1915-4a0a-82eb-9e48b1235a79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            remaining_budget = self.budget - evaluations\n            options = {'maxiter': remaining_budget}  # Dynamic budget allocation change\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds, options=options)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Improved Adaptive Local Search with Dynamic Budget Allocation for Enhanced Convergence.", "configspace": "", "generation": 4, "fitness": 0.8497622344115999, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.101. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.9905466969531717, 0.7992502866575053, 0.759489719624123], "final_y": [0.0, 2.2081665512367395e-08, 1.9987670468203898e-08]}, "mutation_prompt": null}
{"id": "cb9dab0d-c87f-41e6-bef7-defbdb02e40e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Greedy restart within updated bounds\n            restart_point = best_solution + 0.05 * (ub - lb) * np.random.uniform(-1, 1, self.dim)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Greedy Sampling Adjustment for Improved Convergence.", "configspace": "", "generation": 4, "fitness": 0.7562649322440119, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.756 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.7465045746747969, 0.7983159437108072, 0.7239742783464314], "final_y": [9.752929645107255e-08, 4.0894866246342017e-08, 7.37691553814089e-08]}, "mutation_prompt": null}
{"id": "83545803-19fc-4097-afee-48c24e8cde9d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(12, self.budget // 2)  # Modified for more initial diversity\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6  # Refined for precise early stopping\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='BFGS', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.25  # Adjusted for refined balance between exploration and exploitation\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                step_adjustment = 0.15\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x + step_adjustment * (np.random.rand(self.dim) - 0.5), \n                                   method='BFGS', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Refined Early Stopping and Adaptive Exploration for Improved Convergence.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "979f8ecd-20bc-4815-8c68-fa0cac69ab2d", "metadata": {}, "mutation_prompt": null}
{"id": "ecfd67c7-9a8b-4e2c-82dd-84e691fc344c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-5  # Adjusted for quicker early stopping\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='BFGS', bounds=zip(lb, ub))  # Switched method for potential improvement\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.3  # Adjusted back to original for more balanced exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                step_adjustment = 0.15  # Added an adjustment factor for adaptive steps\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x + step_adjustment * (np.random.rand(self.dim) - 0.5) * (ub - lb),  # Adjust initial point\n                                   method='BFGS', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Refined Hybrid Optimization with Strategic Variable Scaling for Improved Convergence.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('An upper bound is less than the corresponding lower bound.').", "error": "ValueError('An upper bound is less than the corresponding lower bound.')", "parent_id": "979f8ecd-20bc-4815-8c68-fa0cac69ab2d", "metadata": {}, "mutation_prompt": null}
{"id": "20d69aff-6370-4ee9-b1f4-5cd87ae81c61", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Changed line for improved early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.35  # Changed line for refined exploration factor\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Fine-Tuned Exploration Factor for Improved Convergence.", "configspace": "", "generation": 5, "fitness": 0.839566404387129, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.068. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "725988e0-fb4d-4617-a397-bd35db6c47ad", "metadata": {"aucs": [0.8365938503784073, 0.9246062092781088, 0.7574991535048707], "final_y": [6.785416154341331e-08, 1.0418918613885635e-08, 2.3608464109254942e-07]}, "mutation_prompt": null}
{"id": "5e509590-5bef-4365-8c95-31bcb666d301", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 5e-8  # Changed line for improved early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.3  # Changed line for refined exploration factor\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(5):  # Changed line for additional local exploitation\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Additional Local Exploitation and Improved Early Stopping for Faster Convergence.", "configspace": "", "generation": 5, "fitness": 0.7756516031121582, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.059. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "725988e0-fb4d-4617-a397-bd35db6c47ad", "metadata": {"aucs": [0.8056692874360212, 0.6928579322617405, 0.8284275896387129], "final_y": [6.74996617355123e-08, 4.835036735101837e-07, 8.897125365574369e-08]}, "mutation_prompt": null}
{"id": "d6a3b948-a8c3-4ba4-a6ca-d4ad37caefd3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            remaining_budget = self.budget - evaluations\n            options = {'maxiter': remaining_budget}  # Dynamic budget allocation change\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds, options=options)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds using stochastic sampling\n            restart_point = np.random.normal(best_solution, 0.05 * (ub - lb))  \n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Stochastic Sampling for Improved Convergence.", "configspace": "", "generation": 5, "fitness": 0.823173269405201, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.119. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3fef5fe2-1915-4a0a-82eb-9e48b1235a79", "metadata": {"aucs": [0.9904205511063477, 0.7279643717310587, 0.7511348853781965], "final_y": [0.0, 7.907930688798694e-08, 3.0051859756239044e-08]}, "mutation_prompt": null}
{"id": "87c61df1-7bb0-49ac-8e29-8d2a7636b485", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-8  # Changed line for improved early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.4  # Changed line for refined exploration factor\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Fine-Tuned Early Stopping for Improved Convergence.", "configspace": "", "generation": 6, "fitness": 0.8477233360855864, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.062. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "725988e0-fb4d-4617-a397-bd35db6c47ad", "metadata": {"aucs": [0.8682807286493321, 0.9117925144829815, 0.7630967651244457], "final_y": [1.5078611093368874e-10, 4.055324868265573e-09, 4.719740216578074e-08]}, "mutation_prompt": null}
{"id": "3d071d0d-499a-4afd-bbd8-60a8db0e1513", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.3  # Changed line for refined exploration factor\n                adaptive_factor = 0.2  # Introduced adaptive tightening factor\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                new_bounds = [(max(new_bounds[i][0], res.x[i] - adaptive_factor * (new_bounds[i][1] - new_bounds[i][0])), \n                               min(new_bounds[i][1], res.x[i] + adaptive_factor * (new_bounds[i][1] - new_bounds[i][0]))) for i in range(self.dim)]\n\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Improved Hybrid Optimization with adaptive bounds tightening and refined exploration factor for faster convergence.", "configspace": "", "generation": 6, "fitness": 0.7835559535984274, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.005. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "725988e0-fb4d-4617-a397-bd35db6c47ad", "metadata": {"aucs": [0.7764041047123412, 0.7886499937853458, 0.7856137622975952], "final_y": [1.2862300891477547e-07, 8.819262557427925e-08, 6.191568135376644e-08]}, "mutation_prompt": null}
{"id": "fe2c058a-425f-4ccb-b633-241394a4a93d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-5  # Adjusted for quicker early stopping\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='Nelder-Mead', bounds=zip(lb, ub))  # Switched method for potential improvement\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.3  # Adjusted back to original for more balanced exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                step_adjustment = 0.15  # Added an adjustment factor for adaptive steps\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x + step_adjustment * (np.random.rand(self.dim) - 0.5),  # Adjust initial point\n                                   method='BFGS', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Optimized Hybrid Optimization with Enhanced Sampling and Dynamic Adaptive Steps for Swift Convergence using Nelder-Mead for initial guesses.", "configspace": "", "generation": 6, "fitness": 0.796844977103644, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.001. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "979f8ecd-20bc-4815-8c68-fa0cac69ab2d", "metadata": {"aucs": [0.794871210806232, 0.7976528602684121, 0.7980108602362882], "final_y": [1.1397776459345045e-07, 5.934490671431885e-08, 1.1524969379624615e-08]}, "mutation_prompt": null}
{"id": "48ef6039-e744-4e53-92bc-f8fa87d0f528", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with an increased number of points\n        num_initial_points = max(10, self.budget // 8)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        exploration_factor = 0.15  # Adjusted exploration factor\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - exploration_factor * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + exploration_factor * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Dynamic Exploration and Improved Convergence for Black Box Optimization.", "configspace": "", "generation": 6, "fitness": 0.8040555011180887, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.083. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.9145860044690752, 0.7815501228459738, 0.7160303760392168], "final_y": [0.0, 4.0894866246342017e-08, 2.3506021042345032e-08]}, "mutation_prompt": null}
{"id": "35efa956-75be-4929-ab71-feb08b6e29e3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased sampling points\n        num_initial_points = max(7, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Improved Initial Sampling for Better Convergence.", "configspace": "", "generation": 6, "fitness": 0.8636598891148953, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.091. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.9905466969531717, 0.7847114566508062, 0.8157215137407083], "final_y": [0.0, 5.420796848484087e-09, 2.458684032709403e-09]}, "mutation_prompt": null}
{"id": "06f07115-5b69-4b08-bc0d-b2f19fe0b366", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Changed early stopping threshold to 1e-7\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                # Early stopping if improvement is below threshold\n                if best_value < early_stop_threshold:\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.4  # Changed factor from 0.35 to 0.4 for enhanced exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Refined Hybrid Optimization with Modified Early Stopping and Enhanced Exploration for Improved Convergence.", "configspace": "", "generation": 7, "fitness": 0.8241048972339589, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.050. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8400807512175044, 0.8761058711323608, 0.7561280693520113], "final_y": [5.6507393483653464e-08, 8.875433822997619e-09, 6.541976532629522e-08]}, "mutation_prompt": null}
{"id": "5ee41101-c303-4e46-8e5e-bf9c8fdf82dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-8  # Line changed to adaptively adjust the early stopping threshold\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                # Early stopping if improvement is below threshold\n                if best_value < early_stop_threshold:\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.35  # Changed from 0.3 to 0.35 to enhance exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Adaptive Early Stopping Threshold for Improved Convergence.", "configspace": "", "generation": 7, "fitness": 0.7834875791526489, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.005. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.7761989813750059, 0.7886499937853458, 0.7856137622975952], "final_y": [1.2862300891477547e-07, 8.819262557427925e-08, 6.191568135376644e-08]}, "mutation_prompt": null}
{"id": "ee82cddc-376e-4ff0-9e82-88848eb76d52", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased sampling points\n        num_initial_points = max(7, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Strategic Budget Allocation for Improved Convergence.", "configspace": "", "generation": 7, "fitness": 0.7832579953893225, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.027. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35efa956-75be-4929-ab71-feb08b6e29e3", "metadata": {"aucs": [0.7527364426920226, 0.8193479072397035, 0.7776896362362418], "final_y": [1.3954020143911302e-08, 1.8729470656732394e-08, 3.218152935752696e-09]}, "mutation_prompt": null}
{"id": "c005fa61-dde2-4e69-ad9b-fdbd5644aef1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased sampling points\n        num_initial_points = max(10, self.budget // 8)  # Increased initial points\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            if evaluations >= self.budget - 5:  # Early termination check\n                break\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))  # Adjusted refinement rate\n            new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Strategic restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Adaptive Local Search with Strategic Restarts and Adaptive Sampling for Improved Convergence.", "configspace": "", "generation": 7, "fitness": 0.8298954664592105, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.066. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35efa956-75be-4929-ab71-feb08b6e29e3", "metadata": {"aucs": [0.9145860044690752, 0.8211174598555495, 0.7539829350530068], "final_y": [0.0, 4.0894866246342017e-08, 4.9330388892563e-08]}, "mutation_prompt": null}
{"id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.1:  # 10% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Improved Adaptive Local Search with Dynamic Restart Strategies and Enhanced Convergence.", "configspace": "", "generation": 7, "fitness": 0.8855643220016298, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.074. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.9905466969531717, 0.8294042870342222, 0.8367419820174957], "final_y": [0.0, 1.1849455246858328e-08, 2.6336157236748977e-08]}, "mutation_prompt": null}
{"id": "4042be9c-adb8-486f-b6df-c96ac737ee48", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.4  # Changed from 0.35 to 0.4 to further enhance exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                targeted_guess = np.clip(res.x + np.random.randn(self.dim) * 0.05, lb, ub)  # Added line for targeted local search\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, targeted_guess, method='L-BFGS-B', bounds=new_bounds)  # Changed line to use targeted_guess\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Refined Hybrid Optimization with Enhanced Dynamic Bound Adjustment and Targeted Local Search for Improved Convergence.", "configspace": "", "generation": 8, "fitness": 0.8485456506115653, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8365072464112242, 0.8930468477252496, 0.8160828576982223], "final_y": [6.785416154341331e-08, 1.2976952355306787e-08, 4.163179477783078e-08]}, "mutation_prompt": null}
{"id": "7c73aece-0097-4ac4-b60e-6494ce60ee21", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6  # Line added for early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                # Early stopping if improvement is below threshold\n                if best_value < early_stop_threshold:  # Changed line\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.4  # Changed from 0.35 to 0.4 to enhance exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Optimized Hybrid Optimization with Enhanced Dynamic Bound Adjustment and Early Stopping for Improved Convergence.", "configspace": "", "generation": 8, "fitness": 0.8570436677929102, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.043. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8577086266854437, 0.9089979962381007, 0.8044243804551858], "final_y": [2.5856858057579343e-08, 3.9639996656909513e-10, 6.400969135766152e-08]}, "mutation_prompt": null}
{"id": "4f66c6a8-f05a-4408-8884-56f55abea195", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Line changed for more precise early stopping\n\n        # Allocate budget dynamically based on progress\n        restart_budget = self.budget // 3  # Changed to allocate more for restarts\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < restart_budget:  # Adjusted to restrain restarts within budget\n                factor = 0.4  # Changed from 0.35 to 0.4 for better exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(5):  # Increased attempts to 5\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Convergence through Adaptive Restarts and Progress-Based Budget Allocation.", "configspace": "", "generation": 8, "fitness": 0.819753638557119, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.047. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8400807512175044, 0.8639440488404586, 0.7552361156133942], "final_y": [5.6507393483653464e-08, 8.875433822997619e-09, 6.541976532629522e-08]}, "mutation_prompt": null}
{"id": "9f8b8951-11e7-4605-8845-7c04a6dd4f04", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Intermediate restart within updated bounds to improve exploration\n            if evaluations < self.budget // 2:  # New line added\n                restart_point = np.random.uniform(lb, ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Intermediate Restart Strategy for Improved Exploration.", "configspace": "", "generation": 8, "fitness": 0.8641582874034857, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.091. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "316c4371-3567-4a09-8a11-a4784497d592", "metadata": {"aucs": [0.9905466969531717, 0.8188921907868566, 0.7830359744704285], "final_y": [0.0, 2.956086585045926e-10, 1.5021415602983716e-08]}, "mutation_prompt": null}
{"id": "77624e74-cba5-4af7-abae-4975e3882d2c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 12)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))  # Reduced refinement width\n            new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.15:  # Increased global restart chance\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Refined Adaptive Local Search with Dynamic Strategy Refinement and Optimized Sampling for Enhanced Convergence.", "configspace": "", "generation": 8, "fitness": 0.7652877848439914, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.765 with standard deviation 0.029. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.7952635827048797, 0.726705193666253, 0.7738945781608417], "final_y": [2.4328352745470836e-08, 9.169815750025823e-08, 4.462796406477872e-08]}, "mutation_prompt": null}
{"id": "89b6cd31-2505-4af6-adb9-c7e64e3ec316", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6  # Line added for early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                # Early stopping if improvement is below threshold\n                if best_value < early_stop_threshold:  # Changed line\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.33  # Changed from 0.35 to 0.33 to enhance exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Optimized Hybrid Optimization with Enhanced Dynamic Restart Factor for Improved Convergence.", "configspace": "", "generation": 9, "fitness": 0.8410376807078667, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8201587664507883, 0.885365096986851, 0.8175891786859608], "final_y": [7.499757805632363e-08, 3.416553228795072e-09, 7.104883875301757e-08]}, "mutation_prompt": null}
{"id": "60525d66-824d-4215-8af1-9d1d149b563a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6  # Line added for early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                # Early stopping if improvement is below threshold\n                if best_value < early_stop_threshold:  # Changed line\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.45  # Changed from 0.4 to 0.45 to enhance exploration further\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Adjusted Exploration Factor for Optimal Convergence.", "configspace": "", "generation": 9, "fitness": 0.8241048972339589, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.050. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c73aece-0097-4ac4-b60e-6494ce60ee21", "metadata": {"aucs": [0.8400807512175044, 0.8761058711323608, 0.7561280693520113], "final_y": [5.6507393483653464e-08, 8.875433822997619e-09, 6.541976532629522e-08]}, "mutation_prompt": null}
{"id": "c8640cc2-29e0-4728-a409-38658af89625", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Changed line for tighter early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.4  # Changed line to increase exploration capabilities\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Tuned Early Stopping and Dynamic Exploration Factor for Improved Convergence.", "configspace": "", "generation": 9, "fitness": 0.8304898771124969, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.058. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.9118016685019595, 0.8010609237072535, 0.7786070391282777], "final_y": [2.5995297727793476e-09, 1.672242697740384e-07, 7.487843824990101e-08]}, "mutation_prompt": null}
{"id": "cf263b84-4cfc-4624-8c6a-1025339d60c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.05:  # Adjusted restart probability to 5%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Improved Initial Sampling and Dynamic Restart Probability Adjustment for Better Convergence.", "configspace": "", "generation": 9, "fitness": 0.8456956891165177, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.105. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.9905466969531717, 0.8015953362027106, 0.7449450341936708], "final_y": [0.0, 1.1080683334553027e-08, 2.4626795101087582e-08]}, "mutation_prompt": null}
{"id": "2d310631-3bdc-401c-8b74-b123e29ea6db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased sampling points\n        num_initial_points = max(7, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds, options={'learning_rate': 0.1})\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Dynamic Learning Rate Adaptation for Faster Convergence.", "configspace": "", "generation": 9, "fitness": 0.7560084216849178, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.756 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35efa956-75be-4929-ab71-feb08b6e29e3", "metadata": {"aucs": [0.7461787931537757, 0.8001151145615238, 0.721731357339454], "final_y": [1.1151605106086439e-07, 1.641347159435079e-08, 1.4807860462926795e-07]}, "mutation_prompt": null}
{"id": "b458ab36-f026-4e8a-bc33-4a5e2c8f4567", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(15, self.budget // 3)  # Changed to increase initial exploration\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Changed for stricter early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.45  # Further increased for broader exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Optimized Hybrid Optimization with Enhanced Dynamic Bound Adjustment, Early Stopping, and Adaptive Sampling for Improved Convergence.", "configspace": "", "generation": 10, "fitness": 0.8004519913793574, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.045. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c73aece-0097-4ac4-b60e-6494ce60ee21", "metadata": {"aucs": [0.7547470875732073, 0.7845566009439129, 0.862052285620952], "final_y": [1.0672838638908364e-07, 1.1631441674748848e-07, 1.6500365329896208e-08]}, "mutation_prompt": null}
{"id": "9945292f-8ae7-422b-9f4a-aaabd08a4396", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with improved strategy\n        num_initial_points = max(5, self.budget // 8)  # Changed from //10 to //8\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))  # Changed from 0.1 to 0.05\n            new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))  # Changed from 0.1 to 0.05\n            lb, ub = new_lb, new_ub\n            \n            # Intermediate restart with diversity-preserving exploration\n            if evaluations < self.budget // 2:\n                restart_point = np.random.uniform(lb, ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Diversity-Preserving Restart Strategy and Improved Sampling for Robust Exploration.", "configspace": "", "generation": 10, "fitness": 0.7843117394439223, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9f8b8951-11e7-4605-8845-7c04a6dd4f04", "metadata": {"aucs": [0.7648947943537636, 0.8116087983832134, 0.77643162559479], "final_y": [3.1853993138906465e-08, 5.445280444987413e-09, 1.831352308901448e-08]}, "mutation_prompt": null}
{"id": "ea5f4c50-7642-49c7-9c87-fa223b663cce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with adaptive frequency\n        num_initial_points = max(5, int(self.budget * 0.15))\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Intermediate restart within updated bounds to improve exploration\n            if evaluations < self.budget * 0.75:  # Adjusted line for better utilization\n                restart_point = np.random.uniform(lb, ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Adaptive Sampling Frequency and Smooth Restart Strategies for Improved Convergence.", "configspace": "", "generation": 10, "fitness": 0.8454246203662157, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.100. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9f8b8951-11e7-4605-8845-7c04a6dd4f04", "metadata": {"aucs": [0.9862586772714715, 0.7848683327106459, 0.7651468511165298], "final_y": [0.0, 1.657607498408086e-08, 3.545180411853961e-08]}, "mutation_prompt": null}
{"id": "85c16e36-ea88-4bf4-9a14-d5b40292f0b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased sampling points\n        num_initial_points = max(7, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb)) # Changed from 0.1 to 0.05\n            new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb)) # Changed from 0.1 to 0.05\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Strategic Restart and Improved Bound Adjustment for Superior Convergence.", "configspace": "", "generation": 10, "fitness": 0.7812186261055928, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.781 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35efa956-75be-4929-ab71-feb08b6e29e3", "metadata": {"aucs": [0.7684480982472571, 0.7760313403547383, 0.799176439714783], "final_y": [7.213554109784247e-09, 3.005745754091565e-08, 2.0304583363990487e-08]}, "mutation_prompt": null}
{"id": "e445b1a7-e548-44ae-ad45-31aacd8240c4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased sampling points\n        num_initial_points = max(7, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            # Improved initial guess by perturbing the best solution\n            perturbed_start_point = best_solution + np.random.uniform(-0.05, 0.05, self.dim) * (ub - lb)\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, perturbed_start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Fine-Tuned BFGS Initialization to Improve Exploitation and Convergence.", "configspace": "", "generation": 10, "fitness": 0.7684919452218525, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.768 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35efa956-75be-4929-ab71-feb08b6e29e3", "metadata": {"aucs": [0.7710622986202536, 0.7868998676683931, 0.7475136693769109], "final_y": [2.644256887111705e-08, 3.132051084911104e-08, 7.390420662725274e-08]}, "mutation_prompt": null}
{"id": "35040a80-6fd5-4788-a394-1b0fcc1aa54a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6  # Line added for early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                # Early stopping if improvement is below threshold\n                if best_value < early_stop_threshold:  # Changed line\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.4  # Changed from 0.35 to 0.4 to enhance exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Tweaked Dynamic Bounds for Optimized Exploration and Fast Convergence.", "configspace": "", "generation": 11, "fitness": 0.8009014909568145, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8400807512175044, 0.8184864189382348, 0.7441373027147045], "final_y": [5.6507393483653464e-08, 1.3960073927005789e-07, 2.8434737924625556e-07]}, "mutation_prompt": null}
{"id": "d18ddd46-0316-47d5-b10a-ceede7c7205b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.3  # Changed from 0.35 to 0.3 to improve accuracy\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    # Added strict bounds for better accuracy\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds, options={'ftol': 1e-9})\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Optimized Hybrid Optimization with Enhanced Boundary Conditions and Dynamic Restart for Improved Accuracy.", "configspace": "", "generation": 11, "fitness": 0.799128619476755, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.060. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8817925255791079, 0.7430784460644781, 0.7725148867866788], "final_y": [2.6995336462057403e-09, 3.7332537901351593e-07, 2.894324339834052e-07]}, "mutation_prompt": null}
{"id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.15:  # 15% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Improved Adaptive Local Search with Enhanced Probability Restart Strategy for Optimal Global Exploration.", "configspace": "", "generation": 11, "fitness": 0.8641548026431728, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.091. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.9905466969531717, 0.8188921907868566, 0.7830255201894902], "final_y": [0.0, 2.956086585045926e-10, 1.5021415602983716e-08]}, "mutation_prompt": null}
{"id": "9c535bd9-1ae9-4375-86af-3a72f425ca48", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Intermediate restart within updated bounds to improve exploration\n            if evaluations < self.budget * 0.75:  # Changed line (1.9% of code)\n                restart_point = np.random.uniform(lb, ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Refined Adaptive Local Search with Enhanced Restart Frequency for Better Exploration and Convergence.", "configspace": "", "generation": 11, "fitness": 0.8195150125171522, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.086. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9f8b8951-11e7-4605-8845-7c04a6dd4f04", "metadata": {"aucs": [0.9404412402441197, 0.756938753991356, 0.7611650433159809], "final_y": [0.0, 6.027595857590994e-08, 4.286163909788321e-09]}, "mutation_prompt": null}
{"id": "3dfd0e3c-5342-43c0-9067-310b009b4ebc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.15:  # Changed from 10% to 15% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Refined adaptive local search with enhanced dynamic restart probability for improved exploration.", "configspace": "", "generation": 11, "fitness": 0.7692475414188974, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.769 with standard deviation 0.029. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.8105383617315846, 0.7504053380670788, 0.7467989244580289], "final_y": [8.618553597649617e-09, 7.316652153367315e-08, 4.699202308439903e-09]}, "mutation_prompt": null}
{"id": "531a0f60-7843-48fa-b5eb-a2b68cc6c3e2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Line changed for tighter early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.35\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Refined Hybrid Optimization with Enhanced Early Stopping Criterion for Improved Performance.", "configspace": "", "generation": 12, "fitness": 0.8577551900354606, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.042. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8577086266854437, 0.9089979962381007, 0.8065589471828372], "final_y": [2.5856858057579343e-08, 3.9639996656909513e-10, 6.400969135766152e-08]}, "mutation_prompt": null}
{"id": "e5e7fe00-bb81-4fe8-abdb-c1b24603ea1d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased sampling points\n        num_initial_points = max(7, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        step_size = 0.1  # Variable step size for exploration\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            step_size *= 0.9  # Adjust step size dynamically\n            new_lb = np.maximum(lb, best_solution - step_size * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + step_size * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Variable Step Size for Improved Convergence and Exploration.", "configspace": "", "generation": 12, "fitness": 0.7645813166333664, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.765 with standard deviation 0.016. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35efa956-75be-4929-ab71-feb08b6e29e3", "metadata": {"aucs": [0.7440612297933039, 0.7652411602218648, 0.7844415598849303], "final_y": [5.577769120722177e-08, 9.289693788096722e-08, 7.408160099667062e-09]}, "mutation_prompt": null}
{"id": "97906d68-34f5-4df3-a14d-3b4c781d2042", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(10, self.budget // 10)  # Adjusted number of initial points\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.25 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.25 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.2:  # 20% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Improved Initial Sampling and Dynamic Exploration Strategies.", "configspace": "", "generation": 12, "fitness": 0.7801520782292436, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.7960125476204034, 0.8097167018879932, 0.7347269851793341], "final_y": [1.3892738292609267e-08, 4.0894866246342017e-08, 1.4807860462926795e-07]}, "mutation_prompt": null}
{"id": "447df82d-52f2-43b0-88b5-ec41f6c39c84", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 12)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.2:  # Increased restart probability\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Optimized Adaptive Local Search with Enhanced Sampling Strategy and Probability for Restart.", "configspace": "", "generation": 12, "fitness": 0.776159269939169, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.770550503480848, 0.8097167018460674, 0.7482106044905916], "final_y": [4.391277300326394e-08, 4.0894866246342017e-08, 6.905688048103872e-08]}, "mutation_prompt": null}
{"id": "d7089b41-000c-4580-a40b-5f75765ed1ba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Intermediate restart with stochastic perturbation for enhanced exploration\n            if evaluations < self.budget // 2:\n                restart_point = np.random.uniform(lb, ub) + np.random.normal(0, 0.05, self.dim)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Augmented Exploration Using Stochastic Perturbations for Improved Convergence.", "configspace": "", "generation": 12, "fitness": 0.7876292959035269, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9f8b8951-11e7-4605-8845-7c04a6dd4f04", "metadata": {"aucs": [0.7534145049337002, 0.8283117414618159, 0.7811616413150647], "final_y": [4.458863158166787e-08, 1.8347852282045625e-08, 3.232666272888783e-08]}, "mutation_prompt": null}
{"id": "54c5141b-7341-40d3-a9d6-4ecdcea27f86", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.2:  # 20% chance to restart globally, increased from 10%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Improved Adaptive Local Search with Enhanced Global Restart Probability for Optimal Convergence.", "configspace": "", "generation": 13, "fitness": 0.7868479454018239, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.8138958575453686, 0.7976869284516147, 0.7489610502084886], "final_y": [1.6180025023860092e-08, 3.0271887364952394e-09, 6.15561749915316e-08]}, "mutation_prompt": null}
{"id": "b76a79c7-822e-493d-871c-293c51dd9b00", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with increased diversity\n        num_initial_points = max(7, self.budget // 8)  # Adjusted initial sampling\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Smart restart within updated bounds when budget is between certain range\n            if evaluations < self.budget * 0.75:  # Modified restart condition\n                restart_point = np.random.uniform(lb, ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Smart Restart and Adaptive Sampling for Faster Convergence.", "configspace": "", "generation": 13, "fitness": 0.8181824944215214, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.087. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9f8b8951-11e7-4605-8845-7c04a6dd4f04", "metadata": {"aucs": [0.9404412402441197, 0.7475794595034021, 0.7665267835170425], "final_y": [0.0, 4.106025791319165e-08, 2.0661914625115506e-08]}, "mutation_prompt": null}
{"id": "b1709a95-f045-468c-bcfb-3aa2d5bad1e7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))  # Reduced refinement width\n            new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.20:  # Increased chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Refined Restart Strategy and Improved Sampling for Optimal Convergence.", "configspace": "", "generation": 13, "fitness": 0.7744644020202979, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.034. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.8205861922098927, 0.7410557610519853, 0.7617512527990153], "final_y": [2.640931563318096e-09, 1.7249108589841474e-08, 2.639343732230102e-08]}, "mutation_prompt": null}
{"id": "6b69edca-8ba8-43cd-816c-71d7fec863ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            restart_probability = 0.1 / (1 + evaluations / (0.2 * self.budget))  # Annealed restart probability\n            if np.random.rand() < restart_probability:  # 10% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Improved Adaptive Local Search with Dynamic Restart Strategies and Enhanced Convergence using Annealed Restart Probability.", "configspace": "", "generation": 13, "fitness": 0.7487564093849475, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.749 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.7848454768907687, 0.7169965115897907, 0.7444272396742833], "final_y": [4.5027108186996305e-08, 1.49298747735359e-07, 4.6426079284031744e-08]}, "mutation_prompt": null}
{"id": "bd1218da-ddc7-446a-a0fd-4e5bd302ff25", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.25 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.25 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.15:  # 15% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Reinforced Random Restarts and Improved Refinement Strategy for Superior Exploration.", "configspace": "", "generation": 13, "fitness": 0.7736848357887628, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.774 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.7990912755495854, 0.7632172953379489, 0.7587459364787539], "final_y": [2.6789229589131538e-08, 1.9247143057847618e-08, 3.035491668447909e-08]}, "mutation_prompt": null}
{"id": "31395b04-6c4c-4bde-a961-bb06ba9494ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.15:  # Changed from 10% to 15% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Optimized Dynamic Restart Probability for Improved Global Exploration.", "configspace": "", "generation": 14, "fitness": 0.7861134712997085, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.035. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.8272211214888399, 0.7413065465551801, 0.7898127458551054], "final_y": [9.044039650113462e-10, 8.76085101422479e-08, 5.082477925089605e-09]}, "mutation_prompt": null}
{"id": "f251942a-7e3c-4c90-89fb-4f2a07a5a787", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.1:  # Reduced global restart probability to 10%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Improved Adaptive Local Search with Refined Probability Restart Strategy for Optimal Convergence.", "configspace": "", "generation": 14, "fitness": 0.7943743331052042, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.8229254135353142, 0.7960744435723314, 0.7641231422079672], "final_y": [4.009905217601166e-09, 1.0626994445107232e-08, 2.132570000902535e-08]}, "mutation_prompt": null}
{"id": "73490bec-9232-4727-85ea-cd212f5b8f93", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 12)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.15:  # 15% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Improved Adaptive Local Search with Fine-Tuned Restart Probability and Enhanced Initial Sampling Coverage.", "configspace": "", "generation": 14, "fitness": 0.8179060415984459, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.089. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.9404412402441197, 0.7795715145530594, 0.7337053699981589], "final_y": [0.0, 8.397535862039829e-09, 2.8284919332764674e-08]}, "mutation_prompt": null}
{"id": "ef13a512-64a2-4f6e-9fc4-aeaeaca3be0a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.12:  # Reduced global restart chance to 12%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Refined Dynamic Restart Strategy for Improved Global Exploration.", "configspace": "", "generation": 14, "fitness": 0.7782627715791225, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.798516501181814, 0.7663266377262691, 0.7699451758292846], "final_y": [2.6789229589131538e-08, 3.925666077923704e-08, 1.4594412976068973e-08]}, "mutation_prompt": null}
{"id": "bc60a168-61e9-49cd-8118-7dccd2b3b612", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)  # Changed sampling ratio for better initial diversity\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        adaptive_restart_prob = 0.05  # Introduced adaptive restart probability\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))  # Improved refinement width\n            new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < adaptive_restart_prob:  # Adaptive restart frequency\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n                adaptive_restart_prob = min(0.2, adaptive_restart_prob + 0.01)  # Increase probability\n            else:\n                adaptive_restart_prob = max(0.02, adaptive_restart_prob - 0.005)  # Decrease probability\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Adaptive Restart Frequency and Improved Sampling Diversity.", "configspace": "", "generation": 14, "fitness": 0.837311531936621, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.9280670580855649, 0.8329475777715108, 0.7509199599527872], "final_y": [0.0, 1.6897919121484867e-09, 2.1139552578062524e-08]}, "mutation_prompt": null}
{"id": "a92f3863-0164-44bc-bfe1-35ea254167e1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Tightened early stopping criterion\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.3  # Reverted to 0.3 to balance exploration and exploitation\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                num_restarts = 2  # Reduced from 3 to 2 to conserve budget\n                for _ in range(num_restarts):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Hybrid optimization with refined adaptive restart strategy and tighter early stopping for improved convergence.", "configspace": "", "generation": 15, "fitness": 0.8260031193077163, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8349399975640562, 0.8137460923303903, 0.8293232680287026], "final_y": [3.5703965824115464e-08, 4.209837647902909e-08, 3.9547251517088925e-08]}, "mutation_prompt": null}
{"id": "470f9440-6a23-447c-8b0e-42868239f5b1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-5  # Updated early stopping threshold for finer control\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.4  # Adjusted exploration factor to 0.4 for improved exploration\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Enhanced Hybrid Optimization with Adaptive Early Stopping and Improved Dynamic Bounds for Fast Convergence.", "configspace": "", "generation": 15, "fitness": 0.8068290913217169, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.7865851684454356, 0.7940923552564465, 0.8398097502632687], "final_y": [8.313020032342529e-08, 8.57419012336375e-08, 3.287403248710819e-08]}, "mutation_prompt": null}
{"id": "94fa27d0-0a7d-4795-8eb8-30f3443840ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.18:  # 18% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Improved Adaptive Local Search with Optimized Dynamic Restart Probability for Enhanced Global Exploration.", "configspace": "", "generation": 15, "fitness": 0.7641734985730712, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.7818889852718072, 0.7621597284127251, 0.7484717820346811], "final_y": [4.85986793693356e-08, 4.407314184756483e-08, 6.719523304523243e-08]}, "mutation_prompt": null}
{"id": "ea2ca35f-f74b-43b2-8e80-1cd4494eca2a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            restart_probability = 0.1 if evaluations > self.budget * 0.5 else 0.2  # Adjusted restart probability\n            if np.random.rand() < restart_probability:  # Conditional restart probability\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Adaptive Local Search with Enhanced Probability Restart Strategy and Improved Restart Probability Tuning.", "configspace": "", "generation": 15, "fitness": 0.7837447298709389, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.784 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.7855225368229817, 0.7621216493262964, 0.8035900034635388], "final_y": [2.3107992778564142e-08, 1.314576566923645e-07, 2.3504451623557162e-08]}, "mutation_prompt": null}
{"id": "226d0d7f-49a5-440e-ac2a-810e20a296b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with adaptive sampling strategy\n        num_initial_points = max(7, self.budget // 8)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Dual-strategy local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # New strategy to refine search using dual approach\n            if evaluations < self.budget // 2:\n                new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            else:\n                new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Adaptive Sampling and Dual-Strategy Refinement for Improved Convergence.", "configspace": "", "generation": 15, "fitness": 0.8855151453244335, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.074. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "35efa956-75be-4929-ab71-feb08b6e29e3", "metadata": {"aucs": [0.9905466969531717, 0.8292567570026329, 0.8367419820174957], "final_y": [0.0, 1.7520871538948996e-08, 2.6336157236748977e-08]}, "mutation_prompt": null}
{"id": "2949c6c5-1d6c-4198-b55b-f02d4334a340", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-6\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                if best_value < early_stop_threshold:\n                    break\n\n            if evaluations < self.budget:\n                factor = 0.25  # Changed from 0.35 to 0.25 to adapt exploration range\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(5):  # Changed from 3 to 5 for more local exploration\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Improved Hybrid Optimization with Enhanced Restart Strategy and Adaptive Exploration for Optimal Convergence.", "configspace": "", "generation": 16, "fitness": 0.8535292579665086, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.047. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8577086266854437, 0.9089979962381007, 0.7938811509759811], "final_y": [2.5856858057579343e-08, 3.9639996656909513e-10, 1.2328655073011468e-07]}, "mutation_prompt": null}
{"id": "97063f03-c15b-493b-aca5-3d21c6af986a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.18:  # Changed global restart probability to 18%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Optimized Adaptive Local Search with Enhanced Global Restart Strategy for Improved Exploration.", "configspace": "", "generation": 16, "fitness": 0.8673233260764027, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.087. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.9905466969531717, 0.8132856002326406, 0.798137681043396], "final_y": [0.0, 2.2081665512367395e-08, 1.7669366666488355e-09]}, "mutation_prompt": null}
{"id": "e7172ea3-af88-43e5-9f84-9f125ea10efe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 12)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.20:  # 20% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Modified Restart Strategies and Improved Sampling for Better Exploration.", "configspace": "", "generation": 16, "fitness": 0.7802513653240614, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.018. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4c819c18-e65d-452f-8c73-ebb85452ebc6", "metadata": {"aucs": [0.7920144701221472, 0.7548332108458002, 0.7939064150042364], "final_y": [3.04309366837299e-08, 1.164267100235324e-08, 4.311675726920098e-09]}, "mutation_prompt": null}
{"id": "afeb7c69-0d1c-4a2b-bd86-1fc9546692c7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 12)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.05:  # Changed restart probability\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Adaptive Local Search with Refined Restart Probability and Optimized Sampling for Enhanced Convergence.", "configspace": "", "generation": 16, "fitness": 0.8313695609036144, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.077. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.9404412402441197, 0.7824841067086543, 0.7711833357580689], "final_y": [0.0, 3.3884196721614556e-08, 2.8046354836901657e-09]}, "mutation_prompt": null}
{"id": "54b8aaa3-712b-4cb2-87e8-65a7ffa535d3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.15:  # Changed restart probability from 0.1 to 0.15\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Refined Dynamic Restart Probability for Improved Convergence.", "configspace": "", "generation": 16, "fitness": 0.7852497887119902, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.011. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.7993068159374996, 0.7844421063019562, 0.7720004438965148], "final_y": [2.6789229589131538e-08, 3.298243920527984e-08, 7.748866671532938e-09]}, "mutation_prompt": null}
{"id": "8f40c298-ecd8-4486-98fa-b8dc05c2820e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n    \n    def __call__(self, func):\n        num_initial_guesses = min(10, self.budget // 2)\n        best_solution = None\n        best_value = float('inf')\n        \n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        initial_guesses = np.random.uniform(lb, ub, (num_initial_guesses, self.dim))\n        evaluations = 0\n        early_stop_threshold = 1e-7  # Adjusted early stopping criterion for improved performance\n\n        for initial_guess in initial_guesses:\n            if evaluations >= self.budget:\n                break\n            \n            res = minimize(func, initial_guess, method='L-BFGS-B', bounds=zip(lb, ub))\n            evaluations += res.nfev\n\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n                # Early stopping if improvement is below threshold\n                if best_value < early_stop_threshold:\n                    break\n\n            # Adaptive restart with dynamic bounds\n            if evaluations < self.budget:\n                factor = 0.35\n                new_bounds = [(max(lb[i], res.x[i] - factor * (ub[i] - lb[i])), \n                               min(ub[i], res.x[i] + factor * (ub[i] - lb[i]))) for i in range(self.dim)]\n                for _ in range(3):\n                    if evaluations >= self.budget:\n                        break\n                    res = minimize(func, res.x, method='L-BFGS-B', bounds=new_bounds)\n                    evaluations += res.nfev\n                    if res.fun < best_value:\n                        best_value = res.fun\n                        best_solution = res.x\n                        \n        return best_solution", "name": "HybridOptimization", "description": "Optimized Hybrid Optimization with Enhanced Local Search by Adjusting Early Stopping for Fast Convergence.", "configspace": "", "generation": 17, "fitness": 0.8254662867837501, "feedback": "The algorithm HybridOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.825 with standard deviation 0.009. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d6669874-414e-4d17-a189-086e29651f13", "metadata": {"aucs": [0.8349399975640562, 0.8125734427496794, 0.8288854200375145], "final_y": [3.5703965824115464e-08, 4.209837647902909e-08, 3.9547251517088925e-08]}, "mutation_prompt": null}
{"id": "2ab775ad-40e4-4dbc-a260-9be834eaa99d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.2:  # 20% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Dynamic Restart Strategy with Probability Adjustment for Improved Exploration.", "configspace": "", "generation": 17, "fitness": 0.810360279801408, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.8458804378594417, 0.8120759644465859, 0.7731244370981963], "final_y": [9.723107202128235e-09, 1.3241139041674207e-08, 2.031945720052628e-08]}, "mutation_prompt": null}
{"id": "8482e405-09af-4ba1-83c3-4c69eddc227f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with enhanced adaptive sampling strategy\n        num_initial_points = max(7, self.budget // 8)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        initial_samples += (np.random.rand(num_initial_points, self.dim) - 0.5) * 0.1  # Small perturbation for diversity\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Dual-strategy local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # New strategy to refine search using dual approach\n            if evaluations < self.budget // 2:\n                new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            else:\n                new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced convergence by modifying adaptive sampling strategy to prioritize diverse initial points.", "configspace": "", "generation": 17, "fitness": 0.777213680182384, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "226d0d7f-49a5-440e-ac2a-810e20a296b0", "metadata": {"aucs": [0.7631065844273508, 0.8140979515173306, 0.7544365046024708], "final_y": [1.9476062526765358e-08, 3.235847375916192e-08, 3.072405133524767e-08]}, "mutation_prompt": null}
{"id": "70147ef1-52f7-4701-bee5-913feaf8f6b3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with improved adaptive sampling strategy\n        num_initial_points = max(10, self.budget // 8)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Dual-strategy local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # New strategy to refine search using dual approach\n            if evaluations < self.budget // 2:\n                new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            else:\n                new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n\n            lb, ub = new_lb, new_ub\n            \n            # Refined random restart within updated bounds\n            restart_point = np.random.normal(best_solution, 0.1 * (ub - lb))\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Improved Initial Sampling and Refined Restart Strategy for Superior Convergence.", "configspace": "", "generation": 17, "fitness": 0.8131421677411086, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.021. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "226d0d7f-49a5-440e-ac2a-810e20a296b0", "metadata": {"aucs": [0.8101188902308432, 0.840420069498189, 0.7888875434942934], "final_y": [2.151658691459711e-08, 3.420580743582881e-08, 4.436345692458285e-09]}, "mutation_prompt": null}
{"id": "edcda5db-cc78-440e-9020-60d36da5dd1f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with adaptive sampling strategy\n        num_initial_points = max(7, self.budget // 8)\n        sampler = Sobol(d=self.dim, scramble=True)\n        initial_samples = sampler.random_base2(m=int(np.log2(num_initial_points)))\n        initial_samples = lb + (ub - lb) * initial_samples\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Dual-strategy local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # New strategy to refine search using dual approach\n            if evaluations < self.budget // 2:\n                new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            else:\n                new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced sampling initialization by introducing Sobol sequence for better coverage of the search space.", "configspace": "", "generation": 17, "fitness": 0.8078002935088815, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.047. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "226d0d7f-49a5-440e-ac2a-810e20a296b0", "metadata": {"aucs": [0.7529413035218262, 0.8682403255205648, 0.8022192514842535], "final_y": [3.741190934273701e-08, 3.167206780064528e-09, 2.9436139242241743e-08]}, "mutation_prompt": null}
{"id": "a24cdd57-5d62-4df2-9eff-1cccd4398809", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with adaptive sampling strategy\n        num_initial_points = max(10, self.budget // 8)  # Changed from 7 to 10\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Dual-strategy local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # New strategy to refine search using dual approach\n            if evaluations < self.budget // 2:\n                new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            else:\n                new_lb = np.maximum(lb, best_solution - 0.1 * (ub - lb))  # Changed from 0.15 to 0.1\n                new_ub = np.minimum(ub, best_solution + 0.1 * (ub - lb))\n\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Improved Initial Sampling and Dual-Strategy Restart for Optimized Convergence.", "configspace": "", "generation": 18, "fitness": 0.8666032029864338, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.084. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "226d0d7f-49a5-440e-ac2a-810e20a296b0", "metadata": {"aucs": [0.7899763855292494, 0.8265627189973983, 0.9832705044326537], "final_y": [1.8626076510643264e-09, 1.8544671170833247e-08, 0.0]}, "mutation_prompt": null}
{"id": "67e6ffe3-f2f8-4a9a-a6d6-0c35667cef24", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.15:  # 15% chance to restart globally\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Modified Restart Probability for Improved Global Exploration.", "configspace": "", "generation": 18, "fitness": 0.7890635961721175, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.023. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "4b7f9eb0-9e6b-4132-ac0c-01d59a883bcf", "metadata": {"aucs": [0.8211119042872768, 0.7792348303646703, 0.7668440538644052], "final_y": [5.534715657909742e-10, 6.772444508488827e-08, 4.997328977347156e-08]}, "mutation_prompt": null}
{"id": "79034e60-ddec-4002-955b-fa24ea936bda", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.2:  # Changed global restart probability to 20%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Modified Restart Strategy for Balanced Exploration and Exploitation.", "configspace": "", "generation": 18, "fitness": 0.7944563918851676, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97063f03-c15b-493b-aca5-3d21c6af986a", "metadata": {"aucs": [0.8131142435143235, 0.8211174593734019, 0.7491374727677772], "final_y": [2.0160493561601358e-08, 4.0894866246342017e-08, 1.1017317530122514e-07]}, "mutation_prompt": null}
{"id": "4d10e4a8-4474-43d1-ace3-e58b05ac0cfc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 10)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))  # Changed from 0.1 to 0.15\n            new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))  # Changed from 0.1 to 0.15\n            lb, ub = new_lb, new_ub\n            \n            # Intermediate restart within updated bounds to improve exploration\n            if evaluations < self.budget // 2: \n                restart_point = np.random.uniform(lb, ub)  # Ensure restart point is within refined bounds\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced restart strategy and adaptive sampling for efficient local search refinement.", "configspace": "", "generation": 18, "fitness": 0.8310199175414263, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.078. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9f8b8951-11e7-4605-8845-7c04a6dd4f04", "metadata": {"aucs": [0.9404412402441197, 0.7872693762974511, 0.765349136082708], "final_y": [0.0, 3.432016868958626e-08, 1.6407086571915938e-08]}, "mutation_prompt": null}
{"id": "0dbf1a48-26e9-46a8-be43-516d11bfd5fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.2 * (ub - lb))  # Increased refinement width\n            new_ub = np.minimum(ub, best_solution + 0.2 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = best_solution + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)  # <--- Modified initialization\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.18:  # Changed global restart probability to 18%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Optimized Adaptive Local Search with Improved BFGS Initialization for Faster Convergence.", "configspace": "", "generation": 18, "fitness": 0.77153493711173, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97063f03-c15b-493b-aca5-3d21c6af986a", "metadata": {"aucs": [0.7823262306347146, 0.7845776404592788, 0.7477009402411972], "final_y": [4.715071956230777e-09, 1.6230090550454634e-08, 1.189323385598593e-07]}, "mutation_prompt": null}
{"id": "a9b917a1-8b5a-4252-8255-9c82635498af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        num_initial_points = max(5, self.budget // 15)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds, options={'maxiter': 50})  # Limit BFGS iterations\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))  # Reduced refinement width\n            new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            if np.random.rand() < 0.20:  # Adjusted global restart probability to 20%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Improved Dynamic Restart Strategy and Gradient-Based Exploration.", "configspace": "", "generation": 19, "fitness": 0.7773623869183206, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.777 with standard deviation 0.037. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97063f03-c15b-493b-aca5-3d21c6af986a", "metadata": {"aucs": [0.8273607791173632, 0.7650243009229516, 0.7397020807146469], "final_y": [1.0090831164506e-08, 6.380544079241228e-08, 9.24784732829723e-08]}, "mutation_prompt": null}
{"id": "791c33c3-4b5b-4e34-84d2-49204ec7e26e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 15)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # Update bounds based on best solution found to refine search\n            new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))  # Reduced refinement width\n            new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.22:  # Increased global restart probability to 22%\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Optimized Adaptive Local Search with Improved Dynamic Restart Strategy and Enhanced Bound Refinement for Better Convergence.", "configspace": "", "generation": 19, "fitness": 0.7889901002132845, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97063f03-c15b-493b-aca5-3d21c6af986a", "metadata": {"aucs": [0.7987080926377378, 0.7882348343043118, 0.7800273736978041], "final_y": [2.6789229589131538e-08, 1.4339488744052526e-08, 1.8784735799302757e-08]}, "mutation_prompt": null}
{"id": "6bfe1ab7-b23f-4b68-95b0-95c67ab60b81", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with adaptive sampling strategy\n        num_initial_points = max(10, self.budget // 8)  # Changed from 7 to 10\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Dual-strategy local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # New strategy to refine search using dual approach\n            if evaluations < self.budget // 2:\n                new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            else:\n                new_lb = np.maximum(lb, best_solution - 0.08 * (ub - lb))  # Reduced from 0.1 to 0.08\n                new_ub = np.minimum(ub, best_solution + 0.08 * (ub - lb))\n\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Synergistic Restart Radius Adjustment for Optimal Convergence.", "configspace": "", "generation": 19, "fitness": 0.8308396394684793, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.078. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a24cdd57-5d62-4df2-9eff-1cccd4398809", "metadata": {"aucs": [0.9404412402441197, 0.7843397246777902, 0.7677379534835281], "final_y": [0.0, 1.174744564584245e-08, 1.361679545094921e-08]}, "mutation_prompt": null}
{"id": "3d2d4372-0ace-4216-83a2-fa74d45772f1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling\n        num_initial_points = max(5, self.budget // 12)  # Changed sampling ratio\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from multiple initial points\n        def bfgs_optimization(start_points):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            for point in start_points:\n                result = minimize(func, point, method='L-BFGS-B', bounds=bounds)\n                evaluations += result.nfev\n                if result.fun < best_value:\n                    best_value = result.fun\n                    best_solution = result.x\n        \n        # Local search and bounds refinement\n        while evaluations < self.budget:\n            num_points = 2  # Explore from multiple points\n            bfgs_optimization([best_solution] + [np.random.uniform(lb, ub) for _ in range(num_points - 1)])\n            # Update bounds based on best solution found to refine search\n            dynamic_step = 0.15 if np.random.rand() < 0.5 else 0.2  # Adaptive step size\n            new_lb = np.maximum(lb, best_solution - dynamic_step * (ub - lb))\n            new_ub = np.minimum(ub, best_solution + dynamic_step * (ub - lb))\n            lb, ub = new_lb, new_ub\n            \n            # Random restart within updated bounds\n            restart_point = np.random.uniform(lb, ub)\n            bfgs_optimization([restart_point])\n            \n            # Introduce dynamic restart strategy\n            if np.random.rand() < 0.18:  # Kept global restart probability\n                restart_point = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                bfgs_optimization([restart_point])\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Adaptive Step Size and Multi-Point Exploration for Optimized Convergence.", "configspace": "", "generation": 19, "fitness": 0.7641369458283701, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.022. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "97063f03-c15b-493b-aca5-3d21c6af986a", "metadata": {"aucs": [0.7681420263216265, 0.7348047340695772, 0.7894640770939065], "final_y": [4.85986793693356e-08, 1.2800386729525438e-07, 1.2041860620859189e-08]}, "mutation_prompt": null}
{"id": "3a597008-a190-4d52-bb09-2b7aa6f383cb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Extract bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial sampling with adaptive sampling strategy\n        num_initial_points = max(7, self.budget // 8)\n        initial_samples = np.random.uniform(lb, ub, (num_initial_points, self.dim))\n        \n        evaluations = 0\n        \n        # Evaluate initial points\n        for sample in initial_samples:\n            if evaluations >= self.budget:\n                break\n            value = func(sample)\n            evaluations += 1\n            if value < best_value:\n                best_value = value\n                best_solution = sample\n        \n        # BFGS Optimization from best initial point\n        def bfgs_optimization(start_point):\n            nonlocal evaluations, best_value, best_solution\n            bounds = [(low, high) for low, high in zip(lb, ub)]\n            result = minimize(func, start_point, method='L-BFGS-B', bounds=bounds)\n            evaluations += result.nfev\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n        \n        # Dual-strategy local search and bounds refinement\n        while evaluations < self.budget:\n            bfgs_optimization(best_solution)\n            # New strategy to refine search using dual approach\n            if evaluations < self.budget // 2:\n                new_lb = np.maximum(lb, best_solution - 0.05 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.05 * (ub - lb))\n            else:\n                new_lb = np.maximum(lb, best_solution - 0.15 * (ub - lb))\n                new_ub = np.minimum(ub, best_solution + 0.15 * (ub - lb))\n\n            lb, ub = new_lb, new_ub\n            \n            # Random restart or strategic perturbation within updated bounds\n            if np.random.rand() < 0.5:\n                restart_point = np.random.uniform(lb, ub)\n            else:\n                restart_point = best_solution + np.random.normal(0, 0.01, self.dim)  # Perturbation strategy\n            bfgs_optimization(restart_point)\n        \n        return best_solution", "name": "AdaptiveLocalSearch", "description": "Enhanced Adaptive Local Search with Strategic Perturbations for Improved Convergence and Exploitation.", "configspace": "", "generation": 19, "fitness": 0.7468541123851559, "feedback": "The algorithm AdaptiveLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.747 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "226d0d7f-49a5-440e-ac2a-810e20a296b0", "metadata": {"aucs": [0.7311987844489571, 0.7670576145843407, 0.7423059381221697], "final_y": [9.568480432378794e-08, 7.701562083855176e-09, 5.673909502445291e-08]}, "mutation_prompt": null}
