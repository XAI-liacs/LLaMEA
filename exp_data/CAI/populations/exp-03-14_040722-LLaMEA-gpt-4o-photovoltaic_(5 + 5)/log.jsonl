{"id": "5367e109-bd83-44b9-98ed-3332ba9e94a5", "solution": "import numpy as np\n\nclass AdaptiveSpiralOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        center = (lb + ub) / 2.0  # Start from the center of the search space\n        radius = (ub - lb) / 2.0  # Initial search radius\n\n        best_solution = center\n        best_value = func(center)\n        \n        evaluations = 1\n        while evaluations < self.budget:\n            for angle in np.linspace(0, 2*np.pi, self.dim, endpoint=False):\n                direction = np.array([np.cos(angle), np.sin(angle)])\n                step_size = radius * (1 - (evaluations / self.budget))\n                \n                candidate_solution = best_solution + step_size * direction\n                candidate_solution = np.clip(candidate_solution, lb, ub)\n                \n                candidate_value = func(candidate_solution)\n                evaluations += 1\n\n                if candidate_value < best_value:\n                    best_solution = candidate_solution\n                    best_value = candidate_value\n\n                if evaluations >= self.budget:\n                    break\n\n            radius *= 0.9  # Reduce the radius over time to focus on local search\n\n        return best_solution\n\n# To use this class, instantiate it with the budget and dimension, then call it with the function to optimize.", "name": "AdaptiveSpiralOptimization", "description": "This novel algorithm, named Adaptive Spiral Optimization (ASO), dynamically combines spiral-based exploration and adaptive convergence to efficiently navigate the solution space for optimizing black-box functions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 22, in __call__\nValueError: operands could not be broadcast together with shapes (10,) (2,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (10,) (2,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 194, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 288, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 149, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 22, in __call__\nValueError: operands could not be broadcast together with shapes (10,) (2,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "118892db-0ed2-44ac-aec9-683122ba3b00", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "A novel metaheuristic algorithm inspired by swarm intelligence and chaos theory to efficiently explore and exploit the search space for black-box optimization problems.", "configspace": "", "generation": 0, "fitness": 0.8743632591857414, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.013. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": null, "metadata": {"aucs": [0.887093614228129, 0.8797824724488688, 0.8562136908802264], "final_y": [0.11428395387083456, 0.12640431502616678, 0.1274778771901771]}, "mutation_prompt": null}
{"id": "39d91562-3364-401a-86a5-2dd7939a56f0", "solution": "import numpy as np\n\nclass DynamicSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = max(10, dim * 2)\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.velocity_clamp = 0.1  # Velocity clamping factor\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        swarm = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, (self.population_size, self.dim))\n        personal_best_positions = np.copy(swarm)\n        personal_best_scores = np.full(self.population_size, float('inf'))\n\n        global_best_position = None\n        global_best_score = float('inf')\n\n        eval_count = 0\n        while eval_count < self.budget:\n            # Evaluate swarm\n            for i in range(self.population_size):\n                score = func(swarm[i])\n                eval_count += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = np.copy(swarm[i])\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = np.copy(swarm[i])\n                if eval_count >= self.budget:\n                    break\n\n            # Update velocities and positions\n            r1, r2 = np.random.rand(self.population_size, self.dim), np.random.rand(self.population_size, self.dim)\n            velocities = (self.inertia_weight * velocities +\n                          self.cognitive_coeff * r1 * (personal_best_positions - swarm) +\n                          self.social_coeff * r2 * (global_best_position - swarm))\n            velocities = np.clip(velocities, -self.velocity_clamp, self.velocity_clamp)\n            swarm += velocities\n            swarm = np.clip(swarm, lb, ub)\n\n        return global_best_position, global_best_score", "name": "DynamicSwarmOptimizer", "description": "A dynamic swarm-based metaheuristic algorithm that adapts its exploration and exploitation phases based on historical solution improvements to efficiently navigate the search space.", "configspace": "", "generation": 0, "fitness": 0.7215550893573992, "feedback": "The algorithm DynamicSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.722 with standard deviation 0.039. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": null, "metadata": {"aucs": [0.6741811439526507, 0.7701192726768286, 0.7203648514427181], "final_y": [0.21180063297173946, 0.16969102523481272, 0.19043896395443305]}, "mutation_prompt": null}
{"id": "e684e631-d6be-4b8a-8d1a-cc6c5af16a19", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring\n            offspring = []\n            for i in range(num_parents):\n                parent = parents[i]\n                mutation_strength = (func.bounds.ub - func.bounds.lb) / 10.0\n                mutated = parent + np.random.normal(0, mutation_strength, self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "The algorithm combines evolutionary strategies with dynamic neighborhood search to explore and exploit the solution space effectively.", "configspace": "", "generation": 0, "fitness": 0.8434152935851446, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.005. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8448214127166188, 0.8493171492917712, 0.8361073187470437], "final_y": [0.12545458951799737, 0.13473060544440818, 0.13089531139315824]}, "mutation_prompt": null}
{"id": "b6ab50ef-b53b-4b51-bc24-b9cd9ac77645", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia_weight = 0.7\n        self.mutation_rate = 0.1\n        self.bounds = None\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(population)\n        personal_best_scores = np.array([float('inf')] * self.population_size)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        evals = 0\n\n        while evals < self.budget:\n            for i in range(self.population_size):\n                score = func(population[i])\n                evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = population[i]\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = population[i]\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 cognitive_component + social_component)\n\n                population[i] += velocities[i]\n\n                # Adaptive mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    population[i] += mutation_vector\n\n                # Ensure bounds\n                population[i] = np.clip(population[i], lb, ub)\n\n        return global_best_position, global_best_score", "name": "HybridParticleSwarmOptimization", "description": "The algorithm employs a hybrid strategy combining particle swarm dynamics and adaptive mutation to efficiently explore and exploit the search space in high-dimensional optimization problems.", "configspace": "", "generation": 0, "fitness": 0.8647494235478295, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.033. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8901402214097548, 0.8177049758208368, 0.8864030734128966], "final_y": [0.11726836267986795, 0.13920548436438085, 0.11906712457643709]}, "mutation_prompt": null}
{"id": "d66633ab-2da2-4eeb-a73c-07b3e2b2c35e", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                mutation_strength = (func.bounds.ub - func.bounds.lb) / 10.0\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improved Dynamic Evolutionary Optimizer by incorporating crossover to enhance exploration and convergence in black-box optimization problems.", "configspace": "", "generation": 1, "fitness": 0.8593485063140803, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.005. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "e684e631-d6be-4b8a-8d1a-cc6c5af16a19", "metadata": {"aucs": [0.8572611646646442, 0.8543497861579508, 0.8664345681196463], "final_y": [0.12138385340768787, 0.1311744925770587, 0.12493682585619692]}, "mutation_prompt": null}
{"id": "9792c58f-94db-47e3-897d-ad19775cb16e", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia_weight = 0.9  # Adjusted initial inertia weight\n        self.mutation_rate = 0.1\n        self.bounds = None\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(population)\n        personal_best_scores = np.array([float('inf')] * self.population_size)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        evals = 0\n\n        while evals < self.budget:\n            for i in range(self.population_size):\n                score = func(population[i])\n                evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = population[i]\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = population[i]\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = self.c1 * r1 * r1 * (personal_best_positions[i] - population[i])  # Non-linear cognitive component\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 cognitive_component + social_component)\n                population[i] += velocities[i]\n\n                # Adaptive mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    population[i] += mutation_vector\n\n                # Ensure bounds\n                population[i] = np.clip(population[i], lb, ub)\n\n        return global_best_position, global_best_score", "name": "HybridParticleSwarmOptimization", "description": "The algorithm enhances exploration by implementing a time-varying inertia weight strategy and refines exploitation through a non-linear cognitive component adjustment.", "configspace": "", "generation": 1, "fitness": 0.8172341753661496, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.016. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "b6ab50ef-b53b-4b51-bc24-b9cd9ac77645", "metadata": {"aucs": [0.7952615688895414, 0.8302130408730091, 0.8262279163358986], "final_y": [0.15129938962238665, 0.13971750518666493, 0.13733299425651013]}, "mutation_prompt": null}
{"id": "d481e10f-bcab-4ce7-9590-787268da2874", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1_initial = 2.5  # initial cognitive coefficient\n        self.c1_final = 1.5  # final cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia_weight_initial = 0.9\n        self.inertia_weight_final = 0.4\n        self.mutation_rate = 0.1\n        self.bounds = None\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(population)\n        personal_best_scores = np.array([float('inf')] * self.population_size)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        evals = 0\n\n        while evals < self.budget:\n            inertia_weight = self.inertia_weight_initial - evals / self.budget * (self.inertia_weight_initial - self.inertia_weight_final)\n            c1 = self.c1_initial - (self.c1_initial - self.c1_final) * (evals / self.budget)\n\n            for i in range(self.population_size):\n                score = func(population[i])\n                evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = population[i]\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = population[i]\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                velocities[i] = (inertia_weight * velocities[i] +\n                                 cognitive_component + social_component)\n\n                population[i] += velocities[i]\n\n                # Adaptive mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    population[i] += mutation_vector\n\n                # Ensure bounds\n                population[i] = np.clip(population[i], lb, ub)\n\n        return global_best_position, global_best_score", "name": "HybridParticleSwarmOptimization", "description": "The algorithm improves exploration by introducing a time-varying inertia weight and enhances exploitation by using a nonlinear decreasing cognitive coefficient.", "configspace": "", "generation": 1, "fitness": 0.8155797768333808, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.013. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "b6ab50ef-b53b-4b51-bc24-b9cd9ac77645", "metadata": {"aucs": [0.8092173154680575, 0.8335126700028217, 0.8040093450292632], "final_y": [0.1264758358258019, 0.12732525400247197, 0.1461097393607964]}, "mutation_prompt": null}
{"id": "0ebcc89d-33fd-4411-8a2e-3b12335657a1", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 10.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring\n            offspring = []\n            for i in range(num_parents):\n                parent = parents[i]\n                mutated = parent + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "The Enhanced Dynamic Evolutionary Optimizer (EDEO) introduces fitness-based adaptive mutation and elitism to improve convergence in exploring and exploiting the solution space effectively.", "configspace": "", "generation": 1, "fitness": 0.8479275266105452, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.007. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "e684e631-d6be-4b8a-8d1a-cc6c5af16a19", "metadata": {"aucs": [0.8449858525856829, 0.8416390285121895, 0.857157698733763], "final_y": [0.12121052821743383, 0.1296550213147557, 0.12900029265929702]}, "mutation_prompt": null}
{"id": "40a3a667-dfee-4a1e-beb5-7e1fabda42fa", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.adaptive_factor = 0.9  # New adaptive factor\n        self.elite_preservation_rate = 0.1  # New elite preservation rate\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                if i < int(self.population_size * self.elite_preservation_rate):\n                    continue  # Preserve elite solutions\n                \n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                adaptive_inertia = self.inertia_weight * (1 - func_evals / self.budget) * self.adaptive_factor\n                velocity[i] = (\n                    adaptive_inertia * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "Enhanced Chaotic Swarm Optimizer with adaptive coefficients and elite preservation for improved convergence and solution quality.", "configspace": "", "generation": 1, "fitness": 0.8191451422380771, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.819 with standard deviation 0.014. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.8000274433761072, 0.8319261313816895, 0.8254818519564344], "final_y": [0.14690533202157052, 0.14267681727844583, 0.1375130038110688]}, "mutation_prompt": null}
{"id": "51aed368-663e-42f8-9ac5-4377697248df", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover and differential mutation\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                \n                # Apply differential mutation\n                F = 0.5 + np.random.rand() * 0.5  # Adaptive mutation factor\n                child1 += F * (p1 - p2)\n                child2 += F * (p2 - p1)\n\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                mutation_strength = (func.bounds.ub - func.bounds.lb) / 20.0\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "The Enhanced Dynamic Evolutionary Optimizer introduces an adaptive differential mutation strategy and elitism to balance exploration and exploitation, improving convergence and diversity for black-box optimization problems.", "configspace": "", "generation": 2, "fitness": 0.8470770486452824, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.022. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "d66633ab-2da2-4eeb-a73c-07b3e2b2c35e", "metadata": {"aucs": [0.8160980571588121, 0.8628732498912632, 0.8622598388857716], "final_y": [0.13775499325690255, 0.1284272612926438, 0.12509147516399133]}, "mutation_prompt": null}
{"id": "9ef4213e-4b9a-46c0-aa1a-5c8ba0cf15a9", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring\n            offspring = []\n            for i in range(num_parents):\n                parent1, parent2 = parents[np.random.choice(num_parents, 2, replace=False)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n                mutation_strength = (func.bounds.ub - func.bounds.lb) / 10.0\n                mutated = child + np.random.normal(0, mutation_strength, self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Incorporate crossover mechanism by combining elements of two parent solutions to generate more diverse offspring.", "configspace": "", "generation": 2, "fitness": 0.8437326830728896, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.009. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "e684e631-d6be-4b8a-8d1a-cc6c5af16a19", "metadata": {"aucs": [0.8394059863334571, 0.8357798576645756, 0.8560122052206365], "final_y": [0.12957273013256132, 0.1335350647686151, 0.1268976592227289]}, "mutation_prompt": null}
{"id": "79d27094-39a4-4c5d-ba4d-c52f32cff1e5", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                # Adaptive inertia weight\n                self.inertia_weight = 0.9 - 0.5 * (func_evals / self.budget)\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "An enhanced Chaotic Swarm Optimizer incorporating adaptive parameter tuning to improve convergence and diversity for black-box optimization problems.", "configspace": "", "generation": 2, "fitness": 0.8372106885071288, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.024. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.8044106072020283, 0.8587700407131066, 0.848451417606251], "final_y": [0.14296660134850236, 0.13276102506129617, 0.12441553931909077]}, "mutation_prompt": null}
{"id": "bf73e493-a341-4dec-8997-e7d705425733", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                diversity = np.std(population, axis=0)  # Calculate diversity\n                mutation_strength = (func.bounds.ub - func.bounds.lb) / 10.0 * diversity\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhanced Dynamic Evolutionary Optimizer with adaptive mutation based on diversity to improve exploration and convergence in black-box optimization problems.", "configspace": "", "generation": 2, "fitness": 0.8052387376180917, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.085. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.037.", "error": "", "parent_id": "d66633ab-2da2-4eeb-a73c-07b3e2b2c35e", "metadata": {"aucs": [0.685415287416892, 0.8683021776733129, 0.8619987477640706], "final_y": [0.20382755288488952, 0.12463317614842606, 0.1253370285614308]}, "mutation_prompt": null}
{"id": "41569227-5834-44e6-a58c-f9d8cc418f08", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia_weight = 0.7\n        self.mutation_rate = 0.1\n        self.bounds = None\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(population)\n        personal_best_scores = np.array([float('inf')] * self.population_size)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        evals = 0\n\n        while evals < self.budget:\n            for i in range(self.population_size):\n                score = func(population[i])\n                evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = population[i]\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = population[i]\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 cognitive_component + social_component)\n\n                population[i] += velocities[i]\n\n                # Adaptive mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    population[i] += mutation_vector\n\n                # Ensure bounds\n                population[i] = np.clip(population[i], lb, ub)\n\n            # Introduce random perturbation to the global best\n            global_best_position += np.random.normal(0, 0.05, self.dim)\n\n        return global_best_position, global_best_score", "name": "HybridParticleSwarmOptimization", "description": "The algorithm enhances exploration by introducing a random perturbation to the global best position, encouraging diverse solutions and potentially escaping local optima.", "configspace": "", "generation": 2, "fitness": 0.8400780694851216, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.018. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b6ab50ef-b53b-4b51-bc24-b9cd9ac77645", "metadata": {"aucs": [0.8567050623647843, 0.8149670439599594, 0.8485621021306212], "final_y": [0.1284553724929367, 0.1435642352030776, 0.12524284619570036]}, "mutation_prompt": null}
{"id": "8b46d484-0b1b-458a-9ffe-8f4d537b90f9", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 10.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring\n            offspring = []\n            for i in range(num_parents):\n                parent = parents[i]\n                mutated = parent + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.15 * pop_size)  # Increased elite size\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce an elitism strategy by retaining more elite individuals to enhance convergence in the solution space.", "configspace": "", "generation": 3, "fitness": 0.8449270153237746, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.017. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "0ebcc89d-33fd-4411-8a2e-3b12335657a1", "metadata": {"aucs": [0.8497274897872227, 0.8219290183827791, 0.863124537801322], "final_y": [0.11857328626960584, 0.14938811858537993, 0.1324057976985218]}, "mutation_prompt": null}
{"id": "0e868bb3-1261-40c5-9de7-f77e4102133e", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.9  # Line changed to start with a higher inertia weight\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.inertia_weight_decay = 0.99  # Line added for inertia weight decay\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                mutation_factor = chaotic_sequence[i] * np.random.rand()  # Line changed to add adaptive chaotic mutation\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i] + mutation_factor  # Line changed to integrate mutation factor\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n            self.inertia_weight *= self.inertia_weight_decay  # Line added for inertia weight decay\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "An improved Chaotic Swarm Optimizer that integrates adaptive chaotic mutation and dynamic inertia weight adjustments to enhance exploration and convergence in black-box optimization problems.", "configspace": "", "generation": 3, "fitness": 0.8040738025845885, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.026. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.7741916225287615, 0.8383291017249557, 0.7997006835000486], "final_y": [0.144631357634047, 0.1436271827286948, 0.15411038470617366]}, "mutation_prompt": null}
{"id": "ee21f137-6bb0-421c-9128-88ae5c0a507b", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover and differential mutation\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                \n                # Apply differential mutation\n                F = 0.6 + np.random.rand() * 0.4  # Adaptive mutation factor\n                child1 += F * (p1 - p2)\n                child2 += F * (p2 - p1)\n\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                mutation_strength = (func.bounds.ub - func.bounds.lb) / 15.0\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improved Dynamic Evolutionary Optimizer by incorporating diversity preservation and adaptive scaling to enhance exploration and exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.8452355864982741, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.027. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "51aed368-663e-42f8-9ac5-4377697248df", "metadata": {"aucs": [0.8315497913615149, 0.8209032636089197, 0.8832537045243878], "final_y": [0.13492149184311875, 0.14929066948294967, 0.12136477334405449]}, "mutation_prompt": null}
{"id": "39d51e58-ac57-4582-bb2c-e07a70d7d12b", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "The Enhanced Dynamic Evolutionary Optimizer (EDEO) is refined with a novel adaptive crossover mechanism and a more diverse initialization strategy to boost exploration and convergence in black-box optimization problems.", "configspace": "", "generation": 3, "fitness": 0.8490069321448299, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.026. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "0ebcc89d-33fd-4411-8a2e-3b12335657a1", "metadata": {"aucs": [0.8846231640004569, 0.8261429820783103, 0.8362546503557223], "final_y": [0.11637600702352024, 0.1464975663321072, 0.14259730818880367]}, "mutation_prompt": null}
{"id": "b8b20b7a-a8fa-4118-8325-933b3d97e18e", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                mutation_strength = np.std(fitness) / 10.0  # Changed mutation strategy\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhanced Dynamic Evolutionary Optimizer by implementing adaptive mutation strength based on fitness variance to optimize convergence rate effectively.", "configspace": "", "generation": 3, "fitness": 0.8514533450265888, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.034. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "d66633ab-2da2-4eeb-a73c-07b3e2b2c35e", "metadata": {"aucs": [0.8255733494398808, 0.8287202862074544, 0.9000663994324309], "final_y": [0.1360197062926738, 0.14651251605598892, 0.1197327577063323]}, "mutation_prompt": null}
{"id": "77222f98-1436-4cb3-a382-ba5b5176ffa3", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia_weight = 0.7\n        self.mutation_rate = 0.1\n        self.bounds = None\n        self.max_velocity = 0.2 * (func.bounds.ub - func.bounds.lb)  # New\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(population)\n        personal_best_scores = np.array([float('inf')] * self.population_size)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        evals = 0\n\n        while evals < self.budget:\n            for i in range(self.population_size):\n                score = func(population[i])\n                evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = population[i]\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = population[i]\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 cognitive_component + social_component)\n\n                # Clamp velocities to max_velocity\n                velocities[i] = np.clip(velocities[i], -self.max_velocity, self.max_velocity)\n\n                population[i] += velocities[i]\n\n                # Adaptive mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    population[i] += mutation_vector\n\n                # Ensure bounds\n                population[i] = np.clip(population[i], lb, ub)\n            \n            self.inertia_weight = 0.9 - 0.5 * (evals / self.budget)  # New\n\n        return global_best_position, global_best_score", "name": "HybridParticleSwarmOptimization", "description": "Enhanced Hybrid Particle Swarm Optimization by incorporating velocity clamping and dynamic inertia adjustment to improve convergence and exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'func' is not defined\").", "error": "NameError(\"name 'func' is not defined\")", "parent_id": "b6ab50ef-b53b-4b51-bc24-b9cd9ac77645", "metadata": {}, "mutation_prompt": null}
{"id": "aea81b0e-6104-41d7-b3c6-8576c6c4c547", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.9  # Changed from 0.5 to 0.9\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "A refined ChaoticSwarmOptimizer with enhanced inertia weight dynamics for better convergence in black-box optimization problems.", "configspace": "", "generation": 4, "fitness": 0.8093371786407543, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.019. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.7953046865876068, 0.8364856033579661, 0.7962212459766898], "final_y": [0.15477319293027803, 0.14417488895784336, 0.15614132033011996]}, "mutation_prompt": null}
{"id": "73b7c1f8-8579-4bfc-b353-e7cb692c8a96", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Adaptive elitism based on current diversity\n            diversity_level = np.std(population, axis=0).mean()\n            elite_size = int(max(1, diversity_level * 10))  # Change made here\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improved Dynamic Evolutionary Optimizer by incorporating adaptive elitism based on the current diversity level to enhance exploration and convergence.", "configspace": "", "generation": 4, "fitness": 0.8410516228287935, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.024. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "39d51e58-ac57-4582-bb2c-e07a70d7d12b", "metadata": {"aucs": [0.8634048681532349, 0.808179092918553, 0.8515709074145923], "final_y": [0.121623504020405, 0.15466097349968577, 0.13766262940671792]}, "mutation_prompt": null}
{"id": "c822d879-e374-4d87-8b3b-818b902b8f27", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.9  # Changed from 0.5 to 0.9 for adaptive inertia\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "A refined version of ChaoticSwarmOptimizer, integrating adaptive inertia for enhanced exploration and convergence in black-box optimization.", "configspace": "", "generation": 4, "fitness": 0.8113257768889045, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.030. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.7954345626077741, 0.8538631483525084, 0.7846796197064312], "final_y": [0.1531308742955223, 0.13849262089323, 0.16009631215236042]}, "mutation_prompt": null}
{"id": "72a96c6c-cf88-4557-9ff1-969959e16dc4", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                mutation_strength = np.std(fitness) / 10.0\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n            # Dynamically adjust population size\n            pop_size = int(max(5, (np.std(fitness) / np.mean(fitness)) * pop_size))\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhanced Dynamic Evolutionary Optimizer by introducing adaptive population size scaling based on fitness variance to dynamically balance exploration and exploitation in optimization tasks.", "configspace": "", "generation": 4, "fitness": 0.7968226400780969, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.017. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "b8b20b7a-a8fa-4118-8325-933b3d97e18e", "metadata": {"aucs": [0.7773866679333772, 0.7953228976446336, 0.81775835465628], "final_y": [0.16375100122270503, 0.15980488170439755, 0.15049582861332667]}, "mutation_prompt": null}
{"id": "a59f7667-18ed-4294-b12c-0f7f30661da8", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    max(0.3, self.inertia_weight - 0.1 * (func_evals / self.budget)) * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "A refined metaheuristic algorithm, ChaoticSwarmOptimizer, introducing dynamic inertia weight adjustment for improved exploration-exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.846064949818452, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.016. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.8555251091626034, 0.8597824265470411, 0.8228873137457117], "final_y": [0.11765314668337457, 0.12968811111802714, 0.13753200095876783]}, "mutation_prompt": null}
{"id": "efaa1e21-413f-445e-b287-1b87c8f065c0", "solution": "import numpy as np\n\nclass EnhancedChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaotic_maps = [lambda x: 4 * x * (1 - x), lambda x: np.sin(np.pi * x)]  # Added additional chaotic map\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.learning_rate_decay = 0.99  # Adaptive learning rate decay\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n        chaotic_map_selector = np.random.choice(self.chaotic_maps)  # Select chaotic map randomly\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = chaotic_map_selector(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n            \n            self.inertia_weight *= self.learning_rate_decay  # Decay inertia weight\n\n        return global_best", "name": "EnhancedChaoticSwarmOptimizer", "description": "An enhanced chaotic swarm optimizer integrating a chaotic map selection mechanism and adaptive learning rates to improve exploration and convergence.", "configspace": "", "generation": 5, "fitness": 0.8350147784072693, "feedback": "The algorithm EnhancedChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.033. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.7997176148561533, 0.8798030891323685, 0.825523631233286], "final_y": [0.14763322094059128, 0.12237914494228874, 0.13482918901025565]}, "mutation_prompt": null}
{"id": "a2ae48f7-c694-432c-bfa9-444454eac727", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n        \n        adaptive_inertia = self.inertia_weight  # Added line\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                adaptive_chaos = chaotic_sequence[i] * (func_evals / self.budget)  # Changed line\n\n                velocity[i] = (\n                    adaptive_inertia * velocity[i]  # Changed line\n                    + self.cognitive_coeff * adaptive_chaos * (personal_best[i] - pop[i])  # Changed line\n                    + self.social_coeff * adaptive_chaos * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "An enhanced Chaotic Swarm Optimizer incorporating adaptive chaotic intensity to balance exploration and exploitation for improved convergence in black-box optimization problems.", "configspace": "", "generation": 5, "fitness": 0.8415379928417132, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.023. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.8123426523087066, 0.8692470237653953, 0.8430243024510375], "final_y": [0.13063118207871516, 0.12380869598087019, 0.1294578437209909]}, "mutation_prompt": null}
{"id": "c4cd36bc-9ca1-43c3-a8bb-0257be3592a7", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia_weight = 0.7\n        self.mutation_rate = 0.1\n        self.bounds = None\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(population)\n        personal_best_scores = np.array([float('inf')] * self.population_size)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        evals = 0\n\n        while evals < self.budget:\n            for i in range(self.population_size):\n                score = func(population[i])\n                evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = population[i]\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = population[i]\n\n            # Nonlinear inertia weight adaptation\n            self.inertia_weight = 0.9 - (0.7 * evals / self.budget)\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 cognitive_component + social_component)\n\n                population[i] += velocities[i]\n\n                # Dynamic mutation rate based on evaluations\n                if np.random.rand() < self.mutation_rate * (1 - evals / self.budget):\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    population[i] += mutation_vector\n\n                # Ensure bounds\n                population[i] = np.clip(population[i], lb, ub)\n\n        return global_best_position, global_best_score", "name": "HybridParticleSwarmOptimization", "description": "The algorithm combines particle swarm optimization with a novel nonlinear inertia weight adaptation and dynamic mutation strategy to enhance exploration and convergence in complex search spaces.", "configspace": "", "generation": 5, "fitness": 0.8423158719963958, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.014. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "b6ab50ef-b53b-4b51-bc24-b9cd9ac77645", "metadata": {"aucs": [0.8353239736744992, 0.8292675770298171, 0.8623560652848713], "final_y": [0.12930009147326738, 0.12678181202173788, 0.12082464519103253]}, "mutation_prompt": null}
{"id": "fe596be9-5353-41c1-ae5a-80df3cf95c07", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                mutation_strength = np.std(fitness) / 10.0\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            elite_index = np.argmin(fitness)  # Added elitism\n            elite = population[elite_index]\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n            population[-1] = elite  # Preserve the best individual\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improved Dynamic Evolutionary Optimizer by introducing elitism and dynamic population size adjustment to enhance convergence speed and solution quality.", "configspace": "", "generation": 5, "fitness": 0.8394710224760279, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.024. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "b8b20b7a-a8fa-4118-8325-933b3d97e18e", "metadata": {"aucs": [0.8334856352821425, 0.8140271666682983, 0.8709002654776429], "final_y": [0.1391406390864861, 0.15243487831363545, 0.12548727836267626]}, "mutation_prompt": null}
{"id": "b37a7ef2-e256-4638-8826-43c9da8388ed", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Novel stochastic tournament selection\n            tournament_size = 3\n            parents_indices = np.random.choice(num_parents, (num_parents, tournament_size))\n            parents = np.array([population[np.min(idx)] for idx in parents_indices])\n\n            # Adaptive crossover rate based on fitness variance\n            fitness_variance = np.var(fitness)\n            crossover_rate = 0.5 + 0.5 * (fitness_variance / (fitness_variance + 1))\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                if np.random.rand() < crossover_rate:\n                    crossover_point = np.random.randint(1, self.dim)\n                    child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                else:\n                    child = parent1.copy()\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improved DynamicEvolutionaryOptimizer by adding novel stochastic tournament selection and adaptive crossover rates based on fitness variance.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)').", "error": "ValueError('all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)')", "parent_id": "39d51e58-ac57-4582-bb2c-e07a70d7d12b", "metadata": {}, "mutation_prompt": null}
{"id": "492b341d-cfb4-46ea-a6bd-caa4defa0f61", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                mutation_strength = np.std(fitness) * (func.bounds.ub - func.bounds.lb) / 10.0\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "The algorithm improves offspring mutation strategy by introducing adaptive scaling based on fitness variance to enhance exploration and convergence in black-box optimization problems.", "configspace": "", "generation": 6, "fitness": 0.865572651496661, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.021. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "d66633ab-2da2-4eeb-a73c-07b3e2b2c35e", "metadata": {"aucs": [0.8359719908992431, 0.8825705075722095, 0.8781754560185302], "final_y": [0.13513480999112915, 0.11741257042802289, 0.11762758407476448]}, "mutation_prompt": null}
{"id": "add2fc63-2880-4854-b6b3-40f8ff48853f", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia_weight_max = 0.9\n        self.inertia_weight_min = 0.4\n        self.mutation_rate = 0.2\n        self.bounds = None\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(population)\n        personal_best_scores = np.array([float('inf')] * self.population_size)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        evals = 0\n\n        while evals < self.budget:\n            inertia_weight = self.inertia_weight_max - (self.inertia_weight_max - self.inertia_weight_min) * (evals / self.budget)\n            for i in range(self.population_size):\n                score = func(population[i])\n                evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = population[i]\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = population[i]\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                velocities[i] = (inertia_weight * velocities[i] +\n                                 cognitive_component + social_component)\n\n                population[i] += velocities[i]\n\n                # Enhanced mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_strength = 0.1 * (1 - evals / self.budget)\n                    mutation_vector = np.random.normal(0, mutation_strength, self.dim)\n                    population[i] += mutation_vector\n\n                # Ensure bounds\n                population[i] = np.clip(population[i], lb, ub)\n\n        return global_best_position, global_best_score", "name": "HybridParticleSwarmOptimization", "description": "Hybrid Particle Swarm Optimization with dynamic inertia weight and enhanced mutation strategy to improve exploration and convergence.", "configspace": "", "generation": 6, "fitness": 0.823508809698008, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.018. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "b6ab50ef-b53b-4b51-bc24-b9cd9ac77645", "metadata": {"aucs": [0.8374155455354402, 0.7981571234397885, 0.8349537601187953], "final_y": [0.12796919136952845, 0.13992759304871105, 0.13157810096315847]}, "mutation_prompt": null}
{"id": "17be0c82-ae46-45f0-9bfd-c5c45afc4f52", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "The Enhanced Dynamic Evolutionary Optimizer (EDEO) is further refined by introducing a dynamic population size adjustment mechanism based on convergence behavior to enhance exploration and convergence in black-box optimization problems.", "configspace": "", "generation": 6, "fitness": 0.8733364445662707, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.012. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "39d51e58-ac57-4582-bb2c-e07a70d7d12b", "metadata": {"aucs": [0.8581590996543452, 0.8873480428131931, 0.874502191231274], "final_y": [0.120486589414743, 0.11805318491402783, 0.12327035255872643]}, "mutation_prompt": null}
{"id": "739f905d-27f7-4d04-90d5-32fa349d9bc0", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 10.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim - 1)  # Ensure crossover is not at the boundary\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhanced Dynamic Evolutionary Optimizer by optimizing mutation strength adaptation and crossover point selection to improve convergence and exploration balance in black-box optimization problems.", "configspace": "", "generation": 6, "fitness": 0.8662109375636465, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.011. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "39d51e58-ac57-4582-bb2c-e07a70d7d12b", "metadata": {"aucs": [0.8518962910521164, 0.878724668052068, 0.8680118535867554], "final_y": [0.12762245921847204, 0.11912475531512456, 0.11795617655994584]}, "mutation_prompt": null}
{"id": "9bfaaf23-2d3a-45e2-853a-d50f415b2020", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Tournament selection for parents\n            num_parents = pop_size // 2\n            parents = []\n            for _ in range(num_parents):\n                # Tournament of size 3\n                competitors_idx = np.random.choice(pop_size, 3, replace=False)\n                best_idx = competitors_idx[np.argmin(fitness[competitors_idx])]\n                parents.append(population[best_idx])\n            parents = np.array(parents)\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Adaptive Gaussian mutation based on individual fitness\n                mutation_strength = np.std(fitness) * (func.bounds.ub - func.bounds.lb) / 10.0\n                offspring[-2] += np.random.normal(0, mutation_strength * (1.0 / (1.0 + fitness[best_idx])), self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength * (1.0 / (1.0 + fitness[best_idx])), self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "The algorithm enhances offspring diversity by introducing tournament selection and adaptive Gaussian mutation based on individual fitness to improve exploration and convergence in black-box optimization problems.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'parents_indices' is not defined\").", "error": "NameError(\"name 'parents_indices' is not defined\")", "parent_id": "492b341d-cfb4-46ea-a6bd-caa4defa0f61", "metadata": {}, "mutation_prompt": null}
{"id": "d3e0e914-539f-477e-bc18-f28b60f06360", "solution": "import numpy as np\n\nclass HybridParticleSwarmOptimization:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30\n        self.c1 = 2.0  # cognitive coefficient\n        self.c2 = 2.0  # social coefficient\n        self.inertia_weight = 0.7\n        self.mutation_rate = 0.1\n        self.bounds = None\n\n    def __call__(self, func):\n        np.random.seed(42)  # For reproducibility\n        self.bounds = func.bounds\n        lb, ub = self.bounds.lb, self.bounds.ub\n        population = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best_positions = np.copy(population)\n        personal_best_scores = np.array([float('inf')] * self.population_size)\n\n        global_best_position = None\n        global_best_score = float('inf')\n        evals = 0\n\n        while evals < self.budget:\n            for i in range(self.population_size):\n                score = func(population[i])\n                evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best_scores[i] = score\n                    personal_best_positions[i] = population[i]\n                if score < global_best_score:\n                    global_best_score = score\n                    global_best_position = population[i]\n\n            for i in range(self.population_size):\n                r1, r2 = np.random.rand(), np.random.rand()\n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (global_best_position - population[i])\n                velocities[i] = (self.inertia_weight * velocities[i] +\n                                 cognitive_component + social_component)\n\n                population[i] += velocities[i]\n\n                # Adaptive mutation with dynamic mutation rate\n                dynamic_mutation_rate = self.mutation_rate * (1 - evals / self.budget)\n                if np.random.rand() < dynamic_mutation_rate:\n                    mutation_vector = np.random.normal(0, 0.1, self.dim)\n                    population[i] += mutation_vector\n\n                # Ensure bounds\n                population[i] = np.clip(population[i], lb, ub)\n\n        return global_best_position, global_best_score", "name": "HybridParticleSwarmOptimization", "description": "The algorithm refines its adaptive mutation strategy by dynamically adjusting the mutation rate based on convergence progress to improve exploration and exploitation balance.", "configspace": "", "generation": 7, "fitness": 0.839757951572552, "feedback": "The algorithm HybridParticleSwarmOptimization got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.010. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "b6ab50ef-b53b-4b51-bc24-b9cd9ac77645", "metadata": {"aucs": [0.8253995168360013, 0.8451552366939702, 0.8487191011876846], "final_y": [0.1320147979913685, 0.13471999402899626, 0.12491211683900394]}, "mutation_prompt": null}
{"id": "5a22f057-17a2-4caa-9f13-64e3da15cac6", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.9  # Changed from 0.5 to 0.9\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "A refined ChaoticSwarmOptimizer utilizing adaptive inertia weight to balance exploration and exploitation in black-box optimization.", "configspace": "", "generation": 7, "fitness": 0.7939900499773481, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.029. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.7545536176887608, 0.8026930487606014, 0.8247234834826821], "final_y": [0.16019460703803978, 0.1566756852137181, 0.14715875409490897]}, "mutation_prompt": null}
{"id": "6c1b505f-bd1f-4157-b551-5848e2d4d7c7", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.std(fitness) * np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improved Adaptive Mutation Strength by Scaling with Fitness Variance for Enhanced Exploration in Black-Box Optimization.", "configspace": "", "generation": 7, "fitness": 0.8703335451513118, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.007. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "17be0c82-ae46-45f0-9bfd-c5c45afc4f52", "metadata": {"aucs": [0.86157963911105, 0.8795108227844526, 0.869910173558433], "final_y": [0.12060444961430139, 0.11840297408506739, 0.12162900317540071]}, "mutation_prompt": null}
{"id": "757cebf8-dbc6-4f5c-84ff-06ef1f18bcf9", "solution": "# Description: Refined Chaotic Swarm Optimizer with adaptive inertia weight based on fitness variance for improved exploration and convergence.\n# Code: \nimport numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            fitness_variance = np.var(personal_best_scores)\n            self.inertia_weight = 0.9 - 0.5 * (fitness_variance / (1 + fitness_variance))  # Adaptive inertia weight\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "Refined Chaotic Swarm Optimizer with adaptive inertia weight based on fitness variance for improved exploration and convergence.", "configspace": "", "generation": 7, "fitness": 0.8116739024972272, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.003. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.815614744306369, 0.8078598437364883, 0.8115471194488243], "final_y": [0.1474756959482857, 0.15487302087663002, 0.1523910466209103]}, "mutation_prompt": null}
{"id": "8d91ecad-3b60-4df1-97ae-62f7f46a6d12", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                self.cognitive_coeff = 1.0 + chaotic_sequence[i]  # Adjust cognitive coefficient\n                self.social_coeff = 1.0 + chaotic_sequence[i]     # Adjust social coefficient\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "Enhance exploration by adjusting cognitive and social coefficients dynamically based on chaotic behavior.", "configspace": "", "generation": 8, "fitness": 0.8474222240263541, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.019. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.820915276859558, 0.8580217612811762, 0.8633296339383281], "final_y": [0.13256383383769088, 0.12187783620173287, 0.13140870029162977]}, "mutation_prompt": null}
{"id": "52168c88-9ea8-4b64-9e27-516bf270766b", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 11.5) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)  # Adjusted scaling factor\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the Enhanced Dynamic Evolutionary Optimizer by slightly adjusting mutation strength scaling to improve exploration and convergence balance.  ", "configspace": "", "generation": 8, "fitness": 0.8713271764958771, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.019. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "17be0c82-ae46-45f0-9bfd-c5c45afc4f52", "metadata": {"aucs": [0.8456917071418626, 0.891168472365363, 0.8771213499804057], "final_y": [0.1305736836130904, 0.1147072622007691, 0.12090279263906978]}, "mutation_prompt": null}
{"id": "ebc04d8a-6226-4851-a430-7bedb665ff2a", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                iteration_factor = 1 - (self.current_evaluations / self.budget)\n                mutation_strength = np.std(fitness) * (func.bounds.ub - func.bounds.lb) / 10.0 * iteration_factor\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhanced mutation strategy by dynamically adapting mutation strength based on both fitness variance and iteration count for improved exploration and convergence.", "configspace": "", "generation": 8, "fitness": 0.8640458265220624, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.021. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "492b341d-cfb4-46ea-a6bd-caa4defa0f61", "metadata": {"aucs": [0.8358088539090405, 0.8843954696645675, 0.8719331559925795], "final_y": [0.1319209593943781, 0.11984690687963773, 0.12177636353645438]}, "mutation_prompt": null}
{"id": "5f8d8486-feb5-411d-878e-d64d3d2c1f71", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = 10 + 5 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Generate offspring using crossover\n            offspring = []\n            for i in range(num_parents // 2):\n                p1, p2 = parents[2 * i], parents[2 * i + 1]\n                cross_point = np.random.randint(1, self.dim)\n                child1 = np.concatenate((p1[:cross_point], p2[cross_point:]))\n                child2 = np.concatenate((p2[:cross_point], p1[cross_point:]))\n                offspring.extend([child1, child2])\n                \n                # Mutate offspring\n                convergence_rate = np.std(fitness) / np.mean(fitness)\n                mutation_strength = np.std(fitness) * (func.bounds.ub - func.bounds.lb) / 10.0 * (1 + convergence_rate)\n                offspring[-2] += np.random.normal(0, mutation_strength, self.dim)\n                offspring[-1] += np.random.normal(0, mutation_strength, self.dim)\n\n            # Evaluate offspring\n            offspring = [np.clip(child, func.bounds.lb, func.bounds.ub) for child in offspring]\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Select the best individuals to form the new population\n            population = np.vstack((parents, offspring))\n            fitness = np.hstack((fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhanced Dynamic Evolutionary Optimizer by incorporating adaptive mutation strength based on both fitness variance and convergence rate to improve exploration and convergence balance in black-box optimization problems.", "configspace": "", "generation": 8, "fitness": 0.8626324733924151, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.029. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "492b341d-cfb4-46ea-a6bd-caa4defa0f61", "metadata": {"aucs": [0.8226099613161015, 0.8770254628004686, 0.8882619960606752], "final_y": [0.14399754544548649, 0.11788513787986277, 0.11492461178028857]}, "mutation_prompt": null}
{"id": "c65f0894-23fa-46c7-b462-bc5dd4bfdb72", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.std(fitness) * np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim // 2)  # Modify crossover strategy\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance exploration by modifying crossover strategy to consider dimensional diversity in offspring generation.", "configspace": "", "generation": 8, "fitness": 0.8598166638257744, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.014. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "6c1b505f-bd1f-4157-b551-5848e2d4d7c7", "metadata": {"aucs": [0.8409229655245045, 0.866228914906925, 0.8722981110458936], "final_y": [0.13383349974179326, 0.12182794994068058, 0.12003990444489421]}, "mutation_prompt": null}
{"id": "74a5bc94-3f9f-470f-9fb6-d089658880b6", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents with diversity emphasis\n            num_parents = pop_size // 2\n            diversity_factor = np.std(fitness)\n            parents_indices = np.argsort(fitness + diversity_factor)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 11.5) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)  # Adjusted scaling factor\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance the optimizer by integrating a fitness diversity mechanism to retain diversity and improve exploration.", "configspace": "", "generation": 9, "fitness": 0.8712362393442045, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.008. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "52168c88-9ea8-4b64-9e27-516bf270766b", "metadata": {"aucs": [0.8615476376187898, 0.8811486218952845, 0.8710124585185389], "final_y": [0.12078797476968828, 0.11914812808781128, 0.11974335841954065]}, "mutation_prompt": null}
{"id": "ad54562b-8d07-45eb-82e0-8203bc3ae8be", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 10.5) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)  # Adjusted scaling factor\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.15 * pop_size)  # Adjusted elite size\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the Enhanced Dynamic Evolutionary Optimizer by modifying the mutation strength scaling and adjusting the elitism strategy for improved solution quality.", "configspace": "", "generation": 9, "fitness": 0.8663949355273765, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.004. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "52168c88-9ea8-4b64-9e27-516bf270766b", "metadata": {"aucs": [0.8639298064966952, 0.8637830393197089, 0.8714719607657254], "final_y": [0.12002821817234444, 0.12032515356588414, 0.11621382950505543]}, "mutation_prompt": null}
{"id": "0a3483df-076a-4be3-9737-5b73d95384a1", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 10.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Dynamic adjustment based on improvement\n            improvement_factor = np.exp(-1 * (fitness[parents_indices] - fitness[parents_indices[0]]) / (fitness[parents_indices[0]] + 1e-10))\n            mutation_strength *= improvement_factor.reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim - 1)  # Ensure crossover is not at the boundary\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance mutation strength adaptation by dynamically adjusting based on fitness improvement for improved exploration and convergence balance.", "configspace": "", "generation": 9, "fitness": 0.8593639552744011, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.004. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "739f905d-27f7-4d04-90d5-32fa349d9bc0", "metadata": {"aucs": [0.8546771443000445, 0.864093512467405, 0.859321209055754], "final_y": [0.1271438427002729, 0.12281805128216072, 0.12903560688296078]}, "mutation_prompt": null}
{"id": "c6409dd8-fdf7-4de0-b5a1-c1152c8d3461", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the Enhanced Dynamic Evolutionary Optimizer by introducing a fitness-based dynamic mutation strength adjustment to enhance exploration and precision in black-box optimization.", "configspace": "", "generation": 9, "fitness": 0.8715093365842964, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.005. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "17be0c82-ae46-45f0-9bfd-c5c45afc4f52", "metadata": {"aucs": [0.8695221356933387, 0.8788180919050024, 0.8661877821545478], "final_y": [0.12026015112936828, 0.12295964471607979, 0.12238070419186708]}, "mutation_prompt": null}
{"id": "c267965d-3eeb-438d-b4c3-71106a93f483", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 8.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)  # Changed mutation scaling from 10.0 to 8.0\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim - 1)  # Ensure crossover is not at the boundary\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.15 * pop_size)  # Changed elite size from 0.1 to 0.15\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improved Adaptive Strategy for Offspring Diversity and Selection by enhancing mutation distribution and elite preservation to boost exploratory capabilities and convergence precision.", "configspace": "", "generation": 9, "fitness": 0.8665775342972092, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.015. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "739f905d-27f7-4d04-90d5-32fa349d9bc0", "metadata": {"aucs": [0.8487281247739462, 0.8645827303828195, 0.8864217477348619], "final_y": [0.13270755537501955, 0.12188748139631711, 0.12041753871996586]}, "mutation_prompt": null}
{"id": "57f235f4-9792-475d-85ab-959423279967", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 2.0  # Increased from 1.5\n        self.social_coeff = 2.0  # Increased from 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "Enhance the ChaoticSwarmOptimizer by increasing the cognitive and social coefficients to improve convergence speed and solution quality.", "configspace": "", "generation": 10, "fitness": 0.8570737358320683, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.010. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.8547704601179023, 0.8700429207066482, 0.8464078266716539], "final_y": [0.12353793341419683, 0.12224994616618767, 0.13408307187928337]}, "mutation_prompt": null}
{"id": "02d0f564-bebd-44d7-ba5c-55de23b1b2cc", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + (1.5 + chaotic_sequence[i]) * chaotic_sequence[i] * (personal_best[i] - pop[i])  # Changed line\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "Introduced adaptive cognitive and social coefficients for balanced exploration and exploitation in ChaoticSwarmOptimizer.", "configspace": "", "generation": 10, "fitness": 0.8260383147257384, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.017. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.8255716327917746, 0.8470650544796576, 0.8054782569057832], "final_y": [0.13855146880975766, 0.12920166958311508, 0.15265341357932705]}, "mutation_prompt": null}
{"id": "7c9a0a1d-8f71-4b29-8743-b4c68ddc4c6c", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.7  # Adjusted coefficient\n        self.social_coeff = 1.7  # Adjusted coefficient\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "Enhance ChaoticSwarmOptimizer by slightly adjusting cognitive and social coefficients to improve balance between exploration and exploitation.", "configspace": "", "generation": 10, "fitness": 0.8247552053747187, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.825 with standard deviation 0.023. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.7928513855443119, 0.8399106811305053, 0.8415035494493386], "final_y": [0.15452795121300433, 0.12997033590086626, 0.13424324049974368]}, "mutation_prompt": null}
{"id": "f8d409e0-93e3-49f0-8dbf-744a3edd9b3f", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents with diversity emphasis\n            num_parents = pop_size // 2\n            diversity_factor = np.std(fitness)\n            parents_indices = np.argsort(fitness + diversity_factor)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 11.5) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)  # Adjusted scaling factor\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                # Use dynamic crossover point based on current evaluations\n                crossover_point = int(np.random.uniform(1, self.dim * (1 - self.current_evaluations / self.budget)))\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the optimizer by using a fitness diversity mechanism with a dynamic crossover point based on current evaluations to enhance adaptability and exploration.", "configspace": "", "generation": 10, "fitness": 0.8632751110110707, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.009. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "74a5bc94-3f9f-470f-9fb6-d089658880b6", "metadata": {"aucs": [0.866964292256168, 0.8513925045991613, 0.8714685361778824], "final_y": [0.12018989042855788, 0.1262352500385271, 0.12217373989476255]}, "mutation_prompt": null}
{"id": "33004d56-b7c7-4166-84fc-80a137925c85", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Dynamic elitism rate\n            elite_size = int(0.1 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce a dynamic elitism rate based on convergence to improve balance between exploration and exploitation.", "configspace": "", "generation": 10, "fitness": 0.872462592559346, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.015. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "17be0c82-ae46-45f0-9bfd-c5c45afc4f52", "metadata": {"aucs": [0.8515394335490518, 0.8837085278009347, 0.8821398163280514], "final_y": [0.12331130943864554, 0.11618776499469952, 0.11502466393717592]}, "mutation_prompt": null}
{"id": "a90c7ec3-f055-4e9c-b0b5-47363e923e36", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.9\n        self.cognitive_coeff = 2.0\n        self.social_coeff = 2.0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                inertia = self.inertia_weight * (0.5 + 0.5 * chaotic_sequence[i])\n                velocity[i] = (\n                    inertia * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "Enhance the ChaoticSwarmOptimizer by introducing adaptive coefficients and dynamic inertia to improve exploration and exploitation balance.", "configspace": "", "generation": 11, "fitness": 0.8082502948295621, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.045. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.7439243503178004, 0.8404430245117043, 0.8403835096591817], "final_y": [0.16137193594755073, 0.14318401927794722, 0.1398294242771061]}, "mutation_prompt": null}
{"id": "41eb8981-fa8c-4642-b061-cc42bbb9b26c", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                # Modify to use an adaptive crossover point\n                crossover_point = int(self.dim * (1 - (fitness[i] - np.min(fitness)) / (np.max(fitness) - np.min(fitness) + 1e-8)))  \n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the Enhanced Dynamic Evolutionary Optimizer by incorporating an adaptive crossover point technique based on fitness diversity to improve exploration and exploitation balance.", "configspace": "", "generation": 11, "fitness": 0.8697750058843609, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.020. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "17be0c82-ae46-45f0-9bfd-c5c45afc4f52", "metadata": {"aucs": [0.8463932344971983, 0.8943244663552765, 0.8686073168006077], "final_y": [0.12972661182861955, 0.1173445975712668, 0.12168836692365192]}, "mutation_prompt": null}
{"id": "d9b17c26-3268-49d6-9b5a-c5c32fadaa29", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 11.5) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)  # Adjusted scaling factor\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.15 * pop_size)  # Adjusted from 0.1 to 0.15\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Slightly adjust the elitism rate to improve convergence speed and solution quality.", "configspace": "", "generation": 11, "fitness": 0.8699038816359229, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.006. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "52168c88-9ea8-4b64-9e27-516bf270766b", "metadata": {"aucs": [0.8655979245663442, 0.865776046769358, 0.8783376735720669], "final_y": [0.12443792040012225, 0.11942687612569569, 0.11874394815578726]}, "mutation_prompt": null}
{"id": "07086571-d1cc-4bc4-8bce-62e72b80177e", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - ((0.5 * (self.current_evaluations / self.budget)) + np.linspace(0, 0.5, num_parents)).reshape(-1, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Dynamic elitism rate\n            elite_size = int(0.1 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce a dynamic mutation strength scaling based on convergence to enhance exploration and precision.", "configspace": "", "generation": 11, "fitness": 0.8707107242974131, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.008. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "33004d56-b7c7-4166-84fc-80a137925c85", "metadata": {"aucs": [0.8600016977467537, 0.8779209952530622, 0.8742094798924236], "final_y": [0.12236737849178447, 0.12184127451813731, 0.11693129037237593]}, "mutation_prompt": null}
{"id": "e0117327-2322-493f-92f3-2ee34e15408a", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 10.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Dynamic elitism rate\n            elite_size = int(0.12 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improve exploration capabilities by enhancing dynamic mutation strength and fine-tuning elitism rate.", "configspace": "", "generation": 11, "fitness": 0.8720068641827049, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.015. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "33004d56-b7c7-4166-84fc-80a137925c85", "metadata": {"aucs": [0.8600600407849615, 0.8935159574619242, 0.8624445943012291], "final_y": [0.11808386458791087, 0.11254092403400295, 0.12399439945816015]}, "mutation_prompt": null}
{"id": "3807fc94-6281-421d-bfcf-787cd852c85f", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover probability\n            offspring = []\n            diversity_measure = np.std(population, axis=0).mean()  # Calculate diversity\n            crossover_probability = 0.5 + 0.5 * (1 - diversity_measure / (func.bounds.ub - func.bounds.lb).mean())  # Dynamic crossover\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                if np.random.rand() < crossover_probability:  # Use dynamic crossover probability\n                    crossover_point = np.random.randint(1, self.dim)\n                    child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                else:\n                    child = parent1.copy()\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Dynamic elitism rate\n            elite_size = int(0.1 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the strategy by introducing a dynamic crossover probability based on population diversity to enhance convergence and diversity balance.", "configspace": "", "generation": 12, "fitness": 0.870731850882046, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.007. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "33004d56-b7c7-4166-84fc-80a137925c85", "metadata": {"aucs": [0.8617084989215409, 0.8713622134671963, 0.8791248402574008], "final_y": [0.12071835968172695, 0.12592652821582972, 0.11754476009258352]}, "mutation_prompt": null}
{"id": "ff4e97d2-10e9-4e57-8651-88ff9e791fe6", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.6, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                # Change 1: Add dynamic parent selection for crossover\n                parent2 = parents[np.random.choice(num_parents, p=(1/num_parents) * np.ones(num_parents))]\n                crossover_point = np.random.randint(1, self.dim)\n                # Change 2: Alter crossover strategy for diversity\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce an adaptive mutation and crossover strategy to improve exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.874942452562174, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.017. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "17be0c82-ae46-45f0-9bfd-c5c45afc4f52", "metadata": {"aucs": [0.851729132291273, 0.8893860188319436, 0.8837122065633056], "final_y": [0.12105570991583536, 0.12182391950372629, 0.116990470020927]}, "mutation_prompt": null}
{"id": "579a4576-ab0b-43f3-88b2-3647236aebd7", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2\n\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                weight = fitness[parents_indices[i]] / np.sum(fitness[parents_indices])\n                child = weight * parent1 + (1 - weight) * parent2\n                mutated = child + np.random.normal(0, mutation_strength[i] * (0.5 + weight), self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            elite_size = int(0.1 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance Dynamic Evolutionary Optimizer by introducing fitness-based weighted recombination and adaptive mutation scaling for improved exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.8688837048859948, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.869 with standard deviation 0.013. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "33004d56-b7c7-4166-84fc-80a137925c85", "metadata": {"aucs": [0.8512675563532677, 0.8752457133157286, 0.880137844988988], "final_y": [0.11843705893302081, 0.11788933345237007, 0.11714096047602318]}, "mutation_prompt": null}
{"id": "1a080945-17fc-4ed1-8a09-641fe44a643b", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_rate = 0.5 - 0.5 * (np.exp(-fitness_variance)) # Adaptive crossover rate\n                crossover_point = np.random.randint(1, self.dim)\n                if np.random.rand() < crossover_rate:\n                    child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                else:\n                    child = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance Dynamic Evolutionary Optimizer by incorporating adaptive crossover rates to balance exploration and exploitation more effectively.", "configspace": "", "generation": 12, "fitness": 0.8682586479121425, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.014. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6409dd8-fdf7-4de0-b5a1-c1152c8d3461", "metadata": {"aucs": [0.8485604942917545, 0.8770097452884382, 0.8792057041562344], "final_y": [0.11962466201589339, 0.11849722110090366, 0.11911587835488546]}, "mutation_prompt": null}
{"id": "a2db0fdd-83ec-4cf9-8d08-d666a16772ff", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the Enhanced Dynamic Evolutionary Optimizer by adding a probabilistic component to mutation strength adjustment for increased exploration diversity.", "configspace": "", "generation": 12, "fitness": 0.8762087368474628, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.008. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6409dd8-fdf7-4de0-b5a1-c1152c8d3461", "metadata": {"aucs": [0.870699120631524, 0.8699629922054197, 0.8879640977054446], "final_y": [0.11793866728636604, 0.11701616985007235, 0.11731919577740335]}, "mutation_prompt": null}
{"id": "c094aed2-ca1a-4ed4-8b60-4125521089d3", "solution": "import numpy as np\n\nclass ChaoticSwarmOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.chaos_map = lambda x: 4 * x * (1 - x)  # Logistic map for chaotic behavior\n        self.inertia_weight = 0.5\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n        velocity = np.random.uniform(-1, 1, (self.population_size, self.dim))\n        personal_best = pop.copy()\n        personal_best_scores = np.array([func(ind) for ind in pop])\n        global_best_index = np.argmin(personal_best_scores)\n        global_best = personal_best[global_best_index]\n        func_evals = self.population_size\n        chaotic_sequence = np.random.rand(self.population_size)\n\n        while func_evals < self.budget:\n            # Updated line: adaptive inertia weight\n            self.inertia_weight = 0.9 - 0.4 * (func_evals / self.budget)\n            \n            for i in range(self.population_size):\n                chaotic_sequence[i] = self.chaos_map(chaotic_sequence[i])\n                velocity[i] = (\n                    self.inertia_weight * velocity[i]\n                    + self.cognitive_coeff * chaotic_sequence[i] * (personal_best[i] - pop[i])\n                    + self.social_coeff * chaotic_sequence[i] * (global_best - pop[i])\n                )\n                pop[i] += velocity[i]\n                pop[i] = np.clip(pop[i], lb, ub)\n\n                score = func(pop[i])\n                func_evals += 1\n                if score < personal_best_scores[i]:\n                    personal_best[i] = pop[i]\n                    personal_best_scores[i] = score\n                    if score < personal_best_scores[global_best_index]:\n                        global_best_index = i\n                        global_best = pop[i]\n\n                if func_evals >= self.budget:\n                    break\n\n        return global_best", "name": "ChaoticSwarmOptimizer", "description": "Augment the ChaoticSwarmOptimizer by allowing adaptive inertia weight adjustment based on convergence to improve exploration-exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.840872533554286, "feedback": "The algorithm ChaoticSwarmOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.033. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "118892db-0ed2-44ac-aec9-683122ba3b00", "metadata": {"aucs": [0.807089125548713, 0.8858037618922373, 0.8297247132219078], "final_y": [0.1445723747043337, 0.1245847839942762, 0.13280045632305892]}, "mutation_prompt": null}
{"id": "656ccf70-4048-4d89-98a5-3cc6b927965d", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2\n\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n            \n            # Change 1: Enhance mutation strength with non-linear strategy\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0) * (1 - np.linspace(0, 0.3, num_parents)**2).reshape(-1, 1)\n            \n            offspring = []\n            for i in range(num_parents):\n                # Change 2: Apply probabilistic adaptive parent selection\n                probabilities = (fitness[parents_indices] / np.sum(fitness[parents_indices]))**2\n                probabilities /= probabilities.sum()\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents, p=probabilities)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance exploration by introducing a non-linear adaptive mutation strategy and probabilistic adaptive parent selection.", "configspace": "", "generation": 13, "fitness": 0.8755927688384334, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.006. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "ff4e97d2-10e9-4e57-8651-88ff9e791fe6", "metadata": {"aucs": [0.867883831127208, 0.8768734230662714, 0.8820210523218204], "final_y": [0.11583467157370653, 0.12243245500312694, 0.11384542211523596]}, "mutation_prompt": null}
{"id": "264cd52a-0ff4-442a-903c-aad2686b588b", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.6, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                # Change 1: Add dynamic parent selection for crossover\n                parent2 = parents[np.random.choice(num_parents, p=(1/num_parents) * np.ones(num_parents))]\n                crossover_point = np.random.randint(1, self.dim)\n                # Change 2: Alter crossover strategy for diversity\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(np.random.uniform(0.05, 0.15) * pop_size)  # Change 3: Add stochastic element to elitism rate\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce a stochastic elitism rate based on a probabilistic model to enhance exploration-exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.8677085355911176, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.003. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ff4e97d2-10e9-4e57-8651-88ff9e791fe6", "metadata": {"aucs": [0.8654158945829143, 0.8714892203588847, 0.8662204918315539], "final_y": [0.1227753966209103, 0.11762123297832716, 0.12276476354807675]}, "mutation_prompt": null}
{"id": "7a7f7bc5-9cfd-45b2-9cd6-0a7e65de8de7", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0) * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the dynamic elitism rate and introduce a convergence-based adaptive mutation strategy to enhance exploration capabilities.", "configspace": "", "generation": 13, "fitness": 0.8789941200840864, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.006. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "33004d56-b7c7-4166-84fc-80a137925c85", "metadata": {"aucs": [0.8748173193480502, 0.875152216729069, 0.8870128241751399], "final_y": [0.11813935290218092, 0.12083584398672398, 0.11483089223061183]}, "mutation_prompt": null}
{"id": "837fa086-9f45-41c6-a573-0165f6bba24b", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        while self.current_evaluations < self.budget:\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2\n\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.6, num_parents)).reshape(-1, 1)\n            \n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents, p=(1/num_parents) * np.ones(num_parents))]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Change 1: Modify elitism rate based on current best fitness\n            elite_size = int(0.05 * pop_size if fitness.min() < 0.5 else 0.15 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce a fitness-based adaptive elitism rate to improve exploration-exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.871164196887793, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.008. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ff4e97d2-10e9-4e57-8651-88ff9e791fe6", "metadata": {"aucs": [0.8592203527877278, 0.8767432968399901, 0.8775289410356614], "final_y": [0.120280940798565, 0.11739691556215515, 0.1209703580181688]}, "mutation_prompt": null}
{"id": "43d582d3-9994-4eba-88b2-1e9743de5cd7", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0) * (1 - np.random.uniform(0, 0.8, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine the dynamic mutation strength adjustment by introducing better scaling based on a probabilistic model to enhance exploration capabilities.", "configspace": "", "generation": 14, "fitness": 0.8786023905922926, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.009. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "7a7f7bc5-9cfd-45b2-9cd6-0a7e65de8de7", "metadata": {"aucs": [0.8667306690631509, 0.8818232377780345, 0.8872532649356923], "final_y": [0.11558215078734568, 0.1183061599343671, 0.1153871079588149]}, "mutation_prompt": null}
{"id": "76f3e476-d60f-4ac3-b2e9-17b1cc55cffa", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            decay_factor = 0.95  # Change 1: Introduce decay factor\n            mutation_strength = (((func.bounds.ub - func.bounds.lb) / 12.0) * \n                                 (1 - np.linspace(0, 0.6, num_parents)).reshape(-1, 1)) * decay_factor\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                # Change 2: Add diversity-based selection for crossover\n                diversity_indices = np.random.choice(num_parents, size=num_parents, replace=False)\n                parent2 = parents[diversity_indices[i]]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance adaptive mutation by introducing a decay factor and integrate a diversity-based selection mechanism for improved exploration.", "configspace": "", "generation": 14, "fitness": 0.8763882306189039, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.013. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "ff4e97d2-10e9-4e57-8651-88ff9e791fe6", "metadata": {"aucs": [0.8611729125475804, 0.8937972805998825, 0.8741944987092488], "final_y": [0.12627849561322835, 0.11503092693362249, 0.12360775620143527]}, "mutation_prompt": null}
{"id": "5ab8ed2c-d923-460f-b889-3cde4136a293", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.6, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                # Change 1: Add dynamic parent selection for crossover\n                parent2 = parents[np.random.choice(num_parents, p=(1/num_parents) * np.ones(num_parents))]\n                crossover_point = np.random.randint(1, self.dim)\n                # Change 2: Alter crossover strategy for diversity\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                # Directional mutation based on parent1's fitness improvement\n                direction = (np.mean(parents, axis=0) - parent1) * np.random.uniform(0, 0.1)\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim) + direction\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance the offspring generation process by introducing directional mutation for better convergence.", "configspace": "", "generation": 14, "fitness": 0.8740242613129579, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.007. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "ff4e97d2-10e9-4e57-8651-88ff9e791fe6", "metadata": {"aucs": [0.8703414836157932, 0.8834972128332417, 0.8682340874898387], "final_y": [0.12002465558458142, 0.11591227985983643, 0.12028122178425094]}, "mutation_prompt": null}
{"id": "7a798ee7-1027-4860-a21e-ee3a8b3fbd17", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2 + int(0.05 * self.dim)  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.15 * pop_size)  # Increased elitism rate\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce dynamic population size adjustment and adaptive elitism rate to enhance exploration and exploitation balance.", "configspace": "", "generation": 14, "fitness": 0.8766469312918922, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.877 with standard deviation 0.009. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "a2db0fdd-83ec-4cf9-8d08-d666a16772ff", "metadata": {"aucs": [0.8650598085895126, 0.8862975130017399, 0.8785834722844241], "final_y": [0.12103357962579153, 0.11794095387326442, 0.12032136170489482]}, "mutation_prompt": null}
{"id": "13a813a4-f68a-4a2e-bcf3-ac4e7df2a9e2", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness rank\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 12.0) * (1 - np.linspace(0, 0.6, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                # Change 1: Add dynamic parent selection for crossover\n                parent2 = parents[np.random.choice(num_parents, p=(1/num_parents) * np.ones(num_parents))]\n                crossover_point = np.random.randint(1, self.dim)\n                # Change 2: Alter crossover strategy for diversity\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i] * np.std(population, axis=0), self.dim)  # change here\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce a dynamic mutation step size scaling based on population diversity to enhance convergence.", "configspace": "", "generation": 14, "fitness": 0.8394744343218692, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.069. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "ff4e97d2-10e9-4e57-8651-88ff9e791fe6", "metadata": {"aucs": [0.7423820363094582, 0.8818894821306754, 0.8941517845254737], "final_y": [0.17924064183162647, 0.11974558095075227, 0.11685014003712613]}, "mutation_prompt": null}
{"id": "373880a6-02f4-4a7a-9004-75e5cb8f5951", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents using tournament selection\n            num_parents = pop_size // 2\n            parents_indices = [np.random.choice(np.argpartition(fitness, 3)[:3]) for _ in range(num_parents)]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce tournament selection to replace parents selection process to enhance exploration and exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.8791100368733892, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.010. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "a2db0fdd-83ec-4cf9-8d08-d666a16772ff", "metadata": {"aucs": [0.8709675133168487, 0.8937132622322067, 0.872649335071112], "final_y": [0.12306538467382677, 0.11651856357542467, 0.12187303842327579]}, "mutation_prompt": null}
{"id": "b7d38173-582d-4615-aee4-35cec13d9844", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (10.0 + np.sqrt(fitness_variance))) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim - 1)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce nonlinear scaling for mutation strength and adaptive crossover point adjustment for enhanced exploration diversity.", "configspace": "", "generation": 15, "fitness": 0.8736295933873383, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.017. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "a2db0fdd-83ec-4cf9-8d08-d666a16772ff", "metadata": {"aucs": [0.8550870980547107, 0.8956793695035875, 0.8701223126037166], "final_y": [0.12521060260736172, 0.1173814768334841, 0.1200523036656218]}, "mutation_prompt": null}
{"id": "b04f53b5-b286-4f58-ad22-7bde2d98c35a", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            diversity_factor = np.std(population, axis=0).mean()  # New line added\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance + diversity_factor)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance mutation strength adjustment by incorporating a dynamic scaling factor based on solution diversity for better exploration.", "configspace": "", "generation": 15, "fitness": 0.8744639949374425, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.007. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "a2db0fdd-83ec-4cf9-8d08-d666a16772ff", "metadata": {"aucs": [0.8785193039839079, 0.864600882634932, 0.8802717981934876], "final_y": [0.1179191127336785, 0.1213205832115285, 0.11726728621370397]}, "mutation_prompt": null}
{"id": "050e5b40-f63a-4e62-ba0f-67b559d2fc6d", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Neural-inspired adaptation strategy\n            learning_rate = 0.1 + (0.5 * (np.max(fitness) - fitness[parents_indices]) / np.ptp(fitness))\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 8.0) * learning_rate.reshape(-1, 1)\n\n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                diversity_indices = np.random.choice(num_parents, size=num_parents, replace=False)\n                parent2 = parents[diversity_indices[i]]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                \n                # Neural-inspired adaptive mutation\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.15 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Integrate a neural-inspired learning rate adaptation and diversity preservation strategy for enhanced exploration and exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.8663560226627639, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.004. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "76f3e476-d60f-4ac3-b2e9-17b1cc55cffa", "metadata": {"aucs": [0.8600959295244007, 0.8699712749053069, 0.8690008635585843], "final_y": [0.11734393028725432, 0.12279563370829105, 0.12185147942406738]}, "mutation_prompt": null}
{"id": "f3e5dc89-e41c-4c61-9fd8-3650c93cdbb5", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                parent3 = parents[np.random.choice(num_parents)]  # Added third parent\n                crossover_point1, crossover_point2 = sorted(np.random.choice(range(1, self.dim), 2, replace=False))\n                child = np.concatenate([parent1[:crossover_point1], parent2[crossover_point1:crossover_point2], parent3[crossover_point2:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce multi-parent crossover to enhance genetic diversity and potential solution quality.", "configspace": "", "generation": 15, "fitness": 0.8787992110501502, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.012. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "a2db0fdd-83ec-4cf9-8d08-d666a16772ff", "metadata": {"aucs": [0.8655190064511973, 0.8942324802949634, 0.8766461464042901], "final_y": [0.11888521354041781, 0.1158019390898446, 0.11930940667387335]}, "mutation_prompt": null}
{"id": "99b23480-55a2-4230-91be-98fdbf6f85d6", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                parent3 = parents[np.random.choice(num_parents)]  # Added third parent\n                crossover_point1, crossover_point2 = sorted(np.random.choice(range(1, self.dim), 2, replace=False))\n                child = np.concatenate([parent1[:crossover_point1], parent2[crossover_point1:crossover_point2], parent3[crossover_point2:]])\n                mutated = child + np.random.normal(0, mutation_strength[i] * 0.9, self.dim)  # Adjusted Gaussian noise scale\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine mutation strategy by adjusting the Gaussian noise scale for increased adaptability.", "configspace": "", "generation": 16, "fitness": 0.8779211657184088, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.005. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f3e5dc89-e41c-4c61-9fd8-3650c93cdbb5", "metadata": {"aucs": [0.8756119789299863, 0.8730440981056966, 0.8851074201195436], "final_y": [0.11765663522755443, 0.11874269161036233, 0.1131437302626308]}, "mutation_prompt": null}
{"id": "ed5d1528-107c-4b8e-b626-3f2bdcc04f14", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2 + int(0.05 * self.dim)  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            decay_factor = 1 - (self.current_evaluations / self.budget)  # Decay factor introduced\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * \\\n                                np.random.uniform(0.8, 1.2, (num_parents, 1)) * decay_factor\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.15 * pop_size)  # Increased elitism rate\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine adaptive mutation strength by introducing a decay factor based on iteration progress to improve solution convergence.", "configspace": "", "generation": 16, "fitness": 0.8783596967846635, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.011. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "7a798ee7-1027-4860-a21e-ee3a8b3fbd17", "metadata": {"aucs": [0.8667806052963186, 0.8758810543695985, 0.8924174306880736], "final_y": [0.11754488080096892, 0.11874990872553981, 0.11258612701796633]}, "mutation_prompt": null}
{"id": "06de26cd-deaf-4774-8180-85e2a332b299", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                parent3 = parents[np.random.choice(num_parents)]  # Added third parent\n                parent4 = parents[np.random.choice(num_parents)]  # Added fourth parent\n                crossover_weights = np.random.dirichlet(np.ones(4), size=1).flatten()\n                child = (crossover_weights[0] * parent1 + crossover_weights[1] * parent2 +\n                         crossover_weights[2] * parent3 + crossover_weights[3] * parent4)\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce a fitness-based adaptive multi-parent crossover and enhanced mutation strategy to improve solution diversity and convergence rate.", "configspace": "", "generation": 16, "fitness": 0.8762278005879974, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.008. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "f3e5dc89-e41c-4c61-9fd8-3650c93cdbb5", "metadata": {"aucs": [0.8722975656203413, 0.8879892180828116, 0.8683966180608398], "final_y": [0.12170266646548278, 0.11419296044600413, 0.11933445951975441]}, "mutation_prompt": null}
{"id": "49770ac6-d28f-4691-a6c7-05ab1e84f5be", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                parent3 = parents[np.random.choice(num_parents)]  # Added third parent\n                crossover_point1, crossover_point2 = sorted(np.random.choice(range(1, self.dim), 2, replace=False))\n                # Introduce Gaussian perturbation at crossover points\n                child = np.concatenate([parent1[:crossover_point1], parent2[crossover_point1:crossover_point2] + np.random.normal(0, 0.1, crossover_point2-crossover_point1), parent3[crossover_point2:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refine offspring generation by introducing Gaussian perturbation at crossover points to enhance exploration capabilities.", "configspace": "", "generation": 16, "fitness": 0.880927279351106, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.009. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "f3e5dc89-e41c-4c61-9fd8-3650c93cdbb5", "metadata": {"aucs": [0.8690194970639725, 0.8913276334378034, 0.8824347075515422], "final_y": [0.11762691262595026, 0.1137704495323425, 0.115181733505888]}, "mutation_prompt": null}
{"id": "c0d5a20a-d571-4269-af5c-87a2cc33236d", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            fitness_range = np.max(fitness) - np.min(fitness)  # Added line for dynamic scaling\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0 * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)) * (1 + fitness_range)  # Modified line\n\n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.random.choice(np.argsort(fitness)[:2 * elite_size], elite_size, replace=False)  # Modified line for stochastic selection\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance adaptive mutation strategy by incorporating dynamic scaling based on fitness diversity and improve elitism rate with stochastic selection.", "configspace": "", "generation": 16, "fitness": 0.8884895656184343, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.888 with standard deviation 0.014. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "7a7f7bc5-9cfd-45b2-9cd6-0a7e65de8de7", "metadata": {"aucs": [0.8750135761541181, 0.8834956449076057, 0.9069594757935794], "final_y": [0.11877453933668336, 0.11776606611355922, 0.11074130898692658]}, "mutation_prompt": null}
{"id": "8dfa4c48-66db-4ab8-8c89-17e1e39f7741", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0) * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i] * np.sqrt(i+1)/num_parents, self.dim)  # Modify perturbation mechanism\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Optimize the offspring generation by incorporating an adaptive Gaussian perturbation mechanism.", "configspace": "", "generation": 17, "fitness": 0.8692377028472448, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.869 with standard deviation 0.022. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "7a7f7bc5-9cfd-45b2-9cd6-0a7e65de8de7", "metadata": {"aucs": [0.8385874316400024, 0.889209712652148, 0.8799159642495836], "final_y": [0.1291591340805287, 0.11222790720066089, 0.11962180209393714]}, "mutation_prompt": null}
{"id": "cb23bf48-d3a8-404e-bd0b-e6abb540f2df", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0) * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * ((self.current_evaluations / self.budget) ** 1.2)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance the dynamic elitism by introducing a non-linear scaling factor based on budget utilization for better exploitation of elite solutions.", "configspace": "", "generation": 17, "fitness": 0.8806414380223758, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.010. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "7a7f7bc5-9cfd-45b2-9cd6-0a7e65de8de7", "metadata": {"aucs": [0.8661484280459256, 0.8860801842754071, 0.8896957017457947], "final_y": [0.11871132903028014, 0.11712233783771497, 0.11320429595534631]}, "mutation_prompt": null}
{"id": "7fa3f6ee-5689-4786-a16a-1e4485a2aed0", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            fitness_range = np.max(fitness) - np.min(fitness)  # Added line for dynamic scaling\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0 * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)) * (1 + fitness_range / np.sum(fitness))  # Modified line\n\n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.random.choice(np.argsort(fitness)[:2 * elite_size], elite_size, replace=False)  # Modified line for stochastic selection\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance mutation strategy by incorporating fitness-proportional scaling to improve solution diversity.", "configspace": "", "generation": 17, "fitness": 0.8893551945164759, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.008. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c0d5a20a-d571-4269-af5c-87a2cc33236d", "metadata": {"aucs": [0.8855417355746087, 0.8821542237875324, 0.9003696241872868], "final_y": [0.11535954350932198, 0.1180433743180902, 0.1128499432033101]}, "mutation_prompt": null}
{"id": "03ebad18-f621-4bdf-b31f-de063fa3297d", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents using tournament selection\n            num_parents = pop_size // 2\n            parents_indices = [np.random.choice(np.argpartition(fitness, 3)[:3]) for _ in range(num_parents)]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            learning_rate = 0.1  # Introduced learning rate for fine-tuning\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * \\\n                                np.random.uniform(0.8, 1.2, (num_parents, 1)) * learning_rate\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance exploitation by introducing a learning rate factor to refine mutation strength dynamically based on current fitness.", "configspace": "", "generation": 17, "fitness": 0.8549115856817169, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.043. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "373880a6-02f4-4a7a-9004-75e5cb8f5951", "metadata": {"aucs": [0.7950576276740912, 0.8949164280303817, 0.8747607013406776], "final_y": [0.145772752152741, 0.1136883245538205, 0.11820586789763876]}, "mutation_prompt": null}
{"id": "8f7c4155-917d-43b8-97c4-0778d08cf32f", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength with non-linear decay\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0) * (1 - np.logspace(0, -0.8, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance adaptive mutation strategy by introducing non-linear mutation strength decay for improved exploration-exploitation balance.", "configspace": "", "generation": 17, "fitness": 0.8779224711954008, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.014. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "7a7f7bc5-9cfd-45b2-9cd6-0a7e65de8de7", "metadata": {"aucs": [0.8624057902950819, 0.8755044816311633, 0.8958571416599572], "final_y": [0.11970478662096995, 0.11688800161793311, 0.11358651956609178]}, "mutation_prompt": null}
{"id": "66252164-70b6-4541-a205-6859256bb706", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Covariance matrix initialization\n        cov_matrix = np.eye(self.dim)\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0) * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.multivariate_normal(np.zeros(self.dim), cov_matrix * mutation_strength[i])\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Update covariance matrix based on successful offspring\n            successful_offspring = offspring[np.argsort(offspring_fitness)[:elite_size]]\n            cov_matrix = np.cov(successful_offspring, rowvar=False)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * ((self.current_evaluations / self.budget) ** 1.2)))\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Incorporate a covariance matrix adaptation strategy to improve mutation distribution by learning dependencies between dimensions.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'elite_size' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'elite_size' referenced before assignment\")", "parent_id": "cb23bf48-d3a8-404e-bd0b-e6abb540f2df", "metadata": {}, "mutation_prompt": null}
{"id": "fa4af44b-ab90-4e55-92fc-b09d280c6e34", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            fitness_range = np.max(fitness) - np.min(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0 * \n                                 (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)) * (1 + fitness_range)\n\n            # Generate offspring with fitness-weighted crossover\n            offspring = []\n            total_fitness = np.sum(1 / (fitness[parents_indices] + 1e-8))  # Avoid division by zero\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_prob = (1 / (fitness[parents_indices[i]] + 1e-8)) / total_fitness\n                crossover_point = np.random.randint(1, self.dim) if np.random.rand() < crossover_prob else 0\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.random.choice(np.argsort(fitness)[:2 * elite_size], elite_size, replace=False)\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Implemented fitness-weighted mutation and crossover to enhance solution quality and diversity.", "configspace": "", "generation": 18, "fitness": 0.8454035205235471, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.008. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c0d5a20a-d571-4269-af5c-87a2cc33236d", "metadata": {"aucs": [0.8335632636893956, 0.8520943172795363, 0.8505529806017091], "final_y": [0.1302111807989298, 0.12338498185623215, 0.12955174015486615]}, "mutation_prompt": null}
{"id": "6063daa9-6e24-4f92-94ba-1adbaa52203a", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents using tournament selection\n            num_parents = pop_size // 2\n            parents_prob = np.exp(-fitness / np.std(fitness))  # Fitness-based weighting\n            parents_indices = np.random.choice(range(pop_size), size=num_parents, p=parents_prob / parents_prob.sum())\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Enhance parent selection by implementing a fitness-based weighting to improve solution quality.", "configspace": "", "generation": 18, "fitness": 0.8626943332307072, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.010. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "373880a6-02f4-4a7a-9004-75e5cb8f5951", "metadata": {"aucs": [0.8556227241701806, 0.8556629900101849, 0.8767972855117562], "final_y": [0.1295684967726809, 0.12147687700526333, 0.11687141470416307]}, "mutation_prompt": null}
{"id": "909b28e0-2d4e-4e31-8c81-ce505cdfdb96", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        last_best_fitness = np.min(fitness)\n\n        while self.current_evaluations < self.budget:\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2\n\n            num_parents = pop_size // 2\n            parents_indices = [np.random.choice(np.argpartition(fitness, 3)[:3]) for _ in range(num_parents)]\n            parents = population[parents_indices]\n\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            current_best_fitness = np.min(fitness)\n            if current_best_fitness < last_best_fitness:\n                elite_size = int(0.15 * pop_size)\n                last_best_fitness = current_best_fitness\n            else:\n                elite_size = int(0.1 * pop_size)\n\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Integrate adaptive elitism by adjusting elite retention rate based on current fitness improvement for better adaptation and solution quality.", "configspace": "", "generation": 18, "fitness": 0.8625930719524247, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.001. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "373880a6-02f4-4a7a-9004-75e5cb8f5951", "metadata": {"aucs": [0.863455977963324, 0.8634583872869358, 0.8608648506070146], "final_y": [0.12961887641554026, 0.12234765868710162, 0.1229775253467823]}, "mutation_prompt": null}
{"id": "1cff0c03-960c-4bdd-9179-51d1c8a7f47d", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents using tournament selection\n            num_parents = pop_size // 2\n            parents_indices = [np.random.choice(np.argpartition(fitness, 3)[:3]) for _ in range(num_parents)]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_prob = np.clip(1 - fitness[i] / np.max(fitness), 0.1, 0.9)  # Adaptive crossover probability\n                crossover_point = np.random.randint(1, self.dim) if np.random.rand() < crossover_prob else self.dim\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce fitness-based adaptive crossover probability to dynamically adjust exploration and exploitation balance.", "configspace": "", "generation": 18, "fitness": 0.8607688697641379, "feedback": "The algorithm DynamicEvolutionaryOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.011. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "373880a6-02f4-4a7a-9004-75e5cb8f5951", "metadata": {"aucs": [0.8484774745445438, 0.8582561382184936, 0.8755729965293766], "final_y": [0.13232696639581343, 0.1256251269199743, 0.11480669125958531]}, "mutation_prompt": null}
{"id": "d6f4f56f-8024-432b-abe3-e98bee5356db", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            fitness_range = np.max(fitness) - np.min(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0 * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)) * (1 + fitness_range)\n\n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_rate = 0.5 + (0.5 * (fitness[i] - np.min(fitness)) / fitness_range)  # Modified line\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:int(crossover_point * crossover_rate)], parent2[int(crossover_point * crossover_rate):]])  # Modified line\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            diversity_weights = 1.0 / (1.0 + np.std(population, axis=0))  # Modified line\n            elite_indices = np.random.choice(np.argsort(fitness)[:2 * elite_size], elite_size, replace=False, p=diversity_weights / np.sum(diversity_weights))  # Modified line\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce fitness-adaptive crossover rate and improve stochastic elite selection by incorporating diversity-based weighting.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "c0d5a20a-d571-4269-af5c-87a2cc33236d", "metadata": {}, "mutation_prompt": null}
{"id": "6d040fb2-0800-44b3-b9e1-6cd8235601fe", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents using tournament selection\n            num_parents = pop_size // 2\n            parents_indices = [np.random.choice(np.argpartition(fitness, 3)[:3]) for _ in range(num_parents)]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / (12.0 + fitness_variance)) * \\\n                                (1 - np.linspace(0, 0.5, num_parents)).reshape(-1, 1) * np.random.uniform(0.8, 1.2, (num_parents, 1))\n            \n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Elitism: Keep a proportion of the best from the previous generation\n            elite_size = int(0.1 * pop_size)\n            elite_indices = np.argsort(fitness)[:elite_size]\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Introduce diversity preservation in offspring selection by prioritizing novel genetic structures.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "373880a6-02f4-4a7a-9004-75e5cb8f5951", "metadata": {}, "mutation_prompt": null}
{"id": "d26ea648-ebea-450a-900b-67b9b4ac329b", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 3  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Adaptive mutation strength based on fitness variance\n            fitness_variance = np.var(fitness)\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 10.0 * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)) * (1 + fitness_variance / np.sum(fitness))\n\n            # Generate offspring with fitness-based crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.where(np.random.rand(self.dim) < 0.5, parent1, parent2)\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.random.choice(np.argsort(fitness)[:2 * elite_size], elite_size, replace=False)\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Integrate adaptive population resizing and fitness-based crossover to enhance exploration and exploitation balance for improved optimization.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "7fa3f6ee-5689-4786-a16a-1e4485a2aed0", "metadata": {}, "mutation_prompt": null}
{"id": "9f4af488-9cb9-46d6-9142-e2bf7f29062b", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Select parents\n            num_parents = pop_size // 2\n            parents_indices = np.argsort(fitness)[:num_parents]\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            fitness_range = np.max(fitness) - np.min(fitness)  # Added line for dynamic scaling\n            mutation_strength = (((func.bounds.ub - func.bounds.lb) / 15.0 * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)) * (1 + fitness_range)) ** 1.5  # Modified line\n\n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.random.choice(np.argsort(fitness)[:2 * elite_size], elite_size, replace=False)  # Modified line for stochastic selection\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Refined adaptive mutation by introducing a power-law distribution for mutation strength to enhance exploration.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "c0d5a20a-d571-4269-af5c-87a2cc33236d", "metadata": {}, "mutation_prompt": null}
{"id": "2fefad68-d436-4de8-8a51-49e13b7188cf", "solution": "import numpy as np\n\nclass DynamicEvolutionaryOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_evaluations = 0\n\n    def __call__(self, func):\n        # Initialize population with increased diversity\n        pop_size = 12 + 6 * self.dim\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, (pop_size, self.dim))\n        fitness = np.array([func(ind) for ind in population])\n        self.current_evaluations += pop_size\n\n        # Evolutionary loop\n        while self.current_evaluations < self.budget:\n            # Adjust population size based on convergence\n            if self.current_evaluations % (self.budget // 5) == 0:\n                pop_size += 2  # Increment population size periodically\n\n            # Enhanced tournament selection for parents\n            num_parents = pop_size // 2\n            tournament_size = max(2, self.dim // 4)  # Added line for tournament size\n            parents_indices = [min(np.random.choice(pop_size, tournament_size, replace=False), key=lambda i: fitness[i]) for _ in range(num_parents)]  # Modified line for tournament selection\n            parents = population[parents_indices]\n\n            # Convergence-based adaptive mutation strength\n            fitness_range = np.std(fitness)  # Modified line for better diversity monitoring\n            mutation_strength = ((func.bounds.ub - func.bounds.lb) / 15.0 * (1 - np.linspace(0, 0.8, num_parents)).reshape(-1, 1)) * (1 + fitness_range / np.mean(fitness))  # Modified line\n\n            # Generate offspring with adaptive crossover\n            offspring = []\n            for i in range(num_parents):\n                parent1 = parents[i]\n                parent2 = parents[np.random.choice(num_parents)]\n                crossover_point = np.random.randint(1, self.dim)\n                child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n                mutated = child + np.random.normal(0, mutation_strength[i], self.dim)\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n                offspring.append(mutated)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(ind) for ind in offspring])\n            self.current_evaluations += len(offspring)\n\n            # Refined dynamic elitism rate\n            elite_size = int(0.15 * pop_size * (0.5 + 0.5 * (self.current_evaluations / self.budget)))\n            elite_indices = np.random.choice(np.argsort(fitness)[:2 * elite_size], elite_size, replace=False)  # Modified line for stochastic selection\n            elite = population[elite_indices]\n\n            # Select the best individuals to form the new population\n            population = np.vstack((elite, parents, offspring))\n            fitness = np.hstack((fitness[elite_indices], fitness[parents_indices], offspring_fitness))\n            best_indices = np.argsort(fitness)[:pop_size]\n            population = population[best_indices]\n            fitness = fitness[best_indices]\n\n        # Return the best solution found\n        best_index = np.argmin(fitness)\n        return population[best_index]", "name": "DynamicEvolutionaryOptimizer", "description": "Improve exploration and convergence by introducing adaptive mutation scaling, fitness diversity monitoring, and tournament-based parent selection.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError(\"'a' and 'p' must have same size\").", "error": "ValueError(\"'a' and 'p' must have same size\")", "parent_id": "c0d5a20a-d571-4269-af5c-87a2cc33236d", "metadata": {}, "mutation_prompt": null}
