{"id": "d6b20a5d-49c7-43c2-a364-5554140bea74", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for exploration and local gradient-based optimization for refinement, with adaptive layer growth to manage complexity.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 21, in _mutation\nNameError: name 'lb' is not defined\n.", "error": "NameError(\"name 'lb' is not defined\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 21, in _mutation\nNameError: name 'lb' is not defined\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "ffb1afe1-96c1-49f8-8bce-d27e2e0ebbea", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * self.dim\n        self.f = 0.8  # DE mutation factor\n        self.cr = 0.9  # DE crossover probability\n        self.local_search_steps = 5\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, pop, lb, ub):\n        new_pop = np.copy(pop)\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            fitness_trial = func(trial) - self.robustness_factor * np.std(trial)\n            self.eval_count += 1\n            if fitness_trial < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_search(self, func, individual, lb, ub):\n        best = np.copy(individual)\n        best_fitness = func(best)\n        for _ in range(self.local_search_steps):\n            if self.eval_count >= self.budget:\n                break\n            candidate = best + np.random.normal(0, 0.1, size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            fitness_candidate = func(candidate) - self.robustness_factor * np.std(candidate)\n            self.eval_count += 1\n            if fitness_candidate < best_fitness:\n                best, best_fitness = candidate, fitness_candidate\n        return best\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            pop = self.differential_evolution(func, pop, lb, ub)\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                pop[i] = self.local_search(func, pop[i], lb, ub)\n                layer_expansion = min(self.dim, int(self.eval_count / self.budget * self.dim))\n                pop[i][:layer_expansion] = self.local_search(func, pop[i][:layer_expansion], lb[:layer_expansion], ub[:layer_expansion])\n\n        best_idx = np.argmin([func(individual) for individual in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution (DE) for global exploration and Local Search for refinement, with dynamic layer expansion and robustness metrics for optimizing high-dimensional noisy black-box functions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 57, in __call__\n  File \"<string>\", line 37, in local_search\nValueError: operands could not be broadcast together with shapes (2,) (10,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 57, in __call__\n  File \"<string>\", line 37, in local_search\nValueError: operands could not be broadcast together with shapes (2,) (10,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "10cdfc2f-bf44-4a96-89aa-6d47dbfcf080", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.f = 0.5  # DE scaling factor\n        self.cr = 0.9  # Crossover probability\n        self.num_evals = 0  # Track the number of function evaluations\n\n    def differential_evolution(self, pop, bounds, func):\n        for i in range(len(pop)):\n            idxs = [idx for idx in range(len(pop)) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.f * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.num_evals < self.budget:\n                trial_fitness = func(trial)\n                self.num_evals += 1\n                if trial_fitness > func(pop[i]):\n                    pop[i] = trial\n        return pop\n\n    def local_search(self, solution, bounds, func, complexity_increment):\n        perturbed_solution = solution.copy()\n        complexity = complexity_increment\n        while complexity < self.dim:\n            layer_indices = np.random.choice(self.dim, complexity, replace=False)\n            perturbation = np.random.uniform(-0.05, 0.05, size=complexity)\n            perturbed_solution[layer_indices] += perturbation\n            perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n            if self.num_evals < self.budget:\n                perturbed_fitness = func(perturbed_solution)\n                self.num_evals += 1\n                if perturbed_fitness > func(solution):\n                    solution = perturbed_solution.copy()\n            complexity += complexity_increment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        pop_fitness = np.array([func(indiv) for indiv in pop])\n        self.num_evals += self.population_size\n\n        while self.num_evals < self.budget:\n            pop = self.differential_evolution(pop, bounds, func)\n            best_idx = np.argmax(pop_fitness)\n            best_solution = pop[best_idx]\n            best_solution = self.local_search(best_solution, bounds, func, complexity_increment=5)\n            if self.num_evals >= self.budget:\n                break\n\n        best_idx = np.argmax(pop_fitness)\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a custom local search that adapts layer optimization complexity dynamically to refine solutions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 57, in __call__\n  File \"<string>\", line 37, in local_search\nValueError: operands could not be broadcast together with shapes (2,) (10,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 57, in __call__\n  File \"<string>\", line 37, in local_search\nValueError: operands could not be broadcast together with shapes (2,) (10,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "9a5626fb-4c6d-42f6-ae1f-98c49063723a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        # Local optimization using L-BFGS-B\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "The algorithm combines Differential Evolution for global search with a local gradient-based optimizer, utilizing adaptive layer growth and modular role preservation to efficiently solve high-dimensional noisy optimization problems.", "configspace": "", "generation": 0, "fitness": 0.8282923639136798, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.019. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8018498897936357, 0.8370999992855056, 0.845927202661898], "final_y": [0.145815078080763, 0.12759667853814116, 0.12878514495237603]}, "mutation_prompt": null}
{"id": "756512a0-bd09-4a46-91e6-90ac5e4433d7", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                trial = np.where(np.random.rand(len(lb)) < 0.9, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                if bounds.lb <= neighbor.all() <= bounds.ub:\n                    neighbor_fitness = func(neighbor)\n                    if neighbor_fitness < best_fitness:\n                        best_solution, best_fitness = neighbor, neighbor_fitness\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "A hybrid metaheuristic combining Differential Evolution for global exploration with a local search refinement phase, incorporating adaptive layer expansion and robustness metrics to efficiently tackle noisy, high-dimensional optimization problems.", "configspace": "", "generation": 0, "fitness": 0.7995412716771816, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.018. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": null, "metadata": {"aucs": [0.774773245635916, 0.8172039386114534, 0.8066466307841753], "final_y": [0.15419598580607174, 0.14225366052859933, 0.14293641416552938]}, "mutation_prompt": null}
{"id": "297e90b1-1216-4a87-8685-69dc0c5ca65a", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * self.dim\n        self.f = 0.8  # DE mutation factor\n        self.cr = 0.9  # DE crossover probability\n        self.local_search_steps = 5\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, pop, lb, ub):\n        new_pop = np.copy(pop)\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            fitness_trial = func(trial) - self.robustness_factor * np.std(trial)\n            self.eval_count += 1\n            if fitness_trial < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_search(self, func, individual, lb, ub):\n        best = np.copy(individual)\n        best_fitness = func(best)\n        for _ in range(self.local_search_steps):\n            if self.eval_count >= self.budget:\n                break\n            candidate = best + np.random.normal(0, 0.1, size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            fitness_candidate = func(candidate) - self.robustness_factor * np.std(candidate)\n            self.eval_count += 1\n            if fitness_candidate < best_fitness:\n                best, best_fitness = candidate, fitness_candidate\n        return best\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            pop = self.differential_evolution(func, pop, lb, ub)\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                pop[i] = self.local_search(func, pop[i], lb, ub)\n                layer_expansion = min(self.dim, int(self.eval_count / self.budget * self.dim))\n                expanded_lb = lb[:layer_expansion] if layer_expansion > 0 else lb\n                expanded_ub = ub[:layer_expansion] if layer_expansion > 0 else ub\n                pop[i][:layer_expansion] = self.local_search(func, pop[i][:layer_expansion], expanded_lb, expanded_ub)\n\n        best_idx = np.argmin([func(individual) for individual in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic combining Differential Evolution (DE) for global exploration and Local Search for refinement, with dynamic layer expansion and corrected dimensionality handling for optimizing high-dimensional noisy black-box functions.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "ffb1afe1-96c1-49f8-8bce-d27e2e0ebbea", "metadata": {}, "mutation_prompt": null}
{"id": "79742364-547d-4d70-9f04-24aed7b6d313", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.f = 0.5  # DE scaling factor\n        self.cr = 0.9  # Crossover probability\n        self.num_evals = 0  # Track the number of function evaluations\n\n    def differential_evolution(self, pop, bounds, func):\n        for i in range(len(pop)):\n            idxs = [idx for idx in range(len(pop)) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.f * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.num_evals < self.budget:\n                trial_fitness = func(trial)\n                self.num_evals += 1\n                if trial_fitness > func(pop[i]):\n                    pop[i] = trial\n        return pop\n\n    def local_search(self, solution, bounds, func, complexity_increment):\n        perturbed_solution = solution.copy()\n        complexity = complexity_increment\n        while complexity < self.dim:\n            layer_indices = np.random.choice(self.dim, complexity, replace=False)\n            perturbation = np.random.uniform(-0.05, 0.05, size=complexity)\n            perturbed_solution[layer_indices] += perturbation  # Ensure broadcasting\n            perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n            if self.num_evals < self.budget:\n                perturbed_fitness = func(perturbed_solution)\n                self.num_evals += 1\n                if perturbed_fitness > func(solution):\n                    solution = perturbed_solution.copy()\n            complexity += complexity_increment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        pop_fitness = np.array([func(indiv) for indiv in pop])\n        self.num_evals += self.population_size\n\n        while self.num_evals < self.budget:\n            pop = self.differential_evolution(pop, bounds, func)\n            best_idx = np.argmax(pop_fitness)\n            best_solution = pop[best_idx]\n            best_solution = self.local_search(best_solution, bounds, func, complexity_increment=5)\n            if self.num_evals >= self.budget:\n                break\n\n        best_idx = np.argmax(pop_fitness)\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a custom local search that adapts layer optimization complexity dynamically to refine solutions, ensuring correct broadcasting during local search perturbations.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "10cdfc2f-bf44-4a96-89aa-6d47dbfcf080", "metadata": {}, "mutation_prompt": null}
{"id": "c0eb338d-240f-476e-affa-0e2946a84988", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.f = 0.5  # DE scaling factor\n        self.cr = 0.9  # Crossover probability\n        self.num_evals = 0  # Track the number of function evaluations\n\n    def differential_evolution(self, pop, bounds, func):\n        for i in range(len(pop)):\n            idxs = [idx for idx in range(len(pop)) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.f * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.num_evals < self.budget:\n                trial_fitness = func(trial)\n                self.num_evals += 1\n                if trial_fitness > func(pop[i]):\n                    pop[i] = trial\n        return pop\n\n    def local_search(self, solution, bounds, func, complexity_increment):\n        perturbed_solution = solution.copy()\n        complexity = complexity_increment\n        while complexity < self.dim:\n            layer_indices = np.random.choice(self.dim, complexity, replace=False)\n            for index in layer_indices:  # Line change 1\n                perturbation = np.random.uniform(-0.05, 0.05)  # Line change 2\n                perturbed_solution[index] += perturbation\n            perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n            if self.num_evals < self.budget:\n                perturbed_fitness = func(perturbed_solution)\n                self.num_evals += 1\n                if perturbed_fitness > func(solution):\n                    solution = perturbed_solution.copy()\n            complexity += complexity_increment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        pop_fitness = np.array([func(indiv) for indiv in pop])\n        self.num_evals += self.population_size\n\n        while self.num_evals < self.budget:\n            pop = self.differential_evolution(pop, bounds, func)\n            best_idx = np.argmax(pop_fitness)\n            best_solution = pop[best_idx]\n            best_solution = self.local_search(best_solution, bounds, func, complexity_increment=5)\n            if self.num_evals >= self.budget:\n                break\n\n        best_idx = np.argmax(pop_fitness)\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a custom local search that refines solutions by correctly handling layer-wise perturbation.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "10cdfc2f-bf44-4a96-89aa-6d47dbfcf080", "metadata": {}, "mutation_prompt": null}
{"id": "a8a842dd-51e4-41d4-adc2-620d8a91cca2", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20  # Initial population size\n        self.f = 0.5  # DE scaling factor\n        self.cr = 0.9  # Crossover probability\n        self.num_evals = 0  # Track the number of function evaluations\n\n    def differential_evolution(self, pop, bounds, func):\n        for i in range(len(pop)):\n            idxs = [idx for idx in range(len(pop)) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.f * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if self.num_evals < self.budget:\n                trial_fitness = func(trial)\n                self.num_evals += 1\n                if trial_fitness > func(pop[i]):\n                    pop[i] = trial\n        return pop\n\n    def local_search(self, solution, bounds, func, complexity_increment):\n        perturbed_solution = solution.copy()\n        complexity = complexity_increment\n        while complexity < self.dim:\n            layer_indices = np.random.choice(self.dim, min(complexity, self.dim), replace=False)  # Changed line\n            perturbation = np.random.uniform(-0.05, 0.05, size=complexity)\n            perturbed_solution[layer_indices] += perturbation\n            perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n            if self.num_evals < self.budget:\n                perturbed_fitness = func(perturbed_solution)\n                self.num_evals += 1\n                if perturbed_fitness > func(solution):\n                    solution = perturbed_solution.copy()\n            complexity += complexity_increment\n        return solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        pop_fitness = np.array([func(indiv) for indiv in pop])\n        self.num_evals += self.population_size\n\n        while self.num_evals < self.budget:\n            pop = self.differential_evolution(pop, bounds, func)\n            best_idx = np.argmax(pop_fitness)\n            best_solution = pop[best_idx]\n            best_solution = self.local_search(best_solution, bounds, func, complexity_increment=5)\n            if self.num_evals >= self.budget:\n                break\n\n        best_idx = np.argmax(pop_fitness)\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "Improved layer selection strategy in local search for enhanced solution refinement.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "10cdfc2f-bf44-4a96-89aa-6d47dbfcf080", "metadata": {}, "mutation_prompt": null}
{"id": "22171ad4-51a8-44a6-890e-1b5e2a1cbf71", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                trial = np.where(np.random.rand(len(lb)) < 0.9, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                if bounds.lb <= neighbor.all() <= bounds.ub:\n                    neighbor_fitness = func(neighbor)\n                    if neighbor_fitness < best_fitness:\n                        best_solution, best_fitness = neighbor, neighbor_fitness\n                        step_size *= 0.5  # Adaptive step size reduction for more precise refinement\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "Enhanced local search by introducing adaptive step size reduction for more precise refinement in high-dimensional optimization.", "configspace": "", "generation": 1, "fitness": 0.8116545043906424, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.005. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "756512a0-bd09-4a46-91e6-90ac5e4433d7", "metadata": {"aucs": [0.8190654946446334, 0.8087263613083637, 0.80717165721893], "final_y": [0.13252314410900634, 0.1446035067468564, 0.14380558686885603]}, "mutation_prompt": null}
{"id": "9a7c1ca8-4cce-407a-92dd-feb3a734268e", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * self.dim\n        self.f = 0.8  # DE mutation factor\n        self.cr = 0.9  # DE crossover probability\n        self.local_search_steps = 5\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, pop, lb, ub):\n        new_pop = np.copy(pop)\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            fitness_trial = func(trial) - self.robustness_factor * np.std(trial)\n            self.eval_count += 1\n            if fitness_trial < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_search(self, func, individual, lb, ub):\n        best = np.copy(individual)\n        best_fitness = func(best)\n        for _ in range(self.local_search_steps):\n            if self.eval_count >= self.budget:\n                break\n            candidate = best + np.random.normal(0, 0.1, size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            fitness_candidate = func(candidate) - self.robustness_factor * np.std(candidate)\n            self.eval_count += 1\n            if fitness_candidate < best_fitness:\n                best, best_fitness = candidate, fitness_candidate\n        return best\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            pop = self.differential_evolution(func, pop, lb, ub)\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                pop[i] = self.local_search(func, pop[i], lb, ub)\n                layer_expansion = min(self.dim, int(self.eval_count / self.budget * self.dim))\n                pop[i][:layer_expansion] = self.local_search(func, pop[i][:layer_expansion], lb[:layer_expansion], ub[:layer_expansion])\n\n        best_idx = np.argmin([func(individual) for individual in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "An enhanced hybrid metaheuristic combining Differential Evolution and Local Search, with dynamic layer expansion and robustness metrics for high-dimensional noisy black-box optimization, refining layer expansion logic to ensure correct dimensionality.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "ffb1afe1-96c1-49f8-8bce-d27e2e0ebbea", "metadata": {}, "mutation_prompt": null}
{"id": "35bccd59-540b-4e16-ae0a-15e20a0ca0db", "solution": "import numpy as np\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * self.dim\n        self.f = 0.8  # DE mutation factor\n        self.cr = 0.9  # DE crossover probability\n        self.local_search_steps = 5\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, pop, lb, ub):\n        new_pop = np.copy(pop)\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.f * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            fitness_trial = func(trial) - self.robustness_factor * np.std(trial)\n            self.eval_count += 1\n            if fitness_trial < func(pop[i]):\n                new_pop[i] = trial\n        return new_pop\n\n    def local_search(self, func, individual, lb, ub):\n        best = np.copy(individual)\n        best_fitness = func(best)\n        for _ in range(self.local_search_steps):\n            if self.eval_count >= self.budget:\n                break\n            candidate = best + np.random.normal(0, 0.1, size=self.dim)\n            candidate = np.clip(candidate, lb, ub)\n            fitness_candidate = func(candidate) - self.robustness_factor * np.std(candidate)\n            self.eval_count += 1\n            if fitness_candidate < best_fitness:\n                best, best_fitness = candidate, fitness_candidate\n        return best\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        pop = np.random.uniform(lb, ub, (self.pop_size, self.dim))\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            pop = self.differential_evolution(func, pop, lb, ub)\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                pop[i] = self.local_search(func, pop[i], lb, ub)\n                layer_expansion = min(self.dim, int(self.eval_count / self.budget * self.dim))\n                if layer_expansion > 0:  # Change made here to ensure valid slicing\n                    pop[i][:layer_expansion] = self.local_search(func, pop[i][:layer_expansion], lb[:layer_expansion], ub[:layer_expansion])\n\n        best_idx = np.argmin([func(individual) for individual in pop])\n        return pop[best_idx]", "name": "HybridMetaheuristic", "description": "An optimized hybrid metaheuristic combining DE and local search with improved modular layer expansion.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) ')", "parent_id": "ffb1afe1-96c1-49f8-8bce-d27e2e0ebbea", "metadata": {}, "mutation_prompt": null}
{"id": "9d372ad7-4bb8-42b0-aff5-2dd353b89061", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n        \n        F = 0.8 # Initial mutation factor\n        CR = 0.9 # Initial crossover rate\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = np.random.uniform(0.5, 1.0)  # Self-adaptive mutation factor\n                CR = np.random.uniform(0.7, 1.0)  # Self-adaptive crossover rate\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                trial = np.where(np.random.rand(len(lb)) < CR, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                if bounds.lb <= neighbor.all() <= bounds.ub:\n                    neighbor_fitness = func(neighbor)\n                    if neighbor_fitness < best_fitness:\n                        best_solution, best_fitness = neighbor, neighbor_fitness\n                        step_size *= 0.5  # Adaptive step size reduction for more precise refinement\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "Enhanced PhotonicOptimizer by incorporating a self-adaptive mutation factor and crossover rate in Differential Evolution for improved global search.", "configspace": "", "generation": 2, "fitness": 0.8042071047360745, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.011. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "22171ad4-51a8-44a6-890e-1b5e2a1cbf71", "metadata": {"aucs": [0.790505331555519, 0.8035057535788883, 0.8186102290738162], "final_y": [0.13672753446147767, 0.14196771490301197, 0.1356115376777962]}, "mutation_prompt": null}
{"id": "deb311b1-bb8a-40c4-b12d-609e56c5d799", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([self.evaluate_individual(func, ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < self.variable_crossover_rate(i, fitness)\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_fitness = self.evaluate_individual(func, trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def evaluate_individual(self, func, ind):\n        perturbation = 0.001 * (np.random.rand(self.dim) - 0.5)\n        return func(ind) + func(ind + perturbation)\n    \n    def variable_crossover_rate(self, index, fitness):\n        return self.CR * (1 - (fitness[index] / (np.max(fitness) + 1e-8)))\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "The algorithm enhances the existing hybrid strategy by integrating robustness metrics in the fitness evaluation and dynamically adjusting crossover probability to efficiently balance exploration and exploitation in noisy, high-dimensional optimization problems.", "configspace": "", "generation": 2, "fitness": 0.8167273649532181, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.020. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "9a5626fb-4c6d-42f6-ae1f-98c49063723a", "metadata": {"aucs": [0.8415014799775922, 0.7928207890074893, 0.8158598258745726], "final_y": [0.12638180026817047, 0.15540647188672307, 0.13904501701193317]}, "mutation_prompt": null}
{"id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Incorporate boundary variables into mutation function to fix undefined variable error and improve performance.", "configspace": "", "generation": 2, "fitness": 0.844117433504563, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.017. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "d6b20a5d-49c7-43c2-a364-5554140bea74", "metadata": {"aucs": [0.8604618111398389, 0.8505316487227661, 0.8213588406510841], "final_y": [0.12395641956436876, 0.13481537692154621, 0.14173451873047327]}, "mutation_prompt": null}
{"id": "c889209d-b8b7-49cf-87b7-11fd93361207", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                # Mutation with adaptive F\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                self.F = 0.5 + 0.3 * np.random.rand()  # Adjusted mutation factor\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        # Local optimization using L-BFGS-B\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "The algorithm improves Differential Evolution by dynamically adjusting the mutation factor (F) based on the convergence rate for efficient exploration in high-dimensional noisy optimization problems.", "configspace": "", "generation": 3, "fitness": 0.8401858071209073, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.008. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9a5626fb-4c6d-42f6-ae1f-98c49063723a", "metadata": {"aucs": [0.8378147147968175, 0.8503432003402099, 0.8323995062256945], "final_y": [0.13259396094434694, 0.11574814784617493, 0.12941259696451346]}, "mutation_prompt": null}
{"id": "1b91695c-3335-4867-af99-daf3d04c151c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            self.F = 0.5 + 0.3 * (1 - self.evaluations / self.budget)  # Change: Dynamic F adjustment\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        # Local optimization using L-BFGS-B\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "Enhance global exploration by dynamically adjusting the mutation factor F based on convergence speed to improve performance in noisy high-dimensional optimization problems.", "configspace": "", "generation": 3, "fitness": 0.8601279604766133, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.009. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "9a5626fb-4c6d-42f6-ae1f-98c49063723a", "metadata": {"aucs": [0.8637641517110197, 0.8483345765639327, 0.8682851531548875], "final_y": [0.12142676304619959, 0.12897842627622003, 0.1235924922308681]}, "mutation_prompt": null}
{"id": "f547b812-d8ac-4485-9dde-86f249671288", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), lb, ub)\n                trial = np.where(np.random.rand(len(lb)) < 0.9, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                # Stochastic acceptance for exploration\n                if trial_fitness < fitness[i] or np.random.rand() < 0.05:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                if bounds.lb <= neighbor.all() <= bounds.ub:\n                    neighbor_fitness = func(neighbor)\n                    if neighbor_fitness < best_fitness:\n                        best_solution, best_fitness = neighbor, neighbor_fitness\n                        step_size *= 0.5  # Adaptive step size reduction for more precise refinement\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "Improved exploration and exploitation balance by integrating stochastic acceptance and diversity preservation in population update.", "configspace": "", "generation": 3, "fitness": 0.8013501636482708, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.031. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "22171ad4-51a8-44a6-890e-1b5e2a1cbf71", "metadata": {"aucs": [0.7592038852428338, 0.8305223841403624, 0.8143242215616165], "final_y": [0.15618430426911067, 0.1345906642464757, 0.13047334905194274]}, "mutation_prompt": null}
{"id": "8e46d4d6-6cb9-4236-bea9-0f60bd903b13", "solution": "import numpy as np\n\nclass PhotonicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.global_pop_size = 50\n        self.local_refinement_steps = 10\n        self.dim_increment = max(dim // 10, 1)\n        self.robustness_factor = 0.01\n\n    def differential_evolution(self, func, bounds, pop_size, max_iter):\n        lb, ub = bounds.lb, bounds.ub\n        pop = lb + (ub - lb) * np.random.rand(pop_size, len(lb))\n        fitness = np.apply_along_axis(func, 1, pop)\n        F = 0.8  # Mutation factor\n        CR = 0.9  # Crossover rate\n\n        for iteration in range(max_iter):\n            if len(fitness) >= self.budget:\n                break\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.3 * np.random.rand()  # Self-adaptive mutation factor\n                mutant = np.clip(a + F * (b - c), lb, ub)\n                CR = 0.8 + 0.2 * np.random.rand()  # Self-adaptive crossover rate\n                trial = np.where(np.random.rand(len(lb)) < CR, mutant, pop[i])\n                trial_fitness = func(trial)\n\n                if len(fitness) >= self.budget:\n                    break\n\n                if trial_fitness < fitness[i]:\n                    pop[i], fitness[i] = trial, trial_fitness\n\n        return pop, fitness\n\n    def local_search(self, func, solution, bounds):\n        best_solution = solution\n        best_fitness = func(solution)\n        step_size = (bounds.ub - bounds.lb) / 100.0\n        perturbation_scale = 0.05\n\n        for _ in range(self.local_refinement_steps):\n            neighbors = [solution + step_size * np.random.randn(self.dim) for _ in range(5)]\n            for neighbor in neighbors:\n                neighbor = np.clip(neighbor, bounds.lb, bounds.ub)  # Ensure within bounds\n                neighbor_fitness = func(neighbor)\n                if neighbor_fitness < best_fitness:\n                    best_solution, best_fitness = neighbor, neighbor_fitness\n                    step_size *= 0.5  # Adaptive step size reduction\n                # Perturbation analysis for robustness\n                if np.random.rand() < perturbation_scale:\n                    test_perturbation = neighbor + perturbation_scale * np.random.randn(self.dim)\n                    test_perturbation = np.clip(test_perturbation, bounds.lb, bounds.ub)\n                    test_fitness = func(test_perturbation)\n                    if test_fitness < best_fitness:\n                        best_solution, best_fitness = test_perturbation, test_fitness\n        return best_solution, best_fitness\n\n    def __call__(self, func):\n        bounds = func.bounds\n        current_dim = self.dim_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim:\n            pop, fitness = self.differential_evolution(func, bounds, self.global_pop_size, self.budget // 10)\n            for solution in pop:\n                if len(fitness) >= self.budget:\n                    break\n                refined_solution, refined_fitness = self.local_search(func, solution, bounds)\n                if refined_fitness < best_fitness:\n                    best_solution, best_fitness = refined_solution, refined_fitness\n\n            current_dim += self.dim_increment\n\n        return best_solution", "name": "PhotonicOptimizer", "description": "Enhance global exploration with self-adaptive parameters and improve local search robustness using a perturbation analysis.", "configspace": "", "generation": 3, "fitness": 0.8094943415542372, "feedback": "The algorithm PhotonicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.009. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "22171ad4-51a8-44a6-890e-1b5e2a1cbf71", "metadata": {"aucs": [0.8126721137171306, 0.8188239593919615, 0.7969869515536195], "final_y": [0.13868786520877585, 0.14051009566658856, 0.14553394596061975]}, "mutation_prompt": null}
{"id": "91dd06fe-c754-43b4-a2ce-c6b7ecc14199", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Incorporate adaptive mutation factor to the mutation function for dynamic exploration.", "configspace": "", "generation": 3, "fitness": 0.8494358070442006, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.030. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "metadata": {"aucs": [0.859587013062876, 0.8081252793980878, 0.8805951286716381], "final_y": [0.13231383423396315, 0.14879109103059263, 0.11562708227383123]}, "mutation_prompt": null}
{"id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adding stochastic variation to the mutation factor (F) for improved performance in noisy environments.", "configspace": "", "generation": 4, "fitness": 0.87147445234096, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.013. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "metadata": {"aucs": [0.8621642141076042, 0.8898760434739269, 0.8623830994413487], "final_y": [0.13230291199926159, 0.11430257077042005, 0.11499709662251312]}, "mutation_prompt": null}
{"id": "7a1de8b3-c89b-4b99-8d35-acc9ca80208e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n        self.layer_increase_steps = [10, 20, 32]  # Change: Adaptive layer increase\n    \n    def differential_evolution(self, func, bounds):\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            self.F = 0.5 + 0.3 * (1 - 0.7 * self.evaluations / self.budget)  # Change: Adjust F more aggressively\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                modularity_weights = self.detect_layer_modularity(b, c)  # Change: Modularity detection\n                mutant = np.clip(a + self.F * (b - c) * modularity_weights, bounds.lb, bounds.ub)\n                \n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n            \n            if self.evaluations < self.budget:\n                self.increase_layers(func)  # Change: Dynamic layer increase\n\n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def detect_layer_modularity(self, b, c):\n        # Simplified modularity detection\n        modularity = np.where(np.abs(b - c) < 0.1, 0.5, 1.0)\n        return modularity\n\n    def increase_layers(self, func):\n        # Change: Simplified layer increase logic\n        for step in self.layer_increase_steps:\n            if self.evaluations < self.budget:\n                self.dim = step\n                break\n\n    def local_refinement(self, func, x0, bounds):\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "Integrate layer modularity detection and adaptive layer increase with dynamic F adjustment to enhance exploration and exploitation balance in noisy high-dimensional optimization tasks.", "configspace": "", "generation": 4, "fitness": 0.8235434797637565, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.006. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "1b91695c-3335-4867-af99-daf3d04c151c", "metadata": {"aucs": [0.8155945832673379, 0.8313035063618822, 0.8237323496620491], "final_y": [0.1380620222669786, 0.1305964352677954, 0.1290216275429834]}, "mutation_prompt": null}
{"id": "677d6a4c-b992-4ca2-ab2f-d5319c046729", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= 1.05  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate diversity-driven selection and incremental mutation rate adjustment to enhance exploration and exploitation balance in high-dimensional noisy optimization problems.", "configspace": "", "generation": 4, "fitness": 0.860142114830602, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.006. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "metadata": {"aucs": [0.8542010510066691, 0.8690369081119077, 0.8571883853732293], "final_y": [0.1268270653314142, 0.11579278556962713, 0.12330603494782066]}, "mutation_prompt": null}
{"id": "0ebca1a9-ce83-4a81-b3ba-2d8c61e09e5a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by adjusting population size dynamically based on evaluation progress.", "configspace": "", "generation": 4, "fitness": 0.855912694473271, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.029. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "91dd06fe-c754-43b4-a2ce-c6b7ecc14199", "metadata": {"aucs": [0.8160655539757351, 0.8836766772608554, 0.8679958521832226], "final_y": [0.1427436732031837, 0.11440523332392827, 0.11716880808480534]}, "mutation_prompt": null}
{"id": "a526cc74-0399-40c1-8791-631760b3fc1e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.evals = 0  # Evaluation count\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Initial differential weight\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _adapt_parameters(self, population, func):\n        fitness = np.array([func(ind) for ind in population])\n        mean_fitness = np.mean(fitness)\n        diversity = np.std(fitness)\n        self.cr = 0.8 + 0.2 * (diversity / mean_fitness)\n        self.f = 0.3 + 0.7 * (1 - diversity / mean_fitness)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            self._adapt_parameters(population, func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance HybridMetaheuristic by incorporating dynamic adaptation of crossover and mutation rates based on population diversity for improved exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8511693556678211, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.033. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "987a72e9-ae5a-451d-81e7-546d58ec0d31", "metadata": {"aucs": [0.874314666387711, 0.8741839566032918, 0.8050094440124608], "final_y": [0.1125315874467907, 0.12240690825931011, 0.14713611282060446]}, "mutation_prompt": null}
{"id": "184eeb15-3da7-48bc-b7b3-779d632be2ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))\n        modular_scaling = 1 + 0.1 * (self.dim // 10)  # New scaling factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c]) * modular_scaling\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        learning_rate = 0.01  # New learning rate\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                trial = trial + learning_rate * (best_solution - trial)\n                trial = np.clip(trial, lb, ub)  # Ensure bounds are respected\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration-exploitation balance by integrating adaptive learning rates and modular structure recognition.", "configspace": "", "generation": 5, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\").", "error": "TypeError(\"unsupported operand type(s) for -: 'NoneType' and 'float'\")", "parent_id": "0ebca1a9-ce83-4a81-b3ba-2d8c61e09e5a", "metadata": {}, "mutation_prompt": null}
{"id": "72b75382-e075-413a-a387-9997ab4f9164", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 20\n        self.CR = 0.9\n        self.F = 0.8\n        self.evaluations = 0\n    \n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        population = np.random.rand(self.pop_size, self.dim)\n        population = bounds.lb + (bounds.ub - bounds.lb) * population\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += self.pop_size\n        \n        while self.evaluations < self.budget:\n            self.F = 0.5 + 0.3 * (1 - self.evaluations / self.budget)  # Change: Dynamic F adjustment\n            self.CR = 0.7 + 0.2 * (1 - self.evaluations / self.budget)  # Change: Dynamic CR adjustment\n            for i in range(self.pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = np.clip(a + self.F * (b - c), bounds.lb, bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n        \n        best_idx = np.argmin(fitness)\n        return population[best_idx], fitness[best_idx]\n\n    def local_refinement(self, func, x0, bounds):\n        # Local optimization using L-BFGS-B\n        result = minimize(func, x0, method='L-BFGS-B', bounds=[(l, u) for l, u in zip(bounds.lb, bounds.ub)])\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution, best_fitness = self.differential_evolution(func, bounds)\n        if self.evaluations < self.budget:\n            refined_solution, refined_fitness = self.local_refinement(func, best_solution, bounds)\n            if refined_fitness < best_fitness:\n                best_solution, best_fitness = refined_solution, refined_fitness\n        return best_solution", "name": "HybridOptimizer", "description": "Enhance exploration and exploitation by dynamically adjusting the crossover rate (CR) based on convergence speed.", "configspace": "", "generation": 5, "fitness": 0.8431162819217825, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.014. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "1b91695c-3335-4867-af99-daf3d04c151c", "metadata": {"aucs": [0.8392205654450562, 0.8620548767553178, 0.8280734035649733], "final_y": [0.12545305574340992, 0.12494699946964594, 0.13227963391595932]}, "mutation_prompt": null}
{"id": "ca6753cf-2617-4fea-85b4-7b5e955bedad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        if np.random.rand() < 0.5:  # Probabilistic selection of mutation strategies\n            mutant = population[a] + self.f * (population[b] - population[c])\n        else:\n            mutant = population[a] + self.f * (population[c] - population[b])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= 1.05  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a probabilistic selection of mutation strategies to enhance diversity and adaptivity.", "configspace": "", "generation": 5, "fitness": 0.8146300614028118, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.015. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "677d6a4c-b992-4ca2-ab2f-d5319c046729", "metadata": {"aucs": [0.8201485905620551, 0.793597501311604, 0.8301440923347759], "final_y": [0.14002868441858762, 0.15901116267818372, 0.13476626095628785]}, "mutation_prompt": null}
{"id": "cdad852a-5597-47c6-b10d-93f7d273ca5d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability based on evaluations\n        cr_dynamic = self.cr * (1 - self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < cr_dynamic\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing a dynamic crossover probability (CR) to balance exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.8541918856592522, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.017. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {"aucs": [0.8786468016591737, 0.8424034690743086, 0.8415253862442744], "final_y": [0.1175637870016869, 0.13475532970694926, 0.1256407667429673]}, "mutation_prompt": null}
{"id": "44262d5a-31a5-4468-8a34-01c582320bf8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= 1.05  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n            self.cr = min(0.9, diversity * 10)  # Adjust crossover probability based on diversity\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve the exploitation phase by adjusting the crossover probability based on population diversity.", "configspace": "", "generation": 5, "fitness": 0.8160140078171748, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.054. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.021.", "error": "", "parent_id": "677d6a4c-b992-4ca2-ab2f-d5319c046729", "metadata": {"aucs": [0.7400036853084662, 0.8535546368861788, 0.8544837012568791], "final_y": [0.17767096583001174, 0.1353091656494696, 0.12954783505590728]}, "mutation_prompt": null}
{"id": "acba7027-e925-4118-adea-9a009c4261a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adjust crossover probability dynamically based on best score\n        self.cr = 0.9 * (1 - best_score / (best_score + 1))\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic adjustment to crossover probability based on current best score to enhance exploitation.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'best_score' is not defined\").", "error": "NameError(\"name 'best_score' is not defined\")", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {}, "mutation_prompt": null}
{"id": "fc995f73-2e24-4f1f-bd89-02ea41a9f827", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive CR based on population diversity\n        diversity = np.mean(np.std(population, axis=0))\n        adaptive_cr = max(0.1, min(1.0, self.cr * (1 + 0.1 * diversity)))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration and exploitation by introducing adaptive crossover probability (CR) based on population diversity.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {}, "mutation_prompt": null}
{"id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Optimize layer role preservation by tweaking the local refinement step's starting point to better acknowledge modular structure in solutions.", "configspace": "", "generation": 6, "fitness": 0.9004245782625578, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.900 with standard deviation 0.015. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "0ebca1a9-ce83-4a81-b3ba-2d8c61e09e5a", "metadata": {"aucs": [0.9010884023304909, 0.9185055262960179, 0.8816798061611646], "final_y": [0.1123535478478741, 0.10961088632201743, 0.11936737091160643]}, "mutation_prompt": null}
{"id": "b73aba40-edd1-4fdd-90ae-1aafe7062059", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability based on evaluations\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability (CR) based on convergence speed to balance exploration and exploitation better.", "configspace": "", "generation": 6, "fitness": 0.8608220765228026, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.014. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {"aucs": [0.8610981973619523, 0.8783123234438452, 0.8430557087626106], "final_y": [0.13230291199926159, 0.12363227099790697, 0.13479717088846366]}, "mutation_prompt": null}
{"id": "4f24b478-b715-4066-855f-2a88713d668f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= (1.05 + 0.05 * np.std(population))  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Boosts algorithm robustness by adjusting mutation rate incrementally based on population diversity.", "configspace": "", "generation": 6, "fitness": 0.8834310299192957, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.883 with standard deviation 0.019. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "677d6a4c-b992-4ca2-ab2f-d5319c046729", "metadata": {"aucs": [0.8587345517852281, 0.903439502135345, 0.888119035837314], "final_y": [0.11874623896654146, 0.11280560128133565, 0.11715331069114965]}, "mutation_prompt": null}
{"id": "c467b973-7e3c-4cad-bfc2-44c537160683", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= (1.01 + 0.09 * np.std(population))  # Adjust mutation rate\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance performance by fine-tuning the adaptive mutation rate mechanism to respond more dynamically to population diversity.", "configspace": "", "generation": 7, "fitness": 0.8430089416150919, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.061. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "4f24b478-b715-4066-855f-2a88713d668f", "metadata": {"aucs": [0.7577242554096693, 0.8787232412888896, 0.8925793281467168], "final_y": [0.12692950484237053, 0.11765310435649301, 0.11523426950575022]}, "mutation_prompt": null}
{"id": "d538fb58-8381-4808-930d-0e16bb2711ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability based on evaluations\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            population_scores = [func(ind) for ind in population]\n            best_idx = np.argmin(population_scores)\n            worst_idx = np.argmax(population_scores)\n            d = np.linalg.norm(population[best_idx] - population[worst_idx])\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim and d > 0.1:  # Perform more frequent refinement\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by conducting it more frequently based on the distance between the best and worst solutions.", "configspace": "", "generation": 7, "fitness": 0.8639908958142194, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.025. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "b73aba40-edd1-4fdd-90ae-1aafe7062059", "metadata": {"aucs": [0.8489010127069337, 0.8991772270918841, 0.8438944476438404], "final_y": [0.12714622914597795, 0.11707723262386827, 0.13378333374135687]}, "mutation_prompt": null}
{"id": "31219726-2b61-491d-a103-20c08fc8ec03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        # Incorporate Gaussian perturbation to enhance diversity\n        mutant += np.random.normal(0, 0.1, size=self.dim) \n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability based on evaluations\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance the mutation diversity by incorporating a Gaussian perturbation to the mutant vector.", "configspace": "", "generation": 7, "fitness": 0.8601301157125167, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.014. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "b73aba40-edd1-4fdd-90ae-1aafe7062059", "metadata": {"aucs": [0.840714608865526, 0.8733918601086235, 0.8662838781634009], "final_y": [0.1266764053817595, 0.1233020955493902, 0.11874051483766968]}, "mutation_prompt": null}
{"id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhancing layer preservation by adapting local refinement starting point to boost solution modularity.", "configspace": "", "generation": 7, "fitness": 0.8655177573652127, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.023. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8815292722325243, 0.8332689864243323, 0.8817550134387816], "final_y": [0.11406019785333421, 0.14099211055737637, 0.1196331673388944]}, "mutation_prompt": null}
{"id": "2a3050e2-53b1-45ef-86a9-cc41e0ad8787", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub): \n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Improved adaptive mutation scaling\n        adaptive_f = self.f + (0.2 * np.random.rand() - 0.1)\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance solution diversity by using a tournament selection mechanism and adaptive mutation scaling for improved exploration.", "configspace": "", "generation": 7, "fitness": 0.8874832449969204, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.019. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b73aba40-edd1-4fdd-90ae-1aafe7062059", "metadata": {"aucs": [0.8603209907889252, 0.8983715677868771, 0.9037571764149588], "final_y": [0.12896247807868166, 0.11149418894288365, 0.11440568420096908]}, "mutation_prompt": null}
{"id": "e45d43a9-ad39-4889-8deb-4c23a76cdcae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Dynamic crossover probability adjustment based on population diversity\n        diversity = np.std(np.asarray(self.population))\n        self.cr = max(0.1, 1 - diversity)\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing dynamic crossover probability adjustment based on population diversity.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {}, "mutation_prompt": null}
{"id": "5c44ec5e-60da-45c9-a918-f7ffbdaf8f4a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= (1.05 + 0.05 * np.std(population))  # Incremental mutation rate adjustment\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            self.population_size = int(max(5 * self.dim, self.population_size * (1 + 0.1 * np.std(population))))  # Dynamically adjust population size\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve diversity by dynamically adjusting population size based on convergence rate and diversity.  ", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 145 is out of bounds for axis 0 with size 100').", "error": "IndexError('index 145 is out of bounds for axis 0 with size 100')", "parent_id": "4f24b478-b715-4066-855f-2a88713d668f", "metadata": {}, "mutation_prompt": null}
{"id": "7c0cffcd-61be-4b30-a223-59b111b20f05", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Added stochastic variation to the mutation factor\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    # Dynamically adjust crossover probability\n    def _update_crossover_rate(self, population):\n        diversity = np.std(population, axis=0).mean()\n        self.cr = 0.5 + 0.4 * (1 - diversity / np.max([diversity, 1e-5]))\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            self._update_crossover_rate(population)  # Update crossover probability\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by dynamically adjusting crossover probability based on population diversity.", "configspace": "", "generation": 8, "fitness": 0.8304828027650851, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.030. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {"aucs": [0.8308773368608033, 0.7934153127927578, 0.8671557586416945], "final_y": [0.13436275413858567, 0.1601017669387199, 0.12386786449268328]}, "mutation_prompt": null}
{"id": "a0d44580-3f15-464b-897c-7ccfe82c1081", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        diversity = np.std(population, axis=0).mean()  # Measure diversity\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * diversity  # Dynamic weighting\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance algorithm efficiency by incorporating dynamic weighting of exploration and exploitation based on solution diversity and convergence speed.", "configspace": "", "generation": 8, "fitness": 0.7668012291602179, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.767 with standard deviation 0.020. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "metadata": {"aucs": [0.7446046465223465, 0.7934153127927578, 0.7623837281655493], "final_y": [0.1619617580379672, 0.1601017669387199, 0.1550651084314263]}, "mutation_prompt": null}
{"id": "89017de2-0af8-4f05-a66b-dff1ff396204", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr * (1 - (self.evals / self.budget))  # Dynamic crossover probability\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by using a dynamic crossover probability that decreases as evaluations progress to balance exploration and exploitation.", "configspace": "", "generation": 8, "fitness": 0.8564011727583027, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.022. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.880612300449196, 0.8608818754352304, 0.8277093423904816], "final_y": [0.11707013798338561, 0.12866184645863143, 0.13484028333830977]}, "mutation_prompt": null}
{"id": "39c16239-528c-45f9-b3d5-9683e48fa1c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        stochastic_f = self.f + 0.1 * np.random.rand()\n        mutant = population[a] + stochastic_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability\n        self.cr = max(0.5, 1 - self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate adaptive crossover probability to enhance exploration and exploitation balance in noisy environments.", "configspace": "", "generation": 9, "fitness": 0.856084918898549, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.032. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "df0fb3b6-aa2d-457b-b6fd-86dfa5428d72", "metadata": {"aucs": [0.8263489527082508, 0.9004872661253703, 0.8414185378620258], "final_y": [0.13768201466286922, 0.11707215330687137, 0.13396429151381228]}, "mutation_prompt": null}
{"id": "cde5d679-c532-4336-8a60-07db38610c72", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  \n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  \n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        self.cr = 0.9 * (1 - self.evals / self.budget)  # Dynamic crossover probability\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B') \n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  \n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  \n                trial = self._crossover(target, mutant)\n                score = np.mean([func(trial + np.random.normal(0, 1e-3, self.dim)) for _ in range(3)])  # Noise-resilient evaluation\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic crossover probability and incorporate a noise-resilient objective function evaluation to improve stability and exploration.  ", "configspace": "", "generation": 9, "fitness": 0.848486084457665, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.034. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "metadata": {"aucs": [0.8408614537155504, 0.8937090673596206, 0.8108877322978238], "final_y": [0.11466254997017433, 0.11440478850166191, 0.14901555914048603]}, "mutation_prompt": null}
{"id": "d29612e0-5160-4f5d-b35f-479a87c71181", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        adaptive_cr = self.cr * (1 - (self.evals / self.budget))  # Adaptive crossover rate\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by incorporating an adaptive crossover rate that decreases as evaluations progress.", "configspace": "", "generation": 9, "fitness": 0.8608835250265633, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.032. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.859714100923415, 0.9008900777432279, 0.8220463964130469], "final_y": [0.12542721014821145, 0.11531543503683184, 0.14324682531244126]}, "mutation_prompt": null}
{"id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by implementing adaptive crossover probability based on evaluation progress for better balance between exploration and exploitation.", "configspace": "", "generation": 9, "fitness": 0.884471572297099, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.884 with standard deviation 0.019. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "metadata": {"aucs": [0.857594500303791, 0.8982164386371677, 0.8976037779503385], "final_y": [0.1251366442596208, 0.11707292865274799, 0.1153766410646837]}, "mutation_prompt": null}
{"id": "1b3e0514-182e-406a-9777-15d1c2eca6ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Changed line:\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - (self.evals / self.budget))  # Adaptive crossover probability\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive crossover probability to enhance exploration and convergence balance.", "configspace": "", "generation": 9, "fitness": 0.8720710319556266, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.007. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "01055dd7-b6e1-42ff-83e4-a4f552bbc27e", "metadata": {"aucs": [0.8639861407954663, 0.8702796696955483, 0.8819472853758656], "final_y": [0.12513733685935102, 0.12667577837178723, 0.11891833923036699]}, "mutation_prompt": null}
{"id": "3cc4f5bc-4ba7-4bb5-8eac-540fcd72c77c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        pop_variance = np.var(population, axis=0).mean()  # Calculate population variance\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * (1 + pop_variance)  # Dynamic mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        pop_variance = np.var(population, axis=0).mean()  # Calculate population variance\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget) * (1 + pop_variance))  # Dynamic crossover\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduces dynamic weighting of crossover and mutation based on evaluation progress and variance in population.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {}, "mutation_prompt": null}
{"id": "21f3937f-1805-4e98-ae7a-69e8217979d6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.elite_fraction = 0.1  # Elite preservation fraction\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        diversity = np.std(population, axis=0).mean()  # Measure diversity\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * (0.5 + 0.5 * diversity)  # Enhanced adaptive mutation\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        diversity = np.std(self.population, axis=0).mean()  # Measure diversity\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget)) * (0.5 + 0.5 * diversity)  # Enhanced adaptive crossover\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            elite_count = int(self.elite_fraction * self.population_size)\n            population_scores = np.array([func(ind) for ind in population])\n            elite_indices = np.argsort(population_scores)[:elite_count]\n            elites = population[elite_indices]\n\n            for i in range(self.population_size):\n                if i in elite_indices:  # Preserve elite\n                    continue\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n            self.population = population  # Keep track of current population for diversity calculation\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce elite preservation mechanism and adaptive mutation/crossover probabilities based on evaluation progress and population diversity to improve exploration and exploitation.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {}, "mutation_prompt": null}
{"id": "9efcc28f-5dce-4adc-9439-2bce1ebc77ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.7 * (self.evals / self.budget))  # Adjusted starting value to enhance performance\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by adjusting adaptive crossover start value for better exploration-exploitation tradeoff.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {}, "mutation_prompt": null}
{"id": "91e39ca3-67d2-458d-bdfc-f1bb12af8597", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        self.cr = 0.9 * (1 - (self.evals / self.budget))  # Dynamic crossover probability\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Optimize crossover mechanism to dynamically adjust based on evaluation progress to balance exploration and exploitation.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {}, "mutation_prompt": null}
{"id": "4b82d98b-4db1-4a4f-913d-80631c934bba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget) * (1 + (1 - np.min([population[a], population[b], population[c]]) / self.budget)))  # Adjusted mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by adjusting adaptive mutation factor to consider both evaluation progress and current best score.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\").", "error": "AttributeError(\"'HybridMetaheuristic' object has no attribute 'population'\")", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {}, "mutation_prompt": null}
{"id": "d28878d9-27d6-421b-82b5-eeb23c9f7a34", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = population[a] + self.f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n                    self.f *= (1.05 + 0.05 * np.std(population) * 0.99)  # Incremental mutation rate adjustment with decay\n\n                if self.evals >= self.budget:\n                    break\n\n            diversity = np.std(population)  # Diversity-driven selection\n            if diversity < 0.01:  # Re-initialize population if diversity is low\n                population = self._initialize_population(lb, ub)\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance robustness by introducing a decay factor to the mutation rate adjustment based on diversity.", "configspace": "", "generation": 11, "fitness": 0.8220082305727648, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.072. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "4f24b478-b715-4066-855f-2a88713d668f", "metadata": {"aucs": [0.7785519616362376, 0.923023058062241, 0.7644496720198158], "final_y": [0.15683897814999082, 0.11295563229749694, 0.12534353757690253]}, "mutation_prompt": null}
{"id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - (self.evals / self.budget))  \n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration and convergence by introducing a self-adjusting mutation factor and elitism to preserve top-performing solutions.", "configspace": "", "generation": 11, "fitness": 0.8865403578346465, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.027. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "1b3e0514-182e-406a-9777-15d1c2eca6ad", "metadata": {"aucs": [0.8863861400669184, 0.9197178871419356, 0.8535170462950856], "final_y": [0.1159699781982223, 0.11176161171073429, 0.13450188755438408]}, "mutation_prompt": null}
{"id": "ac7b3344-8daf-4520-8823-d9d4bc9ea7b0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.7 * (self.evals / self.budget))  # Changed factor to 0.7\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve convergence by incorporating a dynamic crossover rate that more aggressively adapts to early-stage evaluation progress.", "configspace": "", "generation": 11, "fitness": 0.8746944396661741, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.043. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {"aucs": [0.8809164982833515, 0.924118690988634, 0.8190481297265368], "final_y": [0.11489922141345421, 0.1116274851543011, 0.1456678335854451]}, "mutation_prompt": null}
{"id": "4e84e475-c8fa-44bf-87e4-e431fe15bcdf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability increased early in evaluations\n        adaptive_cr = self.cr * (1 + 0.5 * (1 - self.evals / self.budget))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by increasing adaptive crossover probability early in evaluations.", "configspace": "", "generation": 11, "fitness": 0.8763429921894712, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.016. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {"aucs": [0.897337481837662, 0.858504509801483, 0.8731869849292684], "final_y": [0.11338869175252464, 0.12687692538038275, 0.12567390176839743]}, "mutation_prompt": null}
{"id": "cc419866-e0e0-4e8d-8c74-bde26f8809dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size  # Dynamic population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _adaptive_population_size(self):\n        # Adjust population size based on remaining budget\n        self.population_size = max(4, int(self.base_population_size * (1 - self.evals / self.budget))) \n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Multi-strategy mutation: choice of random weighted mutation\n        if np.random.rand() < 0.5:\n            mutant = population[a] + self.f * (population[b] - population[c])\n        else:\n            mutant = population[a] + self.f * (population[b] - population[target_idx])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            self._adaptive_population_size()\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce multi-strategy mutation and variable population sizing to enhance exploration and convergence.", "configspace": "", "generation": 11, "fitness": 0.8868857870214972, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.018. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "2a3050e2-53b1-45ef-86a9-cc41e0ad8787", "metadata": {"aucs": [0.903758020796898, 0.8624448095231103, 0.8944545307444832], "final_y": [0.11692230528864711, 0.1323040088073678, 0.1182750998784814]}, "mutation_prompt": null}
{"id": "b8d4ef00-bcb0-46aa-8bed-1f3ea0b26204", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        diversity = np.std(population, axis=0).mean()  # Calculate population diversity\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * (1 + diversity)  # Adaptive mutation factor with diversity scaling\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by introducing adaptive mutation factor scaling based on population diversity to improve exploration.", "configspace": "", "generation": 12, "fitness": 0.8448813450246576, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.061. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {"aucs": [0.7723962116416053, 0.9224702871116559, 0.8397775363207117], "final_y": [0.14587691375745904, 0.11295136115401105, 0.13643158413982848]}, "mutation_prompt": null}
{"id": "596d60d5-c614-4c6e-9bad-ad4dfa6a05a5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - 0.3 * (self.evals / self.budget))  # Adjusted crossover scaling\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by adjusting crossover probability to increase diversity earlier in the search process.", "configspace": "", "generation": 12, "fitness": 0.8488152364198546, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.023. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {"aucs": [0.8190015198442845, 0.8533786598906312, 0.874065529524648], "final_y": [0.11060422809186532, 0.13391298219188663, 0.12567520946700061]}, "mutation_prompt": null}
{"id": "b7212948-f39c-4040-8f03-1da15ad57b4f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size  # Dynamic population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _adaptive_population_size(self):\n        # Adjust population size based on remaining budget\n        self.population_size = max(4, int(self.base_population_size * (1 - self.evals / self.budget))) \n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Multi-strategy mutation: choice of random weighted mutation\n        if np.random.rand() < 0.5:\n            self.f = 0.5 * (1 - self.evals / self.budget)  # Dynamic differential weight\n            mutant = population[a] + self.f * (population[b] - population[c])\n        else:\n            mutant = population[a] + self.f * (population[b] - population[target_idx])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            self._adaptive_population_size()\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a dynamic differential weight (f) that decreases over time to emphasize exploitation in later stages of the search.", "configspace": "", "generation": 12, "fitness": 0.8559043825983901, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.013. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "cc419866-e0e0-4e8d-8c74-bde26f8809dc", "metadata": {"aucs": [0.8563236041386438, 0.8716246493678964, 0.8397648942886299], "final_y": [0.11680398236243317, 0.1268814919974569, 0.1400867040311492]}, "mutation_prompt": null}
{"id": "639414f9-5d72-4bc5-89de-601ae14eb945", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n        self.local_refinement_frequency = 0.1 * budget  # Dynamic frequency for local refinement\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - (self.evals / self.budget))  \n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim and self.evals >= self.local_refinement_frequency:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic local refinement frequency based on convergence speed to improve exploitation without exceeding evaluation limits.", "configspace": "", "generation": 12, "fitness": 0.8607642007244637, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.047. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {"aucs": [0.8439970638517982, 0.9249235817372908, 0.8133719565843018], "final_y": [0.13094328174709502, 0.11162915986929212, 0.1473989952276199]}, "mutation_prompt": null}
{"id": "dac82457-1f04-485a-a9e5-2524cf4d5441", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _adaptive_population_size(self):\n        self.population_size = max(4, int(self.base_population_size * (1 - self.evals / self.budget)))\n\n    def _dynamic_mutation_factor(self):\n        return self.f + (0.2 * (self.evals / self.budget))  # Dynamic mutation factor\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        dynamic_f = self._dynamic_mutation_factor()\n        if np.random.rand() < 0.5:\n            mutant = population[a] + dynamic_f * (population[b] - population[c])\n        else:\n            mutant = population[a] + dynamic_f * (population[b] - population[target_idx])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def _diversity_preservation(self, population, scores):\n        unique_solutions = np.unique(population, axis=0)  # Keep unique solutions\n        if len(unique_solutions) < self.population_size:\n            extra_needed = self.population_size - len(unique_solutions)\n            extra_solutions = self._initialize_population(func.bounds.lb, func.bounds.ub)[:extra_needed]\n            population = np.vstack((unique_solutions, extra_solutions))\n        return population\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            self._adaptive_population_size()\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n            population = self._diversity_preservation(population, scores)\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing a dynamic mutation factor and integrating a diversity preservation mechanism.  ", "configspace": "", "generation": 12, "fitness": 0.8509567681243091, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.030. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "cc419866-e0e0-4e8d-8c74-bde26f8809dc", "metadata": {"aucs": [0.8449342119550177, 0.8905373683244872, 0.8173987240934224], "final_y": [0.13687025743299375, 0.11716491387768324, 0.14846716431341267]}, "mutation_prompt": null}
{"id": "c5f966aa-7d7a-4a45-95fc-624ef954e755", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        perturbation_scale = np.std(population, axis=0)  # New line to adjust mutation\n        mutant = population[a] + adaptive_f * (population[b] - population[c]) + np.random.normal(0, perturbation_scale)\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve refinement by adjusting mutation strategy to incorporate perturbation scaling for better adaptability.", "configspace": "", "generation": 13, "fitness": 0.8439256632091675, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.035. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8099699583275564, 0.8292024761875799, 0.8926045551123661], "final_y": [0.14235962030503246, 0.1415647975706683, 0.11982547642008967]}, "mutation_prompt": null}
{"id": "271ca99d-af76-45bd-9364-134b6df324d1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size  # Dynamic population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _adaptive_population_size(self):\n        # Adjust population size based on remaining budget\n        self.population_size = max(4, int(self.base_population_size * (1 - self.evals / self.budget))) \n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Multi-strategy mutation: choice of adaptive differential weight and random sampling\n        if np.random.rand() < 0.5:\n            f_adaptive = self.f * (1 - self.evals / self.budget)\n            mutant = population[a] + f_adaptive * (population[b] - population[c])\n        else:\n            d = np.random.choice(indices)\n            mutant = population[a] + self.f * (population[b] - population[d])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            self._adaptive_population_size()\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by using a blend of adaptive differential weight and random sampling in mutation.", "configspace": "", "generation": 13, "fitness": 0.8588889892511483, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.023. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "cc419866-e0e0-4e8d-8c74-bde26f8809dc", "metadata": {"aucs": [0.8887927781927697, 0.8322825739702886, 0.8555916155903867], "final_y": [0.11900236464154967, 0.14158247534850332, 0.13475515003149652]}, "mutation_prompt": null}
{"id": "ba89ae06-8db3-4170-8902-b61f88c8981f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def _local_refinement(self, best, func):\n        perturbed_start = best * (0.99 + 0.02 * np.random.rand(self.dim))  # Dynamic perturbation\n        result = minimize(func, perturbed_start, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by dynamically adjusting the starting point based on previous best solutions and incorporating robustness metrics for perturbation tolerance.", "configspace": "", "generation": 13, "fitness": 0.8697243242244395, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.025. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.838101846532012, 0.8980018857613117, 0.8730692403799947], "final_y": [0.13558923859050687, 0.11474984669134547, 0.12567640228787091]}, "mutation_prompt": null}
{"id": "2b241be2-1d9c-4aee-a172-12b69adb8094", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.98, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve layer role preservation by adjusting the local refinement starting point to 0.98 of the best solution.", "configspace": "", "generation": 13, "fitness": 0.8596644380444524, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.039. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8076754167181437, 0.8701497374190215, 0.9011681599961919], "final_y": [0.14920000103034092, 0.12864109271908553, 0.1148017232563564]}, "mutation_prompt": null}
{"id": "489bdf91-e36c-40a1-a18d-7f4a4c7793cd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        # Adaptive crossover probability based on evaluation progress\n        adaptive_cr = self.cr * (1 - 0.5 * (self.evals / self.budget))\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        # Dynamically adjust starting point based on evaluation progress\n        start_point = best * (0.985 + 0.015 * (self.evals / self.budget))\n        result = minimize(func, start_point, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by introducing dynamic adjustment to the L-BFGS-B method's starting point based on evaluation progress.", "configspace": "", "generation": 13, "fitness": 0.8560516353522031, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.031. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "8bf542bb-7466-4efc-a7bc-281e99b8a992", "metadata": {"aucs": [0.8127864082620662, 0.8821058314028711, 0.8732626663916716], "final_y": [0.14055339308486048, 0.11280545091611516, 0.12669506516679674]}, "mutation_prompt": null}
{"id": "b77254e4-fa4d-4777-acd1-9790e9f1d890", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points = np.random.choice([True, False], size=self.dim, p=[0.1, 0.9])  # Altered for diversity\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance modular structure detection by refining the crossover method and introducing a diversity-preserving mechanism.", "configspace": "", "generation": 14, "fitness": 0.8394301226438281, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.029. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8768966276775383, 0.8351347451575275, 0.8062589950964185], "final_y": [0.12556209328967505, 0.1262266566004272, 0.13251491195097664]}, "mutation_prompt": null}
{"id": "3ef1e475-7792-4bcc-bb8a-9ed28745df0a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Annealing-like decreasing mutation scaling\n        adaptive_f = self.f * (1 - self.evals / self.budget)\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        if (self.evals / self.budget) > 0.5:  # Refine only in later stages\n            result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n            return result.x\n        return best\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance solution diversity and convergence by introducing annealing-like decreasing mutation scaling and refining local optimization conditions.", "configspace": "", "generation": 14, "fitness": 0.8388649780646068, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.011. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "2a3050e2-53b1-45ef-86a9-cc41e0ad8787", "metadata": {"aucs": [0.8450401425345773, 0.8485963284376848, 0.8229584632215585], "final_y": [0.11937328175503503, 0.12102341091366176, 0.11982665273773219]}, "mutation_prompt": null}
{"id": "ba493aec-c65b-46e6-a087-f3c8350cd9c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub): \n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Improved adaptive mutation scaling\n        adaptive_f = self.f + (0.4 * (self.budget - self.evals) / self.budget) * (0.2 * np.random.rand() - 0.1)\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve mutation diversity by incorporating a time-varying scaling factor to better explore the search space.", "configspace": "", "generation": 14, "fitness": 0.8612310347846791, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.007. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "2a3050e2-53b1-45ef-86a9-cc41e0ad8787", "metadata": {"aucs": [0.8541405375312288, 0.8709164420310211, 0.8586361247917873], "final_y": [0.13302844719168205, 0.11196252452080824, 0.1143792604584527]}, "mutation_prompt": null}
{"id": "14614a34-d4d8-4321-8b7f-d783c17047f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim and best_score < np.min([func(ind) for ind in population]) * 0.98:  # Change 1: Add improvement threshold\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance refinement by adding a check to selectively trigger local refinement based on improvement threshold.", "configspace": "", "generation": 14, "fitness": 0.812105507761752, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.036. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.7629703114827673, 0.8259011944357856, 0.8474450173667032], "final_y": [0.15820057227514728, 0.12261516204406375, 0.11209144832042195]}, "mutation_prompt": null}
{"id": "a7f774d8-5e96-4b3e-b6b7-440f8f5c48ac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size  # Dynamic population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _adaptive_population_size(self):\n        # Adjust population size based on remaining budget\n        self.population_size = max(4, int(self.base_population_size * (1 - self.evals / self.budget))) \n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Multi-strategy mutation: adaptation based on evaluation progress\n        dynamic_f = self.f + 0.5 * (self.evals / self.budget)\n        if np.random.rand() < 0.5:\n            mutant = population[a] + dynamic_f * (population[b] - population[c])\n        else:\n            mutant = population[a] + dynamic_f * (population[b] - population[target_idx])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            self._adaptive_population_size()\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing adaptive mutation strategies based on evaluation progress to improve diversity.", "configspace": "", "generation": 14, "fitness": 0.8589685834859728, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.037. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "cc419866-e0e0-4e8d-8c74-bde26f8809dc", "metadata": {"aucs": [0.9074708884875898, 0.8167013399795752, 0.8527335219907534], "final_y": [0.11235559767502423, 0.13115045287732308, 0.11014909610643808]}, "mutation_prompt": null}
{"id": "73705250-f0ce-4fde-8075-1644da1c3d7f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        diversity = np.std(population, axis=0).mean()  # Calculate diversity\n        self.cr = 0.9 - 0.5 * diversity  # Adjust crossover rate based on diversity\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - (self.evals / self.budget))  \n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploration by dynamically adjusting the crossover rate based on population diversity.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'population' is not defined\").", "error": "NameError(\"name 'population' is not defined\")", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {}, "mutation_prompt": null}
{"id": "04dd7c68-e77f-4433-af38-bb7d6480f339", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub): \n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Improved adaptive mutation scaling\n        adaptive_f = self.f + (0.2 * np.random.rand() - 0.1)\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            # Introduced dynamic population size adaptation\n            self.population_size = int((10 + 5 * (self.evals / self.budget)) * self.dim)\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic population adaptation by varying population size based on evaluation progress to balance exploration and exploitation.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 103 is out of bounds for axis 0 with size 100').", "error": "IndexError('index 103 is out of bounds for axis 0 with size 100')", "parent_id": "2a3050e2-53b1-45ef-86a9-cc41e0ad8787", "metadata": {}, "mutation_prompt": null}
{"id": "eb6dc01e-fec9-42d4-872c-d09ccf865483", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * np.random.uniform(0.5, 1.5)  # Adaptive mutation scaling\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.995, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Robust local search\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce adaptive mutation scaling and robust local search to enhance exploration and convergence efficiency.", "configspace": "", "generation": 15, "fitness": 0.8777049678716807, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.019. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.897833797114126, 0.8830189833045221, 0.8522621231963939], "final_y": [0.11440893649987582, 0.12031510720309246, 0.12567582634562457]}, "mutation_prompt": null}
{"id": "3b2bbbf7-7768-4bbd-bcd8-4213d293c57b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * ((1 - (self.evals / self.budget)) ** 2)  # Enhanced adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 16, 24, 32]  # Adjusted layer growth steps\n        current_dim = layer_steps[0]\n        for next_dim in layer_steps:\n            if self.evals > (self.budget / len(layer_steps)) * (layer_steps.index(next_dim) + 1):\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement and adaptiveness by improving mutation strategy and dynamically adjusting layer growth.", "configspace": "", "generation": 15, "fitness": 0.8609024840331561, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.030. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.837282004197051, 0.8417387884041044, 0.9036866594983128], "final_y": [0.13073381807100226, 0.13779102314121705, 0.11196405694956657]}, "mutation_prompt": null}
{"id": "d2e780f6-628f-41e9-8309-447c39d0042e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        diversity = np.std(target) / (np.mean(target) + 1e-10)  # Dynamic diversity-based adjustment\n        dynamic_cr = self.cr * (1 - (self.evals / self.budget)) * (1 + diversity)\n        cross_points = np.random.rand(self.dim) < dynamic_cr  \n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by dynamically adjusting the crossover probability based on current solution diversity.", "configspace": "", "generation": 15, "fitness": 0.8900997231842046, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.009. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {"aucs": [0.9015066109828556, 0.8808187034448257, 0.8879738551249322], "final_y": [0.11059855770050864, 0.12303480586300897, 0.1193646954578742]}, "mutation_prompt": null}
{"id": "b4c24bef-8b4b-4b3f-8468-03af04d6a574", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub): \n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Improved adaptive mutation scaling\n        guided_walk = np.random.rand(self.dim) * (population[a] - population[c])\n        adaptive_f = self.f + (0.2 * np.random.rand() - 0.1)\n        mutant = population[a] + adaptive_f * (population[b] - population[c]) + guided_walk\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = layer_steps[0]\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        current_dim = min(current_dim + (self.evals // (self.budget // len(layer_steps))), self.dim)\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing guided random walks and optimizing layer transition through improved adaptive layer growth strategy.", "configspace": "", "generation": 16, "fitness": 0.8553016849655591, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.020. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "2a3050e2-53b1-45ef-86a9-cc41e0ad8787", "metadata": {"aucs": [0.8342589541226114, 0.8500678901134822, 0.8815782106605836], "final_y": [0.14008202141195802, 0.13475703055565746, 0.1193688263572582]}, "mutation_prompt": null}
{"id": "2eb213d8-6822-4cdc-b6bf-d4af861c8d20", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        diversity = np.std(population, axis=0).mean() / (np.mean(population) + 1e-10)  # Added diversity\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget)) * (1 + diversity)  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        diversity = np.std(target) / (np.mean(target) + 1e-10)  # Dynamic diversity-based adjustment\n        dynamic_cr = self.cr * (1 - (self.evals / self.budget)) * (1 + diversity)\n        cross_points = np.random.rand(self.dim) < dynamic_cr  \n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce variability in mutation factor based on both progress and diversity for enhanced exploration.", "configspace": "", "generation": 16, "fitness": 0.8208208834383742, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.018. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "d2e780f6-628f-41e9-8309-447c39d0042e", "metadata": {"aucs": [0.8168803118949921, 0.845083244509953, 0.8004990939101776], "final_y": [0.14175228756152625, 0.13754828029467392, 0.15304976258891323]}, "mutation_prompt": null}
{"id": "4bf78b02-4e22-4706-a4a5-5787f5ff0c40", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        half_pop = self.population_size // 2\n        random_half = np.random.uniform(lb, ub, (half_pop, self.dim))\n        grid_half = np.linspace(lb, ub, half_pop).reshape(half_pop, self.dim)\n        return np.vstack((random_half, grid_half))  # Hybrid initialization\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        adaptive_cr = self.cr * (1 - np.abs(np.mean(target - mutant)))  # Adaptive crossover rate\n        cross_points = np.random.rand(self.dim) < adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance optimization by introducing an adaptive crossover rate and a hybrid population initialization scheme.", "configspace": "", "generation": 16, "fitness": 0.8671449138139948, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.027. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8874612421075986, 0.8291471960736824, 0.8848263032607033], "final_y": [0.11235082553657127, 0.14034281815059502, 0.11880018347940713]}, "mutation_prompt": null}
{"id": "9da22b46-af3a-4029-a331-6dd8b84287bc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.98, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance layer role preservation by refining the local refinement step's starting point to emphasize modular structure recognition.", "configspace": "", "generation": 16, "fitness": 0.8661266192019162, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.023. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8981587527529491, 0.8572809150012268, 0.8429401898515729], "final_y": [0.11253004963940172, 0.13155945078030162, 0.13301012467615414]}, "mutation_prompt": null}
{"id": "e05bed75-ef09-4a05-ac35-a4f5fa97f777", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 8 * dim  # Reduced adaptive population size\n        self.cr = 0.9  \n        self.f = 0.8  \n        self.evals = 0  \n        self.elite_size = max(1, int(0.1 * self.population_size))  \n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        layer_specific_f = self.f * (1 + 0.1 * (target_idx % self.dim))  # Layer-specific mutation factor\n        mutant = population[a] + layer_specific_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - (self.evals / self.budget))\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve exploitation by introducing adaptive population size and layer-specific mutation.", "configspace": "", "generation": 16, "fitness": 0.828281172299871, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.828 with standard deviation 0.030. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {"aucs": [0.8420117778261978, 0.8565532082268488, 0.7862785308465665], "final_y": [0.1339687558002396, 0.13051743952332218, 0.15443038320135483]}, "mutation_prompt": null}
{"id": "f216c558-68e2-4c34-913b-d95cd6d4f3e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _adaptive_population_size(self):\n        self.population_size = max(4, int(self.base_population_size * (1 - self.evals / self.budget))) \n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Adaptive differential weight\n        adaptive_f = self.f * (1 + 0.5 * np.random.randn())\n        if np.random.rand() < 0.5:\n            mutant = population[a] + adaptive_f * (population[b] - population[c])\n        else:\n            mutant = population[a] + adaptive_f * (population[b] - population[target_idx])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        bounds_tuning = [min(5, dim) for dim in zip(func.bounds.lb, func.bounds.ub)]  # Layer-specific tuning\n        result = minimize(func, best, bounds=bounds_tuning, method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            self._adaptive_population_size()\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration and exploitation balance by introducing adaptive differential weight and layer-specific tuning during local refinement.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: TypeError(\"'<' not supported between instances of 'tuple' and 'int'\").", "error": "TypeError(\"'<' not supported between instances of 'tuple' and 'int'\")", "parent_id": "cc419866-e0e0-4e8d-8c74-bde26f8809dc", "metadata": {}, "mutation_prompt": null}
{"id": "2bb4c00c-3791-4a70-ae4e-f42bbabd7b28", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - (self.evals / self.budget))  \n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:int(self.population_size * (1 + 0.1 * (self.evals/self.budget)))]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by introducing a dynamic population size scaling mechanism based on evaluation count.", "configspace": "", "generation": 17, "fitness": 0.8272043318973502, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.023. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {"aucs": [0.8520908287865833, 0.7970984561490779, 0.8324237107563899], "final_y": [0.13155961682752138, 0.15380347755454915, 0.13823836471850992]}, "mutation_prompt": null}
{"id": "d11dac77-9bf5-4e72-82b8-bf53076496e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Modified line for adaptive differential weight scaling\n        adaptive_f = self.f * (1 - (self.evals / self.budget)) * (1 + np.random.rand() * 0.5)  # Increased diversity\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Optimize layer role preservation by enhancing mutation diversity with an adaptive differential weight scaling based on solution quality.", "configspace": "", "generation": 17, "fitness": 0.8153655625240769, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.058. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8942360191511368, 0.7972815974977732, 0.7545790709233207], "final_y": [0.11747237161213742, 0.15380347755454915, 0.16284750094768063]}, "mutation_prompt": null}
{"id": "5c4d5367-369c-42d9-9925-a88937d1a793", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size  # Dynamic population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _adaptive_population_size(self):\n        # Adjust population size based on remaining budget\n        self.population_size = max(4, int(self.base_population_size * (1 - self.evals / self.budget))) \n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        diversity = np.std(population, axis=0).mean()  # Calculate population diversity\n        f_dynamic = self.f * (1 + diversity)  # Dynamic scaling of differential weight\n        if np.random.rand() < 0.5:\n            mutant = population[a] + f_dynamic * (population[b] - population[c])\n        else:\n            mutant = population[a] + f_dynamic * (population[b] - population[target_idx])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            self._adaptive_population_size()\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic mutation factor scaling based on population diversity to enhance exploration.", "configspace": "", "generation": 17, "fitness": 0.8179352568026655, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.040. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "cc419866-e0e0-4e8d-8c74-bde26f8809dc", "metadata": {"aucs": [0.8608404501958515, 0.7651971962534896, 0.8277681239586553], "final_y": [0.12253601308179973, 0.15310503457296032, 0.13737012640785118]}, "mutation_prompt": null}
{"id": "44b323bc-aa7a-4464-bc5a-61f23a1ab355", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Initial crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub): \n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Dynamic differential weight scaling\n        adaptive_f = self.f + (0.2 * np.random.rand() - 0.1) * (1 - self.evals / self.budget)\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        # Adjust stopping criteria for better efficiency\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), options={'gtol': 1e-5}, method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(population, scores)\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce dynamic differential weight scaling and enhance local refinement efficiency by adjusting stopping criteria.", "configspace": "", "generation": 17, "fitness": 0.8570351354398413, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.059. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "2a3050e2-53b1-45ef-86a9-cc41e0ad8787", "metadata": {"aucs": [0.9337796822897395, 0.7901297015561137, 0.847196022473671], "final_y": [0.11148942393824435, 0.15552323159439052, 0.13160720358422184]}, "mutation_prompt": null}
{"id": "4d5c14b4-aa96-4e88-9d3a-db4c1bb75cf6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        adaptive_cr = self.cr * (0.5 + 0.5 * (1 - self.evals / self.budget))  # Adaptive crossover probability\n        cross_points = np.random.rand(self.dim) < adaptive_cr  # Updated to use adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Integrate adaptive crossover probability to further enhance convergence and exploration balance.", "configspace": "", "generation": 18, "fitness": 0.8922527385974117, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.022. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {"aucs": [0.8610673159578657, 0.907428428979381, 0.9082624708549885], "final_y": [0.12459944084881402, 0.11197743378992764, 0.11149933922055255]}, "mutation_prompt": null}
{"id": "a87094bf-748b-47f9-bd9c-a93038cdbb88", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        diversity = np.std(target) / (np.mean(target) + 1e-10)  # Dynamic diversity-based adjustment\n        dynamic_cr = self.cr * (1 - (self.evals / self.budget)) * (1 + diversity)\n        cross_points = np.random.rand(self.dim) < dynamic_cr  \n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            # Adjust elite size dynamically\n            self.elite_size = max(1, int((0.1 + 0.1 * (self.evals / self.budget)) * self.population_size))\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve convergence by incorporating a gradual increase in the elitism proportion as evaluations progress.", "configspace": "", "generation": 18, "fitness": 0.8610675819699024, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.023. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "d2e780f6-628f-41e9-8309-447c39d0042e", "metadata": {"aucs": [0.8487235155965588, 0.8405954851815264, 0.8938837451316223], "final_y": [0.1327501514667373, 0.13799206171540568, 0.11562245452390563]}, "mutation_prompt": null}
{"id": "d90174ac-c988-4a19-9aef-141affdc3200", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.base_population_size = 10 * dim\n        self.population_size = self.base_population_size\n        self.cr = 0.9\n        self.f = 0.5\n        self.evals = 0\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _adaptive_population_size(self):\n        diversity = np.std(self.population, axis=0).mean()\n        self.population_size = max(4, int(self.base_population_size * (1 - self.evals / self.budget * diversity)))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f + 0.3 * np.random.rand()\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        dynamic_cr = self.cr - 0.5 * (self.evals / self.budget)\n        cross_points = np.random.rand(self.dim) < dynamic_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _tournament_selection(self, population, scores, k=3):\n        selected_idx = np.random.choice(self.population_size, k, replace=False)\n        best_idx = selected_idx[np.argmin(scores[selected_idx])]\n        return population[best_idx]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n        scores = np.full(self.population_size, np.inf)\n\n        while self.evals < self.budget:\n            self._adaptive_population_size()\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = self._tournament_selection(self.population, scores)\n                mutant = self._mutation(i, self.population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                scores[i] = score\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    self.population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce self-adaptive differential weight and population size scaling based on diversity to balance exploration and convergence.", "configspace": "", "generation": 18, "fitness": 0.8931525394446181, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.893 with standard deviation 0.009. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "cc419866-e0e0-4e8d-8c74-bde26f8809dc", "metadata": {"aucs": [0.8822746785029376, 0.8932796171923807, 0.9039033226385361], "final_y": [0.12567662206746022, 0.1176598839234344, 0.11162692147948883]}, "mutation_prompt": null}
{"id": "58a8c303-5fd4-4469-9cad-7feee5ece6dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - 0.3 * (self.evals / self.budget))  # Modified time-varying crossover\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:max(self.elite_size, 2)]  # Adjusted elitism to preserve more diversity\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a time-varying crossover probability and adjust the elitism strategy to improve exploration and convergence.", "configspace": "", "generation": 18, "fitness": 0.8837405900574872, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.884 with standard deviation 0.005. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {"aucs": [0.8900600167477936, 0.882884206813861, 0.878277546610807], "final_y": [0.114656840926471, 0.12132509625965393, 0.11430063886489628]}, "mutation_prompt": null}
{"id": "6980b22e-68f3-4546-82d8-2bb7e22ee224", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr * (1 - (self.evals / self.budget))  \n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def _should_restart(self, scores):\n        unique_scores = len(set(scores))\n        return unique_scores < self.population_size * 0.1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n            \n            if self._should_restart(scores):\n                population = self._initialize_population(lb, ub)\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Improve convergence by incorporating a restart mechanism based on solution diversity to escape local optima.", "configspace": "", "generation": 18, "fitness": 0.8561324660151035, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.011. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "15df7dc7-4d38-443d-b8de-8606e5f0269b", "metadata": {"aucs": [0.8593732819837716, 0.8414141815673715, 0.8676099344941675], "final_y": [0.1251515544828442, 0.13902891929827732, 0.12435920453084559]}, "mutation_prompt": null}
{"id": "4cd3db25-f82e-483c-bc1b-8e0f1887d6eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - 0.5 * (self.evals / self.budget))  # Updated self-adjusting factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        adaptive_cr = self.cr * (0.5 + 0.5 * (1 - self.evals / self.budget))  # Adaptive crossover probability\n        cross_points = np.random.rand(self.dim) < adaptive_cr  # Updated to use adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                best_solution = 0.7 * best_solution + 0.3 * refined_solution  # Weighted combination\n                refined_score = func(best_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance refinement step by using a weighted combination of the best solution and its refined version to improve robustness.", "configspace": "", "generation": 19, "fitness": 0.8655865618858468, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.016. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "4d5c14b4-aa96-4e88-9d3a-db4c1bb75cf6", "metadata": {"aucs": [0.8458127005725664, 0.8664095608837157, 0.8845374242012585], "final_y": [0.1191518915073354, 0.12878415700916468, 0.11839122781158662]}, "mutation_prompt": null}
{"id": "0a6063ea-690d-42b9-9d3f-4be3f40f50fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        random_start = np.random.uniform(func.bounds.lb, func.bounds.ub)  # Randomized starting point\n        result = minimize(func, random_start, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance exploration by randomizing the starting point of local refinement to better escape local optima.", "configspace": "", "generation": 19, "fitness": 0.8695136367278812, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.015. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8907971934131713, 0.8625534013392029, 0.8551903154312694], "final_y": [0.1101227595269273, 0.12669397807773397, 0.13073076869964173]}, "mutation_prompt": null}
{"id": "7ab20ed0-4bd2-49b6-8147-32c8a0ca4871", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.8  # Modified differential weight\n        self.evals = 0  # Evaluation count\n        self.elite_size = max(1, int(0.1 * self.population_size))  # Elitism\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        diversity_factor = np.std(population, axis=0).mean() / (ub - lb).mean()\n        adaptive_f = self.f * (0.5 + 0.5 * (1 - diversity_factor))  # Updated self-adjusting factor based on diversity\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        adaptive_cr = self.cr * (0.5 + 0.5 * (1 - self.evals / self.budget))  # Adaptive crossover probability\n        cross_points = np.random.rand(self.dim) < adaptive_cr  # Updated to use adaptive_cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.985, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            scores = [func(ind) for ind in population]\n            self.evals += len(population)\n            elite_indices = np.argsort(scores)[:self.elite_size]\n            elite_population = population[elite_indices]\n\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < scores[i]:\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            population = np.vstack((population, elite_population))[:self.population_size]\n\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance convergence by dynamically adjusting the differential weight based on the current solution diversity.", "configspace": "", "generation": 19, "fitness": 0.881365906234089, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.014. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "4d5c14b4-aa96-4e88-9d3a-db4c1bb75cf6", "metadata": {"aucs": [0.8619827631967167, 0.8858728803825342, 0.8962420751230161], "final_y": [0.1254216817207886, 0.12004771191740937, 0.11603527736665031]}, "mutation_prompt": null}
{"id": "18e63346-3722-43b7-9e52-527bf8a8093a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        result = minimize(func, best * 0.99, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')  # Slightly altered starting point\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement with probabilistic initiation\n            if current_dim == self.dim and np.random.rand() < 0.5:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Introduce a probability-based selection mechanism for local refinement initiation to balance exploration and exploitation.", "configspace": "", "generation": 19, "fitness": 0.8631155133034771, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.019. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8359156020024819, 0.8735863311977656, 0.8798446067101837], "final_y": [0.11403713637625734, 0.125663360173066, 0.11937000782664775]}, "mutation_prompt": null}
{"id": "97649cd4-114c-459f-92d8-027695bdac41", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Adaptive population size\n        self.cr = 0.9  # Crossover probability\n        self.f = 0.5  # Differential weight\n        self.evals = 0  # Evaluation count\n\n    def _initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _mutation(self, target_idx, population, lb, ub):  # Added lb, ub\n        indices = list(range(self.population_size))\n        indices.remove(target_idx)\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        adaptive_f = self.f * (1 - (self.evals / self.budget))  # Adaptive mutation factor\n        mutant = population[a] + adaptive_f * (population[b] - population[c])\n        return np.clip(mutant, lb, ub)\n\n    def _crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def _local_refinement(self, best, func):\n        refine_factor = 0.97 + 0.02 * (self.evals / self.budget)  # Adjusted starting point\n        result = minimize(func, best * refine_factor, bounds=zip(func.bounds.lb, func.bounds.ub), method='L-BFGS-B')\n        return result.x\n\n    def _adaptive_layer_growth(self, func):\n        # Reducing dimensionality by growing layers adaptively\n        layer_steps = [10, 20, 32]\n        current_dim = min(self.dim, layer_steps[0])\n        for next_dim in layer_steps:\n            if self.evals > self.budget * next_dim / self.dim:\n                current_dim = next_dim\n        return current_dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population_size = max(2, int(10 * self.dim * (1 - self.evals / self.budget)))  # Dynamic population size\n        population = self._initialize_population(lb, ub)\n        best_solution = None\n        best_score = float('inf')\n\n        while self.evals < self.budget:\n            current_dim = self._adaptive_layer_growth(func)\n            for i in range(self.population_size):\n                target = population[i]\n                mutant = self._mutation(i, population, lb, ub)  # Added lb, ub\n                trial = self._crossover(target, mutant)\n                score = func(trial)\n                self.evals += 1\n                if score < best_score:\n                    best_score = score\n                    best_solution = trial\n\n                if score < func(target):\n                    population[i] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local refinement\n            if current_dim == self.dim:\n                refined_solution = self._local_refinement(best_solution, func)\n                refined_score = func(refined_solution)\n                self.evals += 1\n                if refined_score < best_score:\n                    best_score = refined_score\n                    best_solution = refined_solution\n\n        return best_solution", "name": "HybridMetaheuristic", "description": "Enhance local refinement by adjusting the starting point more precisely using a dynamic factor.", "configspace": "", "generation": 19, "fitness": 0.8613219016728273, "feedback": "The algorithm HybridMetaheuristic got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.035. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "df519b92-ed59-4408-a5af-84f24cc3dfe8", "metadata": {"aucs": [0.8261963771063465, 0.8492431858836558, 0.9085261420284796], "final_y": [0.14483145739652936, 0.1343548375024446, 0.11196306469103956]}, "mutation_prompt": null}
