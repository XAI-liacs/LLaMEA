{"id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution for global search and Covariance Matrix Adaptation Evolution Strategy for local refinement, with adaptive layer complexity and robustness-focused cost adjustments.", "configspace": "", "generation": 0, "fitness": 0.7981341535255518, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.014. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": null, "metadata": {"aucs": [0.7777309213638787, 0.8094998819938464, 0.80717165721893], "final_y": [0.15163348466168347, 0.14233310905584073, 0.14380558686885603]}, "mutation_prompt": null}
{"id": "c8df0c81-c3ef-4359-8a9f-b8c8ceef5397", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'ftol': 1e-6})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 12  # Adjusted to improve convergence\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved hybrid metaheuristic with strategic layer iteration and adaptive local refinement for enhanced photovoltaic structure optimization.", "configspace": "", "generation": 1, "fitness": 0.8015129507503561, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.013. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.7889143202882197, 0.7964900895380488, 0.8191344424248002], "final_y": [0.15276457190049098, 0.15195016731899735, 0.14333941081021206]}, "mutation_prompt": null}
{"id": "b0bfac22-3d58-4683-af60-d1f6642a8284", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.9, CR=0.85):  # Adjusted F and CR\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Hybrid Metaheuristic Optimizer by tuning parameters for improved exploration and exploitation balance in Differential Evolution.", "configspace": "", "generation": 1, "fitness": 0.7951022394582087, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.795 with standard deviation 0.014. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.7818791521117369, 0.8146525989876197, 0.7887749672752693], "final_y": [0.14260087811294286, 0.13455395987237573, 0.1383656265323744]}, "mutation_prompt": null}
{"id": "d68354e9-602f-4db0-a518-e733cc65305d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            adaptive_F = F * (1 - self.evaluations / self.budget)  # Change 1\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)  # Change 2\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        options = {'maxiter': 100, 'ftol': 1e-5}  # Change 3\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options=options)\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic with adaptive mutation rate and improved local refinement strategy for robust optimization.", "configspace": "", "generation": 1, "fitness": 0.8122507932033655, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.010. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.8252323720487486, 0.8095305063087616, 0.8019895012525865], "final_y": [0.13570607668630708, 0.1490614004896933, 0.13839630821131033]}, "mutation_prompt": null}
{"id": "e583d3a7-367e-4efb-9c9b-206d46d2e564", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n\n        # Introduce a diversity measure in the decision process\n        diversity_threshold = np.std(global_solution) * 0.05\n        if local_cost < global_cost and np.std(local_solution) > diversity_threshold:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Optimized HybridMetaheuristicOptimizer by enhancing the selection process and integrating a diversity measure to maintain a robust search space exploration.", "configspace": "", "generation": 1, "fitness": 0.8080735158621732, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.008. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.8160988170751439, 0.8116766501746454, 0.7964450803367303], "final_y": [0.1402211039565121, 0.1450309511288661, 0.13986214604615022]}, "mutation_prompt": null}
{"id": "bf55821b-f29f-4b7e-936b-1b4679178565", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced hybrid metaheuristic algorithm incorporating adaptive population size and improved local refinement to optimize high-dimensional problems efficiently.", "configspace": "", "generation": 1, "fitness": 0.8210739706269568, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.018. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "c048b504-97bb-48c0-ac94-73f1a5b63e9d", "metadata": {"aucs": [0.8160089348132187, 0.8456102722856128, 0.8016027047820387], "final_y": [0.13757969112975543, 0.1243051766495391, 0.1449348999230704]}, "mutation_prompt": null}
{"id": "752b7ce7-6fea-43fc-bc0a-50124bf1365c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Improved mutation strategy\n                mutant = np.clip(a + F * (b - c) + 0.5 * (np.mean(population, axis=0) - population[i]), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer with improved mutation strategy for better global exploration.", "configspace": "", "generation": 2, "fitness": 0.812545042878719, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.009. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8110823915655064, 0.8027880442874331, 0.8237646927832174], "final_y": [0.14847728937049576, 0.1518714731744547, 0.1443017440476132]}, "mutation_prompt": null}
{"id": "83fd4b13-f996-477a-9008-19db4327e8b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved local refinement by switching to L-BFGS-B method for better handling of noisy functions.", "configspace": "", "generation": 2, "fitness": 0.8128259626122333, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.010. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8103611456711237, 0.8263598349258952, 0.8017569072396808], "final_y": [0.13272941013024941, 0.13652185687231133, 0.14340965510832182]}, "mutation_prompt": null}
{"id": "49592da2-5bea-4a9b-9d4c-83416463d8dd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                if np.random.rand() < 0.5:  # Adaptive mutation strategy\n                    F_adapt = 0.7 + 0.3 * np.random.rand()  \n                else:\n                    F_adapt = F\n                mutant = np.clip(a + F_adapt * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid algorithm with adaptive mutation strategies for refined exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.8107650476265751, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.018. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8349541884034922, 0.8054298848987312, 0.7919110695775018], "final_y": [0.1288646091576281, 0.14484987277252936, 0.1395600364660856]}, "mutation_prompt": null}
{"id": "428e5012-ac14-48c5-be6b-15825ec9ad1e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def adaptive_population_size(self, pop_size, complexity_factor):\n        return max(4, int(pop_size * complexity_factor))  # Adaptive population size\n    \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        adaptive_factor = 1 + (np.log(self.dim) / 10)  # New complexity factor\n        pop_size = self.adaptive_population_size(pop_size, adaptive_factor) # Adjusted population\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='trust-constr')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced metaheuristic algorithm employing adaptive layer complexity and refined local search integration to optimize high-dimensional black-box problems effectively.", "configspace": "", "generation": 2, "fitness": 0.7980950394605335, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.798 with standard deviation 0.010. And the mean value of best solutions found was 0.149 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8092042492492153, 0.8006673824003807, 0.7844134867320047], "final_y": [0.13822180500714742, 0.1524829238280343, 0.15655648155159363]}, "mutation_prompt": null}
{"id": "dea2c825-d208-48cd-8762-c82a9162fda3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_adapt = F * (1 - self.evaluations / self.budget)  # Dynamic mutation adaptation\n                mutant = np.clip(a + F_adapt * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')  # Refined local search method\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "A refined hybrid metaheuristic algorithm introducing dynamic mutation adaptation and refined local search strategies to enhance solution quality and convergence speed.", "configspace": "", "generation": 2, "fitness": 0.8084628519487626, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.005. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "bf55821b-f29f-4b7e-936b-1b4679178565", "metadata": {"aucs": [0.8102520510165403, 0.8010242076522477, 0.8141122971774997], "final_y": [0.1292794157435012, 0.14717276738236396, 0.1472213232117362]}, "mutation_prompt": null}
{"id": "b37b1f48-6210-4dbc-8e0c-1a52e3e1e978", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if _ % 10 == 0:  # Dynamic adjustment of population size\n                pop_size = max(4, min(pop_size + 1, 100))\n\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        method = 'TNC' if self.evaluations < self.budget * 0.5 else 'L-BFGS-B'  # Adaptive local search method\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method=method)\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration and exploitation balance using adaptive population size in DE and hybrid local search techniques.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {}, "mutation_prompt": null}
{"id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced differential evolution step by introducing adaptive mutation factor for better exploration.", "configspace": "", "generation": 3, "fitness": 0.8325461120387194, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.008. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {"aucs": [0.8323653370921871, 0.8421632718180765, 0.8231097272058947], "final_y": [0.13468684230909567, 0.1356881254854615, 0.14294488534002814]}, "mutation_prompt": null}
{"id": "3d4db83a-bee7-46de-b913-8e926da1e99b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.95):  # Increased CR to 0.95\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by increasing the crossover rate in differential evolution to improve search diversity.", "configspace": "", "generation": 3, "fitness": 0.8264649567229303, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.015. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {"aucs": [0.8466138294668618, 0.8118446544892252, 0.8209363862127039], "final_y": [0.12842055785334883, 0.14399158281510138, 0.1388574413528797]}, "mutation_prompt": null}
{"id": "fb7d6448-b7f8-4873-9cd4-34d8ea4de4b9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def adaptive_differential_evolution(self, func, bounds, iters, pop_size=50, F=0.5, CR=0.5):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = np.random.uniform(0.5, 1.0)  # Adaptive mutation factor\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        # Noise-resilient sampling strategy\n        perturbed_solutions = [x0 + np.random.normal(0, 1e-2, self.dim) for _ in range(5)]\n        perturbed_fitness = [func(sol) for sol in perturbed_solutions]\n        best_perturbed = perturbed_solutions[np.argmin(perturbed_fitness)]\n        result = minimize(func, best_perturbed, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.adaptive_differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive differential evolution to enhance exploration and integrate noise-resilient sampling in local refinement.", "configspace": "", "generation": 3, "fitness": 0.8051686339007732, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.007. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {"aucs": [0.8002613857415917, 0.7996277448737187, 0.8156167710870095], "final_y": [0.13541169531027342, 0.13942778978111914, 0.14823272972656798]}, "mutation_prompt": null}
{"id": "31241c61-0728-43b6-87f8-0ab393745c4b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size)  # Dynamic population size for better diversity\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration with a dynamic population size in differential evolution for improved solution diversity.", "configspace": "", "generation": 3, "fitness": 0.7985823901441148, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.799 with standard deviation 0.046. And the mean value of best solutions found was 0.146 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "83fd4b13-f996-477a-9008-19db4327e8b9", "metadata": {"aucs": [0.7381688168419774, 0.8495298980318479, 0.808048455558519], "final_y": [0.1603270993644239, 0.13726213638126372, 0.14180046096775345]}, "mutation_prompt": null}
{"id": "273fdc2a-f3b8-4b3a-9106-ed5848f925b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.9 - (0.4 * self.evaluations / self.budget)  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Utilize adaptive crossover rate in differential evolution for enhanced exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.820262888177323, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.006. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8270562416459603, 0.8118965115140968, 0.8218359113719118], "final_y": [0.1354847251388197, 0.14291737801239457, 0.13461577557911364]}, "mutation_prompt": null}
{"id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing differential evolution by adjusting crossover rate adaptively for improved exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8434009885546957, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.021. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8561558812616334, 0.813798918277471, 0.8602481661249826], "final_y": [0.126517292216888, 0.14275315985304304, 0.127082862617104]}, "mutation_prompt": null}
{"id": "a0b10618-3c63-4026-a2f1-868c1ac23eb2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        # Adaptive crossover probability\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            CR = 0.5 + 0.4 * (1 - self.evaluations / self.budget)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n\n        # Layer-wise complexity scaling\n        layer_increment = max(1, self.dim // 4)\n        current_dim = min(self.dim, layer_increment)\n        global_solution, global_cost = None, float('inf')\n\n        while current_dim <= self.dim and self.evaluations < self.budget:\n            global_solution, global_cost = self.differential_evolution(\n                func, bounds, iters=iterations, pop_size=50\n            )\n            current_dim += layer_increment\n\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid exploration-exploitation using adaptive crossover and layer-wise complexity scaling for efficient optimization.", "configspace": "", "generation": 4, "fitness": 0.8329674352940938, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.023. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8035721607474061, 0.8586819703779538, 0.8366481747569218], "final_y": [0.1350729083701281, 0.12850945940264924, 0.12794697613217076]}, "mutation_prompt": null}
{"id": "00b2cdd4-5de5-4038-b3c4-587ce54f99c9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.5 + 0.4 * (1 - (self.evaluations / self.budget))  # Dynamic crossover probability\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved mutation strategy by incorporating dynamic crossover probability adjustment to enhance exploration-exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.837419588318513, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.012. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8545142309972302, 0.8264599324783042, 0.8312846014800046], "final_y": [0.12722180844581166, 0.1306433570570903, 0.13069717960759408]}, "mutation_prompt": null}
{"id": "c95ab682-aed1-47a8-a330-0c8cbfcb7eff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n\n            CR = 0.7 + 0.3 * np.sin(np.pi * self.evaluations / self.budget)  # Dynamic crossover rate\n\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduced dynamic crossover rate adaptation to enhance exploration and exploitation balance in the differential evolution step.", "configspace": "", "generation": 4, "fitness": 0.8331076487811684, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.833 with standard deviation 0.006. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "54655dca-4b93-4bb1-8c2b-0a3281a8eb80", "metadata": {"aucs": [0.8397242573288668, 0.824282963724881, 0.8353157252897576], "final_y": [0.12304681305484022, 0.12292762652019518, 0.13657298976199927]}, "mutation_prompt": null}
{"id": "0a95fbf4-cf29-49a0-b846-e86a60b53613", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i] or np.std(population) < 1e-5:  # Diversity-based selection\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing differential evolution by incorporating diversity-based selection for maintaining higher population diversity.", "configspace": "", "generation": 5, "fitness": 0.8302977548490044, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.006. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.8359788190685546, 0.8322610942091865, 0.8226533512692717], "final_y": [0.13415486317841585, 0.1273858825992228, 0.1312871688821392]}, "mutation_prompt": null}
{"id": "b1d4806a-ffef-4fd3-bd06-c83797aad829", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        # Dynamically adjust pop_size based on progress\n        pop_size = max(4, int(pop_size * (1 - 0.5 * self.evaluations / self.budget)))\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introducing a dynamic population size strategy to enhance the balance between exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.824656044652491, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.825 with standard deviation 0.024. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.7947605240702975, 0.8252359172301436, 0.8539716926570319], "final_y": [0.14697403935359032, 0.12902213713398958, 0.13271852438033727]}, "mutation_prompt": null}
{"id": "0e70ad9b-39e3-4162-b90b-5d56362d5be7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size)  # Reverted to original pop_size for exploration boost\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)  # Adaptive mutation factor\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='TNC')  # Changed method for refinement\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Slightly boosting exploration and local refinement by adjusting population size and refinement method.", "configspace": "", "generation": 5, "fitness": 0.8195227407823497, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.008. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.8087642607453658, 0.8225419055700492, 0.8272620560316336], "final_y": [0.14039346706389044, 0.12986426714879906, 0.12909248255714956]}, "mutation_prompt": null}
{"id": "f5f0f892-52bd-4345-9cc6-75b3fd86b739", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)  # Reduce initial pop_size for faster convergence\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * np.sin(np.pi * (self.evaluations / self.budget)))  # Dynamic mutation factor using sine function\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))  # Adaptive crossover rate\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n        return population[np.argmin(fitness)], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        # Final selection based on cost\n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing differential evolution by dynamically adjusting both crossover and mutation rates for better balance between exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.8175031957723377, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.007. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.8114587158449572, 0.8268082472621467, 0.8142426242099086], "final_y": [0.13538392006845756, 0.12763803509734006, 0.14254149570203312]}, "mutation_prompt": null}
{"id": "1d674843-6145-4abd-9d48-497b45a82ace", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improving differential evolution by dynamically resizing the population size and introducing elitism for enhanced performance.", "configspace": "", "generation": 5, "fitness": 0.8366121957772811, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.019. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "be9a4576-c9a0-4614-8f52-909e2b13ac0f", "metadata": {"aucs": [0.8099082870335662, 0.8518446335447369, 0.84808366675354], "final_y": [0.14249357120078643, 0.12836525878917748, 0.13190941565019154]}, "mutation_prompt": null}
{"id": "e65a89d8-42b0-4bd9-a481-6a59774b3aea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            diversity = np.std(population, axis=0).mean()\n            F = 0.5 + (0.5 * diversity)  # Adaptive F based on diversity\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive F and CR parameters based on population diversity to enhance exploration and exploitation in differential evolution.", "configspace": "", "generation": 6, "fitness": 0.7876293346856821, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.788 with standard deviation 0.025. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.7560456644119335, 0.791022408907229, 0.8158199307378833], "final_y": [0.16018484581985615, 0.1498419562689144, 0.14268465778006434]}, "mutation_prompt": null}
{"id": "2d309796-1af5-475e-9615-60a3fa02db47", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)\n            CR = np.var(fitness) / (np.max(fitness) - np.min(fitness) + 1e-9)  # Change: Adapt CR based on fitness variance\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced differential evolution by adapting crossover rates based on fitness variance for improved exploration and exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.8105972825251859, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.023. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.7802863479462293, 0.8348654528748015, 0.8166400467545271], "final_y": [0.15307554766331544, 0.13114842862047282, 0.13070171541250641]}, "mutation_prompt": null}
{"id": "f497b7ab-25af-44d0-938c-cdcb2336fd7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * (1 - fitness.min() / fitness.max()))  # Adaptive scaling based on fitness spread\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Incorporating adaptive mutation scaling and dynamic crossover probability adjustments in differential evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.8014050083677504, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.008. And the mean value of best solutions found was 0.145 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.7935809818621887, 0.7977138764125951, 0.8129201668284676], "final_y": [0.14724933159932052, 0.14701943488023272, 0.1395262796133322]}, "mutation_prompt": null}
{"id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing differential evolution by introducing adaptive control parameters and refinement through local search for robust optimization.", "configspace": "", "generation": 6, "fitness": 0.8227278770045791, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.823 with standard deviation 0.010. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.8096256672348985, 0.8338941065181726, 0.8246638572606662], "final_y": [0.13338672105048077, 0.1392955080883569, 0.1289146581568651]}, "mutation_prompt": null}
{"id": "c78c39a5-5bc7-4401-bfd0-d85ef53626fb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * self.evaluations / self.budget)\n            CR = 0.8 - (0.3 * (self.evaluations / self.budget))\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = np.random.uniform(0.5, 1.0)  # Adaptive mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance differential evolution by introducing adaptive mutation factors and elitism for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.8157507547448223, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.006. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "1d674843-6145-4abd-9d48-497b45a82ace", "metadata": {"aucs": [0.8246396039602598, 0.8132244944379107, 0.8093881658362967], "final_y": [0.13531674415009642, 0.1441295928613603, 0.14081212344482807]}, "mutation_prompt": null}
{"id": "a6005ff7-4730-47b5-bede-46c1d20dd2ca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size += 2  # Changed to increase by 2 instead of 1\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improving differential evolution by introducing adaptive population size and elitism for enhanced convergence.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {}, "mutation_prompt": null}
{"id": "bbc5ea5a-23d4-4a08-b9b7-b694e97457eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # New line: Perform local search on the best individual every few generations\n            if generation % 5 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Incorporating memetic enhancements into the differential evolution phase by introducing a local search every few iterations for faster convergence and better solution quality.", "configspace": "", "generation": 7, "fitness": 0.9036893301995027, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.029. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {"aucs": [0.8630597554791912, 0.917571719993835, 0.9304365151254814], "final_y": [0.13039511519522407, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "768061c7-f361-4525-9aaf-11176de38a5a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        stagnation_counter = 0  # Counter for stagnation detection\n        best_fitness = np.min(fitness)\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())\n            CR = 0.7 + (0.3 * np.random.rand())\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n\n            current_best_fitness = np.min(fitness)\n            if current_best_fitness >= best_fitness:\n                stagnation_counter += 1\n            else:\n                best_fitness = current_best_fitness\n                stagnation_counter = 0\n\n            if stagnation_counter > 10 and pop_size < 100:  # Change: Adapt population size based on stagnation\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n                stagnation_counter = 0  # Reset on population increase\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introducing a feedback mechanism to adaptively tune population size based on convergence rate in differential evolution.", "configspace": "", "generation": 7, "fitness": 0.8909410626559794, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.047. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {"aucs": [0.8248149528486217, 0.917571719993835, 0.9304365151254814], "final_y": [0.1265917293470573, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a705e1bb-7894-4436-a5d9-9417a592820a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(8, pop_size // 2)  # Adjusted initial pop_size\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = np.random.uniform(0.5, 1.0)  # Adaptive mutation factor\n            CR = np.random.uniform(0.6, 0.9)  # Adaptive crossover rate\n            ranked_indices = np.argsort(fitness)  # Rank population\n            for i in range(pop_size):\n                if self.evaluations >= self.budget:\n                    break\n                if i < pop_size // 2:\n                    a, b, c = population[ranked_indices[np.random.choice(pop_size // 2, 3, replace=False)]]\n                else:\n                    a, b, c = population[np.random.choice(population.shape[0], 3, replace=False)]\n                \n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            \n            if pop_size < 100:\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        best_idx = np.argmin(fitness)\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved differential evolution by incorporating adaptive population ranking and dynamic mutation strategies for enhanced robustness and efficiency.", "configspace": "", "generation": 7, "fitness": 0.9001514846662403, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.900 with standard deviation 0.036. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {"aucs": [0.8503414138310285, 0.917571719993835, 0.9325413201738576], "final_y": [0.12427790472750411, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "41ec4391-9500-4584-8fa0-1b182374008b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for _ in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 12 # Adjust number of iterations\n        global_solution = np.random.rand(self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n        for i in range(2, self.dim + 1, 2): # Gradual dimension increase\n            global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n            local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n            if local_cost < global_cost:\n                global_solution = local_solution\n        return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a layered optimization strategy with gradual complexity increase and robustness checks to enhance the adaptive differential evolution method.", "configspace": "", "generation": 7, "fitness": 0.8843197478530754, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.884 with standard deviation 0.057. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "442a0487-e9e2-4fbe-a92e-0a8dad85e11c", "metadata": {"aucs": [0.8034741952008849, 0.917571719993835, 0.9319133283645062], "final_y": [0.14786207489442504, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a50a389a-073f-46d5-aa67-fec8af27399a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # Adjusted line: Perform local search on the best individual every few generations\n            if generation % 3 == 0:  # Changed frequency from 5 to 3\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search frequency for improved convergence speed and solution quality.", "configspace": "", "generation": 8, "fitness": 0.8499400970727766, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.021. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "bbc5ea5a-23d4-4a08-b9b7-b694e97457eb", "metadata": {"aucs": [0.8641366299374896, 0.8653480154856024, 0.8203356457952379], "final_y": [0.13039511519522407, 0.11621992467546649, 0.13699170449010156]}, "mutation_prompt": null}
{"id": "2efd5b68-8ff9-44bd-802a-a871cda15599", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * np.random.rand())  # Changed for improved adaptive F control\n            CR = 0.8 + (0.1 * np.random.rand())  # Changed for improved adaptive CR control\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # New line: Perform local search on the best individual every few generations\n            if generation % 5 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved adaptive control strategy for the mutation and crossover parameters in the differential evolution phase for balanced exploration and exploitation.", "configspace": "", "generation": 8, "fitness": 0.8272285613088082, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.827 with standard deviation 0.027. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "bbc5ea5a-23d4-4a08-b9b7-b694e97457eb", "metadata": {"aucs": [0.865023410938297, 0.8058914811702891, 0.8107707918178386], "final_y": [0.13039511519522407, 0.1329808832714232, 0.13400740733686733]}, "mutation_prompt": null}
{"id": "01f54182-36de-4dcb-bb10-208e30f1f022", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + 0.3 * np.cos(np.pi * generation / iters)  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Retained adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # New line: Conditional local search based on improvement trend\n            improvement_trend = np.diff(fitness)\n            if generation % 5 == 0 and np.all(improvement_trend[-3:] > 0):\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introducing adaptive mutation scaling and conditional local refinement based on the solution's improvement trend to enhance solution quality and convergence speed.", "configspace": "", "generation": 8, "fitness": 0.8220795850660433, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.007. And the mean value of best solutions found was 0.135 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "bbc5ea5a-23d4-4a08-b9b7-b694e97457eb", "metadata": {"aucs": [0.8255612042568418, 0.8279265688799665, 0.8127509820613217], "final_y": [0.12991741002652124, 0.13737313109973937, 0.13711630366784777]}, "mutation_prompt": null}
{"id": "36831194-804c-404b-9d29-f1743557dcf8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100 and np.std(fitness) > 0.01:  # Dynamically increase population size based on fitness variance\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # New line: Perform local search on the best individual every few generations\n            if generation % 5 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population size increase based on convergence rate to enhance exploration and exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.8304912890399962, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.011. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "bbc5ea5a-23d4-4a08-b9b7-b694e97457eb", "metadata": {"aucs": [0.8423030372872524, 0.8327859438967021, 0.816384885936034], "final_y": [0.13925891227918896, 0.13499492691737824, 0.13732101626086812]}, "mutation_prompt": null}
{"id": "85c63d63-5043-4d69-82f2-b387ad99f58a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # Modified line: Perform local search on the best individual every few generations with increasing intensity\n            if generation % (5 + (generation // 10)) == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced dynamic differential evolution with adaptive mutation and dynamic local search intensity for improved robustness and efficiency.", "configspace": "", "generation": 8, "fitness": 0.836445513177622, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.027. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "bbc5ea5a-23d4-4a08-b9b7-b694e97457eb", "metadata": {"aucs": [0.8655293407104128, 0.8433588353287302, 0.8004483634937227], "final_y": [0.13039511519522407, 0.1312644569920156, 0.13963627226836872]}, "mutation_prompt": null}
{"id": "d32ec906-3a29-4fac-9aee-ad015a5e8ba0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())\n            CR = 0.7 + (0.3 * np.random.rand())\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)  # Adjusted line: Increased by 2 for quicker adaptation\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):  # Adjusted line: Multiple local refinements to improve solution quality\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population size and multi-start local searches to enhance exploration and refinement.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "a50a389a-073f-46d5-aa67-fec8af27399a", "metadata": {}, "mutation_prompt": null}
{"id": "011ca3f7-e4b5-41d5-9366-48d67089366a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=60, F=0.5, CR=0.8):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.3 + 0.7 * np.random.rand()\n            CR = 0.6 + 0.4 * np.random.rand()\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 120:  # Increased population limit\n                pop_size = min(pop_size + 2, 120)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            if generation % 2 == 0:  # Increased local refinement frequency\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 12  # Adjusted iterations per budget\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Adaptive Differential Evolution with Layered Complexity Handling and Robustness Metrics Enhancement.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 61 is out of bounds for axis 0 with size 61').", "error": "IndexError('index 61 is out of bounds for axis 0 with size 61')", "parent_id": "a50a389a-073f-46d5-aa67-fec8af27399a", "metadata": {}, "mutation_prompt": null}
{"id": "75d84003-58b2-4a63-a3b8-968749e5be52", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if generation % 2 == 0 and pop_size < 100:  # Dynamically increase population size more frequently\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # Adjusted line: Perform local search on the best individual every few generations\n            if generation % 3 == 0:  # Changed frequency from 5 to 3\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improve exploration by introducing an adaptive population size strategy in DE.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 61 is out of bounds for axis 0 with size 61').", "error": "IndexError('index 61 is out of bounds for axis 0 with size 61')", "parent_id": "a50a389a-073f-46d5-aa67-fec8af27399a", "metadata": {}, "mutation_prompt": null}
{"id": "0cce2c46-cf3b-4b70-82ce-d1393706fb45", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())\n            CR = 0.7 + (0.3 * np.random.rand())\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # Adjusted line: Perform local search on the best individual based on diversity\n            if generation % 3 == 0 and np.std(fitness) > 0.05:  # Diversity-based trigger\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic local search trigger based on population diversity to enhance convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 61 is out of bounds for axis 0 with size 61').", "error": "IndexError('index 61 is out of bounds for axis 0 with size 61')", "parent_id": "a50a389a-073f-46d5-aa67-fec8af27399a", "metadata": {}, "mutation_prompt": null}
{"id": "3a2d307d-fdf8-461d-826a-2d96eb65f2b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())  # Line changed for adaptive F\n            CR = 0.7 + (0.3 * np.random.rand())  # Line changed for adaptive CR\n            success_count = 0  # Success counter\n            best_idx = np.argmin(fitness)  # Elitism: Identify the best individual\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n                    success_count += 1  # Increase success counter\n            if pop_size < 100:  # Dynamically increase population size\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            # Adjusted line: Perform local search on the best individual every few generations\n            if generation % 3 == 0:  # Changed frequency from 5 to 3\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            # Dynamic adjustment of CR based on success rate\n            if success_count / pop_size > 0.2:  # Change CR if success rate is high\n                CR = min(1.0, CR + 0.1)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Implemented a dynamic CR adjustment based on historical success rates to enhance solution quality.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 61 is out of bounds for axis 0 with size 61').", "error": "IndexError('index 61 is out of bounds for axis 0 with size 61')", "parent_id": "a50a389a-073f-46d5-aa67-fec8af27399a", "metadata": {}, "mutation_prompt": null}
{"id": "16bd905d-534c-4f8e-ba14-ea93cf718b0d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))  # Changed line: Dynamic F adaptation\n            CR = 0.8 + (0.1 * np.cos(generation))  # Changed line: Dynamic CR adaptation\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)  # Adjusted line: Increased by 2 for quicker adaptation\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                fitness = np.append(fitness, func(new_individual))  # Corrected line: Error handling for new ind.\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):  # Adjusted line: Multiple local refinements to improve solution quality\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce dynamic F and CR adaptation and improved error handling to enhance robustness and performance of the optimization algorithm.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "d32ec906-3a29-4fac-9aee-ad015a5e8ba0", "metadata": {}, "mutation_prompt": null}
{"id": "85efcd7f-ad61-4d55-b13d-9541c06b0784", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())\n            CR = 0.7 + (0.3 * np.random.rand())\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)  # Bound pop_size to not exceed 100 or cause index errors\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):  # Adjusted line: Multiple local refinements to improve solution quality\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance population management by ensuring it doesn't exceed bounds during adaptation.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "d32ec906-3a29-4fac-9aee-ad015a5e8ba0", "metadata": {}, "mutation_prompt": null}
{"id": "bf05e98b-bbeb-45ef-813c-479af7fd4a84", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.6 + (0.4 * np.random.rand())  # Adjusted line: Fine-tuning mutation factor for better exploration\n            CR = 0.5 + (0.5 * np.random.rand())\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)  # Adjusted line: Increased by 2 for quicker adaptation\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):  # Adjusted line: Multiple local refinements to improve solution quality\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Utilize adaptive differential evolution with revised mutation rates and enhanced local search integration.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "d32ec906-3a29-4fac-9aee-ad015a5e8ba0", "metadata": {}, "mutation_prompt": null}
{"id": "1ba8afe6-9263-4542-9fe1-38885e15149b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())\n            CR = 0.7 + (0.3 * np.random.rand())\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)  # Adjusted line: Increased by 2 for quicker adaptation\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):  # Adjusted line: Multiple local refinements to improve solution quality\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation and crossover diversity in Differential Evolution to improve exploration-exploitation balance.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "d32ec906-3a29-4fac-9aee-ad015a5e8ba0", "metadata": {}, "mutation_prompt": null}
{"id": "9ea3d870-3d95-4480-916a-cd24a152c137", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.4 + (0.6 * np.random.rand())\n            CR = 0.7 + (0.3 * np.random.rand())\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = a + F * (b - c)\n                mutant = np.clip(mutant, bounds.lb, bounds.ub)  # Adjusted line: Ensured mutant is always clipped\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual))\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance population diversity and include intermediate layer-wise adaptive strategy to improve global and local exploration.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "d32ec906-3a29-4fac-9aee-ad015a5e8ba0", "metadata": {}, "mutation_prompt": null}
{"id": "af537b6c-731c-41d5-8a9b-1750f805da6e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))\n            CR = 0.8 + (0.1 * np.cos(generation))\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])  # Corrected line: Ensure correct fitness calculation\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a correction in the error handling for new individual fitness calculation to ensure robustness.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "16bd905d-534c-4f8e-ba14-ea93cf718b0d", "metadata": {}, "mutation_prompt": null}
{"id": "e4e3a117-1d0e-4271-b2c4-41bcd563e108", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))\n            CR = 0.8 + (0.1 * np.cos(generation))\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                population = np.vstack([population, new_individual])\n                fitness = np.append(fitness, func(new_individual[0])) # Fixed line: Correct fitness function call\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual) # Fixed: Single refinement\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refine error handling and enhance solution quality by adjusting population size and local refinement strategy.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "16bd905d-534c-4f8e-ba14-ea93cf718b0d", "metadata": {}, "mutation_prompt": null}
{"id": "b0348b40-9597-4f9e-ae9f-1bda5930e3bf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))  # Changed line: Dynamic F adaptation\n            CR = 0.8 + (0.1 * np.cos(generation))  # Changed line: Dynamic CR adaptation\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)  # Adjusted line: Increased by 2 for quicker adaptation\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual)  # Adjusted line: Fixed variable name for evaluation\n                fitness = np.append(fitness, new_fitness)  # Corrected line: Error handling for new ind.\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):  # Adjusted line: Multiple local refinements to improve solution quality\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce index boundary check to prevent out-of-bounds error during differential evolution.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "16bd905d-534c-4f8e-ba14-ea93cf718b0d", "metadata": {}, "mutation_prompt": null}
{"id": "ddbcca0b-92ae-4dd6-8ef7-b861f58349fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation * np.pi / 6))  # Updated F adaptation\n            CR = 0.8 + (0.1 * np.sin(generation * np.pi / 6))  # Updated CR adaptation\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual)\n                self.evaluations += 1  # Corrected cumulative evaluations\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                num_refinements = min(5, self.budget // 20)  # Dynamic local search allocation\n                for _ in range(num_refinements):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a layered population strategy and dynamic local search allocation for improved optimization robustness and performance.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "16bd905d-534c-4f8e-ba14-ea93cf718b0d", "metadata": {}, "mutation_prompt": null}
{"id": "108eb9df-a2c5-4493-a01d-a10fe5e0995c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))  # Changed line: Dynamic F adaptation\n            CR = 0.8 + (0.1 * np.cos(generation))  # Changed line: Dynamic CR adaptation\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)  # Adjusted line: Increased by 2 for quicker adaptation\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                fitness = np.append(fitness, func(new_individual))  # Corrected line: Error handling for new ind.\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):  # Adjusted line: Multiple local refinements to improve solution quality\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce dynamic population size adaptation and improved error handling to enhance robustness and performance of the optimization algorithm.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "16bd905d-534c-4f8e-ba14-ea93cf718b0d", "metadata": {}, "mutation_prompt": null}
{"id": "e4387b8c-178e-4d96-bd65-ffee4420e3a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.5 * np.random.rand())  # Changed to increase randomness\n            CR = 0.7 + (0.3 * np.cos(generation))  # Adjusted to improve adaptability\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])  # Corrected line: Ensure correct fitness calculation\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance population diversity and adaptive parameter adjustment in hybrid optimization to maximize robustness and convergence.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "af537b6c-731c-41d5-8a9b-1750f805da6e", "metadata": {}, "mutation_prompt": null}
{"id": "5b2c98d7-28bc-4dba-8fc8-bc4a6c85e2ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))\n            CR = 0.8 + (0.1 * np.cos(generation))\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])  # Corrected line: Ensure correct fitness calculation\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        x0 = np.clip(x0, bounds.lb, bounds.ub)  # Corrected line: Ensure x0 is within bounds\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improve robustness by ensuring correct dimension handling during local refinement.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "af537b6c-731c-41d5-8a9b-1750f805da6e", "metadata": {}, "mutation_prompt": null}
{"id": "86317f83-1772-4bb7-aa4a-367542f7294f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))\n            CR = 0.8 + (0.1 * np.cos(generation))\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])\n                self.evaluations += 1  # Ensure evaluations are counted consistently\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n                best_idx = np.argmin(fitness)  # Recalculate best_idx to prevent out-of-bounds\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a mechanism to ensure population and fitness arrays are synced during growth and refinement to prevent index errors.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 26 is out of bounds for axis 0 with size 26').", "error": "IndexError('index 26 is out of bounds for axis 0 with size 26')", "parent_id": "af537b6c-731c-41d5-8a9b-1750f805da6e", "metadata": {}, "mutation_prompt": null}
{"id": "cce69c13-284c-41f1-ab14-08cb87a37525", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.6 + 0.3 * (generation / iters)\n            best_idx = np.argmin(fitness)\n            for i in range(pop_size):\n                indices = list(range(pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n                    \n            if generation % 4 == 0 and pop_size < 100:\n                pop_size += 2\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Use adaptive population size and mutation strategies to enhance exploration and prevent boundary errors in DE.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 51 is out of bounds for axis 0 with size 51').", "error": "IndexError('index 51 is out of bounds for axis 0 with size 51')", "parent_id": "af537b6c-731c-41d5-8a9b-1750f805da6e", "metadata": {}, "mutation_prompt": null}
{"id": "b63a1b7e-f26d-46d2-85a5-2879b1bfc4ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))\n            CR = 0.8 + (0.1 * np.cos(generation))\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)  # Adjustment: Ensure correct index during mutation\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])  # Corrected line: Ensure correct fitness calculation\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a population size adjustment to prevent out-of-bounds errors during mutation in Differential Evolution.", "configspace": "", "generation": 12, "fitness": 0.9029679208784183, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.903 with standard deviation 0.030. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "af537b6c-731c-41d5-8a9b-1750f805da6e", "metadata": {"aucs": [0.860895527515938, 0.917571719993835, 0.9304365151254814], "final_y": [0.13039511519522407, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "51ed267a-2b55-412d-b6da-dc1009fdcc24", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))\n            CR = 0.8 + (0.1 * np.cos(generation))\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            \n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2 + int(self.evaluations / self.budget * 10), 100)  # Line modified for dynamic adjustment\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the HybridMetaheuristicOptimizer by dynamically adjusting the population size based on evaluation feedback to more effectively balance exploration and exploitation.", "configspace": "", "generation": 13, "fitness": 0.9036831822457536, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.029. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b63a1b7e-f26d-46d2-85a5-2879b1bfc4ff", "metadata": {"aucs": [0.8630413116179443, 0.917571719993835, 0.9304365151254814], "final_y": [0.13039511519522407, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "633b0ab2-7293-4600-a6c8-6b2e18a6e7a9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation))\n            # Modify crossover rate dynamically based on fitness\n            CR = 0.8 * (1 - (np.min(fitness) / np.max(fitness))) + 0.1\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)  # Adjustment: Ensure correct index during mutation\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])  # Corrected line: Ensure correct fitness calculation\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic crossover rate based on individual fitness to enhance exploration and exploitation balance.", "configspace": "", "generation": 13, "fitness": 0.8801792990164268, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.027. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "b63a1b7e-f26d-46d2-85a5-2879b1bfc4ff", "metadata": {"aucs": [0.8547967969162752, 0.917571719993835, 0.8681693801391699], "final_y": [0.1327499911830119, 0.11621992467546649, 0.1255205079114181]}, "mutation_prompt": null}
{"id": "b5f790bc-6241-4854-8680-057929a6463f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation + np.pi/4)))  # Adjusted mutation factor\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))  # Adjusted crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])  # Integrated smoothing\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 2 == 0:  # More frequent local refinement\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration and exploitation balance using variable mutation strategies and adaptive smoothing for noise reduction.", "configspace": "", "generation": 13, "fitness": 0.9043782934032979, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.028. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b63a1b7e-f26d-46d2-85a5-2879b1bfc4ff", "metadata": {"aucs": [0.8651266450905768, 0.917571719993835, 0.9304365151254814], "final_y": [0.13039511519522407, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "cdbe7756-b46f-4a58-8752-7de98a72f229", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.3 * np.sin(generation / 5))  # Adjusted mutation factor\n            CR = 0.8 + (0.1 * np.cos(generation / 5))  # Adjusted crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refinement of crossover and mutation strategies with adaptive parameter control to improve convergence in high-dimensional spaces.", "configspace": "", "generation": 13, "fitness": 0.9035758137141422, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.029. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b63a1b7e-f26d-46d2-85a5-2879b1bfc4ff", "metadata": {"aucs": [0.86271920602311, 0.917571719993835, 0.9304365151254814], "final_y": [0.13039511519522407, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "8e239803-68e2-4ae7-8006-bcb9e9637116", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + 0.3 * (np.max(fitness) - np.min(fitness)) / np.max(fitness)  # Adaptive mutation factor\n            CR = 0.8 + (0.1 * np.cos(generation))\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)  # Adjustment: Ensure correct index during mutation\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = func(new_individual[0])  # Corrected line: Ensure correct fitness calculation\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 3 == 0:\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive mutation factor in Differential Evolution to enhance exploration based on fitness diversity.", "configspace": "", "generation": 13, "fitness": 0.9034381055953755, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.903 with standard deviation 0.030. And the mean value of best solutions found was 0.120 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b63a1b7e-f26d-46d2-85a5-2879b1bfc4ff", "metadata": {"aucs": [0.8623060816668104, 0.917571719993835, 0.9304365151254814], "final_y": [0.13039511519522407, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "554f9c2d-1c0b-49b8-831c-dfbad19bdaec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation + np.pi/4)))  # Adjusted mutation factor\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))  # Adjusted crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])  # Integrated smoothing\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 2 == 0:  # More frequent local refinement\n                best_individual = population[best_idx]\n                for _ in range(3):  # Changed from 2 to 3 for more refinement cycles\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n            # Stochastic gradient update to enhance refinement\n            if generation % 5 == 0:  \n                gradient = np.gradient(fitness)\n                population = np.clip(population - 0.01 * gradient, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance local refinement frequency and feedback by incorporating stochastic gradient-based updates.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "b5f790bc-6241-4854-8680-057929a6463f", "metadata": {}, "mutation_prompt": null}
{"id": "8339195d-5f8c-4046-a123-e9d783168da6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        velocity = np.random.rand(pop_size, self.dim) * 0.1  # Initialize velocity for PSO\n        best_personal = np.copy(population)\n        personal_best_fitness = np.copy(fitness)\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation + np.pi/4)))\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            global_best = population[best_idx]\n\n            for i in range(valid_pop_size):\n                r1, r2 = np.random.rand(2)\n                velocity[i] = 0.5 * velocity[i] + r1 * (best_personal[i] - population[i]) + r2 * (global_best - population[i])  # Update velocity\n                trial = population[i] + velocity[i]  # Move particles\n                trial = np.clip(trial, bounds.lb, bounds.ub)\n\n                mutant = np.clip(population[i] + F * (trial - global_best), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n                    if trial_fitness < personal_best_fitness[i]:  # Update personal best\n                        best_personal[i] = trial\n                        personal_best_fitness[i] = trial_fitness\n\n            if generation % 3 == 0:  # Adjust local refinement frequency\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n            pop_size = min(pop_size + 2, 100)\n            new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n            new_fitness = np.mean([func(new_individual[0]) for _ in range(3)]) \n            fitness = np.append(fitness, new_fitness)\n            population = np.vstack([population, new_individual])\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improve the exploration-exploitation trade-off by integrating adaptive layer perturbation and hybridization with particle swarm optimization for better convergence on high-dimensional problems.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "b5f790bc-6241-4854-8680-057929a6463f", "metadata": {}, "mutation_prompt": null}
{"id": "1e9d8b6e-f8b8-4f6d-974f-403a51bd4a44", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation + np.pi/4)))  # Adjusted mutation factor\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))  # Adjusted crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if generation % 5 == 0 and self.evaluations < self.budget - 10:  # Adaptive restart mechanism\n                population[np.argmax(fitness)] = np.random.rand(self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n\n            pop_size = min(max(8, valid_pop_size // 2 + 5), 150)  # Dynamic population size\n\n            if generation % 2 == 0:  # More frequent local refinement\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improve exploration by dynamic population size and enhance local search with adaptive restart mechanisms.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "b5f790bc-6241-4854-8680-057929a6463f", "metadata": {}, "mutation_prompt": null}
{"id": "fe09bc6d-251c-49fc-a34c-54ace5c675c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation + np.pi/4)))  # Adjusted mutation factor\n            CR = 0.75 + (0.2 * (1 - self.evaluations / self.budget))  # Dynamic crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])  # Integrated smoothing\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 2 == 0:  # More frequent local refinement\n                best_individual = population[best_idx]\n                for _ in range(3):  # Increased local search budget\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refine local refinement strategy by allowing larger local search budget and adapt crossover rate dynamically based on evaluations.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "b5f790bc-6241-4854-8680-057929a6463f", "metadata": {}, "mutation_prompt": null}
{"id": "dce47224-92ac-4f06-9763-41202ec7eb0c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation + np.pi/4)))  # Adjusted mutation factor\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))  # Adjusted crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            \n            if np.std(fitness) > 0.1:  # Adaptive population expansion based on fitness variance\n                pop_size = min(pop_size + 1, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])  # Integrated smoothing\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 2 == 0:  # More frequent local refinement\n                best_individual = population[best_idx]\n                for _ in range(2):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population expansion based on fitness variance to improve the exploration-exploitation balance.", "configspace": "", "generation": 14, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 25 is out of bounds for axis 0 with size 25').", "error": "IndexError('index 25 is out of bounds for axis 0 with size 25')", "parent_id": "b5f790bc-6241-4854-8680-057929a6463f", "metadata": {}, "mutation_prompt": null}
{"id": "0e5d4454-4698-40e5-bf96-ed6a2ddf34d7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:  \n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid optimizer that adapts exploration-exploitation balance and incorporates smoothing to handle noisy evaluations efficiently.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "554f9c2d-1c0b-49b8-831c-dfbad19bdaec", "metadata": {}, "mutation_prompt": null}
{"id": "b08938a4-3d55-4d0c-a7fc-f849788bd7ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.2 * np.abs(np.sin(generation + np.pi/4)))  # Adjusted mutation factor\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))  # Adjusted crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])  # Integrated smoothing\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 2 == 0:  # More frequent local refinement\n                best_individual = population[best_idx]\n                for _ in range(3):  # Changed from 2 to 3 for more refinement cycles\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n            # Stochastic gradient update to enhance refinement\n            if generation % 5 == 0:  \n                gradient = np.gradient(fitness, axis=0)  # Corrected gradient computation\n                population = np.clip(population - 0.01 * gradient, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance mutation strategy and adjust gradient computation method for improved refinement.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "554f9c2d-1c0b-49b8-831c-dfbad19bdaec", "metadata": {}, "mutation_prompt": null}
{"id": "a58a7678-e9f7-45e2-8b77-e6a2dbbcdade", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation / (self.dim + 1))))  # Layer-wise adaptive mutation factor\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 2 == 0:\n                best_individual = population[best_idx]\n                for _ in range(3):\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n            if generation % 5 == 0:\n                learning_rate = 0.01 + 0.005 * np.abs(np.sin(generation / 2))  # Adaptive learning rate\n                gradient = np.gradient(fitness)\n                population = np.clip(population - learning_rate * gradient, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce layer-wise adaptive mutation factor and replace gradient updates with adaptive learning rates for population refinement.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "554f9c2d-1c0b-49b8-831c-dfbad19bdaec", "metadata": {}, "mutation_prompt": null}
{"id": "16e79441-a988-49cd-9d29-8db16f507082", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation + np.pi/4)))  # Adjusted mutation factor\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))  # Adjusted crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F *= np.clip((self.budget - self.evaluations) / self.budget, 0.1, 1.0)  # Adaptive scaling\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])  # Integrated smoothing\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 2 == 0:  # More frequent local refinement\n                best_individual = population[best_idx]\n                for _ in range(3):  # Changed from 2 to 3 for more refinement cycles\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n            # Stochastic gradient update to enhance refinement\n            if generation % 5 == 0:  \n                gradient = np.gradient(fitness)\n                population = np.clip(population - 0.01 * gradient, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive scaling of mutation factor based on budget utilization to enhance diversity.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "554f9c2d-1c0b-49b8-831c-dfbad19bdaec", "metadata": {}, "mutation_prompt": null}
{"id": "bf3a849b-ba30-4a2a-a7fb-37539d253d5c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            F = 0.5 + (0.4 * np.abs(np.sin(generation + np.pi/4)))  # Adjusted mutation factor\n            CR = 0.8 + (0.15 * np.abs(np.cos(generation + np.pi/3)))  # Adjusted crossover rate\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])  # Integrated smoothing\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 2 == 0:  # More frequent local refinement\n                best_individual = population[best_idx]\n                for _ in range(3):  # Changed from 2 to 3 for more refinement cycles\n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n            # Stochastic gradient update to enhance refinement\n            if generation % 10 == 0:  # Updated frequency to reduce conflict in gradient application\n                gradient = np.gradient(fitness)\n                population = np.clip(population - 0.01 * gradient, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refine local improvement effectiveness by adjusting gradient update frequency to enhance convergence. ", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "554f9c2d-1c0b-49b8-831c-dfbad19bdaec", "metadata": {}, "mutation_prompt": null}
{"id": "d337eb62-1f08-4cca-9380-78ab160b0ca2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),  # Change 1: Slight perturbation for initial guess\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration in the differential evolution phase and improve local refinement initialization to handle high-dimensional noisy optimization.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "0e5d4454-4698-40e5-bf96-ed6a2ddf34d7", "metadata": {}, "mutation_prompt": null}
{"id": "ce17d684-25d9-40bc-8253-4aa75d8e76ed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = np.random.normal(scale=0.01, size=fitness.shape)  # Changed line\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid optimizer that adapts exploration-exploitation balance and incorporates adaptive smoothing to handle noisy evaluations efficiently.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "0e5d4454-4698-40e5-bf96-ed6a2ddf34d7", "metadata": {}, "mutation_prompt": null}
{"id": "aecd048e-97ad-426d-ba43-c82e017d6b78", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 6 == 0:  # Adjusted for more frequent noise reduction\n                noise_reduction_factor = 0.02 * np.random.randn(*(fitness.shape))  # Increased noise reduction\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n        \n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimizer incorporating differential evolution with adaptive population size, local refinement, and noise smoothing for efficient exploration-exploitation balance in noisy environments.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "0e5d4454-4698-40e5-bf96-ed6a2ddf34d7", "metadata": {}, "mutation_prompt": null}
{"id": "21bf3da7-56f3-41af-8557-831cb48c27fe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            for i in range(len(population)):  # Fixed broadcast issue in population size handling\n                indices = list(range(len(population)))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:  \n                noise_reduction_factor = 0.01 * np.random.randn(*population.shape)  # Changed noise reduction mechanism\n                population = np.clip(population + noise_reduction_factor, bounds.lb, bounds.ub)  # Alter direction of noise application\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "A refined hybrid optimizer enhancing noise handling and population dynamics for improved black-box optimization.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "0e5d4454-4698-40e5-bf96-ed6a2ddf34d7", "metadata": {}, "mutation_prompt": null}
{"id": "616e94bc-a69b-4cb4-b831-ff1296d4901a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(population.shape))  # Changed line\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid optimization approach employing an adaptive scaling factor for noise reduction and convergence refinement.", "configspace": "", "generation": 16, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "0e5d4454-4698-40e5-bf96-ed6a2ddf34d7", "metadata": {}, "mutation_prompt": null}
{"id": "ea92e28a-ce68-477a-b0cc-e3b935c59d32", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)  # Change: Adjust population increase step to 5\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),  # Change 1: Slight perturbation for initial guess\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Utilize adaptive population resizing in the differential evolution phase to accommodate dynamic exploration and convergence needs.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "d337eb62-1f08-4cca-9380-78ab160b0ca2", "metadata": {}, "mutation_prompt": null}
{"id": "9828a00e-964f-48d2-9ac1-349817f50493", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 4, 100)  # Change 1: Increase population size faster\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 5 == 0:  # Change 2: Refine more frequently\n                best_idx = np.argmin(fitness)  # Change 3: Compute best_idx within loop\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[np.argmin(fitness)], np.min(fitness)  # Change 4: Move return out of loop\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population size and diversify initialization to improve exploration and avoid premature convergence.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "d337eb62-1f08-4cca-9380-78ab160b0ca2", "metadata": {}, "mutation_prompt": null}
{"id": "74acb885-16a8-4e48-82ee-b21eb8a0e3fa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                # Change 1: Enhanced mutation strategy for broader exploration\n                mutant = np.clip(a + F * (b - c) + F * (population[best_idx] - a), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            # Change 2: Adaptive population control based on budget\n            if generation % 5 == 0 and self.evaluations < self.budget * 0.8:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Integrate adaptive population size control and enhanced mutation strategies to improve convergence in high-dimensional noisy optimization.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "d337eb62-1f08-4cca-9380-78ab160b0ca2", "metadata": {}, "mutation_prompt": null}
{"id": "eb84e1a6-aba9-4834-9842-0dee60b51a06", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(population.shape))  # Change 1: Correct shape adjustment\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),  # Slight perturbation for initial guess\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive adjustment in differential evolution's population size to avoid broadcasting errors and improve solution diversity.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "d337eb62-1f08-4cca-9380-78ab160b0ca2", "metadata": {}, "mutation_prompt": null}
{"id": "d656aa29-9885-4f49-89a9-31627fe25405", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F_dynamic = F * (0.5 + 0.5 * np.random.rand())  # Change 1: Dynamic mutation factor\n                mutant = np.clip(a + F_dynamic * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 2, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.005 * np.random.randn(*(population.shape))  # Change 2: Adjust noise reduction scaling\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)), \n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance differential evolution by adapting mutation strategy and noise reduction for high-dimensional optimization.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "d337eb62-1f08-4cca-9380-78ab160b0ca2", "metadata": {}, "mutation_prompt": null}
{"id": "44b07e96-7d77-42c7-99ed-e3e3b328c807", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic mutation factor adaptation in the differential evolution phase to enhance exploration-exploitation balance.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "ea92e28a-ce68-477a-b0cc-e3b935c59d32", "metadata": {}, "mutation_prompt": null}
{"id": "32a8b859-8373-4137-8d70-2ed65ad7e025", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)  # Change: Adjust population increase step to 5\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),  # Change 1: Slight perturbation for initial guess\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance population initialization by adding diversity to enhance exploration and prevent premature convergence.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "ea92e28a-ce68-477a-b0cc-e3b935c59d32", "metadata": {}, "mutation_prompt": null}
{"id": "7ba98d04-96af-4b82-bdbd-b5392ee57395", "solution": "# Description: Introduce an adaptive mutation strategy in differential evolution to balance exploration and convergence.\n# Code: \nimport numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            adaptive_F = F * (0.5 + 0.5 * np.random.rand())  # Change: Introduce adaptive mutation factor\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + adaptive_F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce an adaptive mutation strategy in differential evolution to balance exploration and convergence.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "ea92e28a-ce68-477a-b0cc-e3b935c59d32", "metadata": {}, "mutation_prompt": null}
{"id": "8db42e18-3af6-43fe-9d73-5d92e777c449", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) + 0.01 * np.random.randn(self.dim), bounds.lb, bounds.ub)  # Change 1: Add randomness to mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 3, 100)  # Change 2: Adjust population increase step to 3\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 5 == 0:  # Change 3: Adjust generation interval for refinement\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = np.random.normal(0, 0.01, fitness.shape)  # Change 4: Use normal distribution for noise reduction\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance noise robustness by adjusting mutation strategies in DE and refining population management.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "ea92e28a-ce68-477a-b0cc-e3b935c59d32", "metadata": {}, "mutation_prompt": null}
{"id": "91c2bb68-21b5-4f73-b8b1-a0b8682942fc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                mutant = np.clip(a + F * (b - c) * np.random.rand(self.dim), bounds.lb, bounds.ub)  # Change 1\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(fitness.shape[0])  # Change 2\n                population = np.clip(population - noise_reduction_factor[:, np.newaxis], bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refining mutation vector approach and noise reduction to enhance differential evolution performance.", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "ea92e28a-ce68-477a-b0cc-e3b935c59d32", "metadata": {}, "mutation_prompt": null}
{"id": "817d3ef5-85e8-40af-9868-e9048be61bba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                CR = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic crossover rate adaptation in the differential evolution phase to enhance exploration-exploitation balance.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "44b07e96-7d77-42c7-99ed-e3e3b328c807", "metadata": {}, "mutation_prompt": null}
{"id": "3fab6544-e84f-4dea-90da-2708880a1ae5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100 and self.evaluations < self.budget // 2:  # Changed condition\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Adjust population size increment condition to improve exploration-exploitation balance and prevent premature convergence.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "44b07e96-7d77-42c7-99ed-e3e3b328c807", "metadata": {}, "mutation_prompt": null}
{"id": "ea48280a-7eba-48a8-a523-2127bfd09170", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)  # Change: Increase population size incrementally\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population size adjustment to improve exploration in the early stages and intensify exploitation as the budget is consumed.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "44b07e96-7d77-42c7-99ed-e3e3b328c807", "metadata": {}, "mutation_prompt": null}
{"id": "f5734b44-1e9b-4f19-8476-158f119d9bd2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n            population = population[:valid_pop_size]  # Ensures population size is consistent\n        \n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population sizing for enhanced exploration and exploitation balance in differential evolution.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "44b07e96-7d77-42c7-99ed-e3e3b328c807", "metadata": {}, "mutation_prompt": null}
{"id": "3e42f324-c675-4ce4-a21d-792135c2d029", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()\n                layer_mutation = np.random.choice([0.5, 1.0, 1.5])  # New: Layer-wise mutation\n                mutant = np.clip(a + F * (b - c) * layer_mutation, bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(population.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration and robustness by incorporating adaptive noise reduction and layer-wise mutation.", "configspace": "", "generation": 19, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "44b07e96-7d77-42c7-99ed-e3e3b328c807", "metadata": {}, "mutation_prompt": null}
{"id": "e715ef78-3840-4bdb-b5bb-e2e6e42207d9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                CR = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if generation % 3 == 0 and pop_size < 100:  # Modified line for adaptive population increase\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population size increase in the differential evolution phase to improve solution diversity over generations.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "817d3ef5-85e8-40af-9868-e9048be61bba", "metadata": {}, "mutation_prompt": null}
{"id": "fdb81773-09fc-44c3-91e8-4e04abad4870", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  \n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                CR = 0.5 + 0.5 * np.random.rand()  \n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 5 == 0:  # Change: Reduce noise intervention frequency\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                noise_reduction_factor *= np.linspace(1.0, 0.1, self.dim)  # Change: Layer-specific noise handling\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce layer-specific adaptive mechanisms for noise handling and convergence control in the optimization process.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,) (10,) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,) (10,) (26,) ')", "parent_id": "817d3ef5-85e8-40af-9868-e9048be61bba", "metadata": {}, "mutation_prompt": null}
{"id": "48f5882c-8079-472b-963f-45ba92141867", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                CR = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                # Checking if further improvement is possible before refinement\n                if np.min(fitness) < fitness[best_idx]:  \n                    refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                    if refined_cost < fitness[best_idx]:\n                        population[best_idx] = refined_solution\n                        fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Implement a simple fitness improvement check before local refinement to minimize unnecessary evaluations.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "817d3ef5-85e8-40af-9868-e9048be61bba", "metadata": {}, "mutation_prompt": null}
{"id": "a9d5c3df-6a25-4e9e-9dfa-4b2841dc333f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                CR = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 3, 100)  # Change: Population size increment adjustment\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 5 == 0:  # Change: Adjustment to local refinement frequency\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*(fitness.shape))\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive population size scaling and fine-tuned local search frequency for enhanced optimization efficiency.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "817d3ef5-85e8-40af-9868-e9048be61bba", "metadata": {}, "mutation_prompt": null}
{"id": "83327264-8c0c-453b-bc98-0e668710d35d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        \n    def differential_evolution(self, func, bounds, iters, pop_size=50, F=0.8, CR=0.9):\n        pop_size = max(4, pop_size // 2)\n        population = np.random.rand(pop_size, self.dim)\n        population *= (bounds.ub - bounds.lb)\n        population += bounds.lb\n        fitness = np.array([func(ind) for ind in population])\n        self.evaluations += pop_size\n\n        for generation in range(iters):\n            if self.evaluations >= self.budget:\n                break\n            best_idx = np.argmin(fitness)\n            valid_pop_size = len(population)\n            for i in range(valid_pop_size):\n                indices = list(range(valid_pop_size))\n                indices.remove(i)\n                a, b, c = population[np.random.choice(indices, 3, replace=False)]\n                F = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic mutation factor\n                mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n                CR = 0.5 + 0.5 * np.random.rand()  # Change: Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                trial_fitness = func(trial)\n                self.evaluations += 1\n                if trial_fitness < fitness[i]:\n                    fitness[i] = trial_fitness\n                    population[i] = trial\n            if pop_size < 100:\n                pop_size = min(pop_size + 5, 100)\n                new_individual = np.random.rand(1, self.dim) * (bounds.ub - bounds.lb) + bounds.lb\n                new_fitness = np.mean([func(new_individual[0]) for _ in range(3)])\n                fitness = np.append(fitness, new_fitness)\n                population = np.vstack([population, new_individual])\n\n            if generation % 4 == 0:\n                best_individual = population[best_idx]\n                refined_solution, refined_cost = self.local_refinement(func, bounds, best_individual)\n                if refined_cost < fitness[best_idx]:\n                    population[best_idx] = refined_solution\n                    fitness[best_idx] = refined_cost\n\n            if generation % 7 == 0:\n                noise_reduction_factor = 0.01 * np.random.randn(*population.shape)  # Change: Corrected shape for noise reduction\n                population = np.clip(population - noise_reduction_factor, bounds.lb, bounds.ub)\n\n        return population[best_idx], np.min(fitness)\n\n    def local_refinement(self, func, bounds, x0):\n        result = minimize(func, x0 * (1 + 0.05 * np.random.randn(*x0.shape)),\n                          bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        return result.x, result.fun\n\n    def __call__(self, func):\n        bounds = func.bounds\n        iterations = self.budget // 10\n        global_solution, global_cost = self.differential_evolution(func, bounds, iters=iterations)\n        local_solution, local_cost = self.local_refinement(func, bounds, global_solution)\n        \n        if local_cost < global_cost:\n            return local_solution\n        else:\n            return global_solution", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a noise-reduction technique to smooth population updates and enhance robustness in the differential evolution phase.", "configspace": "", "generation": 20, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (26,10) (26,) ').", "error": "ValueError('operands could not be broadcast together with shapes (26,10) (26,) ')", "parent_id": "817d3ef5-85e8-40af-9868-e9048be61bba", "metadata": {}, "mutation_prompt": null}
