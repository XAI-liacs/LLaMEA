{"id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n      \n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=func)\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            current_layers += 1\n            self.dim = current_layers * 2  # Assuming each layer adds a new dimension pair\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration and Nelder-Mead for local refinement, enhanced with progressive layer introduction and adaptive robustness measures to optimize multilayer photonic structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 49, in progressive_layer_optimization\nUnboundLocalError: local variable 'best_candidate' referenced before assignment\n.", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 49, in progressive_layer_optimization\nUnboundLocalError: local variable 'best_candidate' referenced before assignment\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "1a18194f-a226-4d65-954f-8d1a1c00a410", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_budget = 0\n        self.pop_size = min(50, dim * 5)  # Population size for DE\n        self.population = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        self.fitness = np.full(self.pop_size, np.inf)\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            mutant_vector = np.clip(a + self.mutation_factor * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.crossover_rate\n            trial_vector = np.where(crossover, mutant_vector, self.population[i])\n            trial_fitness = self.evaluate(func, trial_vector)\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial_vector\n                self.fitness[i] = trial_fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial_vector\n\n    def local_search(self, func, solution):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        new_solution = np.clip(solution + perturbation, func.bounds.lb, func.bounds.ub)\n        new_fitness = self.evaluate(func, new_solution)\n        if new_fitness < self.best_fitness:\n            self.best_solution = new_solution\n            self.best_fitness = new_fitness\n\n    def evaluate(self, func, solution):\n        if self.current_budget < self.budget:\n            self.current_budget += 1\n            return func(solution)\n        return np.inf\n\n    def __call__(self, func):\n        layers_step = max(2, self.dim // 10)\n        for current_dim in range(layers_step, self.dim + 1, layers_step):\n            self.population = np.random.uniform(-1, 1, (self.pop_size, current_dim))\n            self.fitness = np.full(self.pop_size, np.inf)\n            self.best_solution = None\n            self.best_fitness = np.inf\n            while self.current_budget < self.budget:\n                self.differential_evolution(func)\n                if self.best_solution is not None:\n                    self.local_search(func, self.best_solution)\n            if self.current_budget >= self.budget:\n                break\n        return self.best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution for global search with a local search strategy, designed to adaptively balance exploration and exploitation while incrementally increasing problem complexity for efficient layer-based optimization.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 20, in differential_evolution\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2169, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/_methods.py\", line 99, in _clip\n    return um.clip(a, min, max, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (2,) (10,) (10,) \n.", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) (10,) ')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\n  File \"<string>\", line 20, in differential_evolution\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2169, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/ian/anaconda3/envs/llm/lib/python3.10/site-packages/numpy/core/_methods.py\", line 99, in _clip\n    return um.clip(a, min, max, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (2,) (10,) (10,) \n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic optimization algorithm combining Differential Evolution (DE) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES) with adaptive layer-wise refinement for efficient exploration and exploitation in high-dimensional noisy optimization problems.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 124, in evaluatePhotonic\n    exec(code, globals())\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'cma'\n.", "error": "ModuleNotFoundError(\"No module named 'cma'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 124, in evaluatePhotonic\n    exec(code, globals())\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'cma'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "b0b05721-45c9-40cd-a045-ac41d5856f9e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.9 # Crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub):\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = minimize(func, best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Hybrid Adaptive Layered Optimization (HALO) combines differential evolution for global exploration with local refinement and adaptive layer addition to efficiently tackle high-dimensional noisy black box problems in photovoltaics.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 32, in _differential_evolution_step\nNameError: name 'func' is not defined\n.", "error": "NameError(\"name 'func' is not defined\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 32, in _differential_evolution_step\nNameError: name 'func' is not defined\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "8d7214f7-7643-4e56-a74f-11a1d4ac8e8e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Placeholder for modular structure detection (not fully implemented)\n        # In a real-case scenario, this method would analyze the layers and adapt strategies\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        # Gradually increase the dimensionality of the problem\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            # Local search on best individual\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            # Adapt dimensionality (if implementing a gradual increase)\n            self.dim = self.adapt_dimensionality(self.dim, target_dim=func.dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid adaptive metaheuristic leveraging Differential Evolution (DE) for global exploration and a local search phase with adaptive tuning, combined with layer-wise modular structure detection and dynamic dimensionality adaptation.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 70, in __call__\nAttributeError: 'ioh.iohcpp.problem.RealSingleObjectiveWrappedProbl' object has no attribute 'dim'\n.", "error": "AttributeError(\"'ioh.iohcpp.problem.RealSingleObjectiveWrappedProbl' object has no attribute 'dim'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 70, in __call__\nAttributeError: 'ioh.iohcpp.problem.RealSingleObjectiveWrappedProbl' object has no attribute 'dim'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "0943bab8-56af-45e0-96b1-b3aaaf9e8ab3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n      \n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=func)  # Ensures assignment is defined\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            current_layers += 1\n            self.dim = current_layers * 2  # Assuming each layer adds a new dimension pair\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic that merges Differential Evolution with Nelder-Mead and includes a fix for best_candidate assignment in the progressive layer optimization strategy for photonic structure optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "metadata": {}, "mutation_prompt": null}
{"id": "65c8084f-d25a-4c9e-8060-a59df01c80ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n      \n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=func, default=population[0])\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            current_layers += 1\n            self.dim = current_layers * 2  # Assuming each layer adds a new dimension pair\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration and Nelder-Mead for local refinement, enhanced with progressive layer introduction, adaptive robustness measures, and corrected best candidate assignment to optimize multilayer photonic structures.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "metadata": {}, "mutation_prompt": null}
{"id": "e398e40c-bd8b-4bb4-bfff-a29abca961da", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.current_budget = 0\n        self.pop_size = min(50, dim * 5)  # Population size for DE\n        self.population = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        self.fitness = np.full(self.pop_size, np.inf)\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            # Fix dimensionality handling in DE mutation\n            mutant_vector = np.clip(a + self.mutation_factor * (b - c)[:self.dim], func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.crossover_rate\n            trial_vector = np.where(crossover, mutant_vector, self.population[i])\n            trial_fitness = self.evaluate(func, trial_vector)\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial_vector\n                self.fitness[i] = trial_fitness\n                if trial_fitness < self.best_fitness:\n                    self.best_fitness = trial_fitness\n                    self.best_solution = trial_vector\n\n    def local_search(self, func, solution):\n        perturbation = np.random.normal(0, 0.1, self.dim)\n        new_solution = np.clip(solution + perturbation, func.bounds.lb, func.bounds.ub)\n        new_fitness = self.evaluate(func, new_solution)\n        if new_fitness < self.best_fitness:\n            self.best_solution = new_solution\n            self.best_fitness = new_fitness\n\n    def evaluate(self, func, solution):\n        if self.current_budget < self.budget:\n            self.current_budget += 1\n            return func(solution)\n        return np.inf\n\n    def __call__(self, func):\n        layers_step = max(2, self.dim // 10)\n        for current_dim in range(layers_step, self.dim + 1, layers_step):\n            self.population = np.random.uniform(-1, 1, (self.pop_size, current_dim))\n            self.fitness = np.full(self.pop_size, np.inf)\n            self.best_solution = None\n            self.best_fitness = np.inf\n            while self.current_budget < self.budget:\n                self.differential_evolution(func)\n                if self.best_solution is not None:\n                    self.local_search(func, self.best_solution)\n            if self.current_budget >= self.budget:\n                break\n        return self.best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimization algorithm refining dimensionality handling in DE mutation to improve performance in high-dimensional noisy optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (2,) (10,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (2,) (10,) (10,) ')", "parent_id": "1a18194f-a226-4d65-954f-8d1a1c00a410", "metadata": {}, "mutation_prompt": null}
{"id": "bc54f4dd-b05a-4826-88fd-288b62776948", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.9 # Crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):  # Added 'func' as a parameter\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = minimize(func, best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Enhanced robustness by correcting the incorrect use of a local variable, ensuring `func` is used in the differential evolution step to evaluate trial solutions.", "configspace": "", "generation": 1, "fitness": 0.8433676956735398, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.012. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "b0b05721-45c9-40cd-a045-ac41d5856f9e", "metadata": {"aucs": [0.828459990053326, 0.8436557437318404, 0.8579873532354527], "final_y": [0.1391576355867108, 0.13919517771094259, 0.13063785618548807]}, "mutation_prompt": null}
{"id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved HybridMetaheuristicOptimizer by fixing a dimensionality attribute access error and enhancing the modular structure detection mechanism.", "configspace": "", "generation": 1, "fitness": 0.8298340928247585, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.037. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "8d7214f7-7643-4e56-a74f-11a1d4ac8e8e", "metadata": {"aucs": [0.8267690064704629, 0.8764662020463148, 0.786267069957498], "final_y": [0.1306471634751013, 0.11743860950629603, 0.11531763794593508]}, "mutation_prompt": null}
{"id": "47f51c6a-eac1-45db-b8cf-77b940470ddb", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            if self.current_evaluations < self.budget:\n                # Noise-handling: evaluate the offspring multiple times and take the mean\n                if np.mean([func(offspring) for _ in range(3)]) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic optimization algorithm combining DE and CMA-ES with adaptive layer-wise refinement, enhanced by incorporating a noise-handling technique to improve stability in noisy environments.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "2733a23f-d9d5-419b-bf4f-9a46a5013e1c", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.95  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 5  # Start with fewer layers for gradual introduction\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refined the adaptive layer refinement strategy to include a gradual introduction of layers and adjusted the DE crossover probability for enhanced diversity in solution space.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "97e75dcc-77fd-49ba-8923-e66316831202", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        best_solution = func(best_solution)  # Pre-evaluate the initial best solution\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive layer refinement with pre-evaluation of the initial best solution to ensure efficient layer-based optimization.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "b679a835-9afd-44ad-ae6e-f8cff1f46aeb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=func)\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            current_layers += 1\n            self.dim = current_layers * 2  # Assuming each layer adds a new dimension pair\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        if self.evaluations < self.budget:\n            best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        else:\n            best_solution = None\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer through improved initialization and error handling, aiming for efficient exploration and robustness in multilayer photonic structure optimization.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "metadata": {}, "mutation_prompt": null}
{"id": "3d76cd6d-3fee-409b-99b1-5eb99069665f", "solution": "import numpy as np\n\ntry:\n    from cma import CMAEvolutionStrategy\nexcept ModuleNotFoundError:\n    CMAEvolutionStrategy = None\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget or CMAEvolutionStrategy is None:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.5, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n            es.disp()\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Improved robustness by adding a check for the presence of the 'cma' module and handling the exception gracefully.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "c28182f6-f09f-4c70-beaa-fb37a9fb2555", "solution": "import numpy as np\nfrom cma import CMAEvolutionStrategy\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 20\n        self.de_mutation_factor = 0.8\n        self.de_crossover_prob = 0.9\n        self.current_evaluations = 0\n\n    def differential_evolution(self, func, pop, bounds):\n        new_pop = np.copy(pop)\n        for i in range(len(pop)):\n            if self.current_evaluations >= self.budget:\n                break\n            # Randomly select three other individuals\n            indices = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[indices]\n            # Create a mutant vector\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.de_crossover_prob\n            offspring = np.where(crossover_mask, mutant, pop[i])\n            # Selection\n            if self.current_evaluations < self.budget:\n                if func(offspring) < func(pop[i]):\n                    new_pop[i] = offspring\n                self.current_evaluations += 1\n        return new_pop\n\n    def cma_es_refinement(self, func, x0, bounds):\n        if self.current_evaluations >= self.budget:\n            return x0\n        es = CMAEvolutionStrategy(x0, 0.4, {'bounds': [bounds.lb, bounds.ub], 'popsize': self.population_size * 2})\n        while not es.stop() and self.current_evaluations < self.budget:\n            solutions = es.ask()\n            fitnesses = [func(sol) for sol in solutions]\n            self.current_evaluations += len(solutions)\n            es.tell(solutions, fitnesses)\n        return es.result[0]\n\n    def adaptive_layer_refinement(self, func, bounds):\n        layers = 10\n        best_solution = np.random.uniform(bounds.lb, bounds.ub, self.dim)\n        while layers <= self.dim and self.current_evaluations < self.budget:\n            # DE phase\n            pop = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, layers))\n            pop = self.differential_evolution(func, pop, bounds)\n            best_solution = pop[np.argmin([func(ind) for ind in pop])]\n            \n            # CMA-ES phase\n            best_solution = self.cma_es_refinement(func, best_solution, bounds)\n            \n            # Increase complexity by adding layers\n            layers = min(layers + 5, self.dim)\n        \n        return best_solution\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best_solution = self.adaptive_layer_refinement(func, bounds)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid optimization algorithm combining DE and CMA-ES with enhanced layer-wise refinement and adaptive exploration-exploitation balance for high-dimensional noisy problems.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: ModuleNotFoundError(\"No module named 'cma'\").", "error": "ModuleNotFoundError(\"No module named 'cma'\")", "parent_id": "6a9c60a4-a768-46a2-a626-3d12fb074f42", "metadata": {}, "mutation_prompt": null}
{"id": "4addc517-43a5-4e36-8afc-338930d503bd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def differential_evolution_step(self, population, func, bounds, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(len(population)):\n            if self.evaluations >= self.budget:\n                break\n            indices = np.random.choice(len(population), 3, replace=False)\n            a, b, c = population[indices]\n            mutant = np.clip(a + F * (b - c), bounds.lb, bounds.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, population[i])\n            trial_fitness = func(trial)\n            self.evaluations += 1\n            if trial_fitness < func(population[i]):\n                new_population[i] = trial\n        return new_population\n\n    def local_refinement(self, candidate, func, bounds):\n        if self.evaluations >= self.budget:\n            return candidate\n        result = minimize(func, candidate, method='Nelder-Mead', bounds=bounds,\n                          options={'maxiter': 10, 'disp': False})\n        self.evaluations += result.nfev\n        return result.x if result.success else candidate\n\n    def progressive_layer_optimization(self, func, bounds, max_layers):\n        population_size = 10\n        current_layers = 10\n        population = np.random.uniform(bounds.lb, bounds.ub, (population_size, self.dim))\n\n        while self.evaluations < self.budget and current_layers <= max_layers:\n            population = self.differential_evolution_step(population, func, bounds)\n            best_candidate = min(population, key=lambda ind: func(ind))\n            self.evaluations += 1\n            best_candidate = self.local_refinement(best_candidate, func, bounds)\n            self.dim = min(current_layers * 2 + 2, max_layers * 2)  # Enhance layer increment logic\n            current_layers += 1\n\n        return best_candidate\n\n    def __call__(self, func):\n        bounds = func.bounds\n        max_layers = self.dim // 2\n        best_solution = self.progressive_layer_optimization(func, bounds, max_layers)\n        return best_solution", "name": "HybridMetaheuristicOptimizer", "description": "Refined HybridMetaheuristicOptimizer with fixed best candidate initialization and enhanced progressive layer adjustment.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\").", "error": "UnboundLocalError(\"local variable 'best_candidate' referenced before assignment\")", "parent_id": "2e1960ec-d5c4-4aaa-bb71-aef0a4607a65", "metadata": {}, "mutation_prompt": null}
{"id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved layer-based optimization by refining local search strategy using adaptive bounds.", "configspace": "", "generation": 3, "fitness": 0.8357854598217216, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.058. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "metadata": {"aucs": [0.755115721391527, 0.8639380610132642, 0.8883025970603735], "final_y": [0.16482810497946265, 0.11855424214845334, 0.11252984710218072]}, "mutation_prompt": null}
{"id": "c29a8655-e0c2-478a-a78a-a42a0b8c9cf2", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.9 # Crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb - 0.1 * (ub - lb), ub + 0.1 * (ub - lb), (self.population_size, self.dim))  # Diversified initialization\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):  # Added 'func' as a parameter\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = differential_evolution(func, bounds=[(lb[i], ub[i]) for i in range(self.dim)], maxiter=10, polish=False)  # Using DE for noisy local refinement\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Improved HALO by enhancing the local refinement phase using a more suitable optimization method for noisy functions and adjusting the population initialization to ensure diverse solutions.", "configspace": "", "generation": 3, "fitness": 0.8099425693445585, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.021. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "bc54f4dd-b05a-4826-88fd-288b62776948", "metadata": {"aucs": [0.8397333848135758, 0.7955905846744983, 0.7945037385456013], "final_y": [0.12445868302892416, 0.1288586721003424, 0.14919383729303382]}, "mutation_prompt": null}
{"id": "851e79e2-8729-4158-8426-137fcd918c8a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced Differential Evolution by adjusting mutation factor and incorporating elitism to improve convergence and solution quality.", "configspace": "", "generation": 3, "fitness": 0.8733299996636849, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.022. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "metadata": {"aucs": [0.8453267136633534, 0.8743949853325829, 0.9002682999951185], "final_y": [0.1256755350269676, 0.11490043736864297, 0.10999641391330417]}, "mutation_prompt": null}
{"id": "d695641e-b382-43e2-ab74-a79de37e2abd", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 30 + dim  # Dynamic population size based on problem dimension\n        self.F = 0.8\n        self.CR = 0.7 + 0.2 * (dim / budget)  # Adaptive crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))  # More focused initialization\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = differential_evolution(func, bounds=[(lb[i], ub[i]) for i in range(self.dim)], maxiter=10, polish=False)\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Enhanced HALO by introducing a dynamic population size and adaptive crossover rate to improve exploration and exploitation balance.", "configspace": "", "generation": 4, "fitness": 0.8023677611577469, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.802 with standard deviation 0.005. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "c29a8655-e0c2-478a-a78a-a42a0b8c9cf2", "metadata": {"aucs": [0.808689117283021, 0.7967023692093704, 0.8017117969808496], "final_y": [0.1285937053735462, 0.14587762493708045, 0.13548532582431028]}, "mutation_prompt": null}
{"id": "0403ab62-e57e-407e-9436-4deaca37f1ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        # Adaptive mutation factor and crossover probability\n        self.F = 0.5 + 0.3 * np.random.rand()\n        self.CR = 0.8 + 0.1 * np.random.rand()\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def robustness_check(self, func, best_solution, bounds):\n        perturbation = np.random.normal(0, 0.01, size=best_solution.shape)\n        perturbed_solution = best_solution + perturbation\n        perturbed_solution = np.clip(perturbed_solution, bounds.lb, bounds.ub)\n        return func(perturbed_solution)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n                # Perturbation-based robustness check\n                perturb_score = self.robustness_check(func, refined_solution, bounds)\n                if perturb_score < refined_score:\n                    self.scores[best_idx] = perturb_score\n            \n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced the optimization by incorporating adaptive mutation and crossover in DE and introducing a perturbation-based robustness check.", "configspace": "", "generation": 4, "fitness": 0.7751564441183038, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.010. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7773143123297447, 0.786646665664334, 0.7615083543608323], "final_y": [0.15012870946666845, 0.1574961695205792, 0.16547492986276247]}, "mutation_prompt": null}
{"id": "629faf6c-ebea-4616-9055-b901df6fa7bb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.5  # Changed from 0.9 to 0.5 for adaptive crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):  # Added 'func' as a parameter\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = minimize(func, best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Enhanced HALO by introducing an adaptive crossover probability to balance exploration and exploitation dynamically.", "configspace": "", "generation": 4, "fitness": 0.8296720921365358, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.031. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "bc54f4dd-b05a-4826-88fd-288b62776948", "metadata": {"aucs": [0.8727094629508108, 0.8156150010486138, 0.8006918124101828], "final_y": [0.12037362367284243, 0.13229063307229305, 0.1481169059808729]}, "mutation_prompt": null}
{"id": "b605ea0f-0e24-48d3-beeb-b87fa6aa8587", "solution": "import numpy as np\nfrom scipy.optimize import differential_evolution\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.85  # Differential evolution scale factor (adjusted)\n        self.CR = 0.9 # Crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb - 0.1 * (ub - lb), ub + 0.1 * (ub - lb), (self.population_size, self.dim))  # Diversified initialization\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):  # Added 'func' as a parameter\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        best_idx = np.argmin(self._evaluate_population(func))\n        best_solution = self.pop[best_idx]\n        result = differential_evolution(func, bounds=[(lb[i], ub[i]) for i in range(self.dim)], maxiter=10, polish=False)  # Using DE for noisy local refinement\n        if result.success and func(result.x) < func(best_solution):\n            self.pop[best_idx] = result.x\n            self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            if self.func_evals < self.budget // 2:\n                self._local_refinement(func, lb, ub)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Improved the HALO algorithm by adjusting the differential evolution scale factor to enhance exploration capabilities.", "configspace": "", "generation": 4, "fitness": 0.8036241498944247, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.804 with standard deviation 0.010. And the mean value of best solutions found was 0.141 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c29a8655-e0c2-478a-a78a-a42a0b8c9cf2", "metadata": {"aucs": [0.7928660502598305, 0.8014203941920448, 0.8165860052313987], "final_y": [0.14185857368928623, 0.14227739783683002, 0.13825670479951246]}, "mutation_prompt": null}
{"id": "427b11eb-9a60-47bb-ba56-c1e4dbc5ffb1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.5 + np.random.rand() * 0.5  # Adaptive DE mutation factor\n        self.CR = 0.8 + np.random.rand() * 0.2  # Adaptive DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                with ThreadPoolExecutor() as executor:\n                    future = executor.submit(self.local_search, func, best_individual, bounds)\n                    refined_solution, refined_score = future.result()\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration-exploitation balance by adaptive mutation and crossover rates and introducing parallel local searches.", "configspace": "", "generation": 4, "fitness": 0.771125153754345, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.018. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7597882862497648, 0.7968690946556014, 0.7567180803576687], "final_y": [0.1578784966371678, 0.14185127619609494, 0.15374384957458465]}, "mutation_prompt": null}
{"id": "bb4be052-2c83-4180-bb67-91c769f2648c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n                self.F = min(1.0, self.F + 0.05)  # Dynamic adaptation of F\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation strategy by introducing dynamic adaptation based on success rates of past generations.", "configspace": "", "generation": 5, "fitness": 0.8082739477424529, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.808 with standard deviation 0.022. And the mean value of best solutions found was 0.142 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.7784414689499943, 0.8314602843886792, 0.8149200898886855], "final_y": [0.15502763482363413, 0.133176384216512, 0.1367218892451637]}, "mutation_prompt": null}
{"id": "096d5182-2304-406d-b357-3c84644c6022", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(min(100, 10 * dim), dim)  # Adaptive population size\n        self.scores = np.full(self.population.shape[0], np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.population.shape[0]):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.population.shape[0]):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.population.shape[0]) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by introducing an adaptive population size based on the current evaluation budget.", "configspace": "", "generation": 5, "fitness": 0.8025149115550398, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.026. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7977780831778373, 0.7738997291025193, 0.8358669223847632], "final_y": [0.14676533191087082, 0.1538454819032281, 0.13130247615310464]}, "mutation_prompt": null}
{"id": "f027c278-2265-49dc-8141-33ff4eb7b3ba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced robustness and exploration through adaptive population diversity control and improved local search integration.", "configspace": "", "generation": 5, "fitness": 0.8588856755515214, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.009. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "metadata": {"aucs": [0.8711350754895022, 0.8572521802743732, 0.8482697708906887], "final_y": [0.1154596403241469, 0.12278307239214503, 0.126996708347776]}, "mutation_prompt": null}
{"id": "0c0f9a0f-4cf4-4d1f-a541-b2209743ef52", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.3  # Adapt F\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by integrating adaptive F and CR parameters into differential evolution for better convergence.", "configspace": "", "generation": 5, "fitness": 0.8469688921982318, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.018. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "9944bff1-f45d-4548-bd40-3fd1f5ced2e2", "metadata": {"aucs": [0.8426589101749061, 0.8702603474266233, 0.8279874189931662], "final_y": [0.12665380362850298, 0.11757503386724621, 0.13670563605805452]}, "mutation_prompt": null}
{"id": "8a5a6e45-eb10-4784-b79e-31781c9668a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HALO:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 50\n        self.F = 0.8  # Differential evolution scale factor\n        self.CR = 0.5  # Changed from 0.9 to 0.5 for adaptive crossover probability\n        self.pop = None\n        self.func_evals = 0\n\n    def _initialize_population(self, lb, ub):\n        self.pop = np.random.uniform(lb, ub, (self.population_size, self.dim))\n\n    def _evaluate_population(self, func):\n        fitness = np.array([func(ind) for ind in self.pop])\n        self.func_evals += self.population_size\n        return fitness\n\n    def _differential_evolution_step(self, fitness, lb, ub, func):\n        next_pop = np.empty_like(self.pop)\n        for i in range(self.population_size):\n            indices = np.random.choice(self.population_size, 3, replace=False)\n            a, b, c = self.pop[indices]\n            self.F = 0.5 + 0.3 * (1 - self.func_evals / self.budget)  # Adaptive mutation scaling\n            mutant = np.clip(a + self.F * (b - c), lb, ub)\n            cross_points = np.random.rand(self.dim) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.pop[i])\n            trial_fitness = func(trial)\n            self.func_evals += 1\n            if trial_fitness < fitness[i]:\n                next_pop[i] = trial\n            else:\n                next_pop[i] = self.pop[i]\n        self.pop = next_pop\n\n    def _local_refinement(self, func, lb, ub):\n        if self.func_evals < (3 * self.budget) // 4:  # Adjusted local refinement frequency\n            best_idx = np.argmin(self._evaluate_population(func))\n            best_solution = self.pop[best_idx]\n            result = minimize(func, best_solution, bounds=[(lb[i], ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n            if result.success and func(result.x) < func(best_solution):\n                self.pop[best_idx] = result.x\n                self.func_evals += 1\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self._initialize_population(lb, ub)\n        fitness = self._evaluate_population(func)\n\n        while self.func_evals < self.budget:\n            self._differential_evolution_step(fitness, lb, ub, func)\n            fitness = self._evaluate_population(func)\n\n        best_idx = np.argmin(fitness)\n        return self.pop[best_idx]", "name": "HALO", "description": "Enhanced HALO by adjusting local refinement frequency and introducing adaptive mutation scaling for improved exploration-exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.8180500383792353, "feedback": "The algorithm HALO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.818 with standard deviation 0.011. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "629faf6c-ebea-4616-9055-b901df6fa7bb", "metadata": {"aucs": [0.8026359294137815, 0.8233679546722035, 0.828146231051721], "final_y": [0.1513343159289644, 0.13331122496725223, 0.1433044930385009]}, "mutation_prompt": null}
{"id": "960fcbb3-78a4-4978-9d3e-b6b7c21a4b79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        adaptive_bounds = list(zip(bounds.lb * 0.9, bounds.ub * 1.1))  # Modified line\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=adaptive_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Improved solution quality by enhancing the local search step with adaptive bounds scaling.", "configspace": "", "generation": 6, "fitness": 0.8049208371043092, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.805 with standard deviation 0.036. And the mean value of best solutions found was 0.144 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.8545021955417702, 0.7919098067848231, 0.7683505089863345], "final_y": [0.12492293036687496, 0.15150708413727154, 0.15695249389771604]}, "mutation_prompt": null}
{"id": "de6e0f31-abc1-4fe1-8914-9f7953cd998e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                denorm = self.denormalize(self.population[i], func.bounds)\n                self.scores[i] = func(denorm)\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.5\n            self.CR = 0.7 + np.random.rand() * 0.3\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = self.denormalize(trial, bounds)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def denormalize(self, individual, bounds):\n        return bounds.lb + individual * (bounds.ub - bounds.lb)\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = self.denormalize(individual, bounds)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n    \n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim and self.evaluations % 50 == 0:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = self.denormalize(self.population[best_idx], bounds)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Integrating adaptive mutation and crossover with modularity detection and staged dimensionality growth for robust solution convergence.", "configspace": "", "generation": 6, "fitness": 0.8235285890672507, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.824 with standard deviation 0.030. And the mean value of best solutions found was 0.137 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "0c0f9a0f-4cf4-4d1f-a541-b2209743ef52", "metadata": {"aucs": [0.8618939235029561, 0.8212768045266877, 0.7874150391721083], "final_y": [0.11716513883326907, 0.13753668833967347, 0.1567498778894313]}, "mutation_prompt": null}
{"id": "d4aafe8b-0047-440d-898a-22640d8767db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.7 + 0.2 * np.random.rand()  # Dynamically adjust DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.005:  # Enhanced variance threshold\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced differential evolution by dynamically adjusting crossover probability and integrating variance-based segment detection.", "configspace": "", "generation": 6, "fitness": 0.7751827501947238, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.019. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.792123456653679, 0.7491992509654856, 0.7842255429650069], "final_y": [0.1452594112217216, 0.16766396069332623, 0.14835540673490322]}, "mutation_prompt": null}
{"id": "d360863d-44e9-44f6-8716-3ffb2abb833d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.005:  # Adjusted threshold for better detection\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def adapt_parameters(self):\n        self.F = max(0.5, min(1.0, self.F + 0.1 * np.random.randn()))  # Adaptive F adjustment\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            self.adapt_parameters()  # Integrate adaptive parameter adjustment\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refined strategy integrating adaptive control for DE parameters and enhanced modular structure detection.", "configspace": "", "generation": 6, "fitness": 0.7655612781885038, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.766 with standard deviation 0.005. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7679577113583833, 0.7587089560063773, 0.7700171672007508], "final_y": [0.15172176385096303, 0.1541110296294529, 0.15863525459471406]}, "mutation_prompt": null}
{"id": "cc361482-a458-4210-9e1c-a7fd8fc1812c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.3  # Adapt F\n            learning_rate = 1.0 - (self.evaluations / self.budget)  # Adaptive learning rate\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + learning_rate * (trial * (bounds.ub - bounds.lb))  # Apply learning rate\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Integrate adaptive learning rate in differential evolution to enhance convergence speed and solution quality.", "configspace": "", "generation": 6, "fitness": 0.7800150251542356, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.051. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "0c0f9a0f-4cf4-4d1f-a541-b2209743ef52", "metadata": {"aucs": [0.8502237368420615, 0.7584023886279003, 0.7314189499927448], "final_y": [0.1233159760076189, 0.14372225012068018, 0.1732193506914651]}, "mutation_prompt": null}
{"id": "831fed62-fe1b-4519-ad06-6d2125ee0ff2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # Adjusted DE mutation factor for better balance\n        self.CR = 0.6  # Adjusted crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            F_dynamic = np.random.uniform(0.5, self.F)  # Dynamic mutation factor\n            mutant = np.clip(a + F_dynamic * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Improved balance between exploration and exploitation by introducing adaptive mutation and crossover rates, and enhancing local search efficiency.", "configspace": "", "generation": 7, "fitness": 0.8404741799068348, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.031. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.8816047820359298, 0.8315655695838102, 0.8082521881007646], "final_y": [0.1168125245196121, 0.13391259713782944, 0.14235737415155614]}, "mutation_prompt": null}
{"id": "7fe638c6-bcfc-416d-970a-7e1ebc65193f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population, axis=0).mean()  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation by dynamically adjusting based on population diversity to improve solution quality.", "configspace": "", "generation": 7, "fitness": 0.8631482091362642, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.026. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "f027c278-2265-49dc-8141-33ff4eb7b3ba", "metadata": {"aucs": [0.8272115684178611, 0.8864357171224745, 0.8757973418684568], "final_y": [0.13317575732393339, 0.11602974249774578, 0.1193655799694725]}, "mutation_prompt": null}
{"id": "851304dc-c239-4932-939b-af80186738e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population, axis=0).mean()\n        self.F = 0.5 + 0.3 * (1 - diversity)  # Adaptive mutation factor based on diversity\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        layer_bounds = [(max(lb, val - 0.1), min(ub, val + 0.1)) for lb, ub, val in zip(bounds.lb, bounds.ub, individual_denorm)]\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=layer_bounds)\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration by introducing adaptive mutation factor in differential evolution based on population diversity.", "configspace": "", "generation": 7, "fitness": 0.8100377356659433, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.810 with standard deviation 0.056. And the mean value of best solutions found was 0.143 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "39318d8e-c48b-4dc2-ae98-c8e634e7484c", "metadata": {"aucs": [0.7552983072733397, 0.7879614935191466, 0.8868534062053438], "final_y": [0.16502089772180706, 0.15076654550788904, 0.1117618770381279]}, "mutation_prompt": null}
{"id": "9e135b1c-daa4-4e7d-838d-405f60372df8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.3  # Adapt F\n            self.CR = 0.8 + 0.2 * np.random.rand()  # Dynamic CR\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced balance between exploration and exploitation by introducing adaptive population size and dynamic crossover strategy.", "configspace": "", "generation": 7, "fitness": 0.8655725553651363, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.000. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "0c0f9a0f-4cf4-4d1f-a541-b2209743ef52", "metadata": {"aucs": [0.8657189690369282, 0.8653183368499274, 0.8656803602085534], "final_y": [0.11682198099812935, 0.12133065013858513, 0.11562445351998007]}, "mutation_prompt": null}
{"id": "dffd9294-b860-473c-9b22-12aa95252f70", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search by adjusting method to 'trust-constr' for improved convergence.", "configspace": "", "generation": 7, "fitness": 0.8503128970749403, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.022. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "f027c278-2265-49dc-8141-33ff4eb7b3ba", "metadata": {"aucs": [0.8212644763472812, 0.8730075385495576, 0.8566666763279824], "final_y": [0.13302422200110064, 0.11622917806710553, 0.12360794847044576]}, "mutation_prompt": null}
{"id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive crossover probability based on population diversity to enhance solution quality.", "configspace": "", "generation": 8, "fitness": 0.8767628949658518, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.877 with standard deviation 0.005. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "dffd9294-b860-473c-9b22-12aa95252f70", "metadata": {"aucs": [0.8698955277260721, 0.8823906159745766, 0.8780025411969066], "final_y": [0.11827487887720545, 0.11706912808476244, 0.11488973560673499]}, "mutation_prompt": null}
{"id": "1936ee1d-2e94-44de-ae30-56bcb52b9b65", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * np.std(self.population)  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved mutation factor adaptation for more effective exploration and exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.8470861194225533, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.025. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "f027c278-2265-49dc-8141-33ff4eb7b3ba", "metadata": {"aucs": [0.8498388019519638, 0.8155364433620383, 0.8758831129536578], "final_y": [0.12638665389089476, 0.12884206498737483, 0.11936609820085842]}, "mutation_prompt": null}
{"id": "d4373a9e-8327-4321-886e-1eb15ce94a14", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        elite_idx = np.argmin(self.scores)  # Added line to preserve top solutions\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget or i == elite_idx:\n                break  # Ensure elite solution is preserved\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced elitism by preserving top solutions across generations to improve convergence speed.", "configspace": "", "generation": 8, "fitness": 0.8394478437434452, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.839 with standard deviation 0.036. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.8016627408146298, 0.8295170168362191, 0.8871637735794865], "final_y": [0.13953098700030486, 0.13503510077223835, 0.11176883220266087]}, "mutation_prompt": null}
{"id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.4  # Increase F range\n            self.CR = 0.7 + 0.3 * np.random.rand()  # Adjust CR range\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='Powell', bounds=list(zip(bounds.lb, bounds.ub)))  # Use Powell method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive local search integration and fine-tuned mutation strategy to improve exploration and exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.8819648251438771, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.014. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "9e135b1c-daa4-4e7d-838d-405f60372df8", "metadata": {"aucs": [0.8633157573307395, 0.886121333592357, 0.8964573845085346], "final_y": [0.11741242719283895, 0.11602711837802027, 0.11163410876986402]}, "mutation_prompt": null}
{"id": "1ef07468-72b7-48f9-b7c7-8f43e54b34e9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * np.std(self.population, axis=0).mean()  # Adapt mutation factor\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation and refined local search by adjusting based on population diversity and using alternative optimization method for improved solution quality.", "configspace": "", "generation": 8, "fitness": 0.8726561831850502, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.013. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "7fe638c6-bcfc-416d-970a-7e1ebc65193f", "metadata": {"aucs": [0.8866066199833994, 0.8546594870681706, 0.8767024425035805], "final_y": [0.11196015421074679, 0.12100481420908493, 0.11531789099840839]}, "mutation_prompt": null}
{"id": "07c2c964-0a44-438b-b597-cfab0d1911b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.4 + np.random.rand() * 0.5  # Modify F range for more adaptability\n            self.CR = 0.6 + 0.4 * np.random.rand()  # Adjust CR range for dynamic crossover\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='Powell', bounds=list(zip(bounds.lb, bounds.ub)))  # Use Powell method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive mutation factor adjustment and dynamic crossover to enhance exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": 0.8403775200391812, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.026. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.8044258661562401, 0.8629557464538627, 0.8537509475074408], "final_y": [0.13537633672592342, 0.1200453776448539, 0.12313319551216406]}, "mutation_prompt": null}
{"id": "be9226cd-dce8-40d0-ba00-9c0fb573ce41", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * np.std(self.population, axis=0).mean()  # Adapt mutation factor\n            self.CR = np.mean(self.scores) / (np.std(self.scores) + 1e-8)  # Dynamic crossover strategy\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation and local search integration by introducing a dynamic crossover strategy based on current evolution success to improve solution quality.", "configspace": "", "generation": 9, "fitness": 0.8573506701725883, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.015. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "1ef07468-72b7-48f9-b7c7-8f43e54b34e9", "metadata": {"aucs": [0.8369479260866394, 0.8615353059219873, 0.8735687785091378], "final_y": [0.11889317441579672, 0.11836876513578143, 0.11562211855157722]}, "mutation_prompt": null}
{"id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)  # Adapt mutation factor based on fitness variance\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved convergence by introducing adaptive F and CR based on fitness variance.", "configspace": "", "generation": 9, "fitness": 0.8811960294783878, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.014. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.86210139586791, 0.886201743530128, 0.8952849490371253], "final_y": [0.12047515982366708, 0.11280370858096356, 0.11162669748550225]}, "mutation_prompt": null}
{"id": "888af7bb-9a4c-4433-8d7f-1a60cf2bcd9a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.3  # Adapt F\n            self.CR = 0.8 + 0.2 * np.random.rand()  # Dynamic CR\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c) + 0.1 * (np.random.rand(self.dim) - 0.5), 0, 1)  # Added diversity\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration by modifying the mutation strategy to consider more diverse candidate solutions.", "configspace": "", "generation": 9, "fitness": 0.8430406181881457, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.021. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9e135b1c-daa4-4e7d-838d-405f60372df8", "metadata": {"aucs": [0.8254755332670043, 0.8307064028517137, 0.8729399184457192], "final_y": [0.13594241033076915, 0.13391165390511006, 0.11936340833562509]}, "mutation_prompt": null}
{"id": "31d80c54-c460-427b-9d89-7314aed88413", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            top_indices = self.scores.argsort()[:max(1, self.pop_size // 10)]\n            top_diversity = np.std(self.population[top_indices], axis=0).mean()  # Diversity in top individuals\n            self.F = 0.5 + 0.4 * top_diversity  # Adapt mutation factor based on top diversity\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if np.random.rand() < 0.7:  # Probabilistic trust region approach\n            perturbed_individual = individual_denorm + np.random.uniform(-0.01, 0.01, size=self.dim)\n            perturbed_individual = np.clip(perturbed_individual, bounds.lb, bounds.ub)\n            perturbed_score = func(perturbed_individual)\n            if perturbed_score < result.fun:\n                return perturbed_individual, perturbed_score\n        return (result.x, result.fun) if result.success else (individual_denorm, func(individual_denorm))\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation factor adaptation by incorporating diversity within top individuals and refined local search using a probabilistic trust region approach. ", "configspace": "", "generation": 9, "fitness": 0.8519126388256691, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.029. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "1ef07468-72b7-48f9-b7c7-8f43e54b34e9", "metadata": {"aucs": [0.8578462959080333, 0.8841301089329308, 0.8137615116360432], "final_y": [0.12132363915430355, 0.11602564839288765, 0.13130462670352216]}, "mutation_prompt": null}
{"id": "45eb0df6-4ffd-40a6-80c3-0d575ef0316e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.6 + 0.3 * np.std(self.population, axis=0).mean()  # Adjusted mutation factor range\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))  # Changed method\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.02:  # Adjusted threshold\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Adaptive hybrid optimizer combining dynamic group-wise mutation with noise-resilient local refinement strategies to enhance performance and robustness.", "configspace": "", "generation": 10, "fitness": 0.8452565416258716, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.032. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "1ef07468-72b7-48f9-b7c7-8f43e54b34e9", "metadata": {"aucs": [0.8452678950787044, 0.8062967493646388, 0.8842049804342715], "final_y": [0.13230358287893318, 0.14033588833797128, 0.11465502970219899]}, "mutation_prompt": null}
{"id": "79dd4397-59a5-49e4-947f-cd5d2e5da1b7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            F_dynamic = self.F * (1 - (self.evaluations / self.budget))  # Adaptive mutation factor\n            mutant = np.clip(a + F_dynamic * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            if self.evaluations % 10 == 0:  # More frequent modular structure detection\n                self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Refined integration of adaptive mutation factor and layer-wise optimization to enhance solution quality while preserving modular structures.", "configspace": "", "generation": 10, "fitness": 0.8470148340870963, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.016. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.8671605827634905, 0.8456568806429852, 0.8282270388548132], "final_y": [0.1233693386777659, 0.12430185044224551, 0.13300904615620446]}, "mutation_prompt": null}
{"id": "9063ce01-c05a-43a1-b3e9-74a42b5c6408", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * diversity  # Adapt mutation factor based on diversity\n            self.CR = np.clip(0.5 + 0.3 * diversity, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced population diversity management and adaptive local search integration to improve convergence and solution quality.", "configspace": "", "generation": 10, "fitness": 0.873605753196193, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.008. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.8759845356565938, 0.8633219397731584, 0.8815107841588271], "final_y": [0.11711004740699138, 0.12261289168441458, 0.10960406080680574]}, "mutation_prompt": null}
{"id": "ede60589-0481-4b95-8c6d-bb791dc0e946", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            self.pop_size = max(20, self.pop_size - 1)  # Adaptive reduction of population size\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive reduction of the population size to focus exploration-convergence balance.", "configspace": "", "generation": 10, "fitness": 0.8354164386728099, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.036. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.7938436862131855, 0.8309068717817308, 0.8814987580235134], "final_y": [0.14831081495218568, 0.12836374464137879, 0.11252871197363301]}, "mutation_prompt": null}
{"id": "64264a44-4189-4601-9d05-b540b0f2cf53", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * (np.std(self.scores) / np.mean(self.scores))  # Modify F to use score ratio\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced DE mutation factor adaptation and introduced adaptive population size to improve exploration-exploitation trade-off.", "configspace": "", "generation": 10, "fitness": 0.8690649666359594, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.869 with standard deviation 0.014. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8852650795569288, 0.8717712113342985, 0.850158609016651], "final_y": [0.10960406081538887, 0.11419311471333116, 0.12345361026897184]}, "mutation_prompt": null}
{"id": "7584318f-889a-431f-9ec3-cce788f856cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.9  # Increased DE mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n        self.success_rate = 0.5  # Initialize success rate for adaptive F\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = self.success_rate * 0.8 + 0.1  # Adapt F based on success rate\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n                self.success_rate = (self.success_rate + 1) / 2  # Update success rate\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        best_idx_overall = np.argmin(self.scores)\n        best_score_overall = self.scores[best_idx_overall]\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if best_score_overall > self.scores[best_idx]:\n                best_score_overall = self.scores[best_idx]\n                best_idx_overall = best_idx\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_solution = bounds.lb + self.population[best_idx_overall] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx_overall]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance mutation strategy by incorporating parameter adaptation based on historical success rates.", "configspace": "", "generation": 11, "fitness": 0.8561605922059953, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.030. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "851e79e2-8729-4158-8426-137fcd918c8a", "metadata": {"aucs": [0.8768210907366383, 0.8785230649016038, 0.8131376209797436], "final_y": [0.1144093930675778, 0.1141970840789569, 0.13963570453648688]}, "mutation_prompt": null}
{"id": "8568db44-e3a7-4b49-9742-8f209f24d2a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * diversity  # Adapt mutation factor based on diversity\n            self.CR = np.clip(0.5 + 0.3 * diversity, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        step_size = 1.0 / (1 + 0.1 * self.evaluations)  # Adaptive step size\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'eps': step_size})\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            if self.evaluations % 10 == 0:  # Dynamic refinement frequency\n                best_idx = np.argmin(self.scores)\n                best_individual = self.population[best_idx]\n                if self.evaluations < self.budget:\n                    refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                    if refined_score < self.scores[best_idx]:\n                        self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                        self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved local search strategy by incorporating adaptive step sizes and dynamic refinement frequency based on evaluation progress.", "configspace": "", "generation": 11, "fitness": 0.8484235509476945, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.033. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "9063ce01-c05a-43a1-b3e9-74a42b5c6408", "metadata": {"aucs": [0.8410061948007892, 0.81189452532298, 0.8923699327193141], "final_y": [0.12308930362665216, 0.14071744667522057, 0.11012146232977305]}, "mutation_prompt": null}
{"id": "9072a45b-7de4-4496-b867-5a3aace599a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + 0.4 * np.mean(self.population, axis=0)[i % self.dim]  # Layer-specific F\n            self.CR = 0.7 + 0.3 * np.random.rand()\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))  # Use L-BFGS-B method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Optimized exploration by introducing adaptive layer-specific mutation and improving local refinement through a dynamic local search strategy.", "configspace": "", "generation": 11, "fitness": 0.8423874325082349, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.032. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.8006992723153652, 0.8491777245696694, 0.87728530063967], "final_y": [0.14331771416733596, 0.12132687952198173, 0.11936396664755011]}, "mutation_prompt": null}
{"id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Adaptive mutation and crossover based on fitness improvement to enhance solution diversity and convergence.", "configspace": "", "generation": 11, "fitness": 0.8777531280361242, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.019. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "9063ce01-c05a-43a1-b3e9-74a42b5c6408", "metadata": {"aucs": [0.8502638614306415, 0.8909225841479369, 0.8920729385297942], "final_y": [0.1266735775531136, 0.11012525147572216, 0.11196526904007942]}, "mutation_prompt": null}
{"id": "971bd855-50bf-4388-8642-fad44c999a63", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c) + 0.1 * (np.mean(self.population, axis=0) - a), 0, 1)  # Added mean-based strategy\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration by fine-tuning the mutation strategy using the mean population value.", "configspace": "", "generation": 11, "fitness": 0.8437593533448157, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.032. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.8694262057828023, 0.7979350661921174, 0.8639167880595277], "final_y": [0.1230646839612578, 0.14523341385334554, 0.11874029480727977]}, "mutation_prompt": null}
{"id": "f6a4aaf3-b871-4a94-a52c-bf404ffdf60f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        kmeans = KMeans(n_clusters=max(2, self.pop_size // 10)).fit(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  \n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Adaptive diversity enhancement through clustering to promote solution exploration and exploitation.", "configspace": "", "generation": 12, "fitness": 0.8752692865834463, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.009. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.885618557662832, 0.8639642617655688, 0.876225040321938], "final_y": [0.1119687889295381, 0.1168005145284825, 0.11474850928239422]}, "mutation_prompt": null}
{"id": "e6cc7fcd-7573-4587-bc38-eb44f0de2e51", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.3 * improvement + 0.1 * np.random.rand()  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce dynamic adjustment of the mutation factor to improve exploration and convergence.", "configspace": "", "generation": 12, "fitness": 0.8424667443330699, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.020. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.8657787426932995, 0.8449252374646279, 0.8166962528412823], "final_y": [0.12336449470846944, 0.12541981613474484, 0.13775174138584856]}, "mutation_prompt": null}
{"id": "5998ac12-9ce7-48ac-ae11-2361b61feb79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1  # Slower dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced balance of exploration and exploitation by introducing adaptive dimensionality increase and improved local search.", "configspace": "", "generation": 12, "fitness": 0.8455862730608851, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.846 with standard deviation 0.012. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.8308755607331187, 0.8590670034328357, 0.8468162550167009], "final_y": [0.11296702264415837, 0.12345361033315361, 0.1271417894990836]}, "mutation_prompt": null}
{"id": "7822e5f5-4219-4579-ab96-87d76fb33c19", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.6 + 0.2 * np.std(self.scores)  # Tighter adaptation range for mutation\n            self.CR = np.clip(0.5 + 0.2 * np.std(self.population), 0, 1)  # Tighter adaptation range for crossover\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            perturbed_individual = individual_denorm * (1 + 0.05 * (np.random.rand(self.dim) - 0.5))  # Weighted perturbation\n            return perturbed_individual, func(perturbed_individual)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced algorithm with robust adaptation of DE parameters and weighted local search to improve solution robustness and convergence.", "configspace": "", "generation": 12, "fitness": 0.8647941578195626, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.013. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8575269527370042, 0.8828026572477692, 0.8540528634739142], "final_y": [0.1227214876501953, 0.10960406080643514, 0.12345361044940106]}, "mutation_prompt": null}
{"id": "35d4b9a2-472c-4a04-9ec1-211dd6b6478e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n        self.local_search_threshold = 0.95  # Frequency control for local search\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.4  # Increase F range\n            self.CR = 0.7 + 0.3 * np.random.rand()  # Adjust CR range\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='Powell', bounds=list(zip(bounds.lb, bounds.ub)))  # Use Powell method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget and np.random.rand() < self.local_search_threshold:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved convergence by adjusting local search frequency based on fitness improvement trends.", "configspace": "", "generation": 12, "fitness": 0.8310109033568351, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.014. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.8171950032855576, 0.8254656748779773, 0.8503720319069706], "final_y": [0.12715906692729406, 0.12303411580881818, 0.12330756372965646]}, "mutation_prompt": null}
{"id": "b3b2cdb0-6ad9-4fe5-9712-8b10bd346a8f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n        # Adjust population size dynamically\n        self.pop_size = max(10, int(self.pop_size * (1.1 if improvement > 0.1 else 0.9)))\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Integrate dynamic population size adjustment based on convergence rate to improve solution diversity and convergence. ", "configspace": "", "generation": 13, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 100 is out of bounds for axis 0 with size 100').", "error": "IndexError('index 100 is out of bounds for axis 0 with size 100')", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {}, "mutation_prompt": null}
{"id": "1383d13a-21ef-4402-ae95-06fb3e3e98a5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.6 + 0.2 * np.std(self.scores)  # Adjusted mutation factor\n            self.CR = np.clip(0.4 + 0.4 * np.std(self.population), 0, 1)  # Adjusted crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))  # Changed method\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced the adaptive mutation strategy and local search effectiveness by refining scaling factors and considering solution diversity.", "configspace": "", "generation": 13, "fitness": 0.8158508229413091, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.040. And the mean value of best solutions found was 0.139 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8727606353396923, 0.7889634241743391, 0.7858284093098957], "final_y": [0.11765680492542507, 0.1512432873358076, 0.148576635246511]}, "mutation_prompt": null}
{"id": "581aada4-9274-4f43-aaf8-75e7a86e4a04", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        kmeans = KMeans(n_clusters=max(2, self.pop_size // 10)).fit(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  \n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)), options={'maxiter': 100})  # Changed line\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced local search improvement by increasing iterations for refinement in the optimization process.", "configspace": "", "generation": 13, "fitness": 0.8293425720568562, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.030. And the mean value of best solutions found was 0.132 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "f6a4aaf3-b871-4a94-a52c-bf404ffdf60f", "metadata": {"aucs": [0.8496520891626065, 0.7873753394926791, 0.851000287515283], "final_y": [0.1268140527344812, 0.14984949550552906, 0.12078279221767141]}, "mutation_prompt": null}
{"id": "c73664eb-414a-417e-9a12-322e5beee9db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n        self.local_search_freq = 0.1\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.5 * np.var(self.scores)  # Adjusted mutation factor\n            self.CR = np.clip(0.3 + 0.6 * np.var(self.population), 0, 1)  # Adjusted crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget and np.random.rand() < self.local_search_freq:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive local search frequency and refined mutation strategy using fitness landscape analysis to improve exploration and early convergence.", "configspace": "", "generation": 13, "fitness": 0.8222732315194207, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.018. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.826651701478606, 0.7989409726334161, 0.8412270204462399], "final_y": [0.12305663952740287, 0.14713317197907683, 0.11474850928611346]}, "mutation_prompt": null}
{"id": "d98ac85c-511c-4c22-ad13-1de623e3b0a6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)  # Adapt mutation factor based on fitness variance\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success and np.abs(self.scores.min() - result.fun) > 1e-6:  # Stopping criterion based on fitness improvement\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search by introducing a stopping criterion based on a threshold fitness improvement.", "configspace": "", "generation": 13, "fitness": 0.8122852318699745, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.812 with standard deviation 0.029. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8504271064086384, 0.7792636529672332, 0.8071649362340517], "final_y": [0.1175606701523354, 0.15063324505792186, 0.12613791484313996]}, "mutation_prompt": null}
{"id": "d7dfc5f7-a5ba-44e4-a931-4052ad0c5012", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        self.pop_size = int(min(100, 5 + self.evaluations // 10))  # Adaptive population size\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration and exploitation by adaptive population size and enhanced mutation strategy.", "configspace": "", "generation": 14, "fitness": 0.807078075569371, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.022. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.8386313893288275, 0.7890492491362417, 0.7935535882430438], "final_y": [0.13300904616156417, 0.13964343551816816, 0.1359356644211649]}, "mutation_prompt": null}
{"id": "95ca52a8-e82f-44e0-b656-cc4e8479c927", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.4  # Increase F range\n            self.CR = 0.7 + 0.3 * np.random.rand()  # Adjust CR range\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='Powell', bounds=list(zip(bounds.lb, bounds.ub)))  # Use Powell method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget and np.var(self.scores) > 0.01:  # Adaptive local search frequency\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive local search frequency based on population diversity to enhance convergence.", "configspace": "", "generation": 14, "fitness": 0.7703384176656863, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.770 with standard deviation 0.009. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.757938545015694, 0.778970890691923, 0.7741058172894422], "final_y": [0.15754110582872605, 0.1518647959863768, 0.14290931047874544]}, "mutation_prompt": null}
{"id": "019750dc-161a-4722-ac97-6b3fbfa7db45", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c) * np.var(self.population), 0, 1)  # Scale mutation with variance\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved mutation strategy by introducing population variance scaling to enhance diversity and convergence.", "configspace": "", "generation": 14, "fitness": 0.8255794675451722, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.047. And the mean value of best solutions found was 0.138 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.8770857984067126, 0.8358703410868957, 0.7637822631419081], "final_y": [0.11706863732139916, 0.13623708243622412, 0.15958759537272882]}, "mutation_prompt": null}
{"id": "e2ded0bb-84ef-4367-a362-1b16ca25d865", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.6 + np.random.rand() * 0.3  # Adjust F range\n            self.CR = 0.6 + 0.3 * np.random.rand()  # Adjust CR range\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))  # Use L-BFGS-B method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + np.random.choice([0, 1])  # Random increment\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive strategy with improved local search and flexible dimensionality management for better convergence.", "configspace": "", "generation": 14, "fitness": 0.8221791018337491, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.027. And the mean value of best solutions found was 0.133 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.853885626446277, 0.8239099146076281, 0.7887417644473422], "final_y": [0.12295068232991668, 0.1321342171532408, 0.14320297548074978]}, "mutation_prompt": null}
{"id": "6581c7ba-daca-4bfd-82da-b226f9ddb6db", "solution": "import numpy as np\nfrom scipy.optimize import minimize, differential_evolution\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = differential_evolution(func, bounds=list(zip(bounds.lb, bounds.ub)))  # Changed to a higher-level algorithm\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Implemented enhanced local search by using a higher-level optimization algorithm for refinement.", "configspace": "", "generation": 14, "fitness": 0.8108345581643185, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.811 with standard deviation 0.008. And the mean value of best solutions found was 0.134 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.8058613585108861, 0.821747611332054, 0.8048947046500153], "final_y": [0.1380904568858985, 0.12758340082899933, 0.13688018739543362]}, "mutation_prompt": null}
{"id": "5b3036b8-6f3f-42b8-975d-9f9427494f0f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * np.std(self.population)  # Adjust mutation factor\n            self.CR = np.clip(0.6 + 0.2 * np.std(self.population), 0, 1)  # Refine crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial = np.unique(trial, axis=0)  # Redundancy reduction\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            noisy_eval = np.mean([func(individual_denorm + np.random.normal(0, 1e-3, size=self.dim)) for _ in range(5)])\n            return individual_denorm, noisy_eval\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance convergence and robustness by refining adaptive crossover, introducing redundancy reduction, and incorporating noise-resistant local search.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (9,) (10,) ').", "error": "ValueError('operands could not be broadcast together with shapes (9,) (10,) ')", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {}, "mutation_prompt": null}
{"id": "320f3eab-2f2d-4029-ae7e-a2fc62c6fa3c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.4 * np.std(self.population)  # Adjusted mutation factor\n            self.CR = np.clip(0.6 + 0.3 * np.std(self.population), 0, 1)  # Adjusted crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced mutation and crossover strategy with dynamic parameter adjustments to improve solution quality.", "configspace": "", "generation": 15, "fitness": 0.8730285168099366, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.007. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.8745737092747071, 0.8637125175305335, 0.8807993236245693], "final_y": [0.11176126194602087, 0.11914744011631806, 0.11490471544302405]}, "mutation_prompt": null}
{"id": "e61ef68e-b096-4408-aaa6-280f185446db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.4 + np.random.rand() * 0.4  # Modified F range\n            self.CR = 0.6 + 0.4 * np.random.rand()  # Adjusted CR range\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))  # Use L-BFGS-B method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search integration and adaptive mutation strategy to improve convergence and solution diversity.", "configspace": "", "generation": 15, "fitness": 0.8422694089451946, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.008. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.8372086223443521, 0.854096975901046, 0.8355026285901856], "final_y": [0.131669441685628, 0.1256795801691465, 0.1269950914880702]}, "mutation_prompt": null}
{"id": "feb82cc0-efa2-4d17-ba42-7d4b97e05777", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        # Improved modular structure detection logic\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            # Dynamically adjust population size based on convergence progress\n            if np.std(self.scores) < 1e-3:\n                self.pop_size = max(20, self.pop_size // 2)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "The algorithm enhances solution quality by dynamically adjusting the population size based on convergence speed.", "configspace": "", "generation": 15, "fitness": 0.8643178926270026, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.015. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.8711460851921917, 0.8432726059424445, 0.8785349867463714], "final_y": [0.11756067015103688, 0.128965810253871, 0.11936433935525681]}, "mutation_prompt": null}
{"id": "0c06b9b1-bd8c-4e34-8090-f652197bb9ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        kmeans = KMeans(n_clusters=max(2, self.pop_size // 10)).fit(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        for individual in self.population:\n            if np.std(individual) < 0.1:\n                individual[:] = np.random.rand(self.dim)\n    \n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            return current_dim + np.random.choice([1, 2])\n        return current_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.detect_modular_structure()\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced modular structure detection and adaptive dimensionality for improved exploration and exploitation.", "configspace": "", "generation": 15, "fitness": 0.8468594358623976, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.847 with standard deviation 0.019. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "f6a4aaf3-b871-4a94-a52c-bf404ffdf60f", "metadata": {"aucs": [0.8629015516095767, 0.8572837699004552, 0.820392986077161], "final_y": [0.12335728961796588, 0.12556324713227718, 0.13410481235559235]}, "mutation_prompt": null}
{"id": "d50ed0fd-00d2-448f-8a75-45136651aa58", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)  # Adapt mutation factor based on fitness variance\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved convergence by introducing adaptive F and CR based on fitness variance.", "configspace": "", "generation": 10, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.86210139586791, 0.886201743530128, 0.8952849490371253], "final_y": [0.12047515982366708, 0.11280370858096356, 0.11162669748550225]}, "mutation_prompt": null}
{"id": "c83fc6b3-5840-4a23-9350-92f48f43fe15", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8\n        self.CR = 0.9\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        kmeans = KMeans(n_clusters=max(2, self.pop_size // 10)).fit(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  \n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            if np.std(self.scores) < 1e-3:  # Dynamic population size adjustment based on convergence\n                self.pop_size = min(self.pop_size + 10, 200)  \n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration and exploitation balance by dynamically adjusting population size based on convergence speed.", "configspace": "", "generation": 16, "fitness": 0.858535339043221, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.021. And the mean value of best solutions found was 0.125 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "f6a4aaf3-b871-4a94-a52c-bf404ffdf60f", "metadata": {"aucs": [0.8631300420789225, 0.8816949198686349, 0.8307810551821057], "final_y": [0.12170468650076627, 0.11722279609443376, 0.1354053997324266]}, "mutation_prompt": null}
{"id": "d8fa2298-1229-459d-903f-2e19778b7596", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1  # Slower dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search integration and adaptive dimensionality adjustment to improve convergence and solution quality.", "configspace": "", "generation": 16, "fitness": 0.862898692198771, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.015. And the mean value of best solutions found was 0.119 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.8434261689014362, 0.8644705840703077, 0.8807993236245693], "final_y": [0.1241214007144692, 0.11765467252626083, 0.11490471544302405]}, "mutation_prompt": null}
{"id": "0e9b5d67-bd5a-4495-919b-0c6221a7091b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)  # Adapt mutation factor based on fitness variance\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            perturbation = np.random.normal(0, 0.01, size=trial_denorm.shape)  # Introduce perturbation\n            score = func(trial_denorm + perturbation)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved fitness evaluation by considering robustness against small perturbations in solutions.", "configspace": "", "generation": 16, "fitness": 0.8801615587400778, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.012. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8970485010422503, 0.8739807216241524, 0.869455453553831], "final_y": [0.10960406081149943, 0.11602738075741326, 0.12035903242910595]}, "mutation_prompt": null}
{"id": "5ba9d6ec-9f4a-4a29-adc5-25a67fadd14a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)  # Adapt mutation factor based on fitness variance\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        perturbation = 0.1 * np.random.randn(self.dim)  # Adaptive perturbation for enhanced local search\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb) + perturbation\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive perturbation in the local search for enhanced exploration around the current best solutions.", "configspace": "", "generation": 16, "fitness": 0.8541157265078433, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.038. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8693499501687756, 0.8909844319528951, 0.8020127974018593], "final_y": [0.11235002830995999, 0.11163580280646646, 0.12528227236936695]}, "mutation_prompt": null}
{"id": "2eabec27-501f-47d3-95f9-9466d287c1e6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, max(10, int(10 * self.dim * (1 - np.std(self.scores) / (np.mean(self.scores) + 1e-9)))))  # Dynamic pop_size adjustment\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced dynamic adjustment of population size based on convergence rate to improve the balance between exploration and exploitation.", "configspace": "", "generation": 17, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('cannot convert float NaN to integer').", "error": "ValueError('cannot convert float NaN to integer')", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {}, "mutation_prompt": null}
{"id": "0bac3713-9677-4d9b-9aae-7c50872228ae", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1  # Slower dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)  # Adjust dimensionality control\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced convergence through adaptive dimensionality control and refined local search for improved solution quality.", "configspace": "", "generation": 17, "fitness": 0.8562313105975532, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.029. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.8695980636848171, 0.8827384006737473, 0.8163574674340952], "final_y": [0.1181915683717335, 0.11440710977053359, 0.14000279282582218]}, "mutation_prompt": null}
{"id": "7ded4d7a-7ece-4598-860b-1066c51cc8d5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.6 + 0.3 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.6 + 0.3 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c) + 0.01 * np.random.randn(*a.shape), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget and (self.evaluations % 10 == 0):\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by incorporating a noise-tolerant differential evolution strategy and adaptive local search frequency.", "configspace": "", "generation": 17, "fitness": 0.8737380124519575, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.018. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.873325157514666, 0.8963277977727659, 0.8515610820684403], "final_y": [0.11532087133005664, 0.11012150292882306, 0.12605806457972946]}, "mutation_prompt": null}
{"id": "b05b380b-b8b3-4d9e-96b0-56d562dd3d6f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.3 + 0.4 * np.std(self.scores)  # Adjusted mutation factor for finer control\n            self.CR = np.clip(0.6 + 0.3 * np.std(self.population), 0, 1)  # Tuned crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))  # Switched to L-BFGS-B for precision\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved adaptive strategies by refining DE parameters and enhancing local search precision for better convergence.", "configspace": "", "generation": 17, "fitness": 0.8700516916885501, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.031. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8299615113972321, 0.8737368186618943, 0.9064567450065238], "final_y": [0.12542698459304624, 0.11280468217954853, 0.10961784216109816]}, "mutation_prompt": null}
{"id": "1f8242b9-e798-48b1-8781-ed950d29d90d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.6 + np.random.rand() * 0.3  # Refine F range\n            self.CR = 0.7 + 0.3 * np.random.rand()  # Adjust CR range\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))  # Use L-BFGS-B method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing convergence by adjusting local search method and refining F factor adaptation.", "configspace": "", "generation": 17, "fitness": 0.8573244293529765, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.018. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.8327890577180264, 0.8636539043406446, 0.8755303260002583], "final_y": [0.1330373777031817, 0.12261341044789953, 0.11489965352595533]}, "mutation_prompt": null}
{"id": "87347d95-a099-4173-b739-9eef329c6674", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            self.F = 0.5 + np.random.rand() * 0.4  # Increase F range\n            self.CR = 0.7 + 0.3 * np.random.rand()  # Adjust CR range\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='Powell', bounds=list(zip(bounds.lb, bounds.ub)))  # Use Powell method\n        return result.x, result.fun\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.pop_size = min(100, 10 * self.dim)  # Adaptive pop size\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive local search integration and fine-tuned mutation strategy to improve exploration and exploitation balance.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('list index out of range').", "error": "IndexError('list index out of range')", "parent_id": "4f6960b6-82ba-448a-bfd0-86083ee189e8", "metadata": {"aucs": [0.8633157573307395, 0.886121333592357, 0.8964573845085346], "final_y": [0.11741242719283895, 0.11602711837802027, 0.11163410876986402]}, "mutation_prompt": null}
{"id": "5d903566-90fc-4254-8642-927c32ea91ff", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.population)  # Adapt mutation factor\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def exploitative_refinement(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def resize_population(self, factor):\n        new_size = max(10, int(self.pop_size * factor))\n        self.population = np.random.rand(new_size, self.dim)\n        self.scores = np.full(new_size, np.inf)\n        self.pop_size = new_size\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        resize_factors = [1.0, 0.8, 1.2]  # Different factors to dynamically resize population\n        resize_idx = 0\n        while self.evaluations < self.budget:\n            if self.evaluations % (self.budget // 10) == 0 and resize_idx < len(resize_factors):\n                self.resize_population(resize_factors[resize_idx])\n                resize_idx += 1\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.exploitative_refinement(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptive population resizing and exploitative refinement for improved convergence efficiency.", "configspace": "", "generation": 18, "fitness": 0.8395600268608482, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.013. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "34c9a65b-eb30-4a5b-b2f6-1c8a8c27b1c0", "metadata": {"aucs": [0.8321388072313004, 0.8292245851646358, 0.8573166881866081], "final_y": [0.116038680644453, 0.1330734839046993, 0.12037633326849628]}, "mutation_prompt": null}
{"id": "abd2d195-771e-4fb4-9667-fd84672efc38", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def adapt_population_size(self):  # Newly added method\n        self.pop_size = int(min(100, 10 * self.dim) * (1 - self.evaluations / self.budget)) + 1\n        self.population = self.population[:self.pop_size]  # Adjust population size dynamically\n        self.scores = self.scores[:self.pop_size]\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            self.adapt_population_size()  # Apply adaptive population size adjustment\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced adaptive population size based on evaluation progress to improve convergence rate and solution quality.", "configspace": "", "generation": 18, "fitness": 0.8762775995295019, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.008. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.873822957918023, 0.8682354232383419, 0.8867744174321408], "final_y": [0.11858263338413222, 0.11707987198940295, 0.11546641297649618]}, "mutation_prompt": null}
{"id": "cf05201b-9c7d-4cf0-8a8a-dffacc337a2e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.3 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0.3, 1)  # Refine crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < (self.CR * diversity)  # Fitness-based crossover adjustment\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced fitness-based crossover and refined mutation strategies to enhance adaptation to function landscape dynamics.", "configspace": "", "generation": 18, "fitness": 0.8568349783148896, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.015. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.8695149087757343, 0.8656610956831402, 0.8353289304857943], "final_y": [0.11716958440545588, 0.12049146399220478, 0.13049153128484925]}, "mutation_prompt": null}
{"id": "0dcdeaee-26eb-4360-bf54-29e683fcc2f0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            pop_std = np.std(self.population)\n            self.F = 0.5 + 0.3 * np.std(self.scores) + 0.2 * pop_std  # Further adjust F\n            self.CR = np.clip(0.5 + 0.3 * pop_std, 0, 1)  # Further adjust CR\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced adaptation by dynamically adjusting F and CR based on population diversity and variance.", "configspace": "", "generation": 18, "fitness": 0.884838269301882, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.885 with standard deviation 0.015. And the mean value of best solutions found was 0.114 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8641585859852767, 0.889073627647597, 0.9012825942727722], "final_y": [0.12083798874344231, 0.11163081144043618, 0.10960853161776607]}, "mutation_prompt": null}
{"id": "3ee0050b-59ed-4e13-81e5-b9bde7033553", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n        self.noise_tolerance = 1e-4\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                perturbed_individual = self.population[i] + np.random.normal(0, self.noise_tolerance, self.dim)\n                self.scores[i] = func(perturbed_individual)\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def adjust_population_size(self):\n        if self.evaluations > self.budget / 2:\n            self.pop_size = max(20, self.pop_size // 2)\n        return\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.adjust_population_size()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced algorithm by dynamically adjusting population size based on convergence and incorporating resilience to noise in the fitness evaluation.", "configspace": "", "generation": 19, "fitness": 0.8425657172137315, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.021. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8719469568891728, 0.82288855983527, 0.8328616349167521], "final_y": [0.11680051453235207, 0.13664140726787, 0.1313045207824589]}, "mutation_prompt": null}
{"id": "f99fbbb8-0629-4ddc-a9f8-2b560e3efd26", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n        self.history = []\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n                self.history.append(self.population[i].copy())  # Track history\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)  # Adapt mutation factor based on fitness variance\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb) * (1 + 0.1 * np.std(self.history, axis=0))  # Weighted by history\n            perturbation = np.random.normal(0, 0.01, size=trial_denorm.shape)  # Introduce perturbation\n            score = func(trial_denorm + perturbation)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced convergence by dynamically adapting dimensional weighting and leveraging historical population data.", "configspace": "", "generation": 19, "fitness": 0.8147957641109249, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.815 with standard deviation 0.048. And the mean value of best solutions found was 0.140 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "0e9b5d67-bd5a-4495-919b-0c6221a7091b", "metadata": {"aucs": [0.7485534761987812, 0.8367868093222798, 0.859047006811714], "final_y": [0.16815354136204685, 0.12863658514412013, 0.12261289173589385]}, "mutation_prompt": null}
{"id": "31291f7d-6d51-4fdb-b0e4-1dff2f921f55", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            pop_std = np.std(self.population)\n            self.F = 0.5 + 0.3 * np.std(self.scores) + 0.2 * pop_std  # Further adjust F\n            self.CR = np.clip(0.5 + 0.3 * pop_std, 0, 1)  # Further adjust CR\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if not result.success:  # Combined trust-constr with BFGS for better refinement\n            result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.01:\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search integration by combining trust-constr with the BFGS method for improved refinement.", "configspace": "", "generation": 19, "fitness": 0.8157524809214799, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.816 with standard deviation 0.034. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "0dcdeaee-26eb-4360-bf54-29e683fcc2f0", "metadata": {"aucs": [0.7725152763853192, 0.8201746335511451, 0.8545675328279753], "final_y": [0.134770163351976, 0.13147430622330492, 0.12525026648815307]}, "mutation_prompt": null}
{"id": "ddc1c388-841a-4061-9171-309ce29b7baa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            self.F = 0.5 + 0.3 * np.std(self.scores)  # Adapt mutation factor based on fitness variance\n            self.CR = np.clip(0.5 + 0.3 * np.std(self.population), 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='trust-constr', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:  # Improved local search integration\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        layer_division = np.array_split(self.population, self.dim // 5)\n        for segment in layer_division:\n            if np.var(segment) < 0.005:  # Optimized variance threshold for modular detection\n                continue\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 1\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            self.detect_modular_structure()\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n\n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced modular structure detection by refining variance threshold.", "configspace": "", "generation": 19, "fitness": 0.8354568687501479, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.012. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "6f4c1805-1246-45b4-a6c1-cda9e25bfc82", "metadata": {"aucs": [0.8492676177749323, 0.8196816074717135, 0.8374213810037978], "final_y": [0.12127881788619332, 0.13797761053740865, 0.12699757531162104]}, "mutation_prompt": null}
{"id": "7a44aed8-70fe-4de3-bbc0-455ec271b08b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n        self.pop_size = min(100, 10 * dim)\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.pop_size, dim)\n        self.scores = np.full(self.pop_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.scores[i] == np.inf:\n                self.scores[i] = func(self.population[i])\n                self.evaluations += 1\n\n    def differential_evolution_step(self, func, bounds):\n        diversity = np.std(self.population)\n        self.pop_size = max(10, int(self.pop_size * (1 + 0.1 * (1 - diversity / np.max(self.scores)))))  # Dynamically adjust population size\n        for i in range(self.pop_size):\n            if self.evaluations >= self.budget:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            improvement = (self.scores[i] - np.min(self.scores)) / (np.ptp(self.scores) + 1e-9)\n            self.F = 0.5 + 0.4 * improvement  # Adapt mutation factor based on improvement\n            self.CR = np.clip(0.5 + 0.4 * improvement, 0, 1)  # Adapt crossover probability\n            mutant = np.clip(a + self.F * (b - c), 0, 1)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_denorm = bounds.lb + trial * (bounds.ub - bounds.lb)\n            score = func(trial_denorm)\n            self.evaluations += 1\n            if score < self.scores[i]:\n                self.population[i] = trial\n                self.scores[i] = score\n\n    def local_search(self, func, individual, bounds):\n        individual_denorm = bounds.lb + individual * (bounds.ub - bounds.lb)\n        result = minimize(func, individual_denorm, method='L-BFGS-B', bounds=list(zip(bounds.lb, bounds.ub)))\n        if result.success:\n            return result.x, result.fun\n        else:\n            return individual_denorm, func(individual_denorm)\n\n    def detect_modular_structure(self):\n        return\n\n    def adapt_dimensionality(self, current_dim, target_dim):\n        if current_dim < target_dim:\n            new_dim = current_dim + 2  # Faster dimensionality increase\n        else:\n            new_dim = current_dim\n        return new_dim\n\n    def __call__(self, func):\n        bounds = func.bounds\n        target_dim = bounds.ub.size\n        while self.evaluations < self.budget:\n            self.evaluate_population(func)\n            self.differential_evolution_step(func, bounds)\n            best_idx = np.argmin(self.scores)\n            best_individual = self.population[best_idx]\n            if self.evaluations < self.budget:\n                refined_solution, refined_score = self.local_search(func, best_individual, bounds)\n                if refined_score < self.scores[best_idx]:\n                    self.population[best_idx] = (refined_solution - bounds.lb) / (bounds.ub - bounds.lb)\n                    self.scores[best_idx] = refined_score\n            self.dim = self.adapt_dimensionality(self.dim, target_dim)\n            \n        best_idx = np.argmin(self.scores)\n        best_solution = bounds.lb + self.population[best_idx] * (bounds.ub - bounds.lb)\n        return best_solution, self.scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration by dynamically adjusting population size based on diversity and convergence progress.", "configspace": "", "generation": 19, "fitness": 0.8624718156189711, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.010. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "54fd4555-5799-45b7-a09e-9a3e461ac46a", "metadata": {"aucs": [0.8660757476377878, 0.8487730817542135, 0.8725666174649119], "final_y": [0.12128226883845428, 0.1271417895048068, 0.11579240257464074]}, "mutation_prompt": null}
