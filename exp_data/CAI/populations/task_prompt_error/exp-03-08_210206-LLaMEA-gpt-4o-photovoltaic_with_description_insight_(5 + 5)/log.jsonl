{"id": "5d68566c-6547-4726-a450-417dc65abed8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.budget -= self.population_size\n\n        while self.budget > 0:\n            for i in range(self.population_size):\n                a, b, c = population[np.random.choice(self.population_size, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), bounds.lb, bounds.ub)\n                trial = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                trial_fitness = func(trial)\n                self.budget -= 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                if fitness[i] < self.best_fitness:\n                    self.best_fitness = fitness[i]\n                    self.best_solution = population[i]\n\n                if self.budget <= 0:\n                    break\n\n    def nelder_mead_local_refinement(self, func, bounds):\n        if self.best_solution is not None:\n            result = minimize(func, self.best_solution, method='Nelder-Mead',\n                              bounds=[(low, high) for low, high in zip(bounds.lb, bounds.ub)],\n                              options={'maxiter': self.budget})\n            if result.fun < self.best_fitness:\n                self.best_fitness = result.fun\n                self.best_solution = result.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        layers = min(10, self.dim)\n        \n        while layers <= self.dim:\n            sub_bounds = np.array_split(np.column_stack((bounds.lb, bounds.ub)), layers, axis=0)\n            def sub_func(x):\n                full_x = np.concatenate([x] * (self.dim // layers))[:self.dim]\n                return func(full_x)\n            \n            self.differential_evolution(sub_func, sub_bounds)\n            self.nelder_mead_local_refinement(sub_func, sub_bounds)\n            layers += 10\n\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution (DE) for global exploration and Nelder-Mead for local refinement, incorporating a modularity-preserving mechanism and adaptive layer scaling to efficiently solve high-dimensional black box optimization problems with noisy cost functions.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 13, in differential_evolution\nAttributeError: 'list' object has no attribute 'lb'\n.", "error": "AttributeError(\"'list' object has no attribute 'lb'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"<string>\", line 13, in differential_evolution\nAttributeError: 'list' object has no attribute 'lb'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "3a25f92a-c14d-4f58-b870-d2a220f454dd", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n        self.population = np.random.rand(self.population_size, dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        # Using a simple gradient-based local search\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        # Initial evaluation\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            # DE for exploration\n            self.differential_evolution_step(func)\n            # Local search for refinement\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a local search phase that dynamically adapts layer roles and robustness, leveraging a gradual layer increase to handle complexity and computational costs effectively.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 142, in evaluatePhotonic\n    algorithm = globals()[algorithm_name](budget=budget, dim=dim)\n  File \"<string>\", line 12, in __init__\nNameError: name 'func' is not defined\n.", "error": "NameError(\"name 'func' is not defined\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 142, in evaluatePhotonic\n    algorithm = globals()[algorithm_name](budget=budget, dim=dim)\n  File \"<string>\", line 12, in __init__\nNameError: name 'func' is not defined\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "286df453-21c8-4f7d-8b8c-a2bd92b82bdf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def differential_evolution(self, func, bounds, population_size=20, generations=100):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds.lb + pop * (bounds.ub - bounds.lb)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        for gen in range(generations):\n            for i in range(population_size):\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), bounds.lb, bounds.ub)\n                cross_points = np.random.rand(self.dim) < 0.9\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n            if gen % 10 == 0 and len(generations) * population_size >= self.budget:\n                break\n\n        return best\n\n    def nelder_mead(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxfev': self.budget // 2})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        layer_increment = max(1, self.dim // 10)\n        current_dim = layer_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim and self.budget > 0:\n            subset_func = lambda x: func(np.pad(x, (0, self.dim - len(x)), 'constant'))\n            bounds = np.array([[func.bounds.lb[i], func.bounds.ub[i]] for i in range(current_dim)])\n            de_solution = self.differential_evolution(subset_func, bounds, population_size=20, generations=100)\n            \n            local_solution, local_fitness = self.nelder_mead(subset_func, de_solution)\n            if local_fitness < best_fitness:\n                best_fitness = local_fitness\n                best_solution = np.pad(local_solution, (0, self.dim - len(local_solution)), 'constant')\n\n            current_dim += layer_increment\n        \n        return best_solution", "name": "HybridOptimizer", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution for global exploration with a Nelder-Mead simplex method for local refinement, incorporating a layer-wise adaptive strategy to gradually increase problem complexity in high-dimensional noisy optimization tasks.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\n  File \"<string>\", line 11, in differential_evolution\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n.", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\n  File \"<string>\", line 11, in differential_evolution\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "d9b49482-414f-4257-899e-0fa4b5995e3c", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.f = 0.8  # DE scale factor\n        self.cr = 0.9  # Crossover probability\n        self.local_search_prob = 0.3  # Probability to perform local search\n        self.increment_step = 2  # Layer-wise complexity increment\n        self.current_budget = 0\n\n    def local_search(self, candidate, func, bounds):\n        \"\"\"Perform a simple local search around a candidate solution.\"\"\"\n        best_candidate = candidate.copy()\n        best_score = func(candidate)\n        for d in range(self.dim):\n            perturbation = np.random.uniform(-0.1, 0.1) * (bounds.ub[d] - bounds.lb[d])\n            new_candidate = candidate.copy()\n            new_candidate[d] += perturbation\n            new_candidate[d] = np.clip(new_candidate[d], bounds.lb[d], bounds.ub[d])\n            score = func(new_candidate)\n            if score > best_score:\n                best_score = score\n                best_candidate = new_candidate\n            self.current_budget += 1\n            if self.current_budget >= self.budget:\n                break\n        return best_candidate, best_score\n\n    def differential_evolution(self, population, func, bounds):\n        \"\"\"Perform a single iteration of Differential Evolution.\"\"\"\n        new_population = []\n        for i in range(len(population)):\n            if self.current_budget >= self.budget:\n                break\n            a, b, c = np.random.choice(len(population), 3, replace=False)\n            mutant = population[a] + self.f * (population[b] - population[c])\n            mutant = np.clip(mutant, bounds.lb, bounds.ub)\n            trial = np.copy(population[i])\n            jrand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.cr or j == jrand:\n                    trial[j] = mutant[j]\n            trial_score = func(trial)\n            self.current_budget += 1\n            if trial_score > func(population[i]):\n                new_population.append(trial)\n            else:\n                new_population.append(population[i])\n        return np.array(new_population)\n\n    def __call__(self, func):\n        bounds = func.bounds\n        population = np.random.uniform(bounds.lb, bounds.ub, (self.population_size, self.dim))\n        scores = np.array([func(ind) for ind in population])\n        self.current_budget = self.population_size\n\n        while self.current_budget < self.budget:\n            population = self.differential_evolution(population, func, bounds)\n            if np.random.rand() < self.local_search_prob:\n                for i in range(len(population)):\n                    if self.current_budget >= self.budget:\n                        break\n                    population[i], scores[i] = self.local_search(population[i], func, bounds)\n            \n            # If allowed, increment layer complexity gradually\n            if self.dim + self.increment_step <= self.budget - self.current_budget:\n                self.dim += self.increment_step\n\n        best_idx = np.argmax(scores)\n        return population[best_idx], scores[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid Metaheuristic that combines Differential Evolution with Local Search and Layer-wise Incremental Complexity for optimizing multilayered photonic structures.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 61, in __call__\n  File \"<string>\", line 45, in differential_evolution\nIndexError: index 10 is out of bounds for axis 0 with size 10\n.", "error": "IndexError('index 10 is out of bounds for axis 0 with size 10')Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 61, in __call__\n  File \"<string>\", line 45, in differential_evolution\nIndexError: index 10 is out of bounds for axis 0 with size 10\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "a104b688-1b0b-4ec3-8a75-0441d32c1fbd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds.lb + self.population[i] * (bounds.ub - bounds.lb)\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds.lb, bounds.ub)\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds.lb, bounds.ub)))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "A hybrid evolutionary algorithm combining Differential Evolution for global exploration and a local search strategy using a Sequential Quadratic Programming (SQP) method for refinement, incorporating a dynamic layer expansion technique to balance problem complexity and computational cost.", "configspace": "", "generation": 0, "fitness": -Infinity, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\n  File \"<string>\", line 11, in differential_evolution\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n.", "error": "AttributeError(\"'numpy.ndarray' object has no attribute 'lb'\")Traceback (most recent call last):\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 192, in initialize_single\n    new_individual = self.evaluate_fitness(new_individual)\n  File \"/home/ian/LLaMEA/./llamea/llamea.py\", line 281, in evaluate_fitness\n    updated_individual = self.f(individual, self.logger)\n  File \"/home/ian/LLaMEA/benchmarks/tuto_global_optimization_photonics/exp_photonic.py\", line 143, in evaluatePhotonic\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\n  File \"<string>\", line 11, in differential_evolution\nAttributeError: 'numpy.ndarray' object has no attribute 'lb'\n", "parent_id": null, "metadata": {}, "mutation_prompt": null}
{"id": "c40ee12f-5f26-4942-887c-a688ec60ad0b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def differential_evolution(self, func, bounds, population_size=20, generations=100):\n        pop = np.random.rand(population_size, self.dim)\n        pop = bounds[:, 0] + pop * (bounds[:, 1] - bounds[:, 0])\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n\n        for gen in range(generations):\n            if self.budget <= 0:  # Dynamic budget check\n                break\n            for i in range(population_size):\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), bounds[:, 0], bounds[:, 1])\n                cross_points = np.random.rand(self.dim) < 0.9\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < fitness[best_idx]:\n                        best_idx = i\n                        best = trial\n                self.budget -= 1  # Decrement budget\n            if gen % 10 == 0 and gen * population_size >= self.budget:\n                break\n\n        return best\n\n    def nelder_mead(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxfev': max(1, self.budget // 2)})\n        return result.x, result.fun\n\n    def __call__(self, func):\n        layer_increment = max(1, self.dim // 10)\n        current_dim = layer_increment\n        best_solution = None\n        best_fitness = float('inf')\n\n        while current_dim <= self.dim and self.budget > 0:\n            subset_func = lambda x: func(np.pad(x, (0, self.dim - len(x)), 'constant'))\n            bounds = np.array([[func.bounds.lb[i], func.bounds.ub[i]] for i in range(current_dim)])\n            de_solution = self.differential_evolution(subset_func, bounds, population_size=20, generations=100)\n            \n            local_solution, local_fitness = self.nelder_mead(subset_func, de_solution)\n            if local_fitness < best_fitness:\n                best_fitness = local_fitness\n                best_solution = np.pad(local_solution, (0, self.dim - len(local_solution)), 'constant')\n\n            current_dim += layer_increment\n        \n        return best_solution", "name": "HybridOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution for global exploration with a Nelder-Mead simplex method for local refinement, incorporating dynamic budgeting and robustness metrics in high-dimensional noisy optimization tasks.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('operands could not be broadcast together with shapes (20,10) (2,) ').", "error": "ValueError('operands could not be broadcast together with shapes (20,10) (2,) ')", "parent_id": "286df453-21c8-4f7d-8b8c-a2bd92b82bdf", "metadata": {}, "mutation_prompt": null}
{"id": "329dd65b-ebad-4bb4-b7bb-64fdf91f6423", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n        self.population = None\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm that combines Differential Evolution (DE) for global exploration with a local search phase, dynamically adapting layer roles and robustness, and utilizing a gradual layer increase to handle complexity and computational costs effectively.", "configspace": "", "generation": 1, "fitness": 0.7829874135950948, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.783 with standard deviation 0.031. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "3a25f92a-c14d-4f58-b870-d2a220f454dd", "metadata": {"aucs": [0.7455270479841309, 0.8223419543952348, 0.7810932384059188], "final_y": [0.168414459180121, 0.13945145495321587, 0.1526299038723753]}, "mutation_prompt": null}
{"id": "612d0cb0-589c-4fb1-8959-605fd30fd6b6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds['lb'] + self.population[i] * (bounds['ub'] - bounds['lb'])\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds['lb'], bounds['ub'])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds['lb'], bounds['ub'])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = {'lb': func.bounds.lb, 'ub': func.bounds.ub}\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "A hybrid evolutionary algorithm combining Differential Evolution for global exploration and Sequential Quadratic Programming (SQP) for local refinement, with a corrected initialization of bounds for effective optimization of multilayered photonic structures.", "configspace": "", "generation": 1, "fitness": 0.7719083504124358, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.015. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "a104b688-1b0b-4ec3-8a75-0441d32c1fbd", "metadata": {"aucs": [0.7625278414443453, 0.7934519114875975, 0.7597452983053643], "final_y": [0.16527015440278225, 0.1601017669387199, 0.17185375608456643]}, "mutation_prompt": null}
{"id": "e39b4f7e-cc40-4a5b-891c-42b85ed444c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds[0] + self.population[i] * (bounds[1] - bounds[0])  # Modified this line\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds[0], bounds[1])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds[0], bounds[1])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "A hybrid evolutionary algorithm utilizing Differential Evolution and Sequential Quadratic Programming (SQP) with dynamic layer expansion, now handling bounds using individual elements instead of attributes for better robustness.", "configspace": "", "generation": 1, "fitness": 0.7926921837667541, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.007. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "a104b688-1b0b-4ec3-8a75-0441d32c1fbd", "metadata": {"aucs": [0.7822609733490271, 0.7983776570472522, 0.7974379209039832], "final_y": [0.15108102923921896, 0.1532217058935713, 0.15808392127048776]}, "mutation_prompt": null}
{"id": "00fcbd30-c23a-4366-98e7-32173833819a", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        # Using a simple gradient-based local search\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        # Initial evaluation\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            # DE for exploration\n            self.differential_evolution_step(func)\n            # Local search for refinement\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) for global exploration with a local search phase and a corrected initialization to handle high-dimensional noisy black box optimization problems effectively.", "configspace": "", "generation": 1, "fitness": 0.7898819284426141, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.005. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "3a25f92a-c14d-4f58-b870-d2a220f454dd", "metadata": {"aucs": [0.7823137205120347, 0.7948745380687572, 0.7924575267470503], "final_y": [0.15927623690139803, 0.15944404567468917, 0.14207510990682803]}, "mutation_prompt": null}
{"id": "647fa39d-5592-4043-b137-4a2e891a50fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridMetaheuristic:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.best_solution = None\n        self.best_fitness = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        bounds = np.array(bounds)  # Corrected line to convert bounds to a numpy array\n        population = np.random.uniform(bounds[:, 0], bounds[:, 1], (self.population_size, self.dim))\n        fitness = np.apply_along_axis(func, 1, population)\n        self.budget -= self.population_size\n\n        while self.budget > 0:\n            for i in range(self.population_size):\n                a, b, c = population[np.random.choice(self.population_size, 3, replace=False)]\n                mutant = np.clip(a + 0.8 * (b - c), bounds[:, 0], bounds[:, 1])\n                trial = np.where(np.random.rand(self.dim) < 0.9, mutant, population[i])\n                trial_fitness = func(trial)\n                self.budget -= 1\n                if trial_fitness < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = trial_fitness\n\n                if fitness[i] < self.best_fitness:\n                    self.best_fitness = fitness[i]\n                    self.best_solution = population[i]\n\n                if self.budget <= 0:\n                    break\n\n    def nelder_mead_local_refinement(self, func, bounds):\n        if self.best_solution is not None:\n            result = minimize(func, self.best_solution, method='Nelder-Mead',\n                              bounds=[(low, high) for low, high in zip(bounds[:, 0], bounds[:, 1])],\n                              options={'maxiter': self.budget})\n            if result.fun < self.best_fitness:\n                self.best_fitness = result.fun\n                self.best_solution = result.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        layers = min(10, self.dim)\n        \n        while layers <= self.dim:\n            sub_bounds = np.array_split(np.column_stack((bounds.lb, bounds.ub)), layers, axis=0)\n            def sub_func(x):\n                full_x = np.concatenate([x] * (self.dim // layers))[:self.dim]\n                return func(full_x)\n            \n            self.differential_evolution(sub_func, sub_bounds)\n            self.nelder_mead_local_refinement(sub_func, sub_bounds)\n            layers += 10\n\n        return self.best_solution", "name": "HybridMetaheuristic", "description": "A hybrid metaheuristic algorithm combining Differential Evolution for global exploration and Nelder-Mead for local refinement, with a corrected handling of bounds using numpy arrays to address the AttributeError.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 1 is out of bounds for axis 1 with size 1').", "error": "IndexError('index 1 is out of bounds for axis 1 with size 1')", "parent_id": "5d68566c-6547-4726-a450-417dc65abed8", "metadata": {}, "mutation_prompt": null}
{"id": "00eb89e2-4854-4dfa-8da9-769dd732b608", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.85  # Crossover probability updated for better balance\n        self.current_evaluations = 0\n        self.population = None\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid metaheuristic algorithm with modified crossover probability for enhanced exploration-exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.7848371960035729, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.018. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "329dd65b-ebad-4bb4-b7bb-64fdf91f6423", "metadata": {"aucs": [0.7651841916224582, 0.8082579577310377, 0.7810694386572232], "final_y": [0.15694145052446395, 0.15453064803689986, 0.15072918840542493]}, "mutation_prompt": null}
{"id": "21bd0012-b6af-42c3-a147-e25f9c8f3212", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic algorithm combining Differential Evolution (DE) and Adaptive Gradient Perturbation for improved local search, enhancing the global exploration and refinement balance in high-dimensional noisy black box optimization.", "configspace": "", "generation": 2, "fitness": 0.8003471600757445, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.003. And the mean value of best solutions found was 0.148 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "00fcbd30-c23a-4366-98e7-32173833819a", "metadata": {"aucs": [0.8006687875091604, 0.7961964777523047, 0.8041762149657685], "final_y": [0.15414178000159007, 0.15434990677923066, 0.13472354108920515]}, "mutation_prompt": null}
{"id": "61b5af5f-788c-4e28-a4db-92ffda1887ef", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.8  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        # Using a simple gradient-based local search\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        # Initial evaluation\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            # DE for exploration\n            self.differential_evolution_step(func)\n            # Local search for refinement\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Optimized crossover probability to enhance exploration and diversity in the DE phase.", "configspace": "", "generation": 2, "fitness": 0.7940347961210731, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.012. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "00fcbd30-c23a-4366-98e7-32173833819a", "metadata": {"aucs": [0.79758723175152, 0.8064017604718341, 0.7781153961398652], "final_y": [0.15433240880228816, 0.1506181587946418, 0.16032984966452868]}, "mutation_prompt": null}
{"id": "5d2ad6a7-9058-48ee-8e0b-903ecc4b4a14", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.85  # DE mutation factor adjusted for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds['lb'] + self.population[i] * (bounds['ub'] - bounds['lb'])\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds['lb'], bounds['ub'])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds['lb'], bounds['ub'])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = {'lb': func.bounds.lb, 'ub': func.bounds.ub}\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "Enhance the algorithm by slightly adjusting the DE mutation factor for better exploration balance.", "configspace": "", "generation": 2, "fitness": 0.7916370118701995, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.016. And the mean value of best solutions found was 0.155 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "612d0cb0-589c-4fb1-8959-605fd30fd6b6", "metadata": {"aucs": [0.7695128590972821, 0.804653228787483, 0.8007449477258334], "final_y": [0.16097775109968815, 0.1528905884060644, 0.15192248989724277]}, "mutation_prompt": null}
{"id": "ac4b9a58-4cb8-46ef-a921-981183a58599", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.5 + np.random.rand() * 0.4  # Dynamic mutation factor for better exploration\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds['lb'] + self.population[i] * (bounds['ub'] - bounds['lb'])\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds['lb'], bounds['ub'])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step with noise handling\n                trial_score = func(trial) + np.random.normal(0, 0.01)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(lambda x: func(x) + np.random.normal(0, 0.005), x0, method='SLSQP', bounds=list(zip(bounds['lb'], bounds['ub'])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = {'lb': func.bounds.lb, 'ub': func.bounds.ub}\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "The hybrid algorithm improves solution refinement by integrating a noise-handling mechanism in the local search phase and a dynamic mutation factor in DE.", "configspace": "", "generation": 3, "fitness": 0.7633198475211902, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.763 with standard deviation 0.017. And the mean value of best solutions found was 0.160 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "5d2ad6a7-9058-48ee-8e0b-903ecc4b4a14", "metadata": {"aucs": [0.7401407308630596, 0.7821875432224056, 0.7676312684781055], "final_y": [0.17185816131107357, 0.1482132309836096, 0.1605111894289838]}, "mutation_prompt": null}
{"id": "45c7ab1f-6a43-4474-8843-8a578978a0d1", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.8  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            self.F = np.random.uniform(0.4, 0.9)  # Adaptive mutation factor\n            self.CR = np.random.uniform(0.7, 0.9)  # Adaptive crossover rate\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration and convergence by incorporating adaptive mutation and crossover rates in the Differential Evolution phase.", "configspace": "", "generation": 3, "fitness": 0.8491544948632606, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.062. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "61b5af5f-788c-4e28-a4db-92ffda1887ef", "metadata": {"aucs": [0.7622321721932014, 0.8808685578899188, 0.9043627545066614], "final_y": [0.16724543060826302, 0.11475344227320516, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic leveraging Differential Evolution (DE) with Adaptive Gradient Perturbation, introducing an adaptive crossover probability and a new local search mechanism based on L-BFGS for enhanced convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 3, "fitness": 0.860811754210403, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.041. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "21bd0012-b6af-42c3-a147-e25f9c8f3212", "metadata": {"aucs": [0.8059509466742598, 0.8715855561183212, 0.9048987598386281], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.05, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Hybrid metaheuristic algorithm with an improved local search phase using adaptive Gaussian perturbation, enhancing exploration and exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.8535468376044207, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.049. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "21bd0012-b6af-42c3-a147-e25f9c8f3212", "metadata": {"aucs": [0.7879101712448924, 0.8684123323565255, 0.9043180092118446], "final_y": [0.15708437473640635, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "1d25d39b-a129-4b8e-8181-6bdf1bce0835", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim  # Population size for DE\n        self.F = 0.8  # DE mutation factor\n        self.CR = 0.9  # DE crossover probability\n        self.population = np.random.rand(self.population_size, dim)\n        self.best_solution = None\n        self.best_score = float('inf')\n\n    def differential_evolution(self, func, bounds):\n        # Initialize population within bounds\n        for i in range(self.population_size):\n            self.population[i] = bounds[0] + self.population[i] * (bounds[1] - bounds[0])  # Modified this line\n\n        evaluations = 0\n        while evaluations < self.budget:\n            for i in range(self.population_size):\n                # Mutation step\n                indices = list(range(self.population_size))\n                indices.remove(i)\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                # Introduce linear adaptation of the mutation factor\n                F_adapt = self.F * (1 - evaluations / self.budget)\n                mutant = self.population[a] + F_adapt * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, bounds[0], bounds[1])\n\n                # Crossover step\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection step\n                trial_score = func(trial)\n                evaluations += 1\n                if trial_score < self.best_score:\n                    self.best_score = trial_score\n                    self.best_solution = trial\n\n                if trial_score < func(self.population[i]):\n                    self.population[i] = trial\n\n                if evaluations >= self.budget:\n                    break\n\n    def local_search(self, func, x0, bounds):\n        result = minimize(func, x0, method='SLSQP', bounds=list(zip(bounds[0], bounds[1])))\n        return result.x if result.success else x0\n\n    def __call__(self, func):\n        bounds = (func.bounds.lb, func.bounds.ub)\n\n        # Step 1: Global Optimization with DE\n        self.differential_evolution(func, bounds)\n\n        # Step 2: Local Search for Refinement\n        if self.best_solution is not None:\n            self.best_solution = self.local_search(func, self.best_solution, bounds)\n\n        return self.best_solution", "name": "HybridOptimizer", "description": "Enhanced the DE mutation strategy by introducing a linear adaptation of the mutation factor for improved exploration and exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.7714349685149783, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.771 with standard deviation 0.017. And the mean value of best solutions found was 0.159 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "e39b4f7e-cc40-4a5b-891c-42b85ed444c1", "metadata": {"aucs": [0.7678379371250867, 0.7943141207430209, 0.7521528476768276], "final_y": [0.15436518292089962, 0.1524067893030503, 0.17001760958635304]}, "mutation_prompt": null}
{"id": "80c554e3-ca4c-485d-a298-0cd16c907695", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.8  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            self.F = np.random.uniform(0.4, 0.9)  # Adaptive mutation factor\n            self.CR = np.random.uniform(0.7, 0.9)  # Adaptive crossover rate\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.005, self.dim)  # Improved perturbation variance\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration and convergence by incorporating adaptive mutation and crossover rates in the Differential Evolution phase, with improved perturbation in the local search step.", "configspace": "", "generation": 4, "fitness": 0.7821243514612841, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.013. And the mean value of best solutions found was 0.153 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "45c7ab1f-6a43-4474-8843-8a578978a0d1", "metadata": {"aucs": [0.7680414759311456, 0.7993084489680236, 0.7790231294846833], "final_y": [0.15009670597782232, 0.15762731491642934, 0.15049238635568374]}, "mutation_prompt": null}
{"id": "7c1cc158-5b84-40b3-a850-5113daae3e21", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5\n        self.CR = 0.8\n        self.current_evaluations = 0\n        self.trust_region_radius = 0.1\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            dynamic_F = self.F * np.random.uniform(0.5, 1.5)\n            mutant = np.clip(a + dynamic_F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        # Using a trust region-based local search\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, self.trust_region_radius, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n            else:\n                self.trust_region_radius *= 0.9\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration-exploitation balance by dynamically adjusting mutation factor and integrating a more sophisticated local search using a trust region.", "configspace": "", "generation": 4, "fitness": 0.7971235591426998, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.008. And the mean value of best solutions found was 0.147 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "61b5af5f-788c-4e28-a4db-92ffda1887ef", "metadata": {"aucs": [0.7955010083457192, 0.807671930164911, 0.7881977389174694], "final_y": [0.13765501155724924, 0.1432085912533715, 0.161198493992877]}, "mutation_prompt": null}
{"id": "bcc3ff00-f4d4-413d-bd16-7f677a7cf9f0", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.1 * np.tanh(fitness_variance)\n        \n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.01, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved the mutation strategy by adjusting the differential weight (F) dynamically based on fitness variance to enhance exploration in the DE phase.", "configspace": "", "generation": 4, "fitness": 0.7970503477382215, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.797 with standard deviation 0.026. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "21bd0012-b6af-42c3-a147-e25f9c8f3212", "metadata": {"aucs": [0.768792284005073, 0.8313469989521683, 0.7910117602574234], "final_y": [0.15210038297850548, 0.14167047092808494, 0.1563574252010268]}, "mutation_prompt": null}
{"id": "1e97f05e-1607-44c1-a693-9f9e640554cc", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            adaptive_F = self.F * (1 + (np.min(self.fitness) - self.fitness[i]) / (np.ptp(self.fitness) + 1e-9))\n            mutant = np.clip(a + adaptive_F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            precision_factor = 0.05 * (1.0 / (1.0 + self.fitness[i]))\n            perturb = np.random.normal(0, precision_factor, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration and convergence by integrating adaptive mutation scaling and fitness-based local search precision in DE.", "configspace": "", "generation": 4, "fitness": 0.7867657431173116, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.787 with standard deviation 0.012. And the mean value of best solutions found was 0.154 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7882882555446176, 0.8006516489046838, 0.7713573249026333], "final_y": [0.15516302599788256, 0.15130605708198308, 0.15407134131600475]}, "mutation_prompt": null}
{"id": "cec1fe9c-2f8c-4bca-91e3-d93a6c105a2c", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.8  # Adjusted crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            self.F = np.random.uniform(0.4, 0.9)  # Adaptive mutation factor\n            self.CR = np.random.uniform(0.7, 0.9)  # Adaptive crossover rate\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.015, self.dim)  # Adjusted perturbation noise\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved adaptive mutation and crossover strategies by adjusting perturbation noise during local search for better convergence in noisy environments.", "configspace": "", "generation": 4, "fitness": 0.7764100020411422, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.017. And the mean value of best solutions found was 0.158 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "45c7ab1f-6a43-4474-8843-8a578978a0d1", "metadata": {"aucs": [0.7543110868752925, 0.7945975584680356, 0.7803213607800984], "final_y": [0.16282710478633278, 0.16009569312905536, 0.15065262723100625]}, "mutation_prompt": null}
{"id": "8cd42a79-5273-45d7-ae4b-a00c77dd22eb", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            adapt_factor = np.exp(-self.fitness[i])  # Adaptive scaling factor\n            perturb = np.random.normal(0, 0.05 * adapt_factor, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce an adaptive scaling factor to the Gaussian perturbation in local search for improved exploration efficiency.", "configspace": "", "generation": 5, "fitness": 0.8489121242327687, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.057. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.019.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7709707050462619, 0.8714476584401996, 0.9043180092118446], "final_y": [0.15472202679617364, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "270c0985-a6d1-48ba-ab25-d67e2b113dc2", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.CR_decay = 0.99  # Decay factor for crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        self.CR *= self.CR_decay  # Decay the crossover probability\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.05, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration-exploitation balance by introducing a decay factor to the Differential Evolution's crossover probability, improving convergence in noisy environments.", "configspace": "", "generation": 5, "fitness": 0.8525254278064404, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.050. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7859957829408777, 0.8667159134022812, 0.9048645870761621], "final_y": [0.14905529809509244, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic enhancing DE with Adaptive Gradient Perturbation by incorporating a fitness diversity mechanism and dynamic mutation scaling for improved exploration and convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 5, "fitness": 0.8678666553190683, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.036. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "metadata": {"aucs": [0.8193405943521177, 0.8769942158822075, 0.9072651557228798], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "325813f1-30eb-4f5a-b67f-bf82d59aa9a1", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.05, self.dim) * (1.0 / (1.0 + np.log1p(self.fitness[i])))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic optimizer with adaptive population size and improved Gaussian perturbation scaling to increase exploration and exploitation efficiency.", "configspace": "", "generation": 5, "fitness": 0.8502471308227948, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.064. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.025.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7609798831891648, 0.8791492391796771, 0.9106122700995424], "final_y": [0.16615619053992936, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "ee4a5609-e43a-408e-ad0b-a79d9046839e", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F_initial = 0.5  # Initial differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        F_dynamic = self.F_initial * (1 - self.current_evaluations / self.budget)  # Adaptive F\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing the HybridMetaheuristicOptimizer by introducing an adaptive differential weight with a decay factor to improve convergence in high-dimensional noisy optimization.", "configspace": "", "generation": 5, "fitness": 0.8532777139395448, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.039. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "metadata": {"aucs": [0.8081820973443354, 0.8473330352624542, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing HybridMetaheuristicOptimizer by introducing a temperature-based simulated annealing mechanism to modulate mutation scaling dynamically for improved convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 6, "fitness": 0.866111450863134, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.036. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8183004640583854, 0.8744738217983861, 0.9055600667326306], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "2b5fdcf6-77a1-416c-92f4-9245481d0634", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "An enhanced hybrid metaheuristic that incorporates adaptive dynamic scaling of population size and an enriched local search phase, optimizing convergence in noisy high-dimensional environments.", "configspace": "", "generation": 6, "fitness": 0.8640352619306834, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.042. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8059509466742598, 0.8818368299059458, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "7af471b2-b20e-438c-9b06-4ece30d999e8", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            sigma = 0.05 * (1.0 / (1.0 + self.fitness[i]))  # Dynamic sigma adjustment\n            perturb = np.random.normal(0, sigma, self.dim)\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search by modifying mutation scaling and integrating a dynamic sigma in Gaussian perturbation, improving noise resilience.", "configspace": "", "generation": 6, "fitness": 0.8569369159701482, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.046. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "c4f13578-285f-4402-b4ea-28cf6431c6b8", "metadata": {"aucs": [0.7950450802584001, 0.8714476584401996, 0.9043180092118446], "final_y": [0.15650465425847504, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "501374d5-7594-46ae-b536-609ded4e6c9e", "solution": "import numpy as np\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR = 0.9  # Initial crossover probability\n        self.CR_decay = 0.99  # Decay factor for crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        # Adaptive mutation factor based on population diversity\n        pop_std = np.std(self.population, axis=0)\n        adapt_F = self.F + (np.mean(pop_std) / (np.max(pop_std) + 1e-8))\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + adapt_F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < self.CR\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        self.CR *= self.CR_decay  # Decay the crossover probability\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturb = np.random.normal(0, 0.05, self.dim) * (1.0 / (1.0 + self.fitness[i]))\n            candidate = np.clip(self.population[i] + perturb, func.bounds.lb, func.bounds.ub)\n            candidate_fitness = func(candidate)\n            self.current_evaluations += 1\n\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive mutation factor scaling based on population diversity to enhance exploration-exploitation balance in noisy environments.", "configspace": "", "generation": 6, "fitness": 0.8449663304971242, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.845 with standard deviation 0.063. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.023.", "error": "", "parent_id": "270c0985-a6d1-48ba-ab25-d67e2b113dc2", "metadata": {"aucs": [0.7582999189349621, 0.8722810633445659, 0.9043180092118446], "final_y": [0.16320038087579902, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "34647fa9-4ec3-4052-aafc-afbaf48dda5d", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_diversity = np.std(self.fitness) / np.mean(self.fitness)  # Added line for fitness diversity\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * fitness_diversity * (b - c), func.bounds.lb, func.bounds.ub)  # Adjusted mutation\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by adapting mutation factor based on fitness diversity to improve convergence in high-dimensional noisy optimization.", "configspace": "", "generation": 6, "fitness": 0.8629682631568244, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.041. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "metadata": {"aucs": [0.8075925643764211, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "5b7fedd1-ab2d-4c61-86a0-99afd26478b6", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\nfrom scipy.stats.qmc import Sobol\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        sobol_sampler = Sobol(d=self.dim, scramble=True)\n        self.population = sobol_sampler.random_base2(m=int(np.log2(self.population_size)))\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random(self.dim))  # Layer-wise adaptive mutation\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic enhancing DE with Adaptive Gradient Perturbation, utilizing layer-wise adaptive mutation scaling and incorporating a Sobol sequence initializer for improved exploration and convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 7, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 64 is out of bounds for axis 0 with size 64').", "error": "IndexError('index 64 is out of bounds for axis 0 with size 64')", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {}, "mutation_prompt": null}
{"id": "c31870d7-1cb6-4318-a500-b61891ccb771", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "A hybrid metaheuristic enhancing DE with Adaptive Gradient Perturbation by incorporating a fitness diversity mechanism and dynamic mutation scaling, now with adaptive population size adjustment to improve exploration and convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 7, "fitness": 0.8655767379702812, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.040. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8111396266067811, 0.8812725780922179, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a4ae8dda-897e-4095-8010-d07356bc8823", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        diversity_decay = 1 - self.current_evaluations / self.budget  # New line added\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold * diversity_decay:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introducing a decay factor to the diversity threshold for better adaptation during optimization.", "configspace": "", "generation": 7, "fitness": 0.8616436653035334, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.037. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "metadata": {"aucs": [0.8141982927485253, 0.8664146939502304, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "b44387c1-a0ce-44fa-9556-744d358f0f68", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved local search integration using L-BFGS-B with enhanced gradient estimation for refined solutions in high-dimensional noisy optimization.", "configspace": "", "generation": 7, "fitness": 0.8577612409762754, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.040. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "2b5fdcf6-77a1-416c-92f4-9245481d0634", "metadata": {"aucs": [0.8059509466742598, 0.8627372200236668, 0.9045955562308994], "final_y": [0.13953098700030486, 0.12078533840043593, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a feedback mechanism to dynamically adjust the differential weight `F` based on population fitness variance.", "configspace": "", "generation": 7, "fitness": 0.889093761091551, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.018. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "a8ce709e-87ca-4070-a74f-07ac194cdaff", "metadata": {"aucs": [0.863297142617574, 0.8996661314452346, 0.9043180092118446], "final_y": [0.12104294428798801, 0.11430045219118112, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "6b434e7d-3607-46b8-82af-f08a18663efb", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        max_iter_limit = min(20, self.budget - self.current_evaluations)  # Dynamic iteration limit\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True, maxfun=max_iter_limit)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance local exploration by dynamically adjusting the L-BFGS-B iteration limit based on remaining evaluations.", "configspace": "", "generation": 8, "fitness": 0.8561071008483347, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.053. And the mean value of best solutions found was 0.130 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "2b5fdcf6-77a1-416c-92f4-9245481d0634", "metadata": {"aucs": [0.7820570451412807, 0.8802830317674193, 0.905981225636304], "final_y": [0.1607378870254067, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            # Random walk step added for improved exploration\n            trial = np.where(crossover, mutant + np.random.normal(0, 0.1, self.dim), self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improved exploration by adding a random walk step to enhance diversity and escape local optima.", "configspace": "", "generation": 8, "fitness": 0.8814811240453886, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.018. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "c31870d7-1cb6-4318-a500-b61891ccb771", "metadata": {"aucs": [0.8611082573475979, 0.8790171055767233, 0.9043180092118446], "final_y": [0.12262013111615155, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-np.std(self.fitness))  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)  # Modify line\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Incorporate adaptive learning rate based on population convergence to enhance exploration-exploitation balance.", "configspace": "", "generation": 8, "fitness": 0.8714413097836542, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.032. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "2b5fdcf6-77a1-416c-92f4-9245481d0634", "metadata": {"aucs": [0.8297989560799489, 0.8769942158822075, 0.9075307573888063], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "7c217d15-aa64-450e-a866-1c2e4bad9fe7", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * np.cos(np.pi * self.current_evaluations / (2 * self.budget))  # Temperature decay\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n            else:\n                candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, candidate, bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n                if candidate_fitness < self.fitness[i]:\n                    self.population[i] = candidate\n                    self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce an adaptive temperature-based mutation scaling and reinforce local search with gradient boosting for enhanced convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 8, "fitness": 0.8584993579332018, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.039. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "metadata": {"aucs": [0.8099582316976108, 0.8612218328901498, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "83c4c206-a378-4576-9f5f-1c46aa5a6e24", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - np.std(self.fitness) / self.diversity_threshold)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + np.std(self.fitness))  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive crossover probability and dynamic mutation scaling based on fitness variance to balance exploration and exploitation.", "configspace": "", "generation": 8, "fitness": 0.8524467576915652, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.040. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "c31870d7-1cb6-4318-a500-b61891ccb771", "metadata": {"aucs": [0.8059509466742598, 0.8470713171885914, 0.9043180092118446], "final_y": [0.13953098700030486, 0.13476683816109503, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "e218cc75-38dd-42e5-8e9e-f8babe33f777", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.leap_intensity = 1.5  # Lévy flight intensity\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def levy_flights(self, size, dim):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n                 (np.gamma((1 + beta) / 2) * beta * 2**((beta - 1) / 2)))**(1 / beta)\n        u = np.random.normal(0, sigma, (size, dim))\n        v = np.random.normal(0, 1, (size, dim))\n        step = u / np.abs(v)**(1 / beta)\n        return step\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c) + self.levy_flights(1, self.dim)[0], func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by introducing Lévy flights and maintaining diversity with a novel adaptive mutation strategy.  ", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "metadata": {}, "mutation_prompt": null}
{"id": "29fd54a0-1d1a-49cb-9fc1-2dc959ec19bc", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c) + np.random.normal(0, 0.1, self.dim), func.bounds.lb, func.bounds.ub)  # Gaussian perturbation\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a Gaussian perturbation mechanism to modulate trial vector generation, enhancing exploration and convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "metadata": {}, "mutation_prompt": null}
{"id": "10547c56-a503-405a-bb31-ca9aeeb89f78", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                sample_fitness = [func(self.population[i]) for _ in range(3)]  # Evaluate three times to reduce noise\n                self.fitness[i] = np.mean(sample_fitness)\n                self.current_evaluations += 3\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance fitness evaluation by introducing a noise reduction mechanism to better handle noisy cost functions.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {}, "mutation_prompt": null}
{"id": "df9b74a8-f185-4fb7-ba17-a144dc515392", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True, pgtol=1e-5)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce step-size adaptation in the local search to improve convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {}, "mutation_prompt": null}
{"id": "9af80236-473a-415f-8824-b36643f847e5", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        self.population_size = max(4, int(10 * self.dim * (1 - self.current_evaluations / self.budget)))  # Adaptive population size\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce an adaptive population size strategy, scaling it down as evaluations increase, to improve exploration-exploitation balance.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "metadata": {}, "mutation_prompt": null}
{"id": "776397da-bbaa-415f-95be-f2fbb914e808", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        CR_dynamic = self.CR_initial * (1 - fitness_variance / (fitness_variance + 1))  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce fitness diversity analysis to dynamically adjust both F and CR, enhancing exploration and convergence in noisy high-dimensional optimization.", "configspace": "", "generation": 10, "fitness": 0.8615882334150983, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.043. And the mean value of best solutions found was 0.127 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8027158174069282, 0.877730873626522, 0.9043180092118446], "final_y": [0.1525652054433122, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "6b21f8f2-de90-4a60-8ae1-4b3ce9cb118d", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n                # Update F and CR using self-adaptive learning\n                self.F = 0.5 + 0.2 * np.random.rand()  # New adaptive F\n                self.CR_initial = 0.9 + 0.1 * np.random.rand()  # New adaptive CR\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce self-adaptive mechanisms for CR and F to enhance convergence by optimizing exploration-exploitation balance through dynamic learning.", "configspace": "", "generation": 10, "fitness": 0.8586550806046084, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.040. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "metadata": {"aucs": [0.8059509466742598, 0.8656962859277209, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "4f4ac8a0-7942-4153-af91-3fa4c835669e", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate + 0.01 * np.random.randn(self.dim)  # Memory-based perturbation\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced local search by employing memory-based perturbations to intensify search around promising regions.", "configspace": "", "generation": 10, "fitness": 0.855390805311528, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.036. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "metadata": {"aucs": [0.8231870334328452, 0.8371695716540293, 0.9058158108477093], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "9902bb8d-cd61-4396-ac55-e6a7aefad05d", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.initial_temperature = 1.0  # Initial temperature for simulated annealing\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = self.initial_temperature * (1 - self.current_evaluations / self.budget)  # Temperature decay\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * temperature  # Dynamic mutation with temperature\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            # Modify the selection strategy with a stochastic factor\n            if trial_fitness < self.fitness[i] or (np.std(self.fitness) < self.diversity_threshold and np.random.rand() < 0.5):\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the selection strategy by integrating a stochastic scaling factor to better preserve diversity and avoid premature convergence.", "configspace": "", "generation": 10, "fitness": 0.8674844211157601, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.039. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "46a3705c-62bf-4c3e-a417-230aca89ac21", "metadata": {"aucs": [0.813599438985599, 0.8845358151498366, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a09c5b63-15c9-4046-8f65-385e2054aba3", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            # Random walk step added for improved exploration\n            trial = np.where(crossover, mutant + np.random.normal(0, 0.1, self.dim), self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        local_search_prob = 0.2 if np.std(self.fitness) < self.diversity_threshold else 0.1  # Dynamic local search probability\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget or np.random.rand() > local_search_prob:\n                continue\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploitation by introducing a dynamic local search probability based on population diversity to improve convergence.", "configspace": "", "generation": 10, "fitness": 0.8628970683781333, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.003. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "metadata": {"aucs": [0.8609988839031463, 0.8611011489280526, 0.8665911723032012], "final_y": [0.1203597305622146, 0.11621992467546649, 0.1256768259266513]}, "mutation_prompt": null}
{"id": "90ca4322-2d2b-4b5b-acd0-10efbc38db64", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            # Adaptive random walk scaling\n            random_walk_scale = 0.1 * (1 - np.std(self.fitness) / self.diversity_threshold)\n            trial = np.where(crossover, mutant + np.random.normal(0, random_walk_scale, self.dim), self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Incorporate adaptive random walk scaling based on population convergence to further enhance diversity and escape local optima.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "metadata": {}, "mutation_prompt": null}
{"id": "c52a56f6-830c-4140-a965-8eaf68944fba", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + (np.std(self.population) / np.mean(self.population)))  # Changed line\n            adaptive_lr = np.exp(-np.std(self.fitness))  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the diversity maintenance by adjusting the dynamic mutation scaling factor based on population variance.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {}, "mutation_prompt": null}
{"id": "1e3b9a94-7f00-4568-80e8-2bf9d9f19f36", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        diversity = np.mean(np.std(self.population, axis=0))  # Calculate population diversity\n        self.F = 0.5 + 0.2 * (fitness_variance / (fitness_variance + 1)) * (diversity + 1)  # Adjust F based on variance and diversity\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by refining the mutation strategy with an adaptive scaling factor based on population diversity.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {}, "mutation_prompt": null}
{"id": "980571db-d7a5-4121-a758-c90e1e33e60f", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        ls_freq = max(1, self.budget // (2 * self.population_size))  # Adjust frequency dynamically\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget or i % ls_freq != 0:\n                continue\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine budget utilization by dynamically adjusting the local search frequency to improve convergence efficiency.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {}, "mutation_prompt": null}
{"id": "7b62a552-d592-4cd4-af66-ec24fbbd08af", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)  # Changed line: increased the frequency of local search invocation\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the local search by incorporating a more frequent local search step to improve convergence to local optima.", "configspace": "", "generation": 11, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('scale < 0').", "error": "ValueError('scale < 0')", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {}, "mutation_prompt": null}
{"id": "826f9052-e5f4-4e32-8c4a-c23dfae5f286", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        convergence_rate = np.mean(self.fitness) / np.min(self.fitness)\n        if convergence_rate < 1.5:  # Dynamically adjust population size\n            self.population_size = min(100 * self.dim, self.population_size + self.dim)\n            self.initialize_population(func)\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic population size adjustment based on convergence rate to enhance exploration and exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.8695604685769528, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.050. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.7997856034140207, 0.8945782222588782, 0.9143175800579593], "final_y": [0.14314404442741346, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "c2c33325-3133-47b7-b025-7a015b63a045", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5\n        self.CR_initial = 0.9\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.3 * fitness_variance / (fitness_variance + 1)  \n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        layer_factor = 0.1 + 0.9 * (self.current_evaluations / self.budget)\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c) * layer_factor, func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Incorporate an adaptive mechanism to optimize crossover probability and introduce a layer-wise dynamic F adjustment for improved convergence in complex photonic structure optimization.", "configspace": "", "generation": 12, "fitness": 0.8579964817453263, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.040. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8073754709382419, 0.8622959650858926, 0.9043180092118446], "final_y": [0.13953098700030486, 0.1150697616756905, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "c76b4b4a-a73a-47cf-b072-e65e0d4da8e7", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-np.std(self.fitness))  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturbation = np.random.normal(0, 0.1, size=self.dim)\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i] + perturbation, bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploitation by integrating a perturbation mechanism in local search to diversify solutions.", "configspace": "", "generation": 12, "fitness": 0.8652372270505232, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.038. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {"aucs": [0.8134524966493084, 0.8769942158822075, 0.9052649686200533], "final_y": [0.13953400619163048, 0.11621992467546649, 0.11202858362578216]}, "mutation_prompt": null}
{"id": "dd1108f4-1586-4378-96f5-df9d57289d69", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) * np.exp(-np.std(self.fitness))  # Adaptive mutation\n            adaptive_lr = np.exp(-np.std(self.fitness))  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)  # Modify line\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce an adaptive mutation factor that decreases as the fitness variance increases, enhancing convergence stability.", "configspace": "", "generation": 12, "fitness": 0.8517314198911792, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.045. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {"aucs": [0.7974585260761636, 0.8503935770399976, 0.9073421565573764], "final_y": [0.15081679663920233, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "cbe3c84e-003e-436b-bbd4-c773d7dcee12", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c) + (0.1 * np.random.randn(self.dim)), func.bounds.lb, func.bounds.ub)  # Added noise\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce adaptive mutation scaling for enhanced exploration in high-dimensional spaces. ", "configspace": "", "generation": 12, "fitness": 0.8675743926108556, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.034. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8214109527385147, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13475910408546243, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "31738da4-c1bd-4fb2-8750-69d78c0fc2fd", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random() * np.std(self.fitness)) # Updated line\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            # Random walk step added for improved exploration\n            trial = np.where(crossover, mutant + np.random.normal(0, 0.1, self.dim), self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the adaptive mutation scaling by factoring in fitness variance to improve convergence under noisy conditions.", "configspace": "", "generation": 13, "fitness": 0.8556218663158988, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.856 with standard deviation 0.035. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "metadata": {"aucs": [0.807712876070678, 0.8697599739999033, 0.889392748877115], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "d9b4bd98-020f-4e42-8eab-26bcc2fa217c", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            perturbation_scale = 0.1  # Scaling factor for perturbation\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(\n                lambda x: func(x) + perturbation_scale * np.random.randn(), \n                self.population[i], \n                bounds=list(zip(func.bounds.lb, func.bounds.ub)), \n                approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhancing local search by incorporating a scaling factor to perturb solutions in the local search phase for improved convergence.", "configspace": "", "generation": 13, "fitness": 0.8410123569106127, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.841 with standard deviation 0.048. And the mean value of best solutions found was 0.131 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.7744252372791904, 0.8627076953459007, 0.885904138106747], "final_y": [0.16507221897206015, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "4fcb73d0-4088-4643-ac5d-761811325fea", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n\n        if np.random.rand() < 0.1:  # Introduce random perturbations for diversity\n            self.F *= np.random.uniform(0.8, 1.2)\n\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        budget_left = self.budget - self.current_evaluations  # Dynamic allocation of local search\n        local_budget = max(0, budget_left // (2 * self.population_size))\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True, maxfun=local_budget)\n            self.current_evaluations += local_budget\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance population diversity and robustness by integrating a perturbation mechanism and allocating budget dynamically.", "configspace": "", "generation": 13, "fitness": 0.8306829113390743, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.831 with standard deviation 0.062. And the mean value of best solutions found was 0.136 (0. is the best) with standard deviation 0.030.", "error": "", "parent_id": "826f9052-e5f4-4e32-8c4a-c23dfae5f286", "metadata": {"aucs": [0.7434409289180408, 0.8630826623479146, 0.8855251427512676], "final_y": [0.17872903713207655, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "24f8e601-2478-443e-a41b-77c86ec2b805", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        selection_probs = np.exp(-np.arange(self.population_size) / self.population_size)\n        selection_probs /= selection_probs.sum()  # Normalize probabilities\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = np.random.choice(range(self.population_size), 3, replace=False, p=selection_probs)\n            a, b, c = self.population[indices]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            F_dynamic *= (1 - self.fitness[i] / np.max(self.fitness))  # Weighted mutation\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced hybrid metaheuristic by including stochastic rank-based selection and weighted mutation to improve search efficiency and convergence.", "configspace": "", "generation": 13, "fitness": 0.8517381640109596, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.030. And the mean value of best solutions found was 0.124 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8125271889341058, 0.8572016876665307, 0.8854856154322421], "final_y": [0.14273772859681477, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "b9b1a10b-261c-41a2-99a4-e98ea95b865a", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-np.std(self.fitness))  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)  # Modify line\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n\n            # Stochastic tunneling transformation\n            tunnel_fitness = -np.exp(-func(trial)) + 1\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by integrating a stochastic tunneling mechanism to escape local minima effectively.", "configspace": "", "generation": 13, "fitness": 0.8509205658811839, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.851 with standard deviation 0.037. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {"aucs": [0.7993569657511495, 0.8679191164601601, 0.8854856154322421], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "d73447a2-74bf-4513-b765-2dd803e16aaf", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            # Changed line below\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold * (1 + self.current_evaluations / self.budget):\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introducing a dynamic strategy for adjusting the diversity threshold based on the current budget utilization to better balance exploration and exploitation.", "configspace": "", "generation": 14, "fitness": 0.8602747436334764, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.041. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8059509466742598, 0.8697470123370972, 0.9051262718890725], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "af30bc68-ea88-44e3-ac15-e25a2d681c9c", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.stagnation_counter = 0  # Initialize stagnation counter\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            dynamic_mutation = np.random.uniform(0.3, 0.8)  # Dynamic mutation strategy\n            mutant = np.clip(a + dynamic_mutation * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n                self.stagnation_counter = 0  # Reset stagnation counter on improvement\n            else:\n                self.stagnation_counter += 1\n                if self.stagnation_counter > 5:  # Check for stagnation\n                    self.initialize_population(func)\n                    self.evaluate_population(func)\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a dynamic mutation strategy and incorporate a reset mechanism to enhance exploration in case of stagnation.", "configspace": "", "generation": 14, "fitness": 0.8638865352788123, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.041. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8086574921109926, 0.8769942158822075, 0.9060078978432371], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "2333a66a-ffe1-4879-a9cf-e6995c91e69b", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            if np.random.rand() < 0.7:  # Probabilistic switch to alternative mutation strategy\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            else:\n                mutant = np.clip(b + self.F * (a - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by incorporating a probabilistic mechanism to switch between mutation strategies and dynamic population adjustment.", "configspace": "", "generation": 14, "fitness": 0.8483313325594333, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.029. And the mean value of best solutions found was 0.126 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8079378032903413, 0.8669398206327357, 0.8701163737552231], "final_y": [0.13953098700030486, 0.11621992467546649, 0.12333080627771065]}, "mutation_prompt": null}
{"id": "b966293f-7aba-442c-9eb6-ee7f29daeffc", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            # Change: Dynamic mutation scaling based on fitness variance\n            F_dynamic = self.F * (1.0 + 0.5 * np.tanh(np.std(self.fitness)))  \n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            # Random walk step added for improved exploration\n            trial = np.where(crossover, mutant + np.random.normal(0, 0.1, self.dim), self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a variance-based adaptive control on the mutation scaling factor to improve exploration-exploitation balance.", "configspace": "", "generation": 14, "fitness": 0.8612687912892799, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.042. And the mean value of best solutions found was 0.122 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "metadata": {"aucs": [0.8059509466742598, 0.8714645959413015, 0.9063908312522788], "final_y": [0.13953098700030486, 0.11523365389755746, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "353b8566-048c-47c5-a9e9-8263a2995b3d", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-np.std(self.fitness) * 0.5)  # Improved scaling\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)  # Modify line\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        # Early stopping based on convergence\n        if np.std(self.fitness) < self.diversity_threshold:\n            return self.population[np.argmin(self.fitness)], min(self.fitness)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            if np.std(self.fitness) < self.diversity_threshold:\n                break\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced early stopping based on convergence and improved adaptive learning rate scaling to enhance optimization efficiency.", "configspace": "", "generation": 14, "fitness": 0.8624210572561039, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.041. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {"aucs": [0.8059509466742598, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "dfb48a92-8ae4-4f6b-b8ef-be2407b748fa", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)\n        fitness_variance = np.var(self.fitness)\n        fitness_improvement = np.mean(np.abs(np.diff(self.fitness)))\n        self.F = 0.5 + 0.3 * fitness_variance / (fitness_variance + 1)\n        adaptive_pop_size = max(4, int(self.population_size * (1 + fitness_improvement / (fitness_improvement + 1))))\n        for i in range(adaptive_pop_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced the feedback mechanism by incorporating a dynamic mutation scale and adaptive population size based on fitness improvement rate to improve exploration and convergence.", "configspace": "", "generation": 15, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 100 is out of bounds for axis 0 with size 100').", "error": "IndexError('index 100 is out of bounds for axis 0 with size 100')", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {}, "mutation_prompt": null}
{"id": "6a4643e0-56f5-4e15-8f07-ddecf4528d4a", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True, maxiter=150)  # Increased iterations\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance local search by increasing iterations of L-BFGS-B to improve convergence on refined solutions.", "configspace": "", "generation": 15, "fitness": 0.8627243375828363, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.042. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8059509466742598, 0.8779040568624045, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "063893fa-d04c-4249-868a-0f58f3717d12", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-np.std(self.fitness))  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)  # Modify line\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            neighborhood_fitness = func(candidate + 0.01 * np.random.randn(self.dim))  # Modify line\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n            elif neighborhood_fitness < self.fitness[i]:  # Check neighborhood fitness\n                self.population[i] = candidate + 0.01 * np.random.randn(self.dim)\n                self.fitness[i] = neighborhood_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduced a neighborhood-based local search strategy to enhance solution quality and refinement.", "configspace": "", "generation": 15, "fitness": 0.861728447028875, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.036. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {"aucs": [0.8192109878759632, 0.8580449314838592, 0.9079294217268024], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "e915761b-37b9-474d-8aef-d1f7a46dd93b", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            # Dynamic random walk step for improved exploration\n            trial = np.where(crossover, mutant + np.random.normal(0, 0.1 * np.std(self.fitness), self.dim), self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by modifying the random walk step with dynamic scaling based on fitness variance.", "configspace": "", "generation": 15, "fitness": 0.8644532738810492, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.039. And the mean value of best solutions found was 0.121 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "metadata": {"aucs": [0.8120475965490958, 0.8769942158822075, 0.9043180092118446], "final_y": [0.1354947021293984, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "aad076a6-3ef9-418e-bcba-e123bd9a8ca5", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget) + 0.1 * (fitness_variance / (fitness_variance + 1))\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Incorporate feedback to adjust crossover probability based on fitness variance for improved exploration-exploitation balance.", "configspace": "", "generation": 15, "fitness": 0.8612488421968809, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.861 with standard deviation 0.035. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8207717875327136, 0.8570769060474153, 0.9058978330105139], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "d30fade8-798d-464f-8262-ec10b38ac70e", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        best_idx = np.argmin(self.fitness)  # Preserve the best individual\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i and idx != best_idx]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce elitism by preserving the best individual from the current generation to enhance convergence.", "configspace": "", "generation": 16, "fitness": 0.8092903630364038, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.015. And the mean value of best solutions found was 0.151 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8291737717497611, 0.7945901820563235, 0.8041071353031268], "final_y": [0.13953098700030486, 0.16010176693593792, 0.15358793513971736]}, "mutation_prompt": null}
{"id": "66b365c2-f170-428c-bd54-2cab5ddfee54", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        convergence_rate = np.mean(self.fitness) / np.min(self.fitness)\n        if convergence_rate < 1.5:  # Dynamically adjust population size\n            self.population_size = min(100 * self.dim, self.population_size + self.dim)\n            self.initialize_population(func)\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n                if np.abs(candidate_fitness - self.fitness[i]) < 1e-6:  # Stop if improvement is negligible\n                    break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance local search by incorporating a stopping criterion based on the improvement rate to avoid unnecessary computations.", "configspace": "", "generation": 16, "fitness": 0.7931689943135493, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.793 with standard deviation 0.016. And the mean value of best solutions found was 0.156 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "826f9052-e5f4-4e32-8c4a-c23dfae5f286", "metadata": {"aucs": [0.8120374278654585, 0.7945901820563235, 0.7728793730188657], "final_y": [0.13953098700030486, 0.16010176693593792, 0.16772596860633315]}, "mutation_prompt": null}
{"id": "fc007dc8-dc8b-4b27-8d26-5f6419d12eee", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        diversity_metric = np.std(self.fitness) / (np.mean(self.fitness) + 1e-9)\n        if diversity_metric < 0.1:  # Introduce mutation strategy adaptation based on diversity\n            self.F *= 1.5\n        convergence_rate = np.mean(self.fitness) / np.min(self.fitness)\n        if convergence_rate < 1.5:  # Dynamically adjust population size\n            self.population_size = min(100 * self.dim, self.population_size + self.dim)\n            self.initialize_population(func)\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a fitness diversity mechanism to enhance exploration and convergence by adapting the mutation strategy based on population diversity.", "configspace": "", "generation": 16, "fitness": 0.803477946629444, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.803 with standard deviation 0.006. And the mean value of best solutions found was 0.150 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "826f9052-e5f4-4e32-8c4a-c23dfae5f286", "metadata": {"aucs": [0.8059509466742598, 0.7945901820563235, 0.8098927111577487], "final_y": [0.13953098700030486, 0.16010176693593792, 0.15129970232579693]}, "mutation_prompt": null}
{"id": "7825501f-e474-48ba-ac6f-49487fe5a003", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.convergence_rate_threshold = 1e-5  # New convergence threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)**2  # Modified adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-np.std(self.fitness))  \n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n            if np.std(self.fitness) < self.convergence_rate_threshold:  # Early convergence detection\n                break\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration-exploitation balance and convergence speed by introducing a dynamic crossover rate and early convergence detection.", "configspace": "", "generation": 16, "fitness": 0.8006167656632982, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.801 with standard deviation 0.008. And the mean value of best solutions found was 0.152 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {"aucs": [0.8112580380386132, 0.7958413506910068, 0.7947509082602744], "final_y": [0.13953098700030486, 0.15957994298850153, 0.15716438568516122]}, "mutation_prompt": null}
{"id": "2df3cfd2-4173-4d60-938e-012cd8ae51ff", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n        self.last_best_fitness = np.inf\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        improvement = np.min(self.fitness) < self.last_best_fitness\n        self.last_best_fitness = np.min(self.fitness)\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random()) if improvement else self.F\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            local_iters = 2 if self.last_best_fitness < np.min(self.fitness) else 5\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True, maxfun=local_iters)\n            self.current_evaluations += local_iters\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the HybridMetaheuristicOptimizer with dynamic adjustment of both mutation factor and local search intensity based on fitness improvement trends.", "configspace": "", "generation": 16, "fitness": 0.7660154926808325, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.766 with standard deviation 0.022. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.7423820363094582, 0.7945901820563235, 0.7610742596767157], "final_y": [0.17924064183162647, 0.16010176693593792, 0.17296300850874857]}, "mutation_prompt": null}
{"id": "6773f7f6-8fed-41ee-aa2c-f6440fffa087", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.previous_successful_mutations = []\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            if self.previous_successful_mutations:  # Use memory-based strategy\n                F_selected = np.random.choice(self.previous_successful_mutations)\n            else:\n                F_selected = self.F\n            mutant = np.clip(a + F_selected * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n                self.previous_successful_mutations.append(F_selected)  # Store successful mutation factor\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance diversity during mutation by introducing a memory-based strategy to store and use successful mutation factors.", "configspace": "", "generation": 17, "fitness": 0.8650350538615738, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.038. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8137929364906694, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "100f784e-a313-43f0-b6dd-051b60a93807", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + np.random.uniform(0.3, 0.7))  # Probabilistic dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration by making the mutation scaling factor probabilistic to better escape local optima.", "configspace": "", "generation": 17, "fitness": 0.8649955028078238, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.038. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8136742833294192, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "efa2deac-1ad9-4c05-8b70-497f956fb70b", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            # Random walk step with dynamic standard deviation\n            std_dev = 0.1 * (1 - self.current_evaluations / self.budget)\n            trial = np.where(crossover, mutant + np.random.normal(0, std_dev, self.dim), self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune random walk step by adjusting the standard deviation dynamically based on current evaluations to enhance exploration. ", "configspace": "", "generation": 17, "fitness": 0.8633807383387216, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.863 with standard deviation 0.040. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "metadata": {"aucs": [0.808829989922113, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "a27e8f88-8c60-4018-b7f1-a33744d8aa57", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_std = np.std(self.fitness)\n        CR_dynamic = CR_dynamic * (1 - fitness_std / (fitness_std + self.diversity_threshold))  # Modify line\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-fitness_std)  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)  # Modify line\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or fitness_std < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        # Adaptive dynamic population resizing based on evaluations\n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a fitness diversity measure to adaptively adjust the crossover probability to improve exploration.", "configspace": "", "generation": 17, "fitness": 0.8636378602460629, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.040. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {"aucs": [0.8096013556441366, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "18b3a0dc-ed60-4ed3-95ac-fdcdfe3bcd7f", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        temperature = 1 - self.current_evaluations / self.budget\n        self.F = 0.5 + 0.3 * temperature  # Adjust F based on temperature\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance exploration-exploitation balance by introducing temperature-based mutation scaling and dynamic population size adjustment.", "configspace": "", "generation": 17, "fitness": 0.8657473166558134, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.866 with standard deviation 0.037. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8159297248733879, 0.8769942158822075, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "43d31480-db03-4464-af4a-a7999541bc29", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 0.1  # New diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        fitness_mean = np.mean(self.fitness)  # Calculate mean fitness\n        diversity_factor = fitness_variance / (fitness_mean + 1e-6)  # Calculate diversity factor\n        if diversity_factor < self.diversity_threshold:  # Check against diversity threshold\n            self.population_size = int(self.population_size * 1.1)  # Increase population size\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptation by introducing a fitness diversity threshold to dynamically adjust population size and mutate scaling.  ", "configspace": "", "generation": 18, "fitness": -Infinity, "feedback": "An exception occurred: IndexError('index 102 is out of bounds for axis 0 with size 100').", "error": "IndexError('index 102 is out of bounds for axis 0 with size 100')", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {}, "mutation_prompt": null}
{"id": "64d5ac7a-86ba-4cea-9192-10cb105e403c", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.85  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Fine-tune initial crossover probability to improve exploration-exploitation balance.", "configspace": "", "generation": 18, "fitness": 0.8623269409259112, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.034. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4f6a15c8-6d93-47ff-8ffb-dce2b6b246d1", "metadata": {"aucs": [0.8212860818962098, 0.861376731669679, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "d920171b-c08b-4461-888c-ba30c6a2512f", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget) * np.exp(-np.std(self.fitness))  # Modified line\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-np.std(self.fitness))  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance adaptive learning by incorporating a dynamic crossover rate adjustment based on fitness variance.", "configspace": "", "generation": 18, "fitness": 0.8744022375374509, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.022. And the mean value of best solutions found was 0.115 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "0596c2f5-3584-40db-a125-41b1db7d3eb0", "metadata": {"aucs": [0.8534992408400782, 0.8653894625604301, 0.9043180092118446], "final_y": [0.11680189506080652, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "878088b1-4c39-4abf-8253-ec8cbe56e6fb", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.7 + 0.3 * fitness_variance / (fitness_variance + 1)  # Adjust F with weight for stability\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(0, self.population_size, 2):  # Increase the frequency of local search\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance the hybrid metaheuristic optimizer by introducing weighting for fitness variance-based `F` adjustment and improving local search frequency.", "configspace": "", "generation": 18, "fitness": 0.8531025715928994, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.039. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.8091869411931711, 0.8458027643736823, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "e121adad-c028-4a29-8eaf-e25d76eb09ed", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            mutant = np.clip(a + F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            # Lévy flight step added for enhanced exploration\n            levy_flight = 0.01 * np.random.standard_cauchy(self.dim)\n            trial = np.where(crossover, mutant + levy_flight, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n        # Adaptive population size adjustment\n        self.population_size = max(2, int(self.population_size * (1 - 0.01 * (self.current_evaluations / self.budget))))\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhanced exploration by introducing a Lévy flight step to improve solution diversity and escape local optima effectively.", "configspace": "", "generation": 18, "fitness": 0.8518795638544291, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.039. And the mean value of best solutions found was 0.123 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "4ff9f2f7-9f02-4b82-941d-f72625513bc5", "metadata": {"aucs": [0.8101087306440758, 0.8412119517073668, 0.9043180092118446], "final_y": [0.13953098700030486, 0.11621992467546649, 0.11210677698085747]}, "mutation_prompt": null}
{"id": "f89a8761-8921-45d6-8704-9292ad2ab1b7", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.3 * fitness_variance / (fitness_variance + 1.5)  # Adjust F based on fitness variance\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Improve the exploration-exploitation balance by refining the adaptive adjustment of differential weight `F` and introducing more dynamic mutation strategies.", "configspace": "", "generation": 19, "fitness": 0.8587300722522343, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.859 with standard deviation 0.043. And the mean value of best solutions found was 0.129 (0. is the best) with standard deviation 0.014.", "error": "", "parent_id": "37a6b72d-08d2-478c-b17f-b7637812f1b0", "metadata": {"aucs": [0.7989137611881573, 0.8954626373415753, 0.8818138182269704], "final_y": [0.14901300403868434, 0.1170726385445815, 0.11963348481225788]}, "mutation_prompt": null}
{"id": "dd6e696c-f9de-4b41-89e2-1a8e50c568e8", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget) * np.exp(-0.5 * np.std(self.fitness))  # Modified line\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())  # Dynamic mutation scaling\n            adaptive_lr = np.exp(-np.std(self.fitness))  # New adaptive learning rate\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce a progressive adaptation mechanism in the dynamic crossover rate to improve convergence.", "configspace": "", "generation": 19, "fitness": 0.8552621044428667, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.855 with standard deviation 0.052. And the mean value of best solutions found was 0.128 (0. is the best) with standard deviation 0.020.", "error": "", "parent_id": "d920171b-c08b-4461-888c-ba30c6a2512f", "metadata": {"aucs": [0.7824499587558646, 0.8896459942426348, 0.8936903603301006], "final_y": [0.15648149417872348, 0.11707329561888591, 0.1102237901658486]}, "mutation_prompt": null}
{"id": "dc5b8915-b53d-4474-9625-4b89e6e98224", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget) * np.exp(-np.std(self.fitness))\n        best_idx = np.argmin(self.fitness)  # New line\n        best_individual = self.population[best_idx].copy()  # New line\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + 0.5 * np.random.random())\n            adaptive_lr = np.exp(-np.std(self.fitness))\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n        \n        self.population[0] = best_individual  # New line\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Introduce elite preservation by retaining the best individual across generations to enhance convergence.", "configspace": "", "generation": 19, "fitness": 0.8756801826009352, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.014. And the mean value of best solutions found was 0.117 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d920171b-c08b-4461-888c-ba30c6a2512f", "metadata": {"aucs": [0.8618939235029561, 0.8942824981364554, 0.8708641261633938], "final_y": [0.11716513883326907, 0.11765387244767278, 0.11475629561239464]}, "mutation_prompt": null}
{"id": "09213383-9a2b-466d-af3f-4c8f67c31a68", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget)  # Adaptive CR\n        fitness_variance = np.var(self.fitness)\n        self.F = 0.5 + 0.2 * fitness_variance / (fitness_variance + 1)  # Adjust F based on fitness variance\n        convergence_rate = np.mean(self.fitness) / np.min(self.fitness)\n        if convergence_rate < 1.5:  # Dynamically adjust population size\n            self.population_size = min(100 * self.dim, self.population_size + self.dim)\n            self.initialize_population(func)\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        leader_weight = 0.7\n        leader_idx = np.argmin(self.fitness)\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate_init = leader_weight * self.population[leader_idx] + (1 - leader_weight) * self.population[i]\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, candidate_init, bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                self.population[i] = candidate\n                self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Refine local search by initializing L-BFGS-B with a weighted sum of population leaders and current individuals to enhance convergence.", "configspace": "", "generation": 19, "fitness": 0.8648944803610545, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.865 with standard deviation 0.046. And the mean value of best solutions found was 0.118 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "826f9052-e5f4-4e32-8c4a-c23dfae5f286", "metadata": {"aucs": [0.8064211218915915, 0.9173981930281779, 0.8708641261633938], "final_y": [0.13003557201655536, 0.11012199262683964, 0.11475629561239464]}, "mutation_prompt": null}
{"id": "1398d7a2-cf7f-4d34-b968-c0c0f39b6b8a", "solution": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass HybridMetaheuristicOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = 10 * dim\n        self.F = 0.5  # Differential weight\n        self.CR_initial = 0.9  # Initial crossover probability\n        self.current_evaluations = 0\n        self.diversity_threshold = 1e-4  # Fitness diversity threshold\n        self.fitness_improvement_threshold = 1e-5  # New fitness improvement threshold\n        self.mutation_scale_factor = 0.1  # New mutation scale factor\n\n    def initialize_population(self, func):\n        self.population = np.random.rand(self.population_size, self.dim)\n        self.population = self.population * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n        self.fitness = np.full(self.population_size, np.inf)\n\n    def evaluate_population(self, func):\n        for i in range(self.population_size):\n            if self.fitness[i] == np.inf:\n                self.fitness[i] = func(self.population[i])\n                self.current_evaluations += 1\n\n    def differential_evolution_step(self, func):\n        CR_dynamic = self.CR_initial * (1 - self.current_evaluations / self.budget) * np.exp(-np.std(self.fitness))\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            indices = [idx for idx in range(self.population_size) if idx != i]\n            a, b, c = self.population[np.random.choice(indices, 3, replace=False)]\n            F_dynamic = self.F * (1.0 + self.mutation_scale_factor * np.random.random())  # Adjusted line\n            adaptive_lr = np.exp(-np.std(self.fitness))\n            mutant = np.clip(a + adaptive_lr * F_dynamic * (b - c), func.bounds.lb, func.bounds.ub)\n            crossover = np.random.rand(self.dim) < CR_dynamic\n            trial = np.where(crossover, mutant, self.population[i])\n            trial_fitness = func(trial)\n            self.current_evaluations += 1\n\n            if trial_fitness < self.fitness[i] or np.std(self.fitness) < self.diversity_threshold:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n\n    def local_search(self, func):\n        for i in range(self.population_size):\n            if self.current_evaluations >= self.budget:\n                break\n            candidate, candidate_fitness, _ = fmin_l_bfgs_b(func, self.population[i], bounds=list(zip(func.bounds.lb, func.bounds.ub)), approx_grad=True)\n            self.current_evaluations += 1\n            if candidate_fitness < self.fitness[i]:\n                if self.fitness[i] - candidate_fitness > self.fitness_improvement_threshold:  # New condition\n                    self.population[i] = candidate\n                    self.fitness[i] = candidate_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.evaluate_population(func)\n        \n        if self.current_evaluations > self.budget * 0.5:\n            self.population_size = int(self.population_size * 0.75)\n\n        while self.current_evaluations < self.budget:\n            self.differential_evolution_step(func)\n            self.local_search(func)\n\n        best_idx = np.argmin(self.fitness)\n        return self.population[best_idx], self.fitness[best_idx]", "name": "HybridMetaheuristicOptimizer", "description": "Enhance optimization by implementing a fitness diversity mechanism and adaptive mutation step to maintain exploration, improving global and local search balance.", "configspace": "", "generation": 19, "fitness": 0.8707843352950203, "feedback": "The algorithm HybridMetaheuristicOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.018. And the mean value of best solutions found was 0.116 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "d920171b-c08b-4461-888c-ba30c6a2512f", "metadata": {"aucs": [0.8514843010614235, 0.8954660287526183, 0.865402676071019], "final_y": [0.11765562833661125, 0.11707795493106277, 0.11475130307572212]}, "mutation_prompt": null}
