{"id": "36722668-7b29-41ac-9d3f-a7852eebdd8d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_initial_samples = int(self.budget * 0.1)\n        remaining_budget = self.budget - num_initial_samples\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        \n        # Uniformly sample initial points within bounds\n        samples = np.random.uniform(func.bounds.lb, func.bounds.ub, (num_initial_samples, self.dim))\n        best_sample = None\n        best_sample_value = float('inf')\n        \n        # Evaluate sampled points\n        for sample in samples:\n            value = func(sample)\n            if value < best_sample_value:\n                best_sample_value = value\n                best_sample = sample\n        \n        # Use BFGS local optimization from the best initial sample\n        def wrapped_func(x):\n            nonlocal remaining_budget\n            if remaining_budget <= 0:\n                return float('inf')\n            remaining_budget -= 1\n            return func(x)\n        \n        result = minimize(wrapped_func, best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': remaining_budget})\n        \n        return result.x", "name": "HybridOptimizer", "description": "A hybrid metaheuristic combining uniform sampling for diverse initial guesses and BFGS for fast local convergence in smooth, low-dimensional landscapes.", "configspace": "", "generation": 0, "fitness": 0.8367815240156425, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.114. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.997302974723862, 0.7429003689843958, 0.7701412283386698], "final_y": [0.0, 8.799581984581157e-08, 4.257508418968081e-08]}, "mutation_prompt": null}
{"id": "4b6880da-722a-4581-a203-473911e8175f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ABEL:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n\n    def __call__(self, func):\n        # Initial sampling: Uniform sampling across the parameter space\n        lb, ub = np.array(func.bounds.lb), np.array(func.bounds.ub)\n        best_solution = lb + (ub - lb) * np.random.rand(self.dim)\n        best_value = func(best_solution)\n        self.evaluations += 1\n\n        # Adaptive boundary exploration\n        while self.evaluations < self.budget * 0.5:  # Allocate half budget for exploration\n            candidate = lb + (ub - lb) * np.random.rand(self.dim)\n            candidate_value = func(candidate)\n            self.evaluations += 1\n\n            if candidate_value < best_value:\n                best_solution = candidate\n                best_value = candidate_value\n\n        # Local optimization using BFGS\n        def local_objective(x):\n            nonlocal best_value\n            value = func(x)\n            self.evaluations += 1\n            if value < best_value:\n                best_value = value\n            return value\n\n        options = {'maxiter': self.budget - self.evaluations, 'disp': False}\n        result = minimize(local_objective, best_solution, method='BFGS', bounds=list(zip(lb, ub)), options=options)\n\n        # Return the best solution found\n        return result.x, best_value", "name": "ABEL", "description": "Adaptive Boundary Exploration and Local Optimization (ABEL) algorithm combines global search via adaptive boundary exploration with local refinement using BFGS to efficiently navigate and optimize low-dimensional, smooth landscapes within a given budget.", "configspace": "", "generation": 0, "fitness": 0.6792318936771977, "feedback": "The algorithm ABEL got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.679 with standard deviation 0.228. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9971652762226556, 0.5697038173078318, 0.47082658750110573], "final_y": [0.0, 1.4647961450289086e-08, 1.7483106740549028e-07]}, "mutation_prompt": null}
{"id": "b4f0fab7-55ec-4118-8fbe-540bf98ee181", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//10, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            lb = np.maximum(best_point - (ub - lb) * 0.1, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * 0.1, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Adaptive Multistart Nelder-Mead with Dynamic Bound Adjustment for Fast Convergence on Smooth Landscapes", "configspace": "", "generation": 0, "fitness": 0.6111105445181844, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.611 with standard deviation 0.102. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.47352994592147735, 0.6430931982264336, 0.7167084894066419], "final_y": [2.892788591925166e-06, 9.078113710197516e-07, 1.6033442577217393e-06]}, "mutation_prompt": null}
{"id": "ef3b4549-8562-4066-9046-6d60bea493e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling\n        num_initial_samples = min(10, self.budget // 2)\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.1 * (upper_bounds - lower_bounds)\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "A dynamic boundary-adjusted local search algorithm leveraging uniform sampling for initial exploration and BFGS for quick convergence in smooth landscapes.", "configspace": "", "generation": 0, "fitness": 0.8707076416739725, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.090. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9981490934530588, 0.8163885459972562, 0.7975852855716027], "final_y": [0.0, 5.110444792526362e-08, 1.240891216534901e-07]}, "mutation_prompt": null}
{"id": "745e65a7-98d0-475b-8df0-a09c5752a6f8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Define a local optimization method\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        # Determine the number of local searches based on the budget\n        num_local_searches = min(10, self.budget // 10)\n        function_evals_per_search = self.budget // num_local_searches\n\n        # Perform multiple random initializations with local optimization\n        for _ in range(num_local_searches):\n            # Random initial guess within bounds\n            x0 = np.random.uniform(lb, ub)\n            local_optimize(x0)\n\n            # If budget is exceeded, break\n            if self.budget < 0:\n                break\n\n        return best_solution\n\n# Example of calling the optimizer:\n# Assume `func` is a black-box function with attributes `bounds.lb` and `bounds.ub`.\n# optimizer = HybridLocalGlobalOptimizer(budget=100, dim=2)\n# best_solution = optimizer(func)", "name": "HybridLocalGlobalOptimizer", "description": "A hybrid local-global optimizer leveraging the efficiency of BFGS for rapid convergence in smooth landscapes, supplemented by random sampling to explore diverse initial conditions.", "configspace": "", "generation": 0, "fitness": 0.8479578008776314, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.049. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.9118016685019595, 0.8388228736196509, 0.7932488605112837], "final_y": [2.5995297727793476e-09, 1.3108394644582523e-08, 7.487843824990101e-08]}, "mutation_prompt": null}
{"id": "f65f844e-3ae8-44f9-913a-1df013f91d52", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Define a local optimization method\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)],\n                              options={'maxfun': function_evals_per_search})  # Line 1 change\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        # Determine the number of local searches based on the budget\n        num_local_searches = min(10, self.budget // 20)  # Line 2 change\n        function_evals_per_search = self.budget // (num_local_searches + 1)  # Line 3 change\n\n        # Perform multiple random initializations with local optimization\n        for _ in range(num_local_searches):\n            # Random initial guess within bounds\n            x0 = np.random.uniform(lb, ub)\n            local_optimize(x0)\n\n            # If budget is exceeded, break\n            if self.budget < 0:\n                break\n\n        return best_solution\n\n# Example of calling the optimizer:\n# Assume `func` is a black-box function with attributes `bounds.lb` and `bounds.ub`.\n# optimizer = HybridLocalGlobalOptimizer(budget=100, dim=2)\n# best_solution = optimizer(func)", "name": "HybridLocalGlobalOptimizer", "description": "Enhanced Hybrid Local-Global Optimizer with Adaptive Budget Allocation for Improved Efficiency in Smooth Landscapes.", "configspace": "", "generation": 1, "fitness": 0.6233923622785068, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.623 with standard deviation 0.210. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "745e65a7-98d0-475b-8df0-a09c5752a6f8", "metadata": {"aucs": [0.919950762155853, 0.4645896389660563, 0.48563668571361096], "final_y": [0.0, 0.00038333862348229633, 0.00023662296496546504]}, "mutation_prompt": null}
{"id": "1922b749-7f6a-447f-b68e-a65c6cb7805d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Define a local optimization method\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        # Determine the number of local searches based on the budget\n        num_local_searches = min(10, self.budget // 20)  # Changed to use budget more efficiently\n        function_evals_per_search = self.budget // num_local_searches\n\n        # Perform multiple random initializations with local optimization\n        for _ in range(num_local_searches):\n            # Random initial guess within bounds\n            x0 = np.random.uniform(lb, ub)\n            local_optimize(x0)\n\n            # If budget is exceeded, break\n            if self.budget < 0:\n                break\n\n        return best_solution\n\n# Example of calling the optimizer:\n# Assume `func` is a black-box function with attributes `bounds.lb` and `bounds.ub`.\n# optimizer = HybridLocalGlobalOptimizer(budget=100, dim=2)\n# best_solution = optimizer(func)", "name": "HybridLocalGlobalOptimizer", "description": "Enhanced Hybrid Local-Global Optimizer with Adaptive Sampling to Better Allocate Budget for High-Dimensional Smooth Landscapes.", "configspace": "", "generation": 1, "fitness": 0.5914454433141575, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.591 with standard deviation 0.155. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "745e65a7-98d0-475b-8df0-a09c5752a6f8", "metadata": {"aucs": [0.8106924858256557, 0.47572871476838474, 0.48791512934843195], "final_y": [7.433584176181184e-08, 0.0006036408394878989, 0.0002587988635091075]}, "mutation_prompt": null}
{"id": "6be645e9-ec87-4885-955e-0c2b74af3a51", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Define a local optimization method\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        # Increase the number of local searches to utilize the budget better\n        num_local_searches = min(15, self.budget // 10)  # Changed line\n\n        # Perform multiple random initializations with local optimization\n        for _ in range(num_local_searches):\n            # Random initial guess within bounds\n            x0 = np.random.uniform(lb, ub)\n            local_optimize(x0)\n\n            # If budget is exceeded, break\n            if self.budget < 0:\n                break\n\n        return best_solution\n\n# Example of calling the optimizer:\n# Assume `func` is a black-box function with attributes `bounds.lb` and `bounds.ub`.\n# optimizer = HybridLocalGlobalOptimizer(budget=100, dim=2)\n# best_solution = optimizer(func)", "name": "HybridLocalGlobalOptimizer", "description": "Enhanced Hybrid Local-Global Optimizer with increased local search iterations for improved convergence within budget constraints.", "configspace": "", "generation": 1, "fitness": 0.5918927371072581, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.592 with standard deviation 0.189. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "745e65a7-98d0-475b-8df0-a09c5752a6f8", "metadata": {"aucs": [0.8574362978267573, 0.43400370295336377, 0.4842382105416533], "final_y": [2.0945531009648113e-08, 0.0009553301259959402, 9.380935847081137e-05]}, "mutation_prompt": null}
{"id": "9162a38b-3eec-45e9-aa80-0d961d79a008", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            lb = np.maximum(best_point - (ub - lb) * 0.05, func.bounds.lb)  # Tighter boundary adjustment\n            ub = np.minimum(best_point + (ub - lb) * 0.05, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with Enhanced Sampling Strategy for Efficient Convergence in Smooth, Low-Dimensional Landscapes", "configspace": "", "generation": 1, "fitness": 0.8088325276123637, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.158. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b4f0fab7-55ec-4118-8fbe-540bf98ee181", "metadata": {"aucs": [0.5859318053691729, 0.9176608692727677, 0.9229049081951506], "final_y": [5.16607263883529e-10, 2.423742434121531e-10, 3.918334007216095e-10]}, "mutation_prompt": null}
{"id": "8734f03c-b29c-465f-8cf4-8df0d73be69a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced diversity\n        num_initial_samples = min(15, self.budget // 3)  # Change 1\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.15 * (upper_bounds - lower_bounds)  # Change 2\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Dynamic sampling diversity and boundary refinement for improved local convergence in smooth landscapes.", "configspace": "", "generation": 1, "fitness": 0.8134789488980673, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.041. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef3b4549-8562-4066-9046-6d60bea493e4", "metadata": {"aucs": [0.762715735138284, 0.8626702129857512, 0.815050898570167], "final_y": [3.334522192318905e-07, 2.5147775305459952e-08, 5.235132038340774e-08]}, "mutation_prompt": null}
{"id": "904145fa-0ef9-47a9-af27-12b6bfd267fd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_initial_samples = int(self.budget * 0.2)  # Changed from 0.1 to 0.2\n        remaining_budget = self.budget - num_initial_samples\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        \n        # Uniformly sample initial points within bounds\n        samples = np.random.uniform(func.bounds.lb, func.bounds.ub, (num_initial_samples, self.dim))\n        best_sample = None\n        best_sample_value = float('inf')\n        \n        # Evaluate sampled points\n        for sample in samples:\n            value = func(sample)\n            if value < best_sample_value:\n                best_sample_value = value\n                best_sample = sample\n        \n        # Use BFGS local optimization from the best initial sample\n        def wrapped_func(x):\n            nonlocal remaining_budget\n            if remaining_budget <= 0:\n                return float('inf')\n            remaining_budget -= 1\n            return func(x)\n        \n        result = minimize(wrapped_func, best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': remaining_budget})\n        \n        return result.x", "name": "HybridOptimizer", "description": "Optimized initial sampling strategy by redistributing the budget for a better balance between exploration and exploitation.", "configspace": "", "generation": 2, "fitness": 0.7804079693360965, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.780 with standard deviation 0.139. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "36722668-7b29-41ac-9d3f-a7852eebdd8d", "metadata": {"aucs": [0.7241786871143843, 0.6455535276367725, 0.9714916932571327], "final_y": [2.8945098797792606e-08, 1.7196423570516596e-07, 0.0]}, "mutation_prompt": null}
{"id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.03  # Dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead using dynamic adjustment factor for boundary refinement to improve convergence efficiency.", "configspace": "", "generation": 2, "fitness": 0.8892944052576223, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.073. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9162a38b-3eec-45e9-aa80-0d961d79a008", "metadata": {"aucs": [0.942636956580841, 0.9392461984699756, 0.7860000607220504], "final_y": [6.351489410733045e-10, 6.703931603078574e-10, 7.784898397181606e-10]}, "mutation_prompt": null}
{"id": "2ab44355-f889-4c2e-bbf9-236d9680c2d5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced diversity\n        num_initial_samples = min(20, self.budget // 3)  # Change 1: Increased initial samples to 20\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.10 * (upper_bounds - lower_bounds)  # Change 2: Adjusted epsilon to 0.10 for finer control\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Enhanced Dynamic Local Search with adaptive sampling and strategic exploitation for optimized convergence speed in smooth landscapes.", "configspace": "", "generation": 2, "fitness": 0.8219467670011813, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.025. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8734f03c-b29c-465f-8cf4-8df0d73be69a", "metadata": {"aucs": [0.8147646410052768, 0.7953694048941172, 0.85570625510415], "final_y": [1.1590372920991945e-07, 1.0674258251251654e-07, 1.5207997901462304e-08]}, "mutation_prompt": null}
{"id": "626f431d-35cb-47db-b9ec-93acba017630", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling\n        num_initial_samples = min(10, self.budget // 2)\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.1 * (upper_bounds - lower_bounds)\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n            \n            # Resample promising region\n            if self.evaluations < self.budget:\n                resample = np.random.uniform(lower_bounds, upper_bounds, (1, self.dim))\n                value = func(resample[0])\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = resample[0]\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Dynamic boundary-adjusted local search algorithm with strategic resampling of promising regions for improved convergence.", "configspace": "", "generation": 2, "fitness": 0.7890128101186499, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.789 with standard deviation 0.049. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef3b4549-8562-4066-9046-6d60bea493e4", "metadata": {"aucs": [0.7593249587882471, 0.8578615240552683, 0.7498519475124341], "final_y": [3.334522192318905e-07, 2.7774372928821897e-08, 1.733650539049539e-08]}, "mutation_prompt": null}
{"id": "52db09e8-0441-4654-ab29-d3ee08e0d88c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Define a local optimization method\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        # Determine the number of local searches based on the budget\n        num_local_searches = min(10, self.budget // 10)\n        function_evals_per_search = self.budget // num_local_searches\n\n        # Perform multiple random initializations with local optimization\n        for _ in range(num_local_searches):\n            # Adaptive random initial guess within bounds\n            x0 = np.random.uniform(lb, ub) * 0.9 + 0.1\n            local_optimize(x0)\n\n            # If budget is exceeded, break\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "An enhanced hybrid optimizer using an adaptive random sampling strategy to refine initial guesses before applying L-BFGS-B for rapid convergence.", "configspace": "", "generation": 2, "fitness": 0.7962300383437847, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "745e65a7-98d0-475b-8df0-a09c5752a6f8", "metadata": {"aucs": [0.79515666476363, 0.7790419329537734, 0.8144915173139508], "final_y": [6.136447965200127e-08, 2.5356660525462074e-08, 7.940803839605861e-09]}, "mutation_prompt": null}
{"id": "d7b4bf99-27f7-41ea-b108-c2296bfc5f9c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced diversity\n        num_initial_samples = max(5, min(20, self.budget // 4))  # Change 1: Dynamically adjust initial samples based on budget\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.10 * (upper_bounds - lower_bounds)  # Change 2: Adjusted epsilon to 0.10 for finer control\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Enhanced Dynamic Local Search by optimizing initial sample size dynamically based on available budget for improved early convergence.", "configspace": "", "generation": 3, "fitness": 0.850238948204873, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ab44355-f889-4c2e-bbf9-236d9680c2d5", "metadata": {"aucs": [0.8187483521461147, 0.8908356667236741, 0.8411328257448304], "final_y": [3.9972643764827115e-08, 4.762325687508772e-09, 4.973961573298585e-08]}, "mutation_prompt": null}
{"id": "6dc55069-bb7d-4523-a331-5b77b367da2d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced sampling strategy\n        num_initial_samples = min(25, self.budget // 4)  # Change 1: Adjusted initial samples to 25\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='BFGS', bounds=list(zip(lower_bounds, upper_bounds)))  # Change 2: Switched to BFGS method\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.05 * (upper_bounds - lower_bounds)  # Change 3: Adjusted epsilon to 0.05 for finer control\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Improved Dynamic Local Search with enhanced initial sampling strategy and refined local optimization bounds control for better convergence in smooth landscapes.", "configspace": "", "generation": 3, "fitness": 0.774775641954482, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ab44355-f889-4c2e-bbf9-236d9680c2d5", "metadata": {"aucs": [0.7754780471903054, 0.7843153057471761, 0.7645335729259647], "final_y": [3.6381225504054853e-07, 1.9916517113164936e-07, 5.145423242535653e-07]}, "mutation_prompt": null}
{"id": "f43e052d-d505-4edd-bb92-3b84d20fad2d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced diversity\n        num_initial_samples = min(10 + self.dim, self.budget // 3)  # Change 1: Variable initial samples based on dimension\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.05 * (upper_bounds - lower_bounds)  # Change 2: Adjusted epsilon to 0.05 for tighter control\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Enhanced Dynamic Local Search with variable sample size and tighter epsilon for improved convergence in smooth landscapes.", "configspace": "", "generation": 3, "fitness": 0.8298593791298275, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.830 with standard deviation 0.059. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "2ab44355-f889-4c2e-bbf9-236d9680c2d5", "metadata": {"aucs": [0.8068880657672062, 0.9103292880364563, 0.7723607835858199], "final_y": [6.602383624800775e-08, 1.4647961450289086e-08, 2.657863892761376e-07]}, "mutation_prompt": null}
{"id": "1f1a2820-1e32-4783-a6e0-d48aa2b2fc51", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        num_initial_samples = int(self.budget * 0.1)\n        remaining_budget = self.budget - num_initial_samples\n        bounds = list(zip(func.bounds.lb, func.bounds.ub))\n        \n        # Uniformly sample initial points within bounds\n        samples = np.random.uniform(func.bounds.lb, func.bounds.ub, (num_initial_samples, self.dim))\n        best_sample = None\n        best_sample_value = float('inf')\n        \n        # Evaluate sampled points\n        for sample in samples:\n            value = func(sample)\n            if value < best_sample_value:\n                best_sample_value = value\n                best_sample = sample\n        \n        # Adaptive restart using BFGS local optimization from the best initial sample\n        def wrapped_func(x):\n            nonlocal remaining_budget\n            if remaining_budget <= 0:\n                return float('inf')\n            remaining_budget -= 1\n            return func(x)\n        \n        while remaining_budget > 0:\n            result = minimize(wrapped_func, best_sample, method='L-BFGS-B', bounds=bounds, options={'maxfun': remaining_budget // 2})\n            if result.fun < best_sample_value:\n                best_sample_value = result.fun\n                best_sample = result.x\n            else:\n                # Restart with a new random sample\n                best_sample = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        \n        return best_sample", "name": "HybridOptimizer", "description": "Enhanced HybridOptimizer with adaptive restart mechanism to explore wider search space for improved convergence in smooth landscapes.", "configspace": "", "generation": 3, "fitness": 0.7664810793665717, "feedback": "The algorithm HybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.766 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "36722668-7b29-41ac-9d3f-a7852eebdd8d", "metadata": {"aucs": [0.7736674822605114, 0.7995742583474794, 0.7262014974917246], "final_y": [3.3988626072423296e-08, 7.984268632601008e-09, 9.406407874460984e-08]}, "mutation_prompt": null}
{"id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(15, max(5, self.budget // 15))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.4 * (self.budget / 100)  # Dynamic sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Enhanced Hybrid Local-Global Optimizer using adaptive sampling density and dynamic local search allocation to improve efficiency within the budget constraints.", "configspace": "", "generation": 3, "fitness": 0.8911547351957204, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.020. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "745e65a7-98d0-475b-8df0-a09c5752a6f8", "metadata": {"aucs": [0.9031640364331941, 0.8625761305613968, 0.9077240385925702], "final_y": [2.513709673133527e-09, 1.4719367557172837e-08, 8.337390263090151e-09]}, "mutation_prompt": null}
{"id": "dc68443a-d5fc-4a30-a9ad-81d46e990d64", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.05  # Optimized dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Refined Adaptive Nelder-Mead with optimized dynamic adjustment factor for enhanced boundary refinement and improved convergence.", "configspace": "", "generation": 4, "fitness": 0.7511443997880122, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.751 with standard deviation 0.124. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.9113030871970427, 0.6081320201197187, 0.7339980920472754], "final_y": [1.3803165130252952e-09, 2.36303891945583e-06, 1.3543548617038187e-06]}, "mutation_prompt": null}
{"id": "72caa5b8-df0c-4ebf-93f4-a0f539c11dcb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling\n        num_initial_samples = min(15, self.budget // 3)  # Increased sampling size\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.05 * (upper_bounds - lower_bounds)  # Reduced epsilon for finer adjustments\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Refined Dynamic Local Search with adaptive sampling size and tailored epsilon for enhanced convergence.", "configspace": "", "generation": 4, "fitness": 0.630780442350961, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.631 with standard deviation 0.093. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef3b4549-8562-4066-9046-6d60bea493e4", "metadata": {"aucs": [0.762715735138284, 0.5627946202440628, 0.5668309716705362], "final_y": [3.334522192318905e-07, 1.468920216643374e-07, 6.947894751507793e-08]}, "mutation_prompt": null}
{"id": "97cdb2c4-2ec0-41ef-8b9c-8dfd58051ea8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        num_initial_samples = max(5, self.budget // 3)  # Adjusted initial sampling\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n        \n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            epsilon = 0.15 * (upper_bounds - lower_bounds)  # Increased epsilon for broader exploration\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n            \n            # Introduce restart mechanism\n            if self.evaluations < self.budget // 2:\n                random_restart = np.random.uniform(lower_bounds, upper_bounds)\n                if func(random_restart) < best_value:\n                    best_solution = random_restart\n                    best_value = func(random_restart)\n                    self.evaluations += 1\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Enhanced Dynamic Local Search incorporating adaptive sampling and a restart mechanism to avoid local optima and improve convergence.", "configspace": "", "generation": 4, "fitness": 0.7223691068729571, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.722 with standard deviation 0.187. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "ef3b4549-8562-4066-9046-6d60bea493e4", "metadata": {"aucs": [0.9857474608067893, 0.612376107616695, 0.5689837521953867], "final_y": [0.0, 5.4319540062716693e-08, 2.838306584227967e-07]}, "mutation_prompt": null}
{"id": "2cadc8af-8861-4397-b38a-f467bd0f2e8a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        # Reduced initial exploration to free budget for deeper exploitation\n        exploration_phase = self.budget // 10\n        exploration_points = np.random.uniform(lb, ub, (exploration_phase, self.dim))\n        \n        for p in exploration_points:\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': exploration_phase,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n        while evaluations < self.budget:\n            result = minimize(func, best_point, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution with dynamic step size\n            adjustment_factor = 0.02 + 0.01 * (evaluations / self.budget)\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic step size adjustment and a refined exploration phase for improved convergence.  ", "configspace": "", "generation": 4, "fitness": 0.6780893910327636, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.678 with standard deviation 0.060. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.6916063779471592, 0.5987774179580898, 0.7438843771930417], "final_y": [3.1470588814870043e-06, 2.5723760616964208e-06, 8.646506389822626e-07]}, "mutation_prompt": null}
{"id": "88269467-d502-4344-8361-2462dd1a750e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(15, max(5, int(self.budget / 15)))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.4 * (self.budget / 100)  # Dynamic sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Refined Hybrid Local-Global Optimizer with improved dynamic search strategy by adjusting local search iterations based on budget proximity.", "configspace": "", "generation": 4, "fitness": 0.8141774555981431, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.814 with standard deviation 0.004. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.8183661864295686, 0.8085114430967015, 0.8156547372681591], "final_y": [6.406172241963271e-08, 1.4170645752074892e-07, 2.987655602912346e-08]}, "mutation_prompt": null}
{"id": "8b38a3af-6366-437e-8b5a-d2e6a495c922", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced diversity\n        num_initial_samples = max(5, min(20, self.budget // 4))  # Change 1: Dynamically adjust initial samples based on budget\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.05 * (upper_bounds - lower_bounds)  # Change 2: Adjusted epsilon to 0.05 for finer control\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Optimized boundary adjustment factor to enhance convergence precision in dynamic local search.", "configspace": "", "generation": 5, "fitness": 0.8853460367588765, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.885 with standard deviation 0.077. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d7b4bf99-27f7-41ea-b108-c2296bfc5f9c", "metadata": {"aucs": [0.8008555751357158, 0.8677400540571297, 0.9874424810837836], "final_y": [7.801765758753116e-08, 1.907668457367515e-08, 0.0]}, "mutation_prompt": null}
{"id": "7ecbb857-c53c-426a-a8cf-6e5ee619ed0a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.05  # Dynamic adjustment factor, refined\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with improved dynamic adjustment factor for better convergence efficiency.", "configspace": "", "generation": 5, "fitness": 0.890147779789442, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.9288510026642957, 0.9381049912170758, 0.8034873454869548], "final_y": [8.944573871727007e-10, 6.592679082279586e-10, 7.847987881813648e-10]}, "mutation_prompt": null}
{"id": "7f68f102-8ff3-4941-8230-1d7082b89b57", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced diversity\n        num_initial_samples = max(5, min(20, self.budget // 4))  # Change 1: Dynamically adjust initial samples based on budget\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.05 * (upper_bounds - lower_bounds)  # Change 2: Adjusted epsilon to 0.05 for finer control\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Improved Dynamic Local Search utilizing a refined epsilon scaling factor for more precise boundary adjustments and enhanced convergence.", "configspace": "", "generation": 5, "fitness": 0.7824803989621256, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.054. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d7b4bf99-27f7-41ea-b108-c2296bfc5f9c", "metadata": {"aucs": [0.7593249587882471, 0.8571733408837865, 0.7309428972143432], "final_y": [3.334522192318905e-07, 2.942272210504085e-08, 7.797208749283784e-08]}, "mutation_prompt": null}
{"id": "4d490c15-9dd3-45d3-96f6-2dfbf5448a5b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(15, max(5, self.budget // 15))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.4 * (self.budget / 100)  # Dynamic sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n                self.budget -= function_evals_per_search  # Corrected budget decrement\n\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Enhanced Hybrid Local-Global Optimizer with adaptive evaluation allocation to maximize budget efficiency.", "configspace": "", "generation": 5, "fitness": 0.7942728763238671, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.794 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.7916215041960126, 0.7855280916214877, 0.8056690331541009], "final_y": [7.348974780335351e-08, 1.146071164803176e-07, 2.7965170097742047e-08]}, "mutation_prompt": null}
{"id": "105bc5b4-a5a5-4cf5-95d5-e7e085468c6d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        # Define a local optimization method\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        # Determine the number of local searches based on the budget\n        num_local_searches = min(15, self.budget // 15)  # Adjusted number of local searches\n        function_evals_per_search = self.budget // num_local_searches\n\n        # Perform multiple random initializations with local optimization\n        for _ in range(num_local_searches):\n            # Random initial guess within bounds\n            x0 = np.random.uniform(lb - 0.1*(ub - lb), ub + 0.1*(ub - lb))  # Adjusted sampling distribution\n            local_optimize(x0)\n\n            # If budget is exceeded, break\n            if self.budget < 0:\n                break\n\n        return best_solution\n\n# Example of calling the optimizer:\n# Assume `func` is a black-box function with attributes `bounds.lb` and `bounds.ub`.\n# optimizer = HybridLocalGlobalOptimizer(budget=100, dim=2)\n# best_solution = optimizer(func)", "name": "HybridLocalGlobalOptimizer", "description": "Improved initial sampling distribution and optimized budget allocation across local searches to enhance convergence speed and solution quality.", "configspace": "", "generation": 5, "fitness": 0.7760144967881178, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.776 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "745e65a7-98d0-475b-8df0-a09c5752a6f8", "metadata": {"aucs": [0.7741812021370578, 0.7855280916214877, 0.7683341966058084], "final_y": [8.417571637189845e-08, 1.146071164803176e-07, 1.0859817990442894e-07]}, "mutation_prompt": null}
{"id": "fb2ac9e1-df84-4f84-a747-cffad45a20d1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.05  # Dynamic adjustment factor, refined\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n            # Early stopping if the improvement is minimal\n            if result.success and (best_value - result.fun) < 1e-9:\n                break\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Refined Adaptive Nelder-Mead with improved dynamic adjustment for early stopping based on convergence behavior.", "configspace": "", "generation": 6, "fitness": 0.8868118546433053, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.075. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7ecbb857-c53c-426a-a8cf-6e5ee619ed0a", "metadata": {"aucs": [0.9350229181051019, 0.9442802930304388, 0.7811323527943748], "final_y": [1.1850512515308069e-09, 9.284879335299508e-11, 5.54193089068774e-11]}, "mutation_prompt": null}
{"id": "86211345-77b6-44e6-bfa9-9fe56699459d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds with increased coverage\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//4, self.dim))  # Increased initial samples to budget//4\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point with dynamic tolerance\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-9, 'fatol': 1e-9})  # Refined tolerance levels\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.05  # Larger dynamic adjustment factor for bounds\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with a focus on refining initial sampling and dynamically adjusting convergence thresholds for enhanced performance.", "configspace": "", "generation": 6, "fitness": 0.8861909678710557, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.079. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7743547112944997], "final_y": [3.3110856626351356e-11, 4.058995169303085e-11, 6.353399745737475e-11]}, "mutation_prompt": null}
{"id": "53bd1b67-42f7-479b-af3a-2ab2a865e64c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.05  # Increased dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with an increased dynamic adjustment factor for more effective boundary refinement and faster convergence.", "configspace": "", "generation": 6, "fitness": 0.8861909678710557, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.079. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7743547112944997], "final_y": [2.497786177063166e-10, 4.058995169303085e-11, 6.353399745737475e-11]}, "mutation_prompt": null}
{"id": "3b27715a-1493-489d-93da-2642d82025e3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced diversity\n        num_initial_samples = max(5, min(20, self.budget // 4))  # Change 1: Dynamically adjust initial samples based on budget\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.08 * (upper_bounds - lower_bounds)  # Change 2: Adjusted epsilon to 0.08 for finer control\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Enhanced iterative dynamic boundary adjustment for precision targeting in smooth landscapes using modified epsilon scaling.", "configspace": "", "generation": 6, "fitness": 0.8168981219032169, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.817 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8b38a3af-6366-437e-8b5a-d2e6a495c922", "metadata": {"aucs": [0.8147646410052768, 0.8034831626332533, 0.8324465620711201], "final_y": [1.1590372920991945e-07, 8.082555190733272e-08, 4.305741973880364e-08]}, "mutation_prompt": null}
{"id": "5d0dfe5f-6015-4e99-963f-c64ff713b1db", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicLocalSearch:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.evaluations = 0\n    \n    def __call__(self, func):\n        lower_bounds = np.array(func.bounds.lb)\n        upper_bounds = np.array(func.bounds.ub)\n        best_solution = None\n        best_value = float('inf')\n        \n        # Initial uniform sampling with enhanced diversity\n        num_initial_samples = max(5, min(20, self.budget // 4))  # Change 1: Dynamically adjust initial samples based on budget\n        samples = np.random.uniform(lower_bounds, upper_bounds, (num_initial_samples, self.dim))\n        for sample in samples:\n            if self.evaluations < self.budget:\n                value = func(sample)\n                self.evaluations += 1\n                if value < best_value:\n                    best_value = value\n                    best_solution = sample\n                    \n        # Refine using local optimization (BFGS)\n        while self.evaluations < self.budget:\n            def local_obj(x):\n                if self.evaluations >= self.budget:\n                    return float('inf')\n                value = func(x)\n                self.evaluations += 1\n                return value\n            \n            res = minimize(local_obj, best_solution, method='L-BFGS-B', bounds=list(zip(lower_bounds, upper_bounds)))\n            if res.fun < best_value:\n                best_value = res.fun\n                best_solution = res.x\n            \n            # Dynamically adjust bounds to focus search\n            epsilon = 0.05 * (upper_bounds - lower_bounds) * (1 - self.evaluations / self.budget)  # Change 2: Adaptive epsilon reduction\n            lower_bounds = np.maximum(lower_bounds, best_solution - epsilon)\n            upper_bounds = np.minimum(upper_bounds, best_solution + epsilon)\n        \n        return best_solution, best_value", "name": "DynamicLocalSearch", "description": "Introduce adaptive epsilon reduction based on evaluation progress to refine the search space dynamically.", "configspace": "", "generation": 6, "fitness": 0.8480941601245822, "feedback": "The algorithm DynamicLocalSearch got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "8b38a3af-6366-437e-8b5a-d2e6a495c922", "metadata": {"aucs": [0.8266444430833485, 0.8937126958543833, 0.8239253414360146], "final_y": [3.6995714299637876e-08, 8.628817061435596e-09, 3.519620252371362e-08]}, "mutation_prompt": null}
{"id": "bf20c90e-4f9f-47f3-8d58-8e3c4b1a83d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.075  # Dynamic adjustment factor, refined\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n            # Early stopping if the improvement is minimal\n            if result.success and (best_value - result.fun) < 1e-10:\n                break\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Refined Adaptive Nelder-Mead with enhanced dynamic adjustment for early stopping and adaptive initial sampling.", "configspace": "", "generation": 7, "fitness": 0.6345127058386221, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.635 with standard deviation 0.435. And the mean value of best solutions found was 8.659 (0. is the best) with standard deviation 12.246.", "error": "", "parent_id": "fb2ac9e1-df84-4f84-a747-cffad45a20d1", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.019319925197199384], "final_y": [6.005651922012713e-10, 1.987368197908867e-09, 25.97733385878915]}, "mutation_prompt": null}
{"id": "2b6b1d09-5cbe-42cd-b1b9-b55ba1fc1bc4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.05  # Dynamic adjustment factor, refined\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n            # Early stopping if the improvement is minimal\n            if result.success and (best_value - result.fun) < 1e-10:  # More refined threshold\n                break\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Adaptive Nelder-Mead with refined early stopping criteria based on dynamic improvement thresholds.", "configspace": "", "generation": 7, "fitness": 0.6317810122273788, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.632 with standard deviation 0.427. And the mean value of best solutions found was 7.090 (0. is the best) with standard deviation 10.026.", "error": "", "parent_id": "fb2ac9e1-df84-4f84-a747-cffad45a20d1", "metadata": {"aucs": [0.9288510026642957, 0.9381049912170758, 0.028387042800765094], "final_y": [1.2632859968982266e-09, 6.592679082279586e-10, 21.268647856592104]}, "mutation_prompt": null}
{"id": "6a7078ad-7972-4e6f-98b4-06c6903370c8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.03 * (1.0 - result.fun / best_value)  # Adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic adjustment factor for boundary refinement and adaptive adjustment factor based on convergence rate to improve convergence efficiency.", "configspace": "", "generation": 7, "fitness": 0.6273047979624441, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.627 with standard deviation 0.434. And the mean value of best solutions found was 9.980 (0. is the best) with standard deviation 14.114.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.9385957402199816, 0.9303640355565738, 0.012954618110777139], "final_y": [3.1303997934031395e-10, 1.3079629603060761e-09, 29.940152744854892]}, "mutation_prompt": null}
{"id": "695abf1d-761f-4c13-bd3d-45c5fa631c9a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(15, max(5, self.budget // 15))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.4 * (self.budget / 100)  # Dynamic sampling density\n\n        adaptive_threshold = 0.6 + 0.2 * (self.budget / 100)  # New adaptive threshold\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < adaptive_threshold:  # Adjusted threshold for local search initiation\n                local_optimize(x0)\n\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Improved Hybrid Local-Global Optimizer using adaptive threshold for dynamic local search initiation to enhance performance.", "configspace": "", "generation": 7, "fitness": 0.836985616026666, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.822911584131771, 0.8799037705859243, 0.8081414933623026], "final_y": [5.652947628303842e-08, 1.0969749022164676e-08, 2.271059482440865e-08]}, "mutation_prompt": null}
{"id": "ae26fcd0-083e-4055-ba31-6136f77e96d2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)], options={'maxiter': self.budget // num_local_searches})\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(20, max(5, self.budget // 15))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.4 * (self.budget / 100)  # Dynamic sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Refined Hybrid Local-Global Optimizer with adaptive local search strategies and enhanced initial sampling for increased performance.", "configspace": "", "generation": 7, "fitness": 0.8372540304605133, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.837 with standard deviation 0.026. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.8196433639369014, 0.8745100230759035, 0.8176087043687348], "final_y": [6.581204569474326e-08, 2.9947401078268335e-08, 3.5415232480263834e-08]}, "mutation_prompt": null}
{"id": "58ed3008-5a6a-44ca-a0ed-5c3899969900", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n                # Adaptive restart mechanism when new best is found\n                lb, ub = func.bounds.lb, func.bounds.ub  # Reset bounds\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic adjustment factor and adaptive restart mechanism for improved global exploration.", "configspace": "", "generation": 8, "fitness": 0.8127389531371273, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.813 with standard deviation 0.086. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.7505539054140915, 0.7527538870383126, 0.9349090669589775], "final_y": [4.868253863028618e-10, 2.1490971382421152e-10, 5.10696290702807e-10]}, "mutation_prompt": null}
{"id": "07d7023c-8284-49eb-99b9-a90afc99cc26", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.03 + 0.005 * (self.budget - evaluations) / self.budget  # Adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with adaptive dynamic adjustment factor to refine bounds and enhance convergence efficiency.", "configspace": "", "generation": 8, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "5724d9f3-aab6-4e39-abe2-db72ce0cc864", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_samples = self.budget // 4  # Adjusted sampling size\n        points = np.random.uniform(lb, ub, (initial_samples, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = max(0.01, 0.05 * (evaluations / self.budget))  # Dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with adaptive adjustment factor and dynamic initial sampling size for enhanced convergence.", "configspace": "", "generation": 8, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7ecbb857-c53c-426a-a8cf-6e5ee619ed0a", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "9a967a86-0a8b-4742-9e64-459d42ff9e12", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-5  # New threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with strategic early exit and dynamic restart to refine convergence efficiency.", "configspace": "", "generation": 8, "fitness": 0.890147779789442, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "c6b2e8f0-cc5a-42c8-9134-7ee80d8e83f9", "metadata": {"aucs": [0.9288510026642957, 0.9381049912170758, 0.8034873454869548], "final_y": [8.944573871727007e-10, 6.592679082279586e-10, 7.847987881813648e-10]}, "mutation_prompt": null}
{"id": "d703b16b-f012-4104-9c89-2b0742183155", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = max(5, min(self.budget // 25, 20))  # Adjusted for better budget utilization\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.4 * (self.budget / 100)  # Dynamic sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Refined Hybrid Local-Global Optimizer with adaptive local search count based on budget to enhance convergence within constraints.", "configspace": "", "generation": 8, "fitness": 0.8345551237651155, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.8170793667950801, 0.8745100230759035, 0.8120759814243633], "final_y": [6.276768719091309e-08, 2.9947401078268335e-08, 3.5415232480263834e-08]}, "mutation_prompt": null}
{"id": "2fa049d8-74a4-439e-b8d4-090f1ca1e3d6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 0.001  # Adjusted threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with adjusted restart threshold for enhanced convergence efficiency.", "configspace": "", "generation": 9, "fitness": 0.8751769812890706, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.065. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9a967a86-0a8b-4742-9e64-459d42ff9e12", "metadata": {"aucs": [0.9213296272913395, 0.9206860557186202, 0.7835152608572523], "final_y": [7.985514475819754e-10, 3.4093642319031846e-10, 5.015725356432183e-10]}, "mutation_prompt": null}
{"id": "4733c956-cc4e-40d1-ac1e-edf3dc8d59d1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Further increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//4, self.dim))  # Changed line\n\n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.05  # Dynamic adjustment factor, refined\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with improved initialization through better sampling density for refined convergence efficiency.", "configspace": "", "generation": 9, "fitness": 0.20604565122120902, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.206 with standard deviation 0.013. And the mean value of best solutions found was 0.328 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7ecbb857-c53c-426a-a8cf-6e5ee619ed0a", "metadata": {"aucs": [0.21583590066276692, 0.21522480515928166, 0.1870762478415785], "final_y": [0.3276013310005208, 0.3276013310005221, 0.3276013310005226]}, "mutation_prompt": null}
{"id": "b81ac8f2-ac45-4c12-8910-0a721028cc9c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_samples = self.budget // 4  # Adjusted sampling size\n        points = np.random.uniform(lb, ub, (initial_samples, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = max(0.01, 0.1 * (1 - evaluations / self.budget))  # Modified dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Refined Adaptive Nelder-Mead with an improved dynamic adjustment factor for better exploitation of local minima.", "configspace": "", "generation": 9, "fitness": 0.890095752582833, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.068. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5724d9f3-aab6-4e39-abe2-db72ce0cc864", "metadata": {"aucs": [0.9409746620404078, 0.9350931328476569, 0.7942194628604344], "final_y": [9.856826843121916e-10, 6.113215511260483e-10, 5.62026952294134e-10]}, "mutation_prompt": null}
{"id": "096081b7-f1fb-4b59-8ce4-5f9052d89631", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(15, max(5, self.budget // 15))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.3 * (self.budget / 100)  # Improved dynamic sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Improved dynamic sampling density calculation to better allocate budget and enhance convergence efficiency.", "configspace": "", "generation": 9, "fitness": 0.8357713845184896, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.836 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.8208060747304747, 0.8745100230759035, 0.8119980557490905], "final_y": [6.276768719091309e-08, 2.9947401078268335e-08, 3.5415232480263834e-08]}, "mutation_prompt": null}
{"id": "43a7fde8-a36b-485a-a1d2-151267fe8e3c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(15, max(5, self.budget // 15))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.4 * (self.budget / 100)  # Dynamic sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n\n            if self.budget < 0:\n                break\n\n        if best_value > 0.001:  # Adaptive restart if not converged well\n            x0 = np.random.uniform(lb, ub)\n            local_optimize(x0)\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Enhanced Hybrid Local-Global Optimizer with adaptive restart for improved convergence efficiency within budget constraints.", "configspace": "", "generation": 9, "fitness": 0.7920266982240394, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.792 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.7962887197350538, 0.8012495516585373, 0.7785418232785268], "final_y": [4.207236188027107e-08, 8.63526927977068e-08, 8.63526927977068e-08]}, "mutation_prompt": null}
{"id": "eed047be-a649-47c8-bd44-e5d4e8f94b81", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(15, max(5, self.budget // 15))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.8 + 0.4 * (self.budget / 100)  # Dynamic sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n\n            if best_value < 1e-6:  # Early exit condition added\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Enhanced Hybrid Local-Global Optimizer with refined local search strategy using dynamic adjustment of initial sampling size for improved convergence efficiency.", "configspace": "", "generation": 10, "fitness": 0.8288176100209269, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.829 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.8088267919841892, 0.8744604129777047, 0.8031656251008868], "final_y": [9.95717691892276e-08, 2.9947401078268335e-08, 6.289350042230976e-08]}, "mutation_prompt": null}
{"id": "a1a0009e-39a2-4b0e-925c-3426b464d4f9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.03 + 0.005 * (self.budget - evaluations) / self.budget  # Adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic sampling probability to improve initial exploration coverage.", "configspace": "", "generation": 10, "fitness": 0.890147779789442, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "07d7023c-8284-49eb-99b9-a90afc99cc26", "metadata": {"aucs": [0.9288510026642957, 0.9381049912170758, 0.8034873454869548], "final_y": [8.944573871727007e-10, 6.592679082279586e-10, 7.847987881813648e-10]}, "mutation_prompt": null}
{"id": "a9ca93f3-29fc-450b-8734-513ad4ad6558", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Enhanced threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with enhanced restart threshold adjustment for better convergence refinement.", "configspace": "", "generation": 10, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9a967a86-0a8b-4742-9e64-459d42ff9e12", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "7cd3b513-db61-49d2-b290-27525733dc79", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # New threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with refined restart logic using a dynamic restart threshold to improve efficiency.", "configspace": "", "generation": 10, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9a967a86-0a8b-4742-9e64-459d42ff9e12", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "7916d0ac-0fc0-4f59-aa7c-7b1e7c4bfa2f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-5  # New threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.03  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with dynamic adjustment factor and restart strategy for optimized convergence efficiency.", "configspace": "", "generation": 10, "fitness": 0.8906468757047828, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.067. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "9a967a86-0a8b-4742-9e64-459d42ff9e12", "metadata": {"aucs": [0.9382823101599385, 0.9374462460172167, 0.7962120709371936], "final_y": [8.743963878505011e-10, 7.898951052592254e-10, 8.104704240447588e-10]}, "mutation_prompt": null}
{"id": "15eb2422-a80a-47b1-ba96-8864dccf1889", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_samples = self.budget // 3  # Adjusted sampling size\n        points = np.random.uniform(lb, ub, (initial_samples, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = max(0.02, 0.05 * (evaluations / self.budget))  # Dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with improved sampling strategy for more efficient initial exploration.", "configspace": "", "generation": 11, "fitness": 0.8802893161299242, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.057. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5724d9f3-aab6-4e39-abe2-db72ce0cc864", "metadata": {"aucs": [0.9165388881864787, 0.9244376707698994, 0.7998913894333944], "final_y": [6.86589599416513e-10, 9.679537384515674e-10, 7.223526334690275e-10]}, "mutation_prompt": null}
{"id": "fd926642-f533-42d4-99cc-8c8fd31528af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_samples = self.budget // 4  # Adjusted sampling size\n        points = np.random.uniform(lb, ub, (initial_samples, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = max(0.01, 0.1 * (evaluations / self.budget))  # Increased dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with refined dynamic adjustment factor for improved convergence efficiency.", "configspace": "", "generation": 11, "fitness": 0.8858674370982008, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "5724d9f3-aab6-4e39-abe2-db72ce0cc864", "metadata": {"aucs": [0.9297785330395517, 0.9276165302265602, 0.8002072480284907], "final_y": [1.1064206365898393e-09, 5.93306735554184e-10, 9.719893539439511e-10]}, "mutation_prompt": null}
{"id": "9994fd09-ebd5-46eb-9212-6a1e5128ceba", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-5  # Adjusted threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.1  # Modified dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n            # Add dynamic restart based on exploration of neighborhood\n            if evaluations < self.budget * 0.75 and evaluations % 10 == 0:\n                points = np.vstack((points, np.random.uniform(lb, ub, (self.budget//10, self.dim))))\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic neighborhood exploration and adaptive restart for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.8821054020920642, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.062. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a9ca93f3-29fc-450b-8734-513ad4ad6558", "metadata": {"aucs": [0.9320243583100452, 0.9198768022791975, 0.7944150456869502], "final_y": [5.058046395817353e-10, 2.7531102881038135e-10, 4.5782702368843746e-10]}, "mutation_prompt": null}
{"id": "05f13d48-6465-4725-937f-e328adff3e41", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.04 + 0.005 * (self.budget - evaluations) / self.budget  # Enhanced adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with enhanced dynamic adjustment factor for better bound refinement and convergence.", "configspace": "", "generation": 11, "fitness": 0.9213457861309146, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "07d7023c-8284-49eb-99b9-a90afc99cc26", "metadata": {"aucs": [0.9193185846126254, 0.9247803982501883, 0.9199383755299299], "final_y": [1.0802019464968401e-09, 8.512022803199786e-10, 9.701341793534397e-10]}, "mutation_prompt": null}
{"id": "1003089f-4a62-4750-aadb-6756d81cab01", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridLocalGlobalOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        best_solution = None\n        best_value = float('inf')\n\n        def local_optimize(x0):\n            nonlocal best_solution, best_value\n            result = minimize(func, x0, method='L-BFGS-B', bounds=[(lb[i], ub[i]) for i in range(self.dim)])\n            if result.fun < best_value:\n                best_value = result.fun\n                best_solution = result.x\n\n        num_local_searches = min(20, max(5, self.budget // 20))  # Adjusted for flexibility\n        function_evals_per_search = self.budget // num_local_searches\n        \n        sampling_density = 0.7 + 0.5 * (self.budget / 100)  # Refined sampling density\n\n        for _ in range(num_local_searches):\n            x0 = np.random.uniform(lb, ub)\n            if np.random.rand() < sampling_density:  # Conditional local search initiation\n                local_optimize(x0)\n\n            if self.budget < 0:\n                break\n\n        return best_solution", "name": "HybridLocalGlobalOptimizer", "description": "Improved Hybrid Local-Global Optimizer using adaptive restart strategy and refined sampling density for enhanced efficiency and convergence.", "configspace": "", "generation": 11, "fitness": 0.8581383205733428, "feedback": "The algorithm HybridLocalGlobalOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.033. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "1a9faf34-a8ec-4d73-8a98-d4daa97f1309", "metadata": {"aucs": [0.8878289572197617, 0.8745100230759035, 0.8120759814243633], "final_y": [0.0, 2.9947401078268335e-08, 3.5415232480263834e-08]}, "mutation_prompt": null}
{"id": "da6d3728-b4b9-484b-b255-e473d300b9ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': (self.budget - evaluations) // (len(points) - points.tolist().index(p)),\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.03 + 0.005 * (self.budget - evaluations) / self.budget  # Adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic evaluation allocation for improved convergence efficiency.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "07d7023c-8284-49eb-99b9-a90afc99cc26", "metadata": {}, "mutation_prompt": null}
{"id": "a016ee7e-3e26-407d-812a-857c14749dc0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # New threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-10, 'fatol': 1e-10})  # Tighter convergence criteria\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with tighter convergence criteria for improved solution refinement.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "7cd3b513-db61-49d2-b290-27525733dc79", "metadata": {}, "mutation_prompt": null}
{"id": "e2a64b77-af00-43d3-88f0-ba44d91c5a99", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_samples = self.budget // 4  # Adjusted sampling size\n        points = np.random.uniform(lb, ub, (initial_samples, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = max(0.01, 0.1 * (evaluations / self.budget))  # Optimized dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with optimized adaptive adjustment factor for improved convergence efficiency.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "5724d9f3-aab6-4e39-abe2-db72ce0cc864", "metadata": {}, "mutation_prompt": null}
{"id": "a52bd5d4-bb46-4a62-9a6b-1784c1bd96a9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Enhanced threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n            \n            adjustment_factor = 0.05 * (np.linalg.norm(p - best_point) / np.linalg.norm(ub - lb))  # Dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Refined Adaptive Nelder-Mead with dynamic adjustment factor based on distance from current best solution for enhanced convergence.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "a9ca93f3-29fc-450b-8734-513ad4ad6558", "metadata": {}, "mutation_prompt": null}
{"id": "18783a3d-d3eb-4b92-bde1-8b89d7b414c1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        initial_samples = self.budget // 3  # Adjusted sampling size\n        points = np.random.uniform(lb, ub, (initial_samples, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = max(0.01, 0.05 * (evaluations / self.budget))  # Dynamic adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with improved dynamic sampling size allocation for better initial exploration.", "configspace": "", "generation": 12, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()').", "error": "ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')", "parent_id": "5724d9f3-aab6-4e39-abe2-db72ce0cc864", "metadata": {}, "mutation_prompt": null}
{"id": "d70e90d8-a3c5-4bd8-ba35-250bb1d0530b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # New threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.03  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with refined restart logic and adaptive adjustment factor for improved efficiency.", "configspace": "", "generation": 13, "fitness": 0.8778769019215366, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.061. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7cd3b513-db61-49d2-b290-27525733dc79", "metadata": {"aucs": [0.9137168251931892, 0.9274244624673845, 0.7924894181040357], "final_y": [9.748152854752324e-10, 1.0529450741001394e-09, 1.1796681609542329e-09]}, "mutation_prompt": null}
{"id": "c7cfbf7c-4915-432a-ab6f-837dc60845f3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.04 + 0.005 * (self.budget - evaluations) / self.budget  # Enhanced adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with refined initial sampling strategy for enhanced convergence and exploration.", "configspace": "", "generation": 13, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05f13d48-6465-4725-937f-e328adff3e41", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "f7c57553-d7f1-4b35-aa7e-3e79f669c383", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Enhanced threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.03  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with decreased adjustment factor for improved convergence precision.", "configspace": "", "generation": 13, "fitness": 0.911185774485614, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.911 with standard deviation 0.014. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a9ca93f3-29fc-450b-8734-513ad4ad6558", "metadata": {"aucs": [0.8999046093730788, 0.9027317110274716, 0.9309210030562916], "final_y": [1.3419358940598948e-09, 1.1045433374436206e-09, 1.8464016230029174e-09]}, "mutation_prompt": null}
{"id": "1eb701f4-7938-411d-ad75-e4eb6e1787d4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Enhanced threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05 * (1 - evaluations / self.budget)  # Variable adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with variable adjustment factor based on iteration count for better convergence.", "configspace": "", "generation": 13, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a9ca93f3-29fc-450b-8734-513ad4ad6558", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "c9df32c8-3b3b-470d-8c1c-1833793a38c7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-5  # Adjusted threshold for more dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.1  # Increased dynamic factor for better exploration\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with strategic dynamic threshold adjustments and improved point selection for better convergence.", "configspace": "", "generation": 13, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a9ca93f3-29fc-450b-8734-513ad4ad6558", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "b842e998-7d1e-42b1-9f66-a9731d90f00b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.03 + 0.008 * (self.budget - evaluations) / self.budget  # Enhanced adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with refined dynamic adjustment factor and increased precision for convergence.", "configspace": "", "generation": 14, "fitness": 0.8802893161299242, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.057. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05f13d48-6465-4725-937f-e328adff3e41", "metadata": {"aucs": [0.9165388881864787, 0.9244376707698994, 0.7998913894333944], "final_y": [6.86589599416513e-10, 9.679537384515674e-10, 7.223526334690275e-10]}, "mutation_prompt": null}
{"id": "11208c5c-0586-42ad-9926-4422bd3dc94a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//8, self.dim))  # Adjusted initial sampling size\n\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Enhanced threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Refined dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead by adjusting initial sampling size and introducing a refined dynamic adjustment factor for bounds to enhance convergence.", "configspace": "", "generation": 14, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7c57553-d7f1-4b35-aa7e-3e79f669c383", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "272a00fd-d0d3-448e-bbe0-b0e520d5f3d5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Enhanced threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.03  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n            if np.random.rand() < 0.05:  # Probabilistic re-initialization\n                points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with probabilistic re-initialization to escape potential local minima and improve convergence.", "configspace": "", "generation": 14, "fitness": 0.8890919542046326, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7c57553-d7f1-4b35-aa7e-3e79f669c383", "metadata": {"aucs": [0.9391695857714675, 0.9414216616045112, 0.7866846152379191], "final_y": [5.221044668557169e-10, 8.832486643385659e-10, 8.293774988233163e-10]}, "mutation_prompt": null}
{"id": "291bd4a2-b0e9-4d34-89b1-d6492589e135", "solution": "import numpy as np\nfrom scipy.optimize import minimize, approx_fprime\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Gradient estimation for initial direction enhancement\n            epsilon = np.sqrt(np.finfo(float).eps)\n            grad_est = approx_fprime(p, func, epsilon)\n            p = p - 0.01 * grad_est\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            adjustment_factor = 0.05 + 0.005 * (self.budget - evaluations) / self.budget\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Adaptive Gradient-Nelder-Mead combines gradient estimation with adaptive bound adjustments for improved convergence in smooth landscapes.", "configspace": "", "generation": 14, "fitness": 0.8902257691092169, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.071. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "07d7023c-8284-49eb-99b9-a90afc99cc26", "metadata": {"aucs": [0.9369093613179531, 0.9437256100439542, 0.7900423359657434], "final_y": [4.100839638083422e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "4c23f913-8997-4703-aa7f-48223da10cc7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Dynamic threshold for restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n            restart_threshold = max(1e-8, restart_threshold * 0.5)  # Dynamic adjustment\n\n        # Post-optimization local refinement step\n        final_result = minimize(func, best_point, method='BFGS')\n        if final_result.fun < best_value:\n            best_point = final_result.x\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic restart threshold and post-optimization local refinement for convergence improvement.", "configspace": "", "generation": 14, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a9ca93f3-29fc-450b-8734-513ad4ad6558", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "dce17b70-119d-4735-aca2-f57b541f1cca", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-7  # Refined threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.03  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with refined dynamic restart mechanism for improved convergence precision.", "configspace": "", "generation": 15, "fitness": 0.9213457861309146, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7c57553-d7f1-4b35-aa7e-3e79f669c383", "metadata": {"aucs": [0.9193185846126254, 0.9247803982501883, 0.9199383755299299], "final_y": [1.0802019464968401e-09, 8.512022803199786e-10, 9.701341793534397e-10]}, "mutation_prompt": null}
{"id": "5c76f0e3-f9ee-4570-b9f9-56348930f83d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue\n\n            adjustment_factor = 0.03\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n            restart_threshold = max(1e-8, restart_threshold * 0.9)  # Adjusted line for dynamic threshold\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with variable restart threshold for adaptive convergence precision.", "configspace": "", "generation": 15, "fitness": 0.8802893161299242, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.057. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7c57553-d7f1-4b35-aa7e-3e79f669c383", "metadata": {"aucs": [0.9165388881864787, 0.9244376707698994, 0.7998913894333944], "final_y": [6.86589599416513e-10, 9.679537384515674e-10, 7.223526334690275e-10]}, "mutation_prompt": null}
{"id": "f67cb0ac-db4c-4e69-ad6f-870a266f9b70", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//4, self.dim))  # Adjusted from budget//5 to budget//4\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.04 + 0.005 * (self.budget - evaluations) / self.budget  # Enhanced adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with improved initial sampling density and dynamic refinement for better convergence and precision.", "configspace": "", "generation": 15, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05f13d48-6465-4725-937f-e328adff3e41", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "b2fa154a-959f-4ea1-892a-0980b227b272", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Enhanced threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                restart_threshold *= 0.9  # Adjust restart threshold dynamically\n\n            adjustment_factor = min(0.05 + 0.01 * (evaluations / self.budget), 0.1)  # Dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with varying adjustment factor based on performance to refine convergence efficiently.", "configspace": "", "generation": 15, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a9ca93f3-29fc-450b-8734-513ad4ad6558", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "57d5d060-5037-4afe-8624-6e15d349d1f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//4, self.dim))  # Changed sampling density\n\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if evaluations < self.budget * 0.8:  # Introduced local exploitation condition\n                local_result = minimize(\n                    func, best_point, method='L-BFGS-B', \n                    bounds=list(zip(lb, ub)), options={'maxfun': 10}\n                )\n                evaluations += local_result.nfev\n                if local_result.fun < best_value:\n                    best_value = local_result.fun\n                    best_point = local_result.x\n\n            adjustment_factor = 0.05 + 0.005 * (self.budget - evaluations) / self.budget  # Adjusted factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with improved boundary adjustment using dynamic sampling and local exploitation for better convergence.", "configspace": "", "generation": 15, "fitness": 0.9213457861309146, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "07d7023c-8284-49eb-99b9-a90afc99cc26", "metadata": {"aucs": [0.9193185846126254, 0.9247803982501883, 0.9199383755299299], "final_y": [1.0802019464968401e-09, 8.512022803199786e-10, 9.701341793534397e-10]}, "mutation_prompt": null}
{"id": "5ce408f0-a6bd-4e5c-bd73-54adf0019fec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-7  # Refined threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with refined dynamic adjustment factor for improved convergence efficiency.", "configspace": "", "generation": 16, "fitness": 0.8718182171502574, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.069. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "dce17b70-119d-4735-aca2-f57b541f1cca", "metadata": {"aucs": [0.9165388881864787, 0.9244376707698994, 0.7744780924943939], "final_y": [6.86589599416513e-10, 9.679537384515674e-10, 7.223526334690275e-10]}, "mutation_prompt": null}
{"id": "838dbfc7-7674-4928-bf1e-ec7a1ef00aed", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        \n        # Adjusted initial uniform sampling density based on budget usage\n        sampling_density = max(5, self.budget // (10 * self.dim))\n        points = np.random.uniform(lb, ub, (sampling_density, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            adjustment_factor = 0.03 + 0.005 * (self.budget - evaluations) / self.budget\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with dynamic sampling density adjustment based on budget usage for improved convergence efficiency.", "configspace": "", "generation": 16, "fitness": 0.8765363615659436, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.877 with standard deviation 0.070. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "07d7023c-8284-49eb-99b9-a90afc99cc26", "metadata": {"aucs": [0.9320243583100452, 0.9198768022791975, 0.7777079241085881], "final_y": [5.058046395817353e-10, 2.6540125866209545e-10, 2.6589183244199473e-10]}, "mutation_prompt": null}
{"id": "a4ae3929-36fa-4267-931a-8f6094e02587", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//3, self.dim))  # Adjusted sampling density\n\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if evaluations < self.budget * 0.7:  # Adjusted local exploitation threshold\n                local_result = minimize(\n                    func, best_point, method='L-BFGS-B', \n                    bounds=list(zip(lb, ub)), options={'maxfun': 20}  # Increased local exploration\n                )\n                evaluations += local_result.nfev\n                if local_result.fun < best_value:\n                    best_value = local_result.fun\n                    best_point = local_result.x\n\n            adjustment_factor = 0.1 + 0.01 * (self.budget - evaluations) / self.budget  # Adjusted factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with improved dynamic sampling and exploration strategy for better convergence precision.  ", "configspace": "", "generation": 16, "fitness": 0.8852193118913698, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.885 with standard deviation 0.080. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "57d5d060-5037-4afe-8624-6e15d349d1f4", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7714397433554424], "final_y": [2.496405822396493e-10, 1.0703982756382648e-09, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "7c726821-5376-4fb4-9617-90a089ebf725", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-7  # Refined threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = max(0.01, min(0.05, 0.03 * (self.budget - evaluations) / self.budget))  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with improved boundary adjustment using a more adaptive dynamic factor for better convergence precision.", "configspace": "", "generation": 16, "fitness": 0.929084011579822, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.929 with standard deviation 0.007. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "dce17b70-119d-4735-aca2-f57b541f1cca", "metadata": {"aucs": [0.9391333479943139, 0.9267610897551102, 0.9213575969900416], "final_y": [8.652903828900317e-10, 7.97348597751534e-10, 8.107160374457505e-10]}, "mutation_prompt": null}
{"id": "fd0b2107-c1ff-415b-a4ec-2fbce487a71d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage dynamically adjusted\n        points = np.random.uniform(lb, ub, (max(10, self.budget//10), self.dim))  # Adjusted sampling density\n\n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.04 + 0.005 * (self.budget - evaluations) / self.budget  # Enhanced adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Improved Adaptive Nelder-Mead with a dynamically adjusted sampling density to enhance initial exploration effectiveness.", "configspace": "", "generation": 16, "fitness": 0.8852193118913698, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.885 with standard deviation 0.080. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05f13d48-6465-4725-937f-e328adff3e41", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7714397433554424], "final_y": [2.497786177063166e-10, 1.0703982756382648e-09, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "a170594a-b80f-4c65-81b9-eff588628e7d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-7 \n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue\n\n            adjustment_factor = max(0.01, min(0.05, 0.03 * (self.budget - evaluations) / self.budget))\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n            # Probabilistically restart the search for exploration\n            if np.random.rand() < 0.1:  # New line for probabilistic restart\n                points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with probabilistic restarts and refined adaptive factor for robust convergence.", "configspace": "", "generation": 17, "fitness": 0.8904396995528506, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.073. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c726821-5376-4fb4-9617-90a089ebf725", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7871009063398852], "final_y": [4.3502299855340167e-10, 1.0703982756382648e-09, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "739cf7fd-d8e0-46ed-ab7d-a12e49160cfe", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-5  # Increased threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.03  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with increased restart threshold for improved convergence precision by better exploration.", "configspace": "", "generation": 17, "fitness": 0.8904396995528506, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.890 with standard deviation 0.073. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7c57553-d7f1-4b35-aa7e-3e79f669c383", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7871009063398852], "final_y": [2.497786177063166e-10, 1.0703982756382648e-09, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "980d0592-a794-4470-9493-a3d6e0a44a66", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))  # Changed sampling density\n\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if evaluations < self.budget * 0.7:  # Adjusted local exploitation condition\n                local_result = minimize(\n                    func, best_point, method='BFGS',  # Changed method to BFGS\n                    options={'maxiter': 15}  # Adjusted max iterations\n                )\n                evaluations += local_result.nfev\n                if local_result.fun < best_value:\n                    best_value = local_result.fun\n                    best_point = local_result.x\n\n            adjustment_factor = 0.07 + 0.005 * (self.budget - evaluations) / self.budget  # Adjusted factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Refined Adaptive Nelder-Mead with enhanced adaptive restart and gradient-based adjustment for precise local exploitation.", "configspace": "", "generation": 17, "fitness": 0.9213457861309146, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "57d5d060-5037-4afe-8624-6e15d349d1f4", "metadata": {"aucs": [0.9193185846126254, 0.9247803982501883, 0.9199383755299299], "final_y": [1.0802019464968401e-09, 6.5274292395284e-10, 9.701341793534397e-10]}, "mutation_prompt": null}
{"id": "aa7e3f17-1b46-46b2-b962-0456e9c80777", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-8  # Refined threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with refined restart strategy and dynamic adjustment factor for improved convergence precision.", "configspace": "", "generation": 17, "fitness": 0.8921717269139284, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.892 with standard deviation 0.075. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "f7c57553-d7f1-4b35-aa7e-3e79f669c383", "metadata": {"aucs": [0.9444433699097023, 0.945938966135051, 0.7861328446970318], "final_y": [6.57523413881232e-10, 3.8054318362750257e-10, 7.963935868182387e-10]}, "mutation_prompt": null}
{"id": "0c35fd8f-1f5a-4b9c-9c20-e4ea289d3314", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//4, self.dim))  # Changed sampling density\n\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if evaluations < self.budget * 0.8:  # Introduced local exploitation condition\n                local_result = minimize(\n                    func, best_point, method='L-BFGS-B', \n                    bounds=list(zip(lb, ub)), options={'maxfun': 10}\n                )\n                evaluations += local_result.nfev\n                if local_result.fun < best_value:\n                    best_value = local_result.fun\n                    best_point = local_result.x\n\n            adjustment_factor = 0.05 + 0.005 * (self.budget - evaluations) / self.budget  # Adjusted factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n            if np.linalg.norm(np.gradient(func(best_point))) < 1e-5:  # Early termination based on gradient norm\n                break\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with refined early termination condition based on gradient norm for faster convergence.", "configspace": "", "generation": 17, "fitness": 0.8811843875164129, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.064. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "57d5d060-5037-4afe-8624-6e15d349d1f4", "metadata": {"aucs": [0.9320243583100452, 0.9198768022791975, 0.791652001959996], "final_y": [2.4761851257444084e-10, 2.666973450363839e-10, 2.7428432248868617e-10]}, "mutation_prompt": null}
{"id": "81f0bb30-a811-4096-8fde-01c2139b6dd8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.04 + 0.005 * np.exp(-(self.budget - evaluations) / self.budget)  # Dynamic decay-based adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with optimized convergence by incorporating a dynamic, decay-based adjustment factor for bounds refinement.", "configspace": "", "generation": 18, "fitness": 0.9213457861309146, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05f13d48-6465-4725-937f-e328adff3e41", "metadata": {"aucs": [0.9193185846126254, 0.9247803982501883, 0.9199383755299299], "final_y": [1.0802019464968401e-09, 8.512022803199786e-10, 9.701341793534397e-10]}, "mutation_prompt": null}
{"id": "d2b2bcec-1f10-4c05-9ed6-74c918b30e4d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//4, self.dim))  # Changed sampling density\n\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if evaluations < self.budget * 0.7:\n                local_result = minimize(\n                    func, best_point, method='BFGS',\n                    options={'maxiter': 20}  # Adjusted max iterations\n                )\n                evaluations += local_result.nfev\n                if local_result.fun < best_value:\n                    best_value = local_result.fun\n                    best_point = local_result.x\n\n            adjustment_factor = 0.1 + 0.005 * (self.budget - evaluations) / self.budget  # Adjusted factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with integrated global sampling and adaptive boundary adjustments for improved exploration.", "configspace": "", "generation": 18, "fitness": 0.8802893161299242, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.057. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "980d0592-a794-4470-9493-a3d6e0a44a66", "metadata": {"aucs": [0.9165388881864787, 0.9244376707698994, 0.7998913894333944], "final_y": [6.86589599416513e-10, 9.679537384515674e-10, 7.223526334690275e-10]}, "mutation_prompt": null}
{"id": "d089d299-064a-4c90-afd6-d2400b5d8181", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-7  # Refined threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = max(0.01, min(0.05, 0.02 + 0.03 * (self.budget - evaluations) / self.budget))  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Optimized Adaptive Nelder-Mead with a more precise adjustment factor based on remaining evaluations for enhanced convergence.", "configspace": "", "generation": 18, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c726821-5376-4fb4-9617-90a089ebf725", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "98bdcc33-bbf0-47fb-89a9-dd8a86079f29", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.03 + 0.007 * (self.budget - evaluations) / self.budget  # Enhanced adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with a refined dynamic adjustment factor for improved boundary refinement and convergence precision.", "configspace": "", "generation": 18, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05f13d48-6465-4725-937f-e328adff3e41", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "5053b77e-8b8e-41af-a843-6b370daef00e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-6  # Adjusted threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n            \n            # Introduce adaptive random sampling for better exploration\n            adjustment_factor = 0.05  # Adjusted dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n            new_points = np.random.uniform(lb, ub, (self.budget//10, self.dim))\n            points = np.vstack((points, new_points))\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Introduce a dynamic restart and adaptive sampling strategy to enhance convergence precision and robustness.", "configspace": "", "generation": 18, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "dce17b70-119d-4735-aca2-f57b541f1cca", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "70bdcd5f-9453-4f9c-b893-1175eda7fe54", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//4, self.dim))  # Changed sampling density\n\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if evaluations < self.budget * 0.8:  # Adjusted local exploitation condition\n                local_result = minimize(\n                    func, best_point, method='BFGS',\n                    options={'maxiter': 20}  # Adjusted max iterations\n                )\n                evaluations += local_result.nfev\n                if local_result.fun < best_value:\n                    best_value = local_result.fun\n                    best_point = local_result.x\n\n            adjustment_factor = 0.06 + 0.004 * (self.budget - evaluations) / self.budget  # Adjusted factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Optimized Adaptive Nelder-Mead with refined adaptive restart and dynamic sampling for improved exploration-exploitation balance.", "configspace": "", "generation": 19, "fitness": 0.8802893161299242, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.057. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "980d0592-a794-4470-9493-a3d6e0a44a66", "metadata": {"aucs": [0.9165388881864787, 0.9244376707698994, 0.7998913894333944], "final_y": [6.86589599416513e-10, 9.679537384515674e-10, 7.223526334690275e-10]}, "mutation_prompt": null}
{"id": "c2b40c7d-fb4a-49b0-8aff-250acdee2d15", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Increased sampling density for better coverage\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-9, 'fatol': 1e-9})  # Precision adjustment\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.04 + 0.005 * (self.budget - evaluations) / self.budget  # Enhanced adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Optimized Adaptive Nelder-Mead with refined initialization and adaptive precision adjustment for enhanced convergence.", "configspace": "", "generation": 19, "fitness": 0.8914912884155779, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.066. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05f13d48-6465-4725-937f-e328adff3e41", "metadata": {"aucs": [0.9363625671487052, 0.9405086356743632, 0.797602662423665], "final_y": [7.049308791463217e-11, 7.147430425473605e-10, 5.991232156977594e-10]}, "mutation_prompt": null}
{"id": "7954e010-217d-4f6d-85c9-46fbecab985a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        # Initial uniform sampling within bounds\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Adjust sampling density dynamically based on the available budget\n        points = np.random.uniform(lb, ub, (max(1, self.budget//10), self.dim))\n        \n        # Track best found solution\n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        \n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            # Nelder-Mead optimization from each start point\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            # Adjust bounds around new best found solution\n            adjustment_factor = 0.04 + 0.005 * (self.budget - evaluations) / self.budget  # Enhanced adaptive adjustment factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with adaptive initial sample density based on budget for improved convergence.", "configspace": "", "generation": 19, "fitness": 0.9213457861309146, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.921 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "05f13d48-6465-4725-937f-e328adff3e41", "metadata": {"aucs": [0.9193185846126254, 0.9247803982501883, 0.9199383755299299], "final_y": [1.0802019464968401e-09, 8.512022803199786e-10, 9.701341793534397e-10]}, "mutation_prompt": null}
{"id": "9bc420ba-360f-44f8-b20b-2f9885078cee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Improved initial sampling strategy\n        points = np.random.uniform(lb, ub, (self.budget//4, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-7  # Maintain refined threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                # Stochastic element for restart decision\n                if np.random.rand() > 0.5:\n                    continue  \n\n            adjustment_factor = 0.02  # Fine-tuned dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Refined Adaptive Nelder-Mead using stochastic restart and improved initial sampling for enhanced convergence precision.", "configspace": "", "generation": 19, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "dce17b70-119d-4735-aca2-f57b541f1cca", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
{"id": "e1812694-9acc-4e4c-8cea-0ca21cdb878d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        points = np.random.uniform(lb, ub, (self.budget//5, self.dim))\n        \n        best_point = None\n        best_value = float('inf')\n        \n        evaluations = 0\n        restart_threshold = 1e-7  # Refined threshold for dynamic restart\n\n        for p in points:\n            if evaluations >= self.budget:\n                break\n\n            result = minimize(func, p, method='Nelder-Mead',\n                              options={'maxfev': self.budget - evaluations,\n                                       'xatol': 1e-8, 'fatol': 1e-8})\n            evaluations += result.nfev\n\n            if result.fun < best_value:\n                best_value = result.fun\n                best_point = result.x\n\n            if abs(result.fun - best_value) < restart_threshold:\n                continue  # Early exit if improvement is minimal\n\n            adjustment_factor = max(0.01, min(0.05, 0.02 * (self.budget - evaluations) / self.budget))  # Optimized dynamic factor\n            lb = np.maximum(best_point - (ub - lb) * adjustment_factor, func.bounds.lb)\n            ub = np.minimum(best_point + (ub - lb) * adjustment_factor, func.bounds.ub)\n\n        return best_point", "name": "AdaptiveNelderMead", "description": "Enhanced Adaptive Nelder-Mead with an optimized dynamic factor formula for more precise boundary adjustment.", "configspace": "", "generation": 19, "fitness": 0.8914201760948034, "feedback": "The algorithm AdaptiveNelderMead got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.072. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "7c726821-5376-4fb4-9617-90a089ebf725", "metadata": {"aucs": [0.9404925822747127, 0.9437256100439542, 0.7900423359657434], "final_y": [2.497786177063166e-10, 9.610172279409877e-10, 1.3072766897084637e-09]}, "mutation_prompt": null}
