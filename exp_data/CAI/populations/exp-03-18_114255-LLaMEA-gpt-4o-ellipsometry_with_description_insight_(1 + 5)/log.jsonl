{"id": "b45021fb-f6ac-4ca0-9b3e-00c8b6bbaa7f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.5):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid approach combining Nelder-Mead for local search and iterative boundary adjustment to exploit smooth landscapes efficiently and flexibly update search constraints.", "configspace": "", "generation": 0, "fitness": 0.5250981687770703, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.525 with standard deviation 0.252. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": null, "metadata": {"aucs": [0.16801117029474621, 0.7040464708761414, 0.7032368651603231], "final_y": [1.1143435519024305e-05, 1.980319388368885e-06, 2.8878992849709944e-06]}, "mutation_prompt": null}
{"id": "75d44221-5533-4ef4-9a1f-6243c2cf0cdb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100), 'xatol': 1e-4}  # Added xatol for early stopping\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.5):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "A refined hybrid optimizer using early local search convergence detection to efficiently manage budget and improve solution quality.", "configspace": "", "generation": 1, "fitness": 0.5301269819477916, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.530 with standard deviation 0.373. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "b45021fb-f6ac-4ca0-9b3e-00c8b6bbaa7f", "metadata": {"aucs": [0.6658795491970605, 0.9032688374012963, 0.021232559245018212], "final_y": [6.98756958493765e-06, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "60535850-675f-479f-8539-04fb85755fcc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Adjusted shrink factor\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "A refined adaptive hybrid approach with enhanced boundary adjustment to boost exploitation by dynamically tuning shrink factors based on convergence rates.", "configspace": "", "generation": 1, "fitness": 0.5136322153354219, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.514 with standard deviation 0.342. And the mean value of best solutions found was 5.051 (0. is the best) with standard deviation 7.143.", "error": "", "parent_id": "b45021fb-f6ac-4ca0-9b3e-00c8b6bbaa7f", "metadata": {"aucs": [0.6480121347162875, 0.8482089695021191, 0.044675541787859374], "final_y": [7.215816811889092e-06, 6.019706981167586e-08, 15.152121674719215]}, "mutation_prompt": null}
{"id": "8512613c-8152-4d84-8cb3-777ee2734736", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)  # Changed from 'Nelder-Mead' to 'BFGS'\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.5):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "A refined adaptive hybrid optimizer using BFGS for more efficient local searches, exploiting smoothness in black box optimization.", "configspace": "", "generation": 1, "fitness": 0.5793950356992152, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.579 with standard deviation 0.396. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "b45021fb-f6ac-4ca0-9b3e-00c8b6bbaa7f", "metadata": {"aucs": [0.8136837104513313, 0.9032688374012963, 0.021232559245018212], "final_y": [1.1702101718098186e-07, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "f9c2e662-47f2-4d30-a011-6b49f54e8688", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.4):  # Changed line\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "A refined hybrid optimizer leveraging Nelder-Mead with enhanced boundary adjustments for improved convergence efficiency in smooth landscapes.", "configspace": "", "generation": 1, "fitness": 0.53658405572056, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.537 with standard deviation 0.375. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "b45021fb-f6ac-4ca0-9b3e-00c8b6bbaa7f", "metadata": {"aucs": [0.6852507705153654, 0.9032688374012963, 0.021232559245018212], "final_y": [4.780530029896679e-06, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "d8f1c14b-7afe-49e8-b947-d8d0527b3ae0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)  # Added refinement with BFGS\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.5):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Refinement of AdaptiveHybridOptimizer using the BFGS method after Nelder-Mead for enhanced local search performance.", "configspace": "", "generation": 1, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "b45021fb-f6ac-4ca0-9b3e-00c8b6bbaa7f", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "d7c50909-69e8-44c3-a1e7-d7375f0acb03", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        def approx_grad(x):\n            return np.gradient(wrapped_func(x))  # Custom gradient approximation\n\n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', jac=approx_grad, options=options)  # Use custom gradient\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.5):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhancement of AdaptiveHybridOptimizer by adjusting the BFGS refinement to use a custom gradient approximation for improved convergence.", "configspace": "", "generation": 2, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('zero-size array to reduction operation maximum which has no identity').", "error": "ValueError('zero-size array to reduction operation maximum which has no identity')", "parent_id": "d8f1c14b-7afe-49e8-b947-d8d0527b3ae0", "metadata": {}, "mutation_prompt": null}
{"id": "991b7794-bba3-4244-9c05-fda53489aafc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved AdaptiveHybridOptimizer with adaptive shrinking for better refinement and convergence.", "configspace": "", "generation": 2, "fitness": 0.8379447593640146, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.838 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d8f1c14b-7afe-49e8-b947-d8d0527b3ae0", "metadata": {"aucs": [0.8792377446898341, 0.8097069425227424, 0.8248895908794672], "final_y": [1.150334768924285e-08, 1.168113319410649e-07, 7.253536211343985e-09]}, "mutation_prompt": null}
{"id": "b2b0919c-1ae3-41bd-923f-1dd70b5adaf9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.5, 0.5 * (self.budget - self.calls) / self.budget))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)  # Added refinement with BFGS\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.5):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introduce dynamic shrink factor adjustment during the bounds refinement to improve local search precision.", "configspace": "", "generation": 2, "fitness": 0.6115960454706902, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.612 with standard deviation 0.417. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d8f1c14b-7afe-49e8-b947-d8d0527b3ae0", "metadata": {"aucs": [0.9102867397657561, 0.9032688374012963, 0.021232559245018212], "final_y": [1.7083781471925474e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "ca202742-82e9-497b-8170-bde0b73e7830", "solution": "import numpy as np\nfrom scipy.optimize import minimize, dual_annealing\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = dual_annealing(wrapped_func, bounds=list(zip(bounds[0], bounds[1])), maxiter=50)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)  # Added refinement with BFGS\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.5):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhance the AdaptiveHybridOptimizer by using dual annealing for initial local search to improve exploration.", "configspace": "", "generation": 2, "fitness": 0.6455136620274479, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.646 with standard deviation 0.443. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d8f1c14b-7afe-49e8-b947-d8d0527b3ae0", "metadata": {"aucs": [1.0, 0.9153084268373256, 0.021232559245018212], "final_y": [0.0, 3.9743050989946755e-09, 25.44556613682321]}, "mutation_prompt": null}
{"id": "1b75014f-dcec-4284-bc1f-a5bdb9d72c1b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        if np.linalg.norm(initial_guess - bounds.mean(axis=0)) < 0.1:\n            result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)\n        else:\n            result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n            result = minimize(wrapped_func, result.x, method='BFGS', options=options)  # Added refinement with BFGS\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.5):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introduced a switch to directly use BFGS when close to convergence for faster refinement.", "configspace": "", "generation": 2, "fitness": 0.5729043332215454, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.573 with standard deviation 0.386. And the mean value of best solutions found was 7.446 (0. is the best) with standard deviation 10.531.", "error": "", "parent_id": "d8f1c14b-7afe-49e8-b947-d8d0527b3ae0", "metadata": {"aucs": [0.8233326333293336, 0.8683120567916786, 0.02706830954362416], "final_y": [1.045229188355955e-08, 6.046775342405407e-08, 22.339264346961578]}, "mutation_prompt": null}
{"id": "6183ca07-2dde-4520-b8f3-ed17f1b11405", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3 + 0.1 * (self.calls / self.budget)):  # Dynamic shrink factor\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved AdaptiveHybridOptimizer with dynamic adjustment of shrink factor for enhanced convergence.", "configspace": "", "generation": 3, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'self' is not defined\").", "error": "NameError(\"name 'self' is not defined\")", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {}, "mutation_prompt": null}
{"id": "a42cb217-df4f-4ddc-bd1e-561ba4c3c6e3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0] + 0.1, bounds[1] - 0.1, self.dim)  # Adjusted uniform sampling range\n\n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with a minor tweak for more effective initial sampling.", "configspace": "", "generation": 3, "fitness": 0.4020224558801872, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.402 with standard deviation 0.330. And the mean value of best solutions found was 10.013 (0. is the best) with standard deviation 14.160.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.819948065431498, 0.3723239886007652, 0.013795313608298554], "final_y": [1.0581988974875388e-07, 7.589657320592219e-08, 30.038690942028385]}, "mutation_prompt": null}
{"id": "c0c34052-078d-456c-b408-a9620609cd63", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, self.dynamic_shrink_factor(result.fun))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds\n\n    def dynamic_shrink_factor(self, current_fun):\n        return 0.1 + 0.2 * (1 - np.tanh(current_fun))  # Dynamic shrink factor based on performance", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic shrink factor based on recent improvements, increasing refinement and convergence speed.", "configspace": "", "generation": 3, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "f9f4b816-93e1-4c3f-b091-0ef48ddca957", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='trust-constr', options=options)  # Changed method to 'trust-constr'\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced convergence by incorporating trust-region approach into local search.", "configspace": "", "generation": 3, "fitness": 0.5195918456783509, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.520 with standard deviation 0.372. And the mean value of best solutions found was 6.915 (0. is the best) with standard deviation 9.779.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.5960648682821104, 0.9322961348305365, 0.030414533922405673], "final_y": [1.4534866899469062e-07, 1.0836686616862609e-08, 20.744636172174793]}, "mutation_prompt": null}
{"id": "ea59870f-ee23-4442-973a-2a16ae9b5c86", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        initial_guess = bounds[0] + (bounds[1] - bounds[0]) * np.random.rand(self.dim)  # Altered line for better initial guess\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced Adaptive Hybrid Optimizer with improved local search initiation.", "configspace": "", "generation": 3, "fitness": 0.6415004655487715, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.642 with standard deviation 0.440. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [1.0, 0.9032688374012963, 0.021232559245018212], "final_y": [0.0, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "35144833-fa3a-4763-8d35-d16dcf116edc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)  # Changed method order from 'Nelder-Mead' to 'BFGS'\n        result = minimize(wrapped_func, result.x, method='Nelder-Mead', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by modifying the method order for improved convergence.", "configspace": "", "generation": 4, "fitness": 0.5452208515091778, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.545 with standard deviation 0.381. And the mean value of best solutions found was 10.425 (0. is the best) with standard deviation 14.743.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8764191883518245, 0.7472548623109854, 0.011988503864723277], "final_y": [2.2091438306653663e-09, 1.4655423559597608e-08, 31.27537064429225]}, "mutation_prompt": null}
{"id": "9a4759d2-ba63-400f-8e14-458aaf772881", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Halton\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        sampler = Halton(d=self.dim, scramble=True)\n        sample = sampler.random(n=1)[0]\n        return bounds[0] + sample * (bounds[1] - bounds[0])\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "AdaptiveHybridOptimizer with improved initial guess using Halton sequence for better convergence.", "configspace": "", "generation": 4, "fitness": 0.5295564363527889, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.530 with standard deviation 0.363. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8439212013970889, 0.7235155484162598, 0.021232559245018212], "final_y": [5.869784886093865e-08, 9.161944482443102e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "aafcd9b1-b81d-4a96-b973-b5d17cbcdd2d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n        self.memory = None  # Added memory to store best solutions\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        if self.memory is not None:\n            initial_guess = self.memory  # Use memory if available\n        else:\n            initial_guess = self.uniform_sample(bounds)\n            \n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n                self.memory = result.x  # Store the best solution in memory\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with memory-based initialization for faster convergence.", "configspace": "", "generation": 4, "fitness": 0.548794338968429, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.549 with standard deviation 0.378. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.7356173703457485, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.953277181216433e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "74efc51a-69cc-4d3f-a43c-6538be4ca290", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.refined_initial_guess(result.x, refined_bounds)  # Adjusted initial guess strategy\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def refined_initial_guess(self, best_guess, bounds):  # New method for refined initial guess\n        perturbation = np.random.uniform(-0.1, 0.1, size=self.dim)\n        return np.clip(best_guess + perturbation, bounds[0], bounds[1])\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with adjusted initial guess strategy for improved exploration.", "configspace": "", "generation": 4, "fitness": 0.5876475586576236, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.588 with standard deviation 0.417. And the mean value of best solutions found was 9.456 (0. is the best) with standard deviation 13.372.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [1.0, 0.7465849057161149, 0.016357770256756], "final_y": [0.0, 2.253450559402314e-07, 28.367105276157496]}, "mutation_prompt": null}
{"id": "30cd2855-0f43-41c1-a34b-d2c15ddaae84", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.dynamic_sample(bounds)  # Changed to dynamic_sample\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.dynamic_sample(refined_bounds)  # Changed to dynamic_sample\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        if self.calls < self.budget / 2:  # Adaptive method switching\n            result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        else:\n            result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)\n\n        return result\n    \n    def dynamic_sample(self, bounds):  # Modified method for sampling\n        proportion = min(1.0, self.budget / (self.calls + 1))  # Dynamic sampling proportion\n        return np.random.uniform(bounds[0], bounds[1], self.dim) * proportion\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic sampling proportion and adaptive method switching for efficient convergence.", "configspace": "", "generation": 4, "fitness": 0.5011420278324904, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.501 with standard deviation 0.339. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.7432661333057151, 0.7389273909467378, 0.021232559245018212], "final_y": [2.8194250504816778e-08, 8.369022080206623e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "9d087b23-a45b-4219-bc12-69038c3da7e1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n        self.success_count = 0  # Track successful iterations\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n                self.success_count += 1\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds):\n        shrink_factor = 0.3 if self.success_count == 0 else 0.3 / self.success_count  # Dynamic shrink factor\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introduced a dynamic shrink factor based on the success rate of previous iterations to enhance the exploration-exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.604852175259733, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.9037908792196607, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 9.487972967081414e-09, 25.44556613682321]}, "mutation_prompt": null}
{"id": "ff9cee0f-5197-4dba-8845-ad45bec6a60c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            if self.calls % 20 == 0:  # Added: Randomized restart every 20 evaluations\n                initial_guess = self.uniform_sample(bounds)\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  \n        shrink_factor = 0.2 + 0.1 * np.sin(self.calls)  # Modified: Dynamic shrink factor based on evaluation count\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with randomized restart and dynamic shrink factor for improved exploration and exploitation balance.", "configspace": "", "generation": 5, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "21ad9fb7-a1ec-43fa-a133-138c9513e061", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=None):  # Updated shrink_factor to be dynamic\n        if shrink_factor is None:\n            shrink_factor = max(0.1, min(0.5, 1 - self.calls / self.budget))  # Dynamic shrink factor\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic shrink factor for improved convergence.", "configspace": "", "generation": 5, "fitness": 0.6183567636807868, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.618 with standard deviation 0.420. And the mean value of best solutions found was 3.262 (0. is the best) with standard deviation 4.613.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.9269323940406088, 0.9032688374012963, 0.0248690596004556], "final_y": [3.104447304650078e-09, 2.057274787051421e-08, 9.784921320630621]}, "mutation_prompt": null}
{"id": "3a2e7e49-ffd8-4e9d-80d3-b002b4d5157c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=None):\n        # Dynamically adjust shrink_factor based on remaining budget\n        if shrink_factor is None:\n            shrink_factor = 0.3 if self.calls < self.budget * 0.5 else 0.15\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with a dynamic shrink factor for refined exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.5521264325878871, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.552 with standard deviation 0.370. And the mean value of best solutions found was 7.154 (0. is the best) with standard deviation 10.117.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.817136131243558, 0.810268546382486, 0.028974620137617202], "final_y": [1.0950763113329595e-07, 1.2669327961030374e-07, 21.462447644398146]}, "mutation_prompt": null}
{"id": "a8fbee7d-3363-4613-97fa-73b000330702", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.improved_uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def improved_uniform_sample(self, bounds):  # Changed function to improved_uniform_sample for better sampling\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with improved initial guess sampling for better convergence.", "configspace": "", "generation": 5, "fitness": 0.6048896635610875, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.9039033441237242, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 1.9805292894437975e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "f7cd19ba-ba90-4f89-b9c2-b2a90e524441", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.25 + 0.05 * (self.calls/self.budget)):  # Changed shrink_factor dynamically\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic shrink factor for improved convergence control.", "configspace": "", "generation": 6, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'self' is not defined\").", "error": "NameError(\"name 'self' is not defined\")", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {}, "mutation_prompt": null}
{"id": "84d1f249-2ccb-4919-b5d4-eadceeb12ef1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n            else:\n                initial_guess = self.uniform_sample(bounds)  # Line changed: Re-initializing with global sampling\n\n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with re-initialization strategy for escaping local optima and improving global search.", "configspace": "", "generation": 6, "fitness": 0.5697175669795106, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.570 with standard deviation 0.390. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.7983870543789935, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.9521991292936086e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "1d9010ba-a9c9-4185-861d-6f4b8f1059ef", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options={'gtol': 1e-8})  # Changed gtol to 1e-8 for finer convergence\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with refined local search strategy for improved convergence rates.", "configspace": "", "generation": 6, "fitness": 0.819504051072057, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.820 with standard deviation 0.010. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8063414216901821, 0.8303504083401612, 0.8218203231858277], "final_y": [5.5516531634756796e-08, 2.78945575815988e-08, 1.643592718323406e-07]}, "mutation_prompt": null}
{"id": "20ebeb8f-d817-487c-9ee9-243d2acc7f93", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        # Switched the order of optimization methods for better hybridization\n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)\n        if not result.success:\n            result = minimize(wrapped_func, result.x, method='Nelder-Mead', options=options)\n        \n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer using a hybrid selection strategy for local search methods based on the problem's complexity.", "configspace": "", "generation": 6, "fitness": 0.5185979741742889, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.519 with standard deviation 0.355. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8242837671502039, 0.7102775961276444, 0.021232559245018212], "final_y": [2.8730951137679507e-08, 1.0081018880599098e-06, 25.44556613682321]}, "mutation_prompt": null}
{"id": "1453b50a-fd95-4159-bbae-9768d1612f08", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='L-BFGS-B', options=options)  # Switched to L-BFGS-B for better convergence\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced local search with sequential L-BFGS-B optimization for improved convergence efficiency.", "configspace": "", "generation": 6, "fitness": 0.5091587573043457, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.509 with standard deviation 0.345. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.747694917255522, 0.758548795412497, 0.021232559245018212], "final_y": [6.211735211660523e-07, 3.601118277123251e-07, 25.44556613682321]}, "mutation_prompt": null}
{"id": "0d6554b4-1208-46bf-96d5-179937b7c452", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=None): \n        if shrink_factor is None:  # Dynamic shrink factor adjustment\n            shrink_factor = 0.3 / (1 + self.calls / self.budget)\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer using dynamic shrink factor adjustment for improved convergence.", "configspace": "", "generation": 7, "fitness": 0.5539408353332718, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.554 with standard deviation 0.389. And the mean value of best solutions found was 11.476 (0. is the best) with standard deviation 16.230.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8858269166448357, 0.7683065737722019, 0.007689015582777681], "final_y": [7.819305281318382e-09, 2.132007660222425e-08, 34.428823731826164]}, "mutation_prompt": null}
{"id": "800a3951-85f0-419c-8374-6055b7849549", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            shrink_factor = self.calculate_shrink_factor(result.fun)  # Changed to dynamic shrink factor\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  \n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds\n    \n    def calculate_shrink_factor(self, current_best_fun):  # New method to calculate dynamic shrink factor\n        return max(0.1, min(0.5, 0.5 * current_best_fun))", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic shrink factor adjustment based on convergence progress for more efficient refinement.", "configspace": "", "generation": 7, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "c50600f0-7deb-49cc-815e-efe0d8a1afcd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 150)}  # Updated maxiter for deeper exploration\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.2):  # Dynamically updated shrink_factor\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Refined AdaptiveHybridOptimizer by enhancing local search strategy and dynamic shrink factor for improved convergence.", "configspace": "", "generation": 7, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "dea2f4ac-55d6-4ac6-a5bb-ceaaaad5a9da", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim) * 0.5  # Adjusted initial exploration\n\n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved AdaptiveHybridOptimizer by adjusting initial exploration strategy for better global search.", "configspace": "", "generation": 7, "fitness": 0.8322308199046593, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.832 with standard deviation 0.008. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.821606917838239, 0.8387458461115668, 0.8363396957641723], "final_y": [9.028486619178563e-08, 5.13946363791199e-08, 6.70609731664095e-08]}, "mutation_prompt": null}
{"id": "21c96a5e-3d2e-4a53-8bae-5aca1fd201e4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)\n        result = minimize(wrapped_func, result.x, method='Nelder-Mead', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced convergence by changing optimization method order for better performance.", "configspace": "", "generation": 7, "fitness": 0.5827902974510033, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.583 with standard deviation 0.398. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8238694957066954, 0.9032688374012963, 0.021232559245018212], "final_y": [2.8593933697920555e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "178b20cd-ec8e-400b-9285-6cce428d9c9b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.25):  # Adjusted shrink_factor to 0.25 for enhanced convergence\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with improved bound adjustment for accelerated convergence.", "configspace": "", "generation": 8, "fitness": 0.3888387773574724, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.389 with standard deviation 0.374. And the mean value of best solutions found was 10.425 (0. is the best) with standard deviation 14.743.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8972292864271728, 0.25910039684054054, 0.010186648804703657], "final_y": [2.7040359796205367e-09, 1.2500251802433247e-07, 31.27537064429225]}, "mutation_prompt": null}
{"id": "5694c29f-080b-4276-a8f7-df8b318e119b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 80)}  # Adjusted maxiter to 80 for better balance\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.35):  # Adjusted shrink_factor to 0.35 for refined exploitation\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with refined balance between exploration and exploitation for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.39844662602554887, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.398 with standard deviation 0.364. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.2862325102485491, 0.019574280513577347], "final_y": [3.4166179190589484e-08, 1.236923433704603e-07, 25.44556613682321]}, "mutation_prompt": null}
{"id": "7bf4df37-5edd-4a28-a9f5-20e0f59e7391", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        # Changed to use method='L-BFGS-B' for better performance in bounded problems\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='L-BFGS-B', bounds=bounds.T, options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by refining the local search strategy to improve convergence efficiency.", "configspace": "", "generation": 8, "fitness": 0.019348742542354747, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.019 with standard deviation 0.000. And the mean value of best solutions found was 25.446 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.019573346512606338, 0.019575703829233015, 0.018897177285224886], "final_y": [25.44556613682321, 25.44556613682321, 25.44556613682321]}, "mutation_prompt": null}
{"id": "e2a349ff-4fa2-4369-9225-3ef48eaebc96", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        \n        if self.calls < self.budget:\n            result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.2):  # Dynamic shrink factor for improved search\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhance AdaptiveHybridOptimizer with dual local search methods and dynamic shrink factor for optimized convergence.", "configspace": "", "generation": 8, "fitness": 0.30913717192660906, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.309 with standard deviation 0.410. And the mean value of best solutions found was 16.964 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.018952863015375798, 0.01892556544993118], "final_y": [3.4166179190589484e-08, 25.44556613682321, 25.44556613682321]}, "mutation_prompt": null}
{"id": "7a3d9cbd-c7c9-42a6-942f-2b3ebbcdffd7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.sobol_sample(bounds)  # Changed initial sampling strategy\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def sobol_sample(self, bounds):  # New sampling function using Sobol sequence\n        sampler = Sobol(d=self.dim, scramble=False)\n        sample = sampler.random_base2(1)[0]  # Generate a single Sobol point\n        scaled_sample = bounds[0] + sample * (bounds[1] - bounds[0])\n        return scaled_sample\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by refining the initial guess with a Sobol sequence for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.028188250169387863, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.028 with standard deviation 0.000. And the mean value of best solutions found was 20.745 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.028489543123458638, 0.02759111664916858, 0.02848409073553637], "final_y": [20.744636172174793, 20.744636172174793, 20.744636172174793]}, "mutation_prompt": null}
{"id": "154a54c0-1d0d-4445-84d4-be0e5331b8ee", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n            if np.abs(new_result.fun - result.fun) < 1e-6:  # Early stopping\n                break\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        if self.calls < self.budget * 0.5:  # Use BFGS only if budget allows\n            result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with early stopping and dynamic combination of local search methods for faster convergence.", "configspace": "", "generation": 9, "fitness": 0.3506679956832058, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.351 with standard deviation 0.381. And the mean value of best solutions found was 3.669 (0. is the best) with standard deviation 2.594.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8894775162792653, 0.09049865732818407, 0.07202781344216802], "final_y": [3.4166179190589484e-08, 5.502781361947425, 5.502781361947425]}, "mutation_prompt": null}
{"id": "e178384e-cc2d-42db-9e23-a4c46982c5a2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.adaptive_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.adaptive_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def adaptive_sample(self, bounds):\n        center = (bounds[0] + bounds[1]) / 2\n        radius = (bounds[1] - bounds[0]) * (0.5 if self.calls < self.budget/2 else 0.1)\n        return np.random.uniform(center - radius, center + radius, self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.2):  # adjusted shrink_factor dynamically\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic shrinking and adaptive initial sampling for improved convergence.", "configspace": "", "generation": 9, "fitness": 0.34648550496986363, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.346 with standard deviation 0.392. And the mean value of best solutions found was 3.669 (0. is the best) with standard deviation 2.594.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.9002572533497275, 0.09049865732818407, 0.04870060423167932], "final_y": [1.4781904336376196e-08, 5.502781361947425, 5.502781361947425]}, "mutation_prompt": null}
{"id": "74415274-f86a-43bd-a5ce-dcbdb78f8815", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 1 - self.calls/self.budget))  # Dynamic shrink_factor\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):  # Changed shrink_factor to 0.3 for better refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic shrink factor adjustment for improved convergence.", "configspace": "", "generation": 9, "fitness": 0.33524878442751516, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.335 with standard deviation 0.393. And the mean value of best solutions found was 3.669 (0. is the best) with standard deviation 2.594.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.09049865732818407, 0.025714608639841208], "final_y": [3.4166179190589484e-08, 5.502781361947424, 5.502781361947425]}, "mutation_prompt": null}
{"id": "52499c97-9c3e-4417-a8f6-0e5b9c2f4efc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='L-BFGS-B', options=options)  # Changed to L-BFGS-B\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.25):  # Changed shrink_factor to 0.25\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Optimized AdaptiveHybridOptimizer with refined initial sampling for enhanced convergence.", "configspace": "", "generation": 9, "fitness": 0.3401920226518295, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.340 with standard deviation 0.357. And the mean value of best solutions found was 1.403 (0. is the best) with standard deviation 0.992.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8425445604545598, 0.13307945357449424, 0.04495205392643442], "final_y": [9.249718226267821e-09, 2.104483211704971, 2.1044832117049714]}, "mutation_prompt": null}
{"id": "fa80dab7-0294-4a3b-8941-eaa65f319a1d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds.mean(axis=0))  # Adjusted initial guess to use mean of bounds\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with refined initial guess for improved convergence.", "configspace": "", "generation": 9, "fitness": 0.07854570942766297, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.079 with standard deviation 0.017. And the mean value of best solutions found was 5.503 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.09048760988904403, 0.09049865732818407, 0.05465086106576078], "final_y": [5.502781361947422, 5.502781361947425, 5.502781361947424]}, "mutation_prompt": null}
{"id": "38a9127d-3926-4569-957a-5854f389a67a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.25):  # Changed shrink_factor to 0.25 for faster convergence\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with improved bounds adjustment for faster convergence.", "configspace": "", "generation": 10, "fitness": 0.6098998996776492, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.610 with standard deviation 0.416. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.9051983023866331, 0.9032688374012963, 0.021232559245018212], "final_y": [1.5684214207193832e-09, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "ff16cbd4-f48e-4730-99e4-5f7b61e4d232", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.2):  # Changed shrink_factor for more aggressive refinement\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer using dual refinement strategies for improved convergence and solution accuracy.", "configspace": "", "generation": 10, "fitness": 0.5893979694276293, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.589 with standard deviation 0.367. And the mean value of best solutions found was 2.834 (0. is the best) with standard deviation 4.007.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8474056769119626, 0.8502894823698843, 0.07049874900104125], "final_y": [4.5862563664543724e-08, 3.9438207802083306e-08, 8.500722108881485]}, "mutation_prompt": null}
{"id": "977bcf2d-047f-4032-bff6-c2b8a7cb69ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.initial_sampling(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=0.2 + 0.5 * (1 - self.calls / self.budget))\n            initial_guess = self.initial_sampling(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def initial_sampling(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n\n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by incorporating a dynamic shrink factor and improved initial sampling strategy for refined exploration and convergence.", "configspace": "", "generation": 10, "fitness": 0.6047241077567841, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 2.185 (0. is the best) with standard deviation 3.091.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.02137039855453582], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 6.556499812193869]}, "mutation_prompt": null}
{"id": "71cfcafd-3a57-4a5b-9c7a-0c85e9a5ced3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.hybrid_initialization(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=0.2 + 0.1 * (1 - self.calls/self.budget))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def hybrid_initialization(self, func, bounds):  # New hybrid initialization method\n        best_guess = self.uniform_sample(bounds)\n        best_value = func(best_guess)\n        for _ in range(5):\n            guess = self.uniform_sample(bounds)\n            value = func(guess)\n            if value < best_value:\n                best_guess, best_value = guess, value\n        return best_guess\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Modified AdaptiveHybridOptimizer using hybrid initialization and dynamic shrink factor for better efficiency and convergence.", "configspace": "", "generation": 10, "fitness": 0.6411901119566812, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.641 with standard deviation 0.440. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.9990689392237291, 0.9032688374012963, 0.021232559245018212], "final_y": [0.0, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "85664224-22a6-4b60-a9b1-b5616082e9da", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            shrink_factor = max(0.1, 0.3 * (self.budget - self.calls) / self.budget)  # Dynamic shrink factor\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor):  # Changed to dynamic shrink factor input\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic shrink factor for improved convergence precision.", "configspace": "", "generation": 10, "fitness": 0.5969388281652581, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.597 with standard deviation 0.413. And the mean value of best solutions found was 10.214 (0. is the best) with standard deviation 14.444.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.9029783220686227, 0.874932468980524, 0.01290569344662762], "final_y": [1.2526455345804677e-09, 1.4992684198399428e-08, 30.64068633201035]}, "mutation_prompt": null}
{"id": "cf37acda-0a6b-4a1d-8280-c6cd4e52488f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.dynamic_sample(bounds)  # Changed to dynamic sampling\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.dynamic_sample(refined_bounds)  # Changed to dynamic sampling\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def dynamic_sample(self, bounds):  # Introduced dynamic sampling\n        return np.random.uniform(bounds[0], bounds[1], self.dim) + np.random.normal(0, 0.05, self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.2):  # Adjusted shrink_factor\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic sampling and adaptive learning rate for improved precision and convergence efficiency.", "configspace": "", "generation": 11, "fitness": 0.8520325389075017, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.852 with standard deviation 0.032. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8632426248755352, 0.8838603362181258, 0.8089946556288445], "final_y": [1.7853750423601004e-08, 2.2306305122977487e-08, 1.143184011370334e-07]}, "mutation_prompt": null}
{"id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with refined search strategy using dynamic shrink factor adjustment for improved convergence.", "configspace": "", "generation": 11, "fitness": 0.8623166915470767, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.862 with standard deviation 0.045. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.9023452989903171, 0.8854855678842598, 0.7991192077666529], "final_y": [8.58577995577865e-09, 2.3954201248782634e-08, 2.0760759745730005e-07]}, "mutation_prompt": null}
{"id": "5fb0379a-d1cb-42cd-b53d-d92a574b2a7a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)  # Swapped order with next line\n        result = minimize(wrapped_func, result.x, method='Nelder-Mead', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by switching the local optimization order for improved performance.", "configspace": "", "generation": 11, "fitness": 0.6873848955548029, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.687 with standard deviation 0.231. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8729110916495901, 0.82723195092873, 0.3620116440860881], "final_y": [1.2795050332988897e-08, 2.6624955576901264e-08, 1.5293212114822764e-07]}, "mutation_prompt": null}
{"id": "a3bb4873-3556-489b-81b8-3737bc675aac", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds):  # Removed shrink_factor parameter from function signature\n        shrink_factor = 0.3 * (self.budget - self.calls) / self.budget  # Dynamic shrink_factor based on remaining budget\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by optimizing search strategy using a dynamic shrink factor based on budget utilization.", "configspace": "", "generation": 11, "fitness": 0.5640018807950374, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.564 with standard deviation 0.386. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8895330873145202, 0.781220530255993, 0.021252024814598758], "final_y": [3.4166179190589484e-08, 2.4977577779640813e-09, 25.44556613682321]}, "mutation_prompt": null}
{"id": "c3e8e243-3ac5-42fd-9fba-15f977b5603c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import qmc  # Added this import for Latin Hypercube Sampling\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.lhs_sample(bounds)  # Changed to use lhs_sample instead of uniform_sample\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n\n    def lhs_sample(self, bounds):  # Added this function for Latin Hypercube Sampling\n        sampler = qmc.LatinHypercube(d=self.dim)\n        sample = sampler.random(1)\n        scaled_sample = qmc.scale(sample, bounds[0], bounds[1])[0]\n        return scaled_sample\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved initialization by using Latin Hypercube Sampling for better initial guesses.", "configspace": "", "generation": 11, "fitness": 0.7956715112646657, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.796 with standard deviation 0.049. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "991b7794-bba3-4244-9c05-fda53489aafc", "metadata": {"aucs": [0.8272303398685197, 0.7266154864630914, 0.8331687074623864], "final_y": [8.40557062942548e-08, 1.192054491779211e-07, 9.774110898959411e-08]}, "mutation_prompt": null}
{"id": "4cd776b9-6518-47b4-aa40-32fcb94719b1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        method = 'Nelder-Mead' if np.random.rand() > 0.5 else 'BFGS'  # Randomly switch between methods\n        result = minimize(wrapped_func, initial_guess, method=method, options=options)\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introduced a random strategy switch to alternate between Nelder-Mead and BFGS for enhanced exploration-exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.5647308870821407, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.565 with standard deviation 0.382. And the mean value of best solutions found was 7.854 (0. is the best) with standard deviation 11.108.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.830638688533521, 0.8388801314246044, 0.02467384128829664], "final_y": [9.804462143528249e-08, 1.0360421928327779e-07, 23.563063418303617]}, "mutation_prompt": null}
{"id": "c842859e-e1de-46cc-9d13-83e1469c4a73", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            exploration_factor = np.exp(-self.calls / self.budget)  # New exploration strategy\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=exploration_factor * 0.3)\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n            else:\n                # Implementing mutation strategy to escape local minima\n                initial_guess = self.mutate_guess(result.x, refined_bounds, mutation_rate=0.1 * (1 - exploration_factor))\n                result = self.local_search(func, initial_guess, refined_bounds)\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def mutate_guess(self, guess, bounds, mutation_rate):\n        mutation = np.random.uniform(-mutation_rate, mutation_rate, self.dim)\n        mutated_guess = guess + mutation\n        # Ensure mutated guess stays within bounds\n        return np.clip(mutated_guess, bounds[0], bounds[1])\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved AdaptiveHybridOptimizer with an adaptive mutation strategy and dynamic exploration-exploitation balance for enhanced convergence.", "configspace": "", "generation": 12, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "8f8b0128-adb2-489f-b4d6-f0885b9abada", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.weighted_sample(refined_bounds, result.x)  # Changed to weighted_sample for initial guess\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def weighted_sample(self, bounds, center):\n        weights = np.random.uniform(0, 1, self.dim)\n        return center + weights * (np.random.uniform(bounds[0], bounds[1], self.dim) - center)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introduced a weighted sampling strategy for the initial guess to prioritize unexplored regions and improve solution exploration.", "configspace": "", "generation": 12, "fitness": 0.5691037403171045, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.569 with standard deviation 0.381. And the mean value of best solutions found was 1.310 (0. is the best) with standard deviation 1.852.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8495469779734113, 0.82791311376437, 0.029851129213532168], "final_y": [1.583445417800672e-08, 1.340750514469854e-07, 3.92888170399885]}, "mutation_prompt": null}
{"id": "ed734014-e254-4d2c-8c96-0b510c9345ec", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100), 'learning_rate': 0.1 * (1 - self.calls / self.budget)}  # Adaptive learning rate added\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introduced adaptive learning rate in `local_search` to enhance convergence speed.", "configspace": "", "generation": 12, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "f70c688d-32d4-4b21-af66-26a16dacd27a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.simulated_annealing_sample(func, refined_bounds)  # Changed to simulated annealing\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def simulated_annealing_sample(self, func, bounds):  # New function for simulated annealing sampling\n        temperature = 1.0\n        best_sample = self.uniform_sample(bounds)\n        best_energy = func(best_sample)\n        for _ in range(int(self.budget / 10)):  # Limited iterations\n            candidate = self.uniform_sample(bounds)\n            candidate_energy = func(candidate)\n            if candidate_energy < best_energy or np.random.rand() < np.exp((best_energy - candidate_energy) / temperature):\n                best_sample, best_energy = candidate, candidate_energy\n            temperature *= 0.9  # Cooling schedule\n        return best_sample\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "AdaptiveHybridOptimizer with energy-based refinement using simulated annealing to enhance exploration and exploitation balance.", "configspace": "", "generation": 12, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "4d66c360-759f-4f80-9e6f-3a19acc6748c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds) + np.random.normal(0, 0.05, self.dim)  # Added Gaussian perturbation\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with improved initial sampling by incorporating a Gaussian perturbation to refine initial guesses.", "configspace": "", "generation": 13, "fitness": 0.5617345702181056, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.562 with standard deviation 0.357. And the mean value of best solutions found was 3.888 (0. is the best) with standard deviation 5.499.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8100818330676514, 0.8187564552494453, 0.05636542233721997], "final_y": [1.3565282205615132e-07, 1.1820506324557952e-07, 11.66446942070428]}, "mutation_prompt": null}
{"id": "c95674f8-6309-42ae-8db7-9f398c2a5e8e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            if self.calls < self.budget * 0.7:  # Reinitialize search space strategically\n                initial_guess = self.uniform_sample(bounds)\n            else:\n                initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Optimized Adaptive Hybrid with strategic reinitialization to enhance exploration and convergence.", "configspace": "", "generation": 13, "fitness": 0.8255986699424082, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.027. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8195381186948643, 0.7964300828795964, 0.8608278082527638], "final_y": [9.638348593473027e-08, 1.5771495814771187e-07, 4.3535147818098775e-08]}, "mutation_prompt": null}
{"id": "1738a0b6-60c6-44af-ae58-02606772ec1d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        if result.success or self.calls >= self.budget // 2:  # Selects BFGS based on convergence status or halfway budget\n            result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Incorporate a dynamic selection between local search methods based on convergence speed to enhance exploitation capabilities.", "configspace": "", "generation": 13, "fitness": 0.6213821629821817, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.621 with standard deviation 0.432. And the mean value of best solutions found was 9.456 (0. is the best) with standard deviation 13.372.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [1.0, 0.8477887186897894, 0.016357770256756], "final_y": [0.0, 8.564942874463722e-08, 28.367105276157496]}, "mutation_prompt": null}
{"id": "00c701e0-5b9a-49ce-a2c2-15b618f99c3c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            # Adjusting strategy switch point to enhance adaptability in convergence\n            if self.calls / self.budget > 0.5:  # Changed line\n                refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            else:\n                refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.2, 0.4 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "AdaptiveHybridOptimizer with dynamic strategy switch based on convergence progress for enhanced performance.", "configspace": "", "generation": 13, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "4673c607-463f-44b1-a78d-57b9b123abaa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n        self.sobol_engine = Sobol(d=self.dim, scramble=True)\n\n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        sample = self.sobol_engine.random(n=1)[0]\n        return bounds[0] + sample * (bounds[1] - bounds[0])\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by refining initial guess generation using Sobol sequence for improved convergence.", "configspace": "", "generation": 13, "fitness": 0.5565903065526193, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.557 with standard deviation 0.383. And the mean value of best solutions found was 6.915 (0. is the best) with standard deviation 9.779.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.707060250904916, 0.9322961348305365, 0.030414533922405673], "final_y": [1.885493541963736e-08, 1.0836686616862609e-08, 20.744636172174793]}, "mutation_prompt": null}
{"id": "9cd7efed-cb03-4b30-8466-0d40b0e56b9d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.refined_initial_guess(result.x, refined_bounds)  # Changed line: Use refined initial guess\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def refined_initial_guess(self, best_guess, bounds):\n        return np.clip(best_guess + np.random.normal(scale=0.05, size=self.dim), bounds[0], bounds[1])\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "AdaptiveHybridOptimizer with a refined strategy using a dynamically adjusted initial guess based on previous best solutions for improved convergence.", "configspace": "", "generation": 14, "fitness": 0.4144753459957145, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.414 with standard deviation 0.381. And the mean value of best solutions found was 3.513 (0. is the best) with standard deviation 4.837.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.061480947993178714, 0.23804784045089344, 0.9438972495430714], "final_y": [10.353515219153165, 0.18686448506299164, 4.618574545839296e-14]}, "mutation_prompt": null}
{"id": "e907696a-a70a-4dae-98ac-a5aece3df109", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        method = 'BFGS' if self.budget - self.calls > 50 else 'Nelder-Mead'  # Changed method selection based on remaining budget\n        result = minimize(wrapped_func, initial_guess, method=method, options=options)  # Adjusted local search method\n        \n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with adaptive local search choice based on remaining budget to improve efficiency.", "configspace": "", "generation": 14, "fitness": 0.6124035277898102, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.612 with standard deviation 0.419. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8769399073116995, 0.9390381168127131, 0.021232559245018212], "final_y": [1.3011472177521952e-09, 4.322452908786216e-13, 25.44556613682321]}, "mutation_prompt": null}
{"id": "6f456719-b830-4423-8748-96cdd6a2e2d5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100), 'xatol': 1e-8 * (self.calls / self.budget + 1e-5)}  # Incorporating a variable step size\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with more dynamic convergence control by incorporating a variable step size in local search.", "configspace": "", "generation": 14, "fitness": 0.6355976993634173, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.636 with standard deviation 0.436. And the mean value of best solutions found was 9.016 (0. is the best) with standard deviation 12.750.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.9431564339092354, 0.9451442598316047, 0.018492404349411906], "final_y": [1.2857966706027547e-13, 1.7274947303695242e-13, 27.04695823294979]}, "mutation_prompt": null}
{"id": "bb822c95-16ae-41d6-bc59-cd2b79101f41", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guesses = [self.uniform_sample(bounds) for _ in range(3)]  # Changed to use multiple starting points\n        results = [self.local_search(func, init_guess, bounds) for init_guess in initial_guesses]  # Run local search for each starting point\n        result = min(results, key=lambda r: r.fun)  # Select the best result\n\n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Refined AdaptiveHybridOptimizer with an enhanced initialization strategy using multiple starting points for better exploration.", "configspace": "", "generation": 14, "fitness": 0.6316980523681464, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.632 with standard deviation 0.432. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.9348234810467078, 0.9390381168127131, 0.021232559245018212], "final_y": [0.0, 4.322452908786216e-13, 25.44556613682321]}, "mutation_prompt": null}
{"id": "08ebe96e-9aac-47b6-ade8-e5498b761879", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100), 'xatol': 0.1 * (1 - self.calls / self.budget)}  # Modified line\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved convergence by dynamically adjusting the Nelder-Mead search step size based on remaining budget.", "configspace": "", "generation": 14, "fitness": 0.6304019982449469, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.630 with standard deviation 0.431. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.9309353186771094, 0.9390381168127131, 0.021232559245018212], "final_y": [3.1160843653646714e-09, 4.322452908786216e-13, 25.44556613682321]}, "mutation_prompt": null}
{"id": "c5033d89-1560-46ec-9e8f-5d5faf82c3d8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.05, 0.25 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved AdaptiveHybridOptimizer using a more aggressive shrink factor adjustment for faster convergence.", "configspace": "", "generation": 15, "fitness": 0.5523571795268228, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.552 with standard deviation 0.366. And the mean value of best solutions found was 6.315 (0. is the best) with standard deviation 8.931.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8178581638715282, 0.8046608163057797, 0.034552558403160405], "final_y": [1.5484285718065684e-07, 2.790745466197361e-07, 18.945037290489516]}, "mutation_prompt": null}
{"id": "25a93930-2d1b-44ac-9057-712d93e95179", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.refined_uniform_sample(bounds)  # Changed sampling strategy\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def refined_uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim) * 0.9 + 0.05 * (bounds[1] - bounds[0])\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Refined AdaptiveHybridOptimizer with enhanced initial sampling strategy for improved convergence.", "configspace": "", "generation": 15, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "34ef30a0-7eca-4860-90ef-1cc890576a60", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)  # Focusing sampling more on refined_bounds\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved convergence by adjusting the initial guess sampling strategy to focus more on promising regions.", "configspace": "", "generation": 15, "fitness": 0.610251956979925, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.610 with standard deviation 0.417. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.9062544742934604, 0.9032688374012963, 0.021232559245018212], "final_y": [1.2070549850844911e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "b72f208f-dca7-4809-8b66-a0a6e0ddd5d3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            if np.random.rand() < 0.05:  # Introduce random restart with 5% probability\n                initial_guess = self.uniform_sample(bounds)\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  \n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by introducing a strategic random restart mechanism to escape local minima.", "configspace": "", "generation": 15, "fitness": 0.6046781613202782, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.605 with standard deviation 0.413. And the mean value of best solutions found was 8.482 (0. is the best) with standard deviation 11.995.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8895330873145202, 0.9032688374012963, 0.021232559245018212], "final_y": [3.4166179190589484e-08, 2.057274787051421e-08, 25.44556613682321]}, "mutation_prompt": null}
{"id": "6542ca0b-aec5-4b00-9cfc-664ec7c60dcb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            # Phase 1: Exploratory Sampling\n            if self.calls < 0.5 * self.budget:\n                initial_guess = self.exploratory_sample(bounds)\n            else:\n                refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n                initial_guess = self.uniform_sample(refined_bounds)\n            \n            new_result = self.local_search(func, initial_guess, bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def exploratory_sample(self, bounds):\n        return np.random.normal(loc=(bounds[0] + bounds[1]) / 2, scale=(bounds[1] - bounds[0]) / 4, size=self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introducing a dual-phase search combining exploratory sampling and adaptive local refinement for more efficient convergence.", "configspace": "", "generation": 15, "fitness": 0.6059233853497025, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.606 with standard deviation 0.408. And the mean value of best solutions found was 6.915 (0. is the best) with standard deviation 9.779.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8550594872961651, 0.9322961348305365, 0.030414533922405673], "final_y": [1.8226015486918232e-08, 1.0836686616862609e-08, 20.744636172174793]}, "mutation_prompt": null}
{"id": "4a2eb74d-817d-4366-bba7-7c62f8ca1606", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds, rate=self.calls/self.budget)  # Adaptive sampling rate\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds, rate=0.5):\n        return np.random.uniform(bounds[0] * rate, bounds[1] * rate, self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introduced adaptive sampling rate in the uniform sampling to enhance exploration and convergence.", "configspace": "", "generation": 16, "fitness": 0.8404607231096999, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.840 with standard deviation 0.013. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.830046455441217, 0.8586759937297216, 0.8326597201581611], "final_y": [7.120981973638606e-08, 1.1677159551453694e-08, 7.925636658036895e-08]}, "mutation_prompt": null}
{"id": "8226be7b-59b4-4c02-9d52-86d28607bac0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats.qmc import Sobol\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        sampler = Sobol(d=self.dim, scramble=True)\n        sample = sampler.random_base2(m=1)[0]\n        return bounds[0] + sample * (bounds[1] - bounds[0])\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with refined sampling using Sobol sequence for better initial coverage of the search space.", "configspace": "", "generation": 16, "fitness": 0.8417909752158294, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.842 with standard deviation 0.015. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8594433348725579, 0.8218278755660503, 0.84410171520888], "final_y": [2.2954459576465738e-08, 1.411157572135909e-07, 2.9027379789925573e-08]}, "mutation_prompt": null}
{"id": "59d96140-2772-4106-9a8c-60853d16fee0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = (result.x + self.uniform_sample(refined_bounds)) / 2  # Use adaptive initial guess\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with adaptive initial guess strategy for improved convergence.", "configspace": "", "generation": 16, "fitness": 0.7083210524996345, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.708 with standard deviation 0.127. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.85925148447101, 0.7181491445230976, 0.5475625285047958], "final_y": [3.651672744782418e-08, 4.169508642603635e-08, 2.9426189881453533e-08]}, "mutation_prompt": null}
{"id": "a0cdab1b-cc9d-49c8-99d1-6f281a596a8b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.uniform_sample(bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 150)}  # Increased maxiter\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n        result = minimize(wrapped_func, result.x, method='Powell', bounds=bounds, options=options)  # Added Powell method\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved AdaptiveHybridOptimizer with modified local search strategy and refined sampling for enhanced convergence efficiency.", "configspace": "", "generation": 16, "fitness": 0.6303050013437247, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.630 with standard deviation 0.184. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.841611588223535, 0.3924710152613222, 0.6568324005463169], "final_y": [6.13132498326078e-08, 1.481563800671168e-07, 1.7899048228289883e-07]}, "mutation_prompt": null}
{"id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "AdaptiveHybridOptimizer with improved initial guess strategy by incorporating local gradient information.", "configspace": "", "generation": 16, "fitness": 0.8749551859474832, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.000. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "d38fc4bd-f5d2-463e-a42d-49045f90daec", "metadata": {"aucs": [0.8749551859474832, 0.8749551859474832, 0.8749551859474832], "final_y": [4.0765223313081095e-08, 4.0765223313081095e-08, 4.0765223313081095e-08]}, "mutation_prompt": null}
{"id": "477b9d6e-823f-4e65-b8cd-cd5410b8c496", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.5 * (1 - self.calls / self.budget)))  # Changed shrink_factor from 0.3 to 0.5\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced exploration by dynamically adjusting the shrink factor for bounds refinement.", "configspace": "", "generation": 17, "fitness": 0.8600710950243512, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.012. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8749551859474832, 0.858623898414093, 0.8466342007114778], "final_y": [4.0765223313081095e-08, 3.196080378696009e-08, 7.043289381611877e-08]}, "mutation_prompt": null}
{"id": "23d823de-0f1d-4e95-a155-798df5f396d4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step * (1 - self.calls / self.budget)\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic refinement of initial guess incorporating adaptive gradient scaling.", "configspace": "", "generation": 17, "fitness": 0.8637278565643953, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.864 with standard deviation 0.031. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8341137158196205, 0.8499818305875297, 0.9070880232860354], "final_y": [9.89013429136129e-08, 6.71431080714355e-08, 6.3267279979656604e-09]}, "mutation_prompt": null}
{"id": "50d37d3b-3c07-4930-a31d-60498ee1a41f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.05, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step * 0.5  # Adjusted perturbation factor\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by updating gradient estimation and bounds adjustment for better convergence.", "configspace": "", "generation": 17, "fitness": 0.8603061043619427, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.860 with standard deviation 0.029. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.9010129821476984, 0.8472830208815465, 0.8326223100565832], "final_y": [3.181576703154417e-09, 6.781545491730006e-08, 8.964663370436516e-08]}, "mutation_prompt": null}
{"id": "c848d632-55c3-4b48-83b8-84c0c8f72ee4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.05, 0.2 * (1 - self.calls / self.budget)))  # Adjusted shrink_factor dynamically\n            initial_guess = self.random_re_sample(func, refined_bounds, result.x)  # Added local re-sampling\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 150)}  # Increased max iterations\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step * 0.5  # Adjusted influence of gradient\n\n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds\n\n    def random_re_sample(self, func, bounds, best_guess):\n        noise = np.random.normal(0, 0.1, self.dim)  # Added noise for diversity\n        return np.clip(best_guess + noise, bounds[0], bounds[1])\n", "name": "AdaptiveHybridOptimizer", "description": "Enhanced the adaptive strategy by adding a local re-sampling mechanism and smarter usage of gradient estimates to improve convergence speed.", "configspace": "", "generation": 17, "fitness": 0.8484668875887299, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.848 with standard deviation 0.017. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8560328262016298, 0.8648677815447019, 0.824500055019858], "final_y": [3.475294970825573e-08, 1.3316718946042692e-08, 1.5222072698961814e-07]}, "mutation_prompt": null}
{"id": "96155525-b74e-4778-9fd1-8fac708ce37a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.05, 0.3 * (1 - self.calls**2 / self.budget**2)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced adaptation by incorporating a variable shrink factor that decreases faster as the budget progresses.", "configspace": "", "generation": 17, "fitness": 0.8673763406390268, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.867 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8895632384988338, 0.8872029285125536, 0.8253628549056929], "final_y": [2.458538201917314e-08, 1.6790086098643566e-08, 1.1320044632494878e-07]}, "mutation_prompt": null}
{"id": "1bfdba52-b0c6-4a16-abe0-f1fb725e0aa0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.05, 0.25 * (1 - self.calls / self.budget)))  # More aggressive shrinking\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Powell', options=options)  # Changed to Powell method for local search\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved AdaptiveHybridOptimizer by updating the local search strategy and shrinking bounds more aggressively as budget decreases.", "configspace": "", "generation": 18, "fitness": 0.8439138765163849, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.844 with standard deviation 0.087. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.7212580673882153, 0.9052417810804698, 0.9052417810804698], "final_y": [7.57840152385749e-13, 2.0452330053352046e-12, 7.987204920725267e-13]}, "mutation_prompt": null}
{"id": "c6a0ae40-ee31-4f2e-905d-dc23f1846c1c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-6  # Enhanced step size for gradient estimate\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "AdaptiveHybridOptimizer with enhanced gradient step size for more precise initial guess adjustments.", "configspace": "", "generation": 18, "fitness": 0.867623201922354, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.868 with standard deviation 0.002. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8642341286637447, 0.8693177385516587, 0.8693177385516587], "final_y": [5.56259486746477e-08, 4.4398958002601225e-14, 4.4398958002601225e-14]}, "mutation_prompt": null}
{"id": "becf76e3-417a-4ab3-b16a-a6c92e461dcd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  \n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        # Add a stochastic perturbation to the BFGS initial point\n        stochastic_initial = result.x + np.random.normal(0, 0.01, self.dim)\n        result = minimize(wrapped_func, stochastic_initial, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced the local search strategy by adding a stochastic component to refine searches near promising areas.", "configspace": "", "generation": 18, "fitness": 0.8494676722063327, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.849 with standard deviation 0.028. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8097675395156807, 0.8693177385516587, 0.8693177385516587], "final_y": [5.022493142360715e-08, 4.4398958002601225e-14, 4.4398958002601225e-14]}, "mutation_prompt": null}
{"id": "6e442b26-079d-4220-a470-da414137339f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n        self.best_history = []  # Store history of best found solutions\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        self.best_history.append((result.x, result.fun))  # Track solutions\n\n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.dynamic_initial_guess(refined_bounds)  # Use dynamic initial guess\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n                self.best_history.append((result.x, result.fun))\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n\n    def dynamic_initial_guess(self, bounds):\n        # Leverage historical best solutions to inform initial guess\n        if self.best_history:\n            historical_best = min(self.best_history, key=lambda x: x[1])[0]\n            return self.uniform_sample(bounds) * 0.5 + historical_best * 0.5\n        else:\n            return self.uniform_sample(bounds)\n\n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer integrates historical search data to dynamically adapt search strategies for improved convergence.", "configspace": "", "generation": 18, "fitness": 0.8713043875314436, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.003. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8752776854910133, 0.8693177385516587, 0.8693177385516587], "final_y": [3.8334912933933436e-08, 4.4398958002601225e-14, 4.4398958002601225e-14]}, "mutation_prompt": null}
{"id": "77cf6b2d-f641-4b90-bde7-a26bc97db77a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='Powell', options=options)  # Changed BFGS to Powell\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhance local search strategy by incorporating Powell's method for improved convergence.", "configspace": "", "generation": 18, "fitness": 0.8693177385516587, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.869 with standard deviation 0.000. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8693177385516587, 0.8693177385516587, 0.8693177385516587], "final_y": [4.4398958002601225e-14, 4.4398958002601225e-14, 4.4398958002601225e-14]}, "mutation_prompt": null}
{"id": "68c5da6c-2211-4245-a967-8e5772ffb122", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        scaling_factor = 1 - self.calls / self.budget  # Dynamic scaling factor\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step * scaling_factor\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introducing a dynamic gradient scaling factor to enhance local search efficiency.", "configspace": "", "generation": 19, "fitness": 0.6903723583823572, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.690 with standard deviation 0.030. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.7325135023782442, 0.6751363068669611, 0.6634672659018663], "final_y": [1.195401101867321e-07, 4.823707075657668e-08, 8.058161075223911e-08]}, "mutation_prompt": null}
{"id": "65e05d92-2657-4242-81ec-78778ba74428", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        if self.calls < self.budget / 2:  # Added adaptive method selection\n            result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        else:\n            result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer with dynamic refinement strategy using adaptive local search method selection.", "configspace": "", "generation": 19, "fitness": 0.7560228560993877, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.756 with standard deviation 0.050. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.825047518781963, 0.7327198136867925, 0.7103012358294076], "final_y": [9.60895022675168e-08, 1.239294323085151e-07, 3.739227013109107e-07]}, "mutation_prompt": null}
{"id": "a084604f-0379-4882-a33a-1400a650876d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100), 'gtol': 1e-7}  # Changed one line here\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "AdaptiveHybridOptimizer with modified local search termination criteria to enhance convergence speed.", "configspace": "", "generation": 19, "fitness": 0.7717267197816563, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.772 with standard deviation 0.073. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8749551859474832, 0.7187613583145611, 0.7214636150829243], "final_y": [4.0765223313081095e-08, 1.0448155195257002e-07, 2.0374581459328527e-07]}, "mutation_prompt": null}
{"id": "43a94b9b-835f-441b-9917-8260d73f25a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 50)}  # Reduced maxiter to focus on rapid iterations\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-6  # Reduced step size for gradient estimation\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by refining the step size and improving local search efficiency.", "configspace": "", "generation": 19, "fitness": 0.7430878555158937, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.743 with standard deviation 0.006. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.7424236268677309, 0.7509683990923065, 0.7358715405876437], "final_y": [3.568377424120667e-08, 3.419126969928189e-08, 1.0751378147119591e-07]}, "mutation_prompt": null}
{"id": "e349a4ea-7313-4345-a6f8-cd643e224088", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        method = 'BFGS' if self.calls > self.budget * 0.5 else 'Nelder-Mead'  # Changed method dynamically based on budget usage\n        result = minimize(wrapped_func, initial_guess, method=method, options=options)  # Consolidated minimization to a single line\n        \n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved AdaptiveHybridOptimizer by dynamically adjusting optimization method choice based on remaining budget.", "configspace": "", "generation": 19, "fitness": 0.7294412611407578, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.729 with standard deviation 0.004. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.7334456515325036, 0.7308139674500719, 0.7240641644396977], "final_y": [1.9043892865039365e-07, 1.1475284697805283e-07, 4.251667179919719e-08]}, "mutation_prompt": null}
{"id": "bae17f0d-f78c-4d9d-b30a-7d8fa8f40587", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='BFGS', options=options)  # Switched to BFGS first\n        result = minimize(wrapped_func, result.x, method='Nelder-Mead', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhance refinement by switching to BFGS first to leverage gradient information.", "configspace": "", "generation": 20, "fitness": 0.44736582191159663, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.447 with standard deviation 0.210. And the mean value of best solutions found was 0.193 (0. is the best) with standard deviation 0.273.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.7042707074275436, 0.4479620045537286, 0.18986475375351763], "final_y": [7.563123070924367e-08, 9.37532336789865e-08, 0.5781381618474936]}, "mutation_prompt": null}
{"id": "7063f761-672a-43a1-8acb-be14f29a1539", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.2 * (1 - self.calls / self.budget)))  # Adjusted shrink_factor more dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Improved convergence by dynamically adjusting the optimization strategy based on performance in the remaining budget.", "configspace": "", "generation": 20, "fitness": 0.8749551859474832, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.000. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8749551859474832, 0.8749551859474832, 0.8749551859474832], "final_y": [4.0765223313081095e-08, 4.0765223313081095e-08, 4.0765223313081095e-08]}, "mutation_prompt": null}
{"id": "a63fcdb8-64ca-4bc9-9507-e4f711722b4c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-6  # Optimized step size for better gradient accuracy\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Optimizes the initial step size for gradient estimation, enhancing gradient accuracy in `improved_initial_guess`.", "configspace": "", "generation": 20, "fitness": 0.871381500186237, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.871 with standard deviation 0.005. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8642341286637447, 0.8749551859474832, 0.8749551859474832], "final_y": [5.56259486746477e-08, 4.0765223313081095e-08, 4.0765223313081095e-08]}, "mutation_prompt": null}
{"id": "57b643fd-7df3-4d5a-842c-617d253db8cf", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.05, 0.3 * (1 - self.calls / self.budget)))  # Changed shrink_factor dynamically\n            initial_guess = self.uniform_sample(refined_bounds)\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds):\n        return np.random.uniform(bounds[0], bounds[1], self.dim)\n    \n    def improved_initial_guess(self, func, bounds):\n        # Use middle point in bounds and perturb it based on a small local gradient estimate\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step * 0.5  # Adjust gradient perturbation scale\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Enhanced AdaptiveHybridOptimizer by fine-tuning local search and sampling strategies for faster convergence.", "configspace": "", "generation": 20, "fitness": 0.858136808971608, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.024. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.824500055019858, 0.8749551859474832, 0.8749551859474832], "final_y": [1.5222072698961814e-07, 4.0765223313081095e-08, 4.0765223313081095e-08]}, "mutation_prompt": null}
{"id": "282ade2c-7f27-42f6-807e-45e0320c02eb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.calls = 0\n    \n    def __call__(self, func):\n        bounds = np.array([func.bounds.lb, func.bounds.ub])\n        initial_guess = self.improved_initial_guess(func, bounds)\n        result = self.local_search(func, initial_guess, bounds)\n        \n        while self.calls < self.budget:\n            refined_bounds = self.adjust_bounds(result.x, bounds, shrink_factor=max(0.1, 0.3 * (1 - self.calls / self.budget)))\n            initial_guess = self.uniform_sample(refined_bounds, func)  # Changed to include func for dynamic sampling\n            new_result = self.local_search(func, initial_guess, refined_bounds)\n            \n            if new_result.fun < result.fun:\n                result = new_result\n        \n        return result.x, result.fun\n    \n    def local_search(self, func, initial_guess, bounds):\n        def wrapped_func(x):\n            self.calls += 1\n            return func(x)\n        \n        options = {'maxiter': min(self.budget - self.calls, 100)}\n        result = minimize(wrapped_func, initial_guess, method='Nelder-Mead', options=options)\n        result = minimize(wrapped_func, result.x, method='BFGS', options=options)\n\n        return result\n    \n    def uniform_sample(self, bounds, func):  # Added func to enable gradient-based refinement\n        sample = np.random.uniform(bounds[0], bounds[1], self.dim)\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(sample + step_vector) - func(sample)) / small_step\n        return sample - grad_estimate * small_step  # Refinement using gradient\n    \n    def improved_initial_guess(self, func, bounds):\n        midpoint = (bounds[0] + bounds[1]) / 2\n        small_step = 1e-5\n        grad_estimate = np.zeros(self.dim)\n        for d in range(self.dim):\n            step_vector = np.zeros(self.dim)\n            step_vector[d] = small_step\n            grad_estimate[d] = (func(midpoint + step_vector) - func(midpoint)) / small_step\n        return midpoint - grad_estimate * small_step\n    \n    def adjust_bounds(self, best_guess, bounds, shrink_factor=0.3):\n        new_bounds = np.zeros_like(bounds)\n        for d in range(self.dim):\n            center = best_guess[d]\n            width = bounds[1, d] - bounds[0, d]\n            new_bounds[0, d] = max(bounds[0, d], center - width * shrink_factor / 2)\n            new_bounds[1, d] = min(bounds[1, d], center + width * shrink_factor / 2)\n        return new_bounds", "name": "AdaptiveHybridOptimizer", "description": "Introduce a dynamic gradient-adjusted refinement in the uniform_sampling function to enhance exploration.", "configspace": "", "generation": 20, "fitness": 0.8755968368096888, "feedback": "The algorithm AdaptiveHybridOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.001. And the mean value of best solutions found was 0.000 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "3c14cce1-5ff7-4db2-9d5c-900fe689545f", "metadata": {"aucs": [0.8749551859474832, 0.8768801385341001, 0.8749551859474832], "final_y": [4.0765223313081095e-08, 2.4059507772994856e-08, 4.0765223313081095e-08]}, "mutation_prompt": null}
