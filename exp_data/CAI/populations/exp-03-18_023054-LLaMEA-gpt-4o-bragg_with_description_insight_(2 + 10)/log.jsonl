{"id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm combining Differential Evolution with a local search phase, using symmetric initialization and periodicity encouragement for efficient exploration and fine-tuning.", "configspace": "", "generation": 0, "fitness": 0.9031402163693626, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.903 with standard deviation 0.012. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": null, "metadata": {"aucs": [0.8924551223809727, 0.920222109719009, 0.8967434170081061], "final_y": [0.16509231503723942, 0.16628281963319314, 0.16518953167519723]}, "mutation_prompt": null}
{"id": "a468412a-6e10-45e1-ab8f-2e7b723af0e3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOpt:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, population_size):\n        return np.random.uniform(self.lb, self.ub, (population_size, self.dim))\n\n    def differential_evolution(self, pop, func, F=0.5, CR=0.9):\n        new_pop = np.copy(pop)\n        for i in range(pop.shape[0]):\n            candidates = list(range(pop.shape[0]))\n            candidates.remove(i)\n            a, b, c = pop[np.random.choice(candidates, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), self.lb, self.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) > func(pop[i]):  # Maximize\n                new_pop[i] = trial\n        return new_pop\n\n    def impose_periodicity(self, pop, period_length):\n        for i in range(pop.shape[0]):\n            pop[i] = np.tile(pop[i][:period_length], self.dim // period_length + 1)[:self.dim]\n        return pop\n\n    def local_optimization(self, x, func):\n        result = minimize(lambda x: -func(x), x, bounds=[(self.lb[i], self.ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        population_size = 20\n        period_length = self.dim // 2\n        pop = self.initialize_population(population_size)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func)\n            pop = self.impose_periodicity(pop, period_length)\n            for i in range(pop.shape[0]):\n                pop[i] = self.local_optimization(pop[i], func)\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n        best_solution = max(pop, key=func)\n        return best_solution", "name": "HybridOpt", "description": "A hybrid global-local optimization algorithm combining Differential Evolution for broad exploration and BFGS for local refinement, enhanced by a periodic constraint to encourage constructive interference in multilayer designs.", "configspace": "", "generation": 0, "fitness": 0.4915788861523353, "feedback": "The algorithm HybridOpt got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.492 with standard deviation 0.097. And the mean value of best solutions found was 0.409 (0. is the best) with standard deviation 0.065.", "error": "", "parent_id": null, "metadata": {"aucs": [0.47221191303530896, 0.38390238517069153, 0.6186223602510053], "final_y": [0.419566083161513, 0.48321921230693143, 0.32530467813662567]}, "mutation_prompt": null}
{"id": "79204509-3b41-4847-bb78-cef97c7aff0a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOpt:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, population_size):\n        return np.random.uniform(self.lb, self.ub, (population_size, self.dim))\n\n    def differential_evolution(self, pop, func, F=0.5, CR=0.9, adapt_CR=False):\n        if adapt_CR:  # Adaptive crossover rate\n            CR = 0.9 - 0.8 * (evaluations / self.budget)\n        new_pop = np.copy(pop)\n        for i in range(pop.shape[0]):\n            candidates = list(range(pop.shape[0]))\n            candidates.remove(i)\n            a, b, c = pop[np.random.choice(candidates, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), self.lb, self.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) > func(pop[i]):  # Maximize\n                new_pop[i] = trial\n        return new_pop\n\n    def impose_periodicity(self, pop, period_length):\n        for i in range(pop.shape[0]):\n            pop[i] = np.tile(pop[i][:period_length], self.dim // period_length + 1)[:self.dim]\n        return pop\n\n    def local_optimization(self, x, func):\n        result = minimize(lambda x: -func(x), x, bounds=[(self.lb[i], self.ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        population_size = 20\n        period_length = self.dim // 2\n        pop = self.initialize_population(population_size)\n        best_solution = max(pop, key=func)  # Elitism: track best solution\n        evaluations = 0\n\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func, adapt_CR=True)\n            pop = self.impose_periodicity(pop, period_length)\n            for i in range(pop.shape[0]):\n                pop[i] = self.local_optimization(pop[i], func)\n                if func(pop[i]) > func(best_solution):  # Update best solution\n                    best_solution = pop[i]\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n        return best_solution", "name": "HybridOpt", "description": "Enhanced HybridOpt by integrating elitism to preserve the best-found solutions and adopting adaptive crossover rates for diversity and convergence speed.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "a468412a-6e10-45e1-ab8f-2e7b723af0e3", "metadata": {}, "mutation_prompt": null}
{"id": "9100fadc-8c3e-4e24-97b8-458ac3edfc10", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOpt:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, population_size):\n        return np.random.uniform(self.lb, self.ub, (population_size, self.dim))\n\n    def differential_evolution(self, pop, func, F=0.5, CR=0.9):\n        new_pop = np.copy(pop)\n        for i in range(pop.shape[0]):\n            candidates = list(range(pop.shape[0]))\n            candidates.remove(i)\n            a, b, c = pop[np.random.choice(candidates, 3, replace=False)]\n            F_dynamic = F + (np.random.rand() - 0.5) * 0.1  # Dynamic scaling change\n            mutant = np.clip(a + F_dynamic * (b - c), self.lb, self.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) > func(pop[i]):  # Maximize\n                new_pop[i] = trial\n        return new_pop\n\n    def impose_periodicity(self, pop, period_length):\n        for i in range(pop.shape[0]):\n            pop[i] = np.tile(pop[i][:period_length], self.dim // period_length)  # Refined periodic constraint\n        return pop\n\n    def local_optimization(self, x, func):\n        result = minimize(lambda x: -func(x), x, bounds=[(self.lb[i], self.ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        population_size = 20\n        period_length = self.dim // 2\n        pop = self.initialize_population(population_size)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func)\n            pop = self.impose_periodicity(pop, period_length)\n            for i in range(pop.shape[0]):\n                pop[i] = self.local_optimization(pop[i], func)\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n        best_solution = max(pop, key=func)\n        return best_solution", "name": "HybridOpt", "description": "A refined optimization algorithm that incorporates dynamic scaling of mutation factor and a refined periodic constraint to enhance solution quality in multilayer designs.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "a468412a-6e10-45e1-ab8f-2e7b723af0e3", "metadata": {}, "mutation_prompt": null}
{"id": "15c7cf78-2f32-4896-af9f-7d6eb1d82b4d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOpt:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, population_size):\n        return np.random.uniform(self.lb, self.ub, (population_size, self.dim))\n\n    def differential_evolution(self, pop, func, F=0.8, CR=0.9):  # Increased F for stronger mutation\n        new_pop = np.copy(pop)\n        for i in range(pop.shape[0]):\n            candidates = list(range(pop.shape[0]))\n            candidates.remove(i)\n            a, b, c = pop[np.random.choice(candidates, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), self.lb, self.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) > func(pop[i]):  # Maximize\n                new_pop[i] = trial\n        return new_pop\n\n    def impose_periodicity(self, pop, period_length):\n        for i in range(pop.shape[0]):\n            pop[i] = np.tile(pop[i][:period_length], self.dim // period_length + 1)[:self.dim]\n        return pop\n\n    def local_optimization(self, x, func):\n        result = minimize(lambda x: -func(x), x, bounds=[(self.lb[i], self.ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        population_size = 20\n        period_length = self.dim // 2\n        pop = self.initialize_population(population_size)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func)\n            pop = self.impose_periodicity(pop, period_length)\n            for i in range(pop.shape[0]):\n                pop[i] = self.local_optimization(pop[i], func)\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n        best_solution = max(pop, key=func)\n        return best_solution", "name": "HybridOpt", "description": "A refined hybrid global-local optimization algorithm with improved mutation strategy in Differential Evolution for Bragg mirror optimization.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "a468412a-6e10-45e1-ab8f-2e7b723af0e3", "metadata": {}, "mutation_prompt": null}
{"id": "7533c4ff-4002-43f5-b40a-b95c01a15adb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOpt:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, population_size):\n        return np.random.uniform(self.lb, self.ub, (population_size, self.dim))\n\n    def differential_evolution(self, pop, func, F=0.5, CR=0.9):\n        new_pop = np.copy(pop)\n        for i in range(pop.shape[0]):\n            candidates = list(range(pop.shape[0]))\n            candidates.remove(i)\n            a, b, c = pop[np.random.choice(candidates, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), self.lb, self.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) > func(pop[i]):  # Maximize\n                new_pop[i] = trial\n            CR = 0.9 if func(trial) > func(pop[i]) else 0.7 # Adaptive CR\n        return new_pop\n\n    def impose_periodicity(self, pop, period_length):\n        for i in range(pop.shape[0]):\n            period = np.tile(pop[i][:period_length], self.dim // period_length + 1)[:self.dim]\n            pop[i] = (pop[i] + period) / 2 # Improved periodicity enforcement\n        return pop\n\n    def local_optimization(self, x, func):\n        result = minimize(lambda x: -func(x), x, bounds=[(self.lb[i], self.ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        population_size = 20\n        period_length = self.dim // 2\n        pop = self.initialize_population(population_size)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func)\n            pop = self.impose_periodicity(pop, period_length)\n            for i in range(pop.shape[0]):\n                pop[i] = self.local_optimization(pop[i], func)\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n        best_solution = max(pop, key=func)\n        return best_solution", "name": "HybridOpt", "description": "Enhanced HybridOpt with adaptive crossover rates and improved periodicity enforcement for better exploration and convergence.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "a468412a-6e10-45e1-ab8f-2e7b723af0e3", "metadata": {}, "mutation_prompt": null}
{"id": "28fc039b-4a8b-44ef-ae4a-a665cf2117d1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOpt:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, population_size):\n        return np.random.uniform(self.lb, self.ub, (population_size, self.dim))\n\n    def differential_evolution(self, pop, func, F=0.6, CR=0.85):  # Adjusted F and CR\n        new_pop = np.copy(pop)\n        for i in range(pop.shape[0]):\n            candidates = list(range(pop.shape[0]))\n            candidates.remove(i)\n            a, b, c = pop[np.random.choice(candidates, 3, replace=False)]\n            mutant = np.clip(a + F * (b - c), self.lb, self.ub)\n            cross_points = np.random.rand(self.dim) < CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, pop[i])\n            if func(trial) > func(pop[i]):  # Maximize\n                new_pop[i] = trial\n        return new_pop\n\n    def impose_periodicity(self, pop, period_length):\n        period_length = np.random.randint(2, self.dim // 2 + 1)  # Dynamic period length\n        for i in range(pop.shape[0]):\n            pop[i] = np.tile(pop[i][:period_length], self.dim // period_length + 1)[:self.dim]\n        return pop\n\n    def local_optimization(self, x, func):\n        result = minimize(lambda x: -func(x), x, bounds=[(self.lb[i], self.ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        return result.x\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        population_size = 20\n        period_length = self.dim // 2\n        pop = self.initialize_population(population_size)\n        evaluations = 0\n\n        while evaluations < self.budget:\n            pop = self.differential_evolution(pop, func)\n            pop = self.impose_periodicity(pop, period_length)\n            for i in range(pop.shape[0]):\n                pop[i] = self.local_optimization(pop[i], func)\n                evaluations += 1\n                if evaluations >= self.budget:\n                    break\n\n        best_solution = max(pop, key=func)\n        return best_solution", "name": "HybridOpt", "description": "Enhanced a hybrid global-local algorithm with adaptive DE parameters and dynamic periodicity for improved exploration and convergence.", "configspace": "", "generation": 1, "fitness": -Infinity, "feedback": "An exception occurred: NameError(\"name 'evaluations' is not defined\").", "error": "NameError(\"name 'evaluations' is not defined\")", "parent_id": "a468412a-6e10-45e1-ab8f-2e7b723af0e3", "metadata": {}, "mutation_prompt": null}
{"id": "ca203dcd-9b4c-4828-af89-71388c84bb09", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < (CR + 0.1)  # Updated crossover probability\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A refined hybrid algorithm with optimized crossover strategy in Differential Evolution for better exploration and fine-tuning.", "configspace": "", "generation": 1, "fitness": 0.8347679614581204, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.061. And the mean value of best solutions found was 0.200 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.7627557801777182, 0.830277766479054, 0.9112703377175888], "final_y": [0.2312172130157426, 0.1967863798529782, 0.17245918867084942]}, "mutation_prompt": null}
{"id": "e79959e5-9563-4a33-a7a0-7357ec8b2503", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < (CR + np.random.rand() * 0.1)  # Changed line\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm that combines Differential Evolution with local search, using a dynamically adjusted crossover rate to improve exploration and fine-tuning.", "configspace": "", "generation": 1, "fitness": 0.8570280578499059, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.014. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8608816879943435, 0.8717222524595422, 0.8384802330958319], "final_y": [0.18827616923751522, 0.19114170342440762, 0.19325757159568002]}, "mutation_prompt": null}
{"id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration and exploitation through adaptive mutation strategy in Differential Evolution and improved local search efficiency.", "configspace": "", "generation": 1, "fitness": 0.8942874301331213, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.894 with standard deviation 0.043. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8340956720788025, 0.9231822204534217, 0.9255843978671395], "final_y": [0.19377263123254584, 0.16655593934569735, 0.17636579840819233]}, "mutation_prompt": null}
{"id": "7e876212-3272-48e4-bafb-c9151190f696", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        if res.success:\n            x0 = res.x \n        self.eval_count += res.nfev\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm with adaptive local search strategy and improved periodic encouragement to optimize multilayer Bragg mirrors.", "configspace": "", "generation": 1, "fitness": 0.8350623059466113, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.835 with standard deviation 0.072. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.7486719742665054, 0.9246724351362618, 0.8318425084370668], "final_y": [0.18746166358185223, 0.17041757616386433, 0.1804259931559311]}, "mutation_prompt": null}
{"id": "99dda721-0b57-4930-89a1-f30120fad904", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                # Modify the mutation strategy to encourage periodicity\n                mutant = np.clip((x0 + F * (x1 - x2)) % 1.0, lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "The optimized algorithm integrates periodicity encouragement directly into the mutation strategy of the Differential Evolution phase to enhance the solution's constructive interference properties.", "configspace": "", "generation": 1, "fitness": 0.7254033985225904, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.725 with standard deviation 0.230. And the mean value of best solutions found was 0.276 (0. is the best) with standard deviation 0.148.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.40116926828217403, 0.9113274557514586, 0.8637134715341388], "final_y": [0.485122211165252, 0.16676190175537653, 0.17581641117765667]}, "mutation_prompt": null}
{"id": "ca4625d4-4b6b-4897-a86d-b66428e534ea", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(pop_size, 5, replace=False)\n                x0, x1, x2, x3, x4 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2) + F * (x3 - x4), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved performance by enhancing the candidate solution selection during differential evolution.", "configspace": "", "generation": 2, "fitness": 0.6564945679052592, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.656 with standard deviation 0.072. And the mean value of best solutions found was 0.270 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.756567053319597, 0.6252917282174464, 0.5876249221787342], "final_y": [0.19939422343106727, 0.30946393929225724, 0.3022455084277498]}, "mutation_prompt": null}
{"id": "42ef916c-111a-44c3-a495-684c09b4a2e3", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.random.beta(0.5, 0.5, (size, self.dim))\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                CR = 0.9 * (self.budget - self.eval_count) / self.budget + 0.1\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution algorithm with chaotic initialization and adaptive crossover to improve solution exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.6522032157070617, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.652 with standard deviation 0.115. And the mean value of best solutions found was 0.280 (0. is the best) with standard deviation 0.085.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8055437241758109, 0.5283077501003985, 0.6227581728449756], "final_y": [0.16554128797768275, 0.367342291782497, 0.3072820484978004]}, "mutation_prompt": null}
{"id": "c86d9034-739e-4d95-a7ad-69d6614f8b9f", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop_size = min(pop_size, self.budget - self.eval_count)  # Adapt population size\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm combining Differential Evolution with a local search phase, using symmetric initialization, periodicity encouragement, and adaptive population size for efficient exploration and fine-tuning.", "configspace": "", "generation": 2, "fitness": 0.7612297805825995, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.761 with standard deviation 0.107. And the mean value of best solutions found was 0.233 (0. is the best) with standard deviation 0.041.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8797936260845782, 0.7829200168726678, 0.6209756987905524], "final_y": [0.18224278009636585, 0.2345114170275101, 0.283484515975166]}, "mutation_prompt": null}
{"id": "310d2027-94ef-41d2-869f-6a9c160e957c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.8):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved global search by adjusting the crossover rate in Differential Evolution for better exploration.", "configspace": "", "generation": 2, "fitness": 0.6618968975551982, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.662 with standard deviation 0.108. And the mean value of best solutions found was 0.287 (0. is the best) with standard deviation 0.075.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8119746769593904, 0.6093685233016621, 0.5643474924045422], "final_y": [0.18200845497802431, 0.32820837904502154, 0.3520222347453662]}, "mutation_prompt": null}
{"id": "8d97a490-ffec-4b96-a093-68f68bd11f71", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial) + np.sin(np.pi * np.sum(trial))\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm that enhances Differential Evolution with a local search phase, using symmetric initialization and adaptive periodicity encouragement for efficient exploration and fine-tuning.", "configspace": "", "generation": 2, "fitness": 0.5564088906575463, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.556 with standard deviation 0.033. And the mean value of best solutions found was 0.331 (0. is the best) with standard deviation 0.024.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.5628918126600835, 0.5130250064440942, 0.5933098528684613], "final_y": [0.34226938009957975, 0.3540385558694136, 0.2972643982163542]}, "mutation_prompt": null}
{"id": "76889483-5b10-4dbe-b859-0bd7102ed8c6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.7)  # Adaptive mutation with slightly higher mean\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim // 2)] = True  # Ensure crossover happens earlier in the array\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Refinement of adaptive mutation and crossover strategy in Differential Evolution to improve robustness and convergence speed.", "configspace": "", "generation": 2, "fitness": 0.7070089439281929, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.707 with standard deviation 0.127. And the mean value of best solutions found was 0.270 (0. is the best) with standard deviation 0.072.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8864553736252175, 0.6275508047544958, 0.6070206534048652], "final_y": [0.16795133425717623, 0.3197366119168271, 0.32341297411287717]}, "mutation_prompt": null}
{"id": "d23d84da-7fda-43b0-b03d-0fcae9218000", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        init_pop = center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n        return np.vstack((init_pop, lb + ub - init_pop))[:size]  # Quasi-Oppositional\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F + np.random.rand() * 0.2 - 0.1  # Adaptive F\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhancing the BraggMirrorOptimizer by introducing quasi-oppositional learning in initialization and an adaptive DE strategy for better exploration and exploitation balance.", "configspace": "", "generation": 2, "fitness": 0.6576401658974366, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.658 with standard deviation 0.121. And the mean value of best solutions found was 0.277 (0. is the best) with standard deviation 0.078.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8219098449148919, 0.5362089685542482, 0.6148016842231697], "final_y": [0.18211410139344664, 0.3730968672570677, 0.27531982649446907]}, "mutation_prompt": null}
{"id": "aefd3d6f-edde-43d3-868c-4f0f1df72317", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                CR_dynamic = CR * (1 - self.eval_count / self.budget)  # Dynamic adjustment of CR\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n\n                # Periodicity encouragement penalty\n                periodicity_penalty = np.sum(np.abs(np.diff(trial[:self.dim//2]) - np.diff(trial[self.dim//2:])))\n                f_trial = func(trial) + periodicity_penalty \n\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved exploitation by dynamically adjusting the crossover rate and integrating a periodicity-encouraging penalty in the cost function.", "configspace": "", "generation": 2, "fitness": 0.6881466722155284, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.688 with standard deviation 0.041. And the mean value of best solutions found was 0.274 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.6644016537402919, 0.6538742854995379, 0.7461640774067556], "final_y": [0.28340320917856765, 0.29014549484805874, 0.24993418521263122]}, "mutation_prompt": null}
{"id": "e390f517-eebc-4d2c-a2b8-07d02b3129c2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                CR_adaptive = 0.8 + 0.2 * np.random.rand()  # Adaptive crossover strategy\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            variance = np.var(best)  # Variance-based local search trigger\n            if variance > 0.1:  # Conditional local search\n                best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution with adaptive crossover strategy and variance-based local search for improved reflectivity optimization.", "configspace": "", "generation": 2, "fitness": 0.7138232553100758, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.714 with standard deviation 0.137. And the mean value of best solutions found was 0.254 (0. is the best) with standard deviation 0.059.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.902574525306244, 0.5796149626266438, 0.6592802779973396], "final_y": [0.18152883170834888, 0.32707401074363807, 0.25347388873132193]}, "mutation_prompt": null}
{"id": "dcdcac99-b20f-4eaf-890f-b9efd65a23b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        opp_center = lb + ub - center\n        pop = center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n        opp_pop = opp_center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n        return np.vstack((pop, opp_pop))\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size // 2)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                if np.random.rand() < 0.5:  # Promote periodicity\n                    trial = np.where(cross_points, mutant, pop[i])\n                else:\n                    trial = np.sin(2 * np.pi * mutant)  # Harmonic crossover\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Advanced hybrid optimizer using Quasi-Oppositional initialization and periodicity-promoting crossover in Differential Evolution for Bragg mirror design.", "configspace": "", "generation": 2, "fitness": 0.6725653065877707, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.673 with standard deviation 0.098. And the mean value of best solutions found was 0.247 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8101736852205716, 0.6153388167000898, 0.5921834178426506], "final_y": [0.18964140692688247, 0.2543951394550513, 0.295632993701764]}, "mutation_prompt": null}
{"id": "2cfd2397-2606-4684-aa09-50b63f6c1bfc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n\n        # Dynamic population size adaptation\n        pop_size = min(50, max(10, pop_size + self.eval_count // 100))\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration by adapting population size dynamically in Differential Evolution phase.", "configspace": "", "generation": 3, "fitness": 0.7403327882337174, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.740 with standard deviation 0.118. And the mean value of best solutions found was 0.251 (0. is the best) with standard deviation 0.065.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8960933191933443, 0.7158148410316996, 0.6090902044761081], "final_y": [0.16551952458936847, 0.2652834941168879, 0.32358587527464266]}, "mutation_prompt": null}
{"id": "ea7b9d44-96bd-497b-877a-269090154a50", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        step_size = 1.0 if self.eval_count < self.budget / 2 else 0.5  # Dynamically adjust step size\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': int(self.budget - self.eval_count) * step_size})\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved local search efficiency by dynamically adjusting the L-BFGS-B step size based on remaining budget.", "configspace": "", "generation": 3, "fitness": 0.7673817807309286, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.767 with standard deviation 0.115. And the mean value of best solutions found was 0.236 (0. is the best) with standard deviation 0.070.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8654926143018876, 0.605546541332985, 0.8311061865579127], "final_y": [0.16490315977736014, 0.3312400223749894, 0.21090666065252672]}, "mutation_prompt": null}
{"id": "e39b45b0-99d3-42d4-8b45-36ad308c29c0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            CR = 0.1 + 0.8 * (self.eval_count / self.budget)  # Adaptive CR\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial = np.clip(trial, lb, ub)  # Reflection\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution with adaptive crossover rate and periodic boundary reflection to improve exploration and convergence.", "configspace": "", "generation": 3, "fitness": 0.8257642293909901, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.826 with standard deviation 0.051. And the mean value of best solutions found was 0.201 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8228828356141535, 0.7641947333011752, 0.8902151192576415], "final_y": [0.17428132349909398, 0.24107542804301574, 0.18763638053039544]}, "mutation_prompt": null}
{"id": "95cd6599-f419-41d0-b09b-2c8472e254ce", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.8):  # Changed CR from 0.9 to 0.8\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm combining Differential Evolution with a local search phase, using symmetric initialization and periodicity encouragement, and enhanced by modifying the crossover rate for better exploration.", "configspace": "", "generation": 3, "fitness": 0.7847773998298742, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.067. And the mean value of best solutions found was 0.214 (0. is the best) with standard deviation 0.035.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.7541262021754986, 0.8783486888077405, 0.7218573085063831], "final_y": [0.18267157564495762, 0.19590227743930966, 0.2619291972526038]}, "mutation_prompt": null}
{"id": "8ff88a0a-8ae6-4155-a9c9-09bfc6732004", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5) + 0.1 * np.random.randn()  # Enhance adaptive mutation with random perturbation\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improve convergence by enhancing the adaptive mutation in Differential Evolution with an additional random perturbation.", "configspace": "", "generation": 3, "fitness": 0.8223472060283182, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.822 with standard deviation 0.097. And the mean value of best solutions found was 0.218 (0. is the best) with standard deviation 0.044.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8862578651280332, 0.6852728355420057, 0.8955109174149158], "final_y": [0.18515079872031315, 0.28068895704796193, 0.18817939802843742]}, "mutation_prompt": null}
{"id": "85b82a01-8438-47d5-9e95-62a6c062e6ad", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                # Adjusting CR based on population diversity\n                CR = 0.9 - 0.8 * (np.std(pop) / np.mean(np.abs(pop)))\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved exploration by adjusting the crossover rate adaptively based on the diversity of the population.", "configspace": "", "generation": 3, "fitness": 0.7021814520695214, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.702 with standard deviation 0.112. And the mean value of best solutions found was 0.263 (0. is the best) with standard deviation 0.074.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8358876300530007, 0.7095564778857029, 0.5611002482698609], "final_y": [0.168354242255509, 0.2722940976219905, 0.3489386871556246]}, "mutation_prompt": null}
{"id": "e83334ce-da30-433e-ace2-1ceecb98538a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n            if np.random.rand() < 0.05:  # Random restart strategy\n                pop = self.symmetric_initialization(lb, ub, pop_size)\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Incorporate random restart strategy to avoid local minima and enhance exploration in Differential Evolution.", "configspace": "", "generation": 3, "fitness": 0.8074071027149236, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.074. And the mean value of best solutions found was 0.224 (0. is the best) with standard deviation 0.029.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.9090023447069469, 0.7799195972884794, 0.7332993661493441], "final_y": [0.18421577874884454, 0.23461231465724441, 0.2529568268528636]}, "mutation_prompt": null}
{"id": "90f500d0-0eec-445e-b22f-a3b518eef08e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                CR_dynamic = CR * (0.5 + 0.5 * np.random.rand())  # Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(120, self.budget - self.eval_count)})  # Extended local search\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved exploration and local search by adjusting crossover rate dynamically in Differential Evolution and refining local search strategy.", "configspace": "", "generation": 3, "fitness": 0.7342307058363748, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.734 with standard deviation 0.100. And the mean value of best solutions found was 0.250 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.870039694839463, 0.6314767953325284, 0.7011756273371329], "final_y": [0.16853873658537888, 0.31168647335665245, 0.26966681320511743]}, "mutation_prompt": null}
{"id": "36949f11-14c2-4f1b-93e5-5d60442efd1b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                dynamic_CR = CR * (0.5 + 0.5 * np.random.rand())  # Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved convergence by introducing a dynamic crossover rate in Differential Evolution to enhance exploration and exploitation balance.", "configspace": "", "generation": 3, "fitness": 0.7901905030086587, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.790 with standard deviation 0.091. And the mean value of best solutions found was 0.225 (0. is the best) with standard deviation 0.049.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8939132516896846, 0.6714034676184766, 0.8052547897178148], "final_y": [0.1651572363242808, 0.2846610025752673, 0.22608548674228046]}, "mutation_prompt": null}
{"id": "e5e4f152-3715-410e-95b0-20a3c2aaa98a", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n            pop_size = max(5, int(pop_size * 0.95))  # Dynamically adjust population size\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improvement in exploration and exploitation balance by dynamically adjusting population size during optimization.", "configspace": "", "generation": 3, "fitness": 0.7295608389097445, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.730 with standard deviation 0.111. And the mean value of best solutions found was 0.263 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8704146021785291, 0.5985245246707516, 0.719743389879953], "final_y": [0.1935448283928285, 0.3297006914890551, 0.26476612344952655]}, "mutation_prompt": null}
{"id": "6c69e826-12d1-4f29-9386-9354091f4dcc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.7, CR=0.9):  # Adjusted mutation scaling factor\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm combining Differential Evolution with a local search phase, using symmetric initialization, periodicity encouragement, and adjusted mutation scaling for efficient exploration and fine-tuning.", "configspace": "", "generation": 4, "fitness": 0.8856156772283953, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.886 with standard deviation 0.032. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8657794870469343, 0.8608306207214722, 0.9302369239167796], "final_y": [0.16593304578175616, 0.16578504272191752, 0.1649588465123608]}, "mutation_prompt": null}
{"id": "c62cf9b9-b006-4b72-b955-d40374e02efb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            x_history = [best] # Added line\n            best = self.local_search(func, best, bounds)\n            x_history.append(best) # Added line\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced local search strategy by incorporating a history mechanism to better fine-tune the best solutions found.", "configspace": "", "generation": 4, "fitness": 0.854399782218914, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.854 with standard deviation 0.023. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8625421084731771, 0.8226530102810646, 0.8780042279025001], "final_y": [0.16606858370972166, 0.16634150714540163, 0.16616929087348364]}, "mutation_prompt": null}
{"id": "4222b942-be34-485c-ac5c-acf78f634307", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR * (1 - self.eval_count / self.budget)  # Adjusted line\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A refined hybrid algorithm using Differential Evolution and local search, now with an enhanced adaptive crossover rate to improve convergence efficiency.", "configspace": "", "generation": 4, "fitness": 0.8740133343123103, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.874 with standard deviation 0.038. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8964873961477114, 0.8198481881872856, 0.905704418601934], "final_y": [0.165175404125653, 0.18698096233493045, 0.16497009251751515]}, "mutation_prompt": null}
{"id": "9dfd1466-7bf0-4940-b920-cfaf20296887", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.8, CR=0.9):  # Changed F from 0.5 to 0.8\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm that integrates Differential Evolution with dynamic exploration strategies and periodicity bias to optimize multilayer photonic structures efficiently.", "configspace": "", "generation": 4, "fitness": 0.80039855005427, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.800 with standard deviation 0.104. And the mean value of best solutions found was 0.201 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.6620861059036545, 0.8251049384958746, 0.9140046057632806], "final_y": [0.2312630444463709, 0.20729408052032627, 0.1655051738971809]}, "mutation_prompt": null}
{"id": "698dfbf3-754b-42ab-bd49-619c702785f5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.85):  # Adjusted CR from 0.9 to 0.85\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved solution refinement by adjusting the crossover rate (CR) for enhanced exploration and exploitation balance in Differential Evolution.", "configspace": "", "generation": 4, "fitness": 0.8580201811699709, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.858 with standard deviation 0.046. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8001541286109579, 0.8613719019137904, 0.9125345129851643], "final_y": [0.18017130462144482, 0.16529320596755492, 0.16502871389230034]}, "mutation_prompt": null}
{"id": "aeeb9f35-6931-4cc2-a422-275267cf2ddc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                CR = 0.9 * (1 - self.eval_count / self.budget)  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhancing Differential Evolution by implementing an adaptive crossover rate for improved exploration and convergence.", "configspace": "", "generation": 4, "fitness": 0.8798143555280847, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.045. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8278222739040364, 0.8743318497678341, 0.9372889429123836], "final_y": [0.16508708039657816, 0.16487357271570247, 0.16521222111862988]}, "mutation_prompt": null}
{"id": "9e7f7da7-3d1d-4565-b430-bde935c78983", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.75)  # Adaptive mutation strategy with refined scaling\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(150, self.budget - self.eval_count)})  # Enhanced iterations\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Amplified precision in DE's adaptive mutation by refining control parameter F with scaling, and enhancing L-BFGS-B iterations.", "configspace": "", "generation": 4, "fitness": 0.8727103482854455, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.005. And the mean value of best solutions found was 0.167 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8675635035723989, 0.8711564789301023, 0.8794110623538349], "final_y": [0.17039698113404067, 0.16602260001817215, 0.16500601636078227]}, "mutation_prompt": null}
{"id": "1d173dfe-8f61-4633-976f-018f8b17d6d0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n            pop_size = min(max(5, int(pop_size * 0.95)), 50) # Adaptive population size adjustment\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved exploration by adaptive population size adjustment within Differential Evolution, enhancing solution quality.", "configspace": "", "generation": 4, "fitness": 0.8429930659790964, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.843 with standard deviation 0.020. And the mean value of best solutions found was 0.175 (0. is the best) with standard deviation 0.013.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8256180575602284, 0.8327722243534835, 0.8705889160235776], "final_y": [0.19309076081987941, 0.16650589447826192, 0.1649299105346943]}, "mutation_prompt": null}
{"id": "97f21603-da67-4835-85b9-fcb75eda121d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                dynamic_CR = CR * (1 - np.exp(-0.05 * self.eval_count))  # Dynamic crossover probability\n                cross_points = np.random.rand(self.dim) < dynamic_CR  # Use dynamic_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved exploration and exploitation using dynamic crossover probability in Differential Evolution with seamless integration of local search for fine-tuning.", "configspace": "", "generation": 4, "fitness": 0.8803019604022633, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.880 with standard deviation 0.043. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.821937032380551, 0.8926512152205307, 0.926317633605708], "final_y": [0.20193196944439518, 0.16505805253393147, 0.16491077010965705]}, "mutation_prompt": null}
{"id": "04b36563-e984-4cfe-b8fd-ac726e59433e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())  # Adaptive crossover\n                cross_points = np.random.rand(self.dim) < adaptive_CR  # Change\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count), 'disp': False})  # Change\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration and exploitation using an adaptive crossover rate in Differential Evolution and improved local search with memory.", "configspace": "", "generation": 4, "fitness": 0.8818744256700142, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.033. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8404796232462932, 0.9223817390201958, 0.8827619147435535], "final_y": [0.16627211318689394, 0.16492030008054348, 0.16671908785150957]}, "mutation_prompt": null}
{"id": "66a83f5a-c12f-40f4-8c3c-8b5af66702af", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                \n                # Added line for boundary reflection\n                pop[i] = np.clip(pop[i], lb, ub)\n                \n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm combining Differential Evolution with a local search phase, enhanced by periodic boundary reflection to prevent premature convergence and improve exploration.", "configspace": "", "generation": 5, "fitness": 0.8065266803070793, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.807 with standard deviation 0.086. And the mean value of best solutions found was 0.182 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8884274323400021, 0.6882323695869329, 0.8429202389943031], "final_y": [0.1664391093721127, 0.20346247364753955, 0.1756267144626199]}, "mutation_prompt": null}
{"id": "c78d98c1-8593-4d5b-8fb0-c716a65d9262", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(len(pop)):  # Dynamic Population Size\n                if len(pop) > 2:  # Ensure at least three individuals\n                    idxs = np.random.choice(np.delete(np.arange(len(pop)), i), 3, replace=False)\n                    x0, x1, x2 = pop[idxs]\n                    mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                    f_trial = func(trial)\n                    self.eval_count += 1\n                    if f_trial < func(pop[i]):\n                        pop[i] = trial\n                        if f_trial < func(best):\n                            best = trial\n                if self.eval_count >= self.budget:\n                    break\n            if len(pop) > pop_size // 2:  # Reduce population size adaptively\n                pop = pop[:pop_size // 2] \n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "An enhanced hybrid algorithm incorporating dynamic population size in Differential Evolution and adaptive local search for efficient exploitation.", "configspace": "", "generation": 5, "fitness": 0.7749525318683821, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.775 with standard deviation 0.062. And the mean value of best solutions found was 0.199 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8393194155905498, 0.7935544372907049, 0.691983742723892], "final_y": [0.20407744895979085, 0.16997067519206188, 0.22255075991318718]}, "mutation_prompt": null}
{"id": "e2ad56b6-1889-4075-a5e9-3b3ca61a80e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < (CR * (1 - self.eval_count / self.budget))\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved convergence by dynamically adjusting the crossover rate in Differential Evolution based on the current iteration.", "configspace": "", "generation": 5, "fitness": 0.7355869222109462, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.736 with standard deviation 0.087. And the mean value of best solutions found was 0.198 (0. is the best) with standard deviation 0.027.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8532219424207602, 0.6453779564434934, 0.708160867768585], "final_y": [0.16556037247496536, 0.23123269101604393, 0.19574567553687627]}, "mutation_prompt": null}
{"id": "25dcd18f-6992-44d5-a338-31379b4af0e1", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(150, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved local search efficiency by allowing more iterations in L-BFGS-B phase to enhance fine-tuning without exceeding the budget.", "configspace": "", "generation": 5, "fitness": 0.8089680294058975, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.809 with standard deviation 0.085. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.010.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.9196028378295164, 0.7116351830222576, 0.7956660673659189], "final_y": [0.1700430032348802, 0.18963691019878925, 0.16886221469827967]}, "mutation_prompt": null}
{"id": "448ec325-41aa-4236-a56c-3f1807945415", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='TNC', options={'maxiter': min(100, self.budget - self.eval_count)})  # Changed method to TNC\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced local search stage by using 'TNC' method instead of 'L-BFGS-B' for potentially better convergence properties within limited iteration budget.", "configspace": "", "generation": 5, "fitness": 0.7266573538995926, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.727 with standard deviation 0.087. And the mean value of best solutions found was 0.203 (0. is the best) with standard deviation 0.015.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8438267892496537, 0.7009997431856841, 0.6351455292634398], "final_y": [0.18627118534799159, 0.20055699539278915, 0.22265150561135838]}, "mutation_prompt": null}
{"id": "797185d1-5797-4c8a-8d6b-e8db62d5a844", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                adaptive_CR = CR * (np.random.rand() * 0.5 + 0.5)  # Adaptive crossover rate strategy\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Incorporating an adaptive crossover rate strategy in Differential Evolution to enhance exploration.", "configspace": "", "generation": 5, "fitness": 0.7239339967513189, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.724 with standard deviation 0.108. And the mean value of best solutions found was 0.225 (0. is the best) with standard deviation 0.050.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.875077608767903, 0.6706424952780152, 0.626081886208038], "final_y": [0.1650248627110502, 0.22223988166264996, 0.28770374399424803]}, "mutation_prompt": null}
{"id": "c3700e71-0d34-4661-b483-cd5821727788", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                # Modified line: introduce probability-based adaptive CR\n                cross_points = np.random.rand(self.dim) < (CR * np.random.rand())\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration by introducing a probability-based adaptive crossover rate in Differential Evolution.", "configspace": "", "generation": 5, "fitness": 0.7272700819115278, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.727 with standard deviation 0.081. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8422631676532173, 0.667772280264362, 0.6717747978170043], "final_y": [0.1823927911878418, 0.19132190285039674, 0.20033004731363402]}, "mutation_prompt": null}
{"id": "e718d783-de47-49e7-99e3-b5e36eddb147", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                adaptive_CR = 0.8 + 0.2 * np.random.rand()  # Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(150, self.budget - self.eval_count)})  # Enhanced efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved exploration through dynamic crossover rate in Differential Evolution and enhanced local search efficiency.", "configspace": "", "generation": 5, "fitness": 0.7847234292167031, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.785 with standard deviation 0.083. And the mean value of best solutions found was 0.197 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.901529856010015, 0.7238027237655149, 0.7288377078745792], "final_y": [0.17592921433422049, 0.2185196569371679, 0.19610899950805427]}, "mutation_prompt": null}
{"id": "b78a09f4-938b-4582-a558-4b9313ff87aa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                CR_adaptive = np.random.rand()\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Incorporate adaptive crossover rates in Differential Evolution to enhance the balance between exploration and exploitation.", "configspace": "", "generation": 5, "fitness": 0.7642879593529962, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.764 with standard deviation 0.075. And the mean value of best solutions found was 0.194 (0. is the best) with standard deviation 0.022.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8697206336874126, 0.7079618657462952, 0.7151813786252808], "final_y": [0.1655821228378852, 0.1964286797559056, 0.21896580315030678]}, "mutation_prompt": null}
{"id": "14176c50-5db7-472c-9ec3-4706a30779a0", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n        self.memory = None\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def adaptive_differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        self.memory = best.copy() if self.memory is None else self.memory\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.rand() * 0.5 + 0.5\n                CR = np.random.rand() * 0.5 + 0.5\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                        self.memory = best.copy()\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.adaptive_differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Integrates a memory component and adaptively tunes mutation and crossover rates in Differential Evolution based on performance to optimize Bragg mirror reflectivity.", "configspace": "", "generation": 5, "fitness": 0.6808967474571918, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.681 with standard deviation 0.028. And the mean value of best solutions found was 0.227 (0. is the best) with standard deviation 0.011.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.6457056570642368, 0.6839224078665849, 0.7130621774407535], "final_y": [0.2380016315742043, 0.21255437531959576, 0.23009290320579778]}, "mutation_prompt": null}
{"id": "865e716e-4716-4ba8-8a7a-6a06fe9c6de9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count / self.budget)  # Adaptive mutation factor\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced introduction of adaptive control in the Differential Evolution mutation factor for better exploration and convergence.", "configspace": "", "generation": 6, "fitness": 0.8867097762787483, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.017. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8779514663984331, 0.9100915643860845, 0.8720862980517272], "final_y": [0.1651420390081716, 0.16523659066177598, 0.1649951893995274]}, "mutation_prompt": null}
{"id": "2760f32e-e5ca-4218-83e9-f6c6bc97fa39", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                # Introducing adaptive crossover rate\n                CR = 0.9 * (1 - self.eval_count / self.budget)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Introduce an adaptive crossover rate in Differential Evolution to better balance exploration and exploitation.", "configspace": "", "generation": 6, "fitness": 0.8534845776270105, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.853 with standard deviation 0.043. And the mean value of best solutions found was 0.185 (0. is the best) with standard deviation 0.017.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.9093546076981622, 0.8053282794001466, 0.8457708457827227], "final_y": [0.16502417609727205, 0.20732101714323548, 0.18211128130443333]}, "mutation_prompt": null}
{"id": "3a92fd86-6aa5-4990-a24f-9e3e65619290", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=25, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "An enhanced hybrid algorithm leveraging adaptive population size and improved local search for efficient optimization of multilayer structures.", "configspace": "", "generation": 6, "fitness": 0.8716972663692998, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.043. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.84626651606775, 0.8360909148956059, 0.9327343681445437], "final_y": [0.18227389673124816, 0.16601216132373786, 0.1650332029518532]}, "mutation_prompt": null}
{"id": "23822d75-9075-4f9a-ac8f-53e8d821ff9d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                # Adaptive crossover rate\n                CR = 0.9 * (1 - self.eval_count / self.budget) + 0.1\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution with adaptive crossover rate to improve exploration and exploitation balance.", "configspace": "", "generation": 6, "fitness": 0.8826087201575069, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.883 with standard deviation 0.024. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8836408259451061, 0.8530960442916913, 0.9110892902357235], "final_y": [0.16578744740052787, 0.18194449635062193, 0.16491779898222925]}, "mutation_prompt": null}
{"id": "a0b6ba55-2ebe-4bc3-a7c5-578f5f15206e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.7 + 0.3)  # Refined adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Refined adaptive mutation strategy in Differential Evolution with an improved cross-over mechanism for better solution diversity.", "configspace": "", "generation": 6, "fitness": 0.890965955352277, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.891 with standard deviation 0.027. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.9281084095980667, 0.8661617333231674, 0.878627723135597], "final_y": [0.16762813508960972, 0.16489809269758549, 0.16518397426745968]}, "mutation_prompt": null}
{"id": "3afac3a4-80ba-4469-9045-2b1d4c046acd", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                adaptive_CR = CR * (np.random.rand() * 0.5 + 0.5)  # Adaptive crossover rate\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Introduced adaptive crossover rate in Differential Evolution and enhanced local search initialization to improve convergence and solution refinement.", "configspace": "", "generation": 6, "fitness": 0.8702523713601842, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.870 with standard deviation 0.036. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.002.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.9051966655188757, 0.8210448955151312, 0.8845155530465456], "final_y": [0.16486205047497493, 0.16972594764770943, 0.1683693158905022]}, "mutation_prompt": null}
{"id": "53c37e00-c11b-4bd7-bf6b-46b78a7c6c81", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.75)  # Enhanced adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhance adaptive mutation with dynamic scaling and improve local search by including gradient information for refined tuning in Differential Evolution.", "configspace": "", "generation": 6, "fitness": 0.8788470147241293, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.879 with standard deviation 0.021. And the mean value of best solutions found was 0.173 (0. is the best) with standard deviation 0.007.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.88795092366172, 0.8494866273804946, 0.8991034931301731], "final_y": [0.18254027865224431, 0.17111007110745868, 0.1650376294871192]}, "mutation_prompt": null}
{"id": "609f0d79-a930-492f-9016-bedd90a49ec7", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                adaptive_CR = CR * (np.random.rand() * 0.5 + 0.5)  # Adaptive crossover strategy\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            ls_initial = np.clip(best + np.random.randn(self.dim) * 0.01, bounds.lb, bounds.ub)  # Enhanced initial guess\n            best = self.local_search(func, ls_initial, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Integrate adaptive crossover strategy in Differential Evolution and enhance local search initialization for improved convergence.", "configspace": "", "generation": 6, "fitness": 0.8874792573338174, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.887 with standard deviation 0.016. And the mean value of best solutions found was 0.166 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8661430943181967, 0.8914790561487953, 0.9048156215344602], "final_y": [0.1678131275457907, 0.16486732768639578, 0.16609101431265627]}, "mutation_prompt": null}
{"id": "7a5ad06c-48f5-4f0b-a9b5-c1c214dad602", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                adaptive_CR = CR * (np.random.rand() * 0.5 + 0.5)  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Introducing adaptive crossover rates in Differential Evolution and enhancing local search initialization to improve convergence efficiency.", "configspace": "", "generation": 6, "fitness": 0.8574129343812928, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.857 with standard deviation 0.028. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8410758638895794, 0.8347352765424855, 0.8964276627118136], "final_y": [0.18197501281463047, 0.16719557443034128, 0.1651003818027521]}, "mutation_prompt": null}
{"id": "c4a8e6ad-235e-4ac3-8f83-f56dd2148686", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                adaptive_CR = CR * (0.5 + 0.5 * np.random.rand())  # Dynamic adjustment of CR\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced convergence by dynamically adjusting crossover rate in Differential Evolution to balance exploration and exploitation.", "configspace": "", "generation": 6, "fitness": 0.8727553844111057, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.873 with standard deviation 0.040. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.9288991009106822, 0.8525864045709676, 0.836780647751667], "final_y": [0.1650495526741812, 0.16507715067586648, 0.1823049594444044]}, "mutation_prompt": null}
{"id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved exploitation using adaptive mutation factor in Differential Evolution for better convergence.", "configspace": "", "generation": 7, "fitness": 0.9220807822104863, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.922 with standard deviation 0.033. And the mean value of best solutions found was 0.172 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8760264799661421, 0.9414875307132169, 0.9487283359520995], "final_y": [0.18311740552593636, 0.16602239144738773, 0.16713346394941386]}, "mutation_prompt": null}
{"id": "14a4020d-fa40-4db8-b762-68c4a8ee2ef6", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.95):  # Changed CR from 0.9 to 0.95\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration by increasing the crossover rate (CR) in Differential Evolution for better diversity in the search space.", "configspace": "", "generation": 7, "fitness": 0.8893544662833154, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.889 with standard deviation 0.026. And the mean value of best solutions found was 0.176 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8599687136139949, 0.9223431668544749, 0.8857515183814764], "final_y": [0.17080483578540262, 0.1756122980029743, 0.18292938849527574]}, "mutation_prompt": null}
{"id": "d3931788-8e8e-42c3-899c-d235761e9447", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.95):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "An improved hybrid optimization algorithm using Differential Evolution with quicker convergence by adjusting the crossover rate.", "configspace": "", "generation": 7, "fitness": 0.9026346932276699, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.903 with standard deviation 0.013. And the mean value of best solutions found was 0.179 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.8879928007252995, 0.9199479997899518, 0.8999632791677581], "final_y": [0.18270938166481554, 0.1724222986598286, 0.18065880494253583]}, "mutation_prompt": null}
{"id": "8cbf0b43-91c7-40d9-8f8e-a655d7c6e4fa", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        # Change: Adjust symmetric initialization to better cover boundary regions\n        return lb + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)  # Improved balance \n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "A hybrid algorithm integrating Differential Evolution and local search, with enhanced symmetric initialization for improved exploration and fine-tuning.", "configspace": "", "generation": 7, "fitness": 0.7862488392952399, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.786 with standard deviation 0.116. And the mean value of best solutions found was 0.215 (0. is the best) with standard deviation 0.038.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.6411276868395764, 0.791508270184317, 0.9261105608618264], "final_y": [0.2635898827708316, 0.21239609243419422, 0.17010047173930698]}, "mutation_prompt": null}
{"id": "7d7496ee-0f8c-43d8-8efa-55438e8b2f35", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            # Adapt population size based on remaining budget\n            current_pop_size = max(3, int(pop_size * (1 - self.eval_count / self.budget)))\n            for i in range(current_pop_size):  # adjust loop to use current_pop_size\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial) + self.periodicity_penalty(trial, bounds)  # apply periodicity penalty\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def periodicity_penalty(self, solution, bounds):\n        period = (bounds.ub - bounds.lb) / 2\n        penalty = np.sum(np.abs(np.sin(solution * np.pi / period)))\n        return penalty\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "An enhanced optimizer using adaptive population size and periodicity constraints for improved convergence and solution quality.", "configspace": "", "generation": 7, "fitness": 0.7818039097520803, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.782 with standard deviation 0.166. And the mean value of best solutions found was 0.219 (0. is the best) with standard deviation 0.056.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.5487486664297525, 0.8701058603502803, 0.9265572024762081], "final_y": [0.29839255463049463, 0.19069233092590165, 0.16929974291938832]}, "mutation_prompt": null}
{"id": "ef243e3e-fd8a-4786-9768-3bd8418210d5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            if self.eval_count > self.budget // 2:  # Dynamic population adjustment\n                pop_size = max(10, pop_size // 2)\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})\n        self.eval_count += res.nfev\n        if res.success:  # Additional condition for local search\n            return res.x\n        return x0\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved exploration by dynamic population size adjustment in Differential Evolution and refined local search conditions.", "configspace": "", "generation": 7, "fitness": 0.8500203384103977, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.850 with standard deviation 0.067. And the mean value of best solutions found was 0.191 (0. is the best) with standard deviation 0.016.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.7737147254680032, 0.9376474793070044, 0.8386988104561854], "final_y": [0.20315479480659682, 0.1677089607971487, 0.2006566119562313]}, "mutation_prompt": null}
{"id": "e48bbf66-10af-4309-a3dd-a469aa568c52", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.3 + 0.7)  # Enhanced adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced adaptive mutation scaling in Differential Evolution for improved global exploration.", "configspace": "", "generation": 7, "fitness": 0.8824192687514966, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.882 with standard deviation 0.034. And the mean value of best solutions found was 0.170 (0. is the best) with standard deviation 0.003.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.928417784561438, 0.8482161570392709, 0.8706238646537805], "final_y": [0.16628315599796517, 0.17435448520889785, 0.17040670954032477]}, "mutation_prompt": null}
{"id": "05d13336-fe13-4c10-aca8-e202fb6b262b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  # Adaptive mutation strategy\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                adaptive_CR = CR * (0.5 + np.random.rand() * 0.5)  # Adaptive crossover strategy\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})  # Improved efficiency\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration through adaptive crossover strategy in Differential Evolution for improved optimization performance.", "configspace": "", "generation": 7, "fitness": 0.8806075367856582, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.881 with standard deviation 0.028. And the mean value of best solutions found was 0.178 (0. is the best) with standard deviation 0.006.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8451856315043232, 0.8832900441140866, 0.9133469347385649], "final_y": [0.18190057913122226, 0.18266946903110282, 0.16935613260584637]}, "mutation_prompt": null}
{"id": "b16ecbb6-cf8c-4319-af34-9f12f084a0b5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                adaptive_F = F * (np.random.rand() * 0.5 + 0.5)  \n                adaptive_CR = CR * (np.random.rand() * 0.1 + 0.95)  # Dynamically adjust CR\n                mutant = np.clip(x0 + adaptive_F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < adaptive_CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B', options={'maxiter': min(100, self.budget - self.eval_count)})\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved adaptive mutation strategy by dynamically adjusting crossover rate in Differential Evolution for enhanced exploration and exploitation.", "configspace": "", "generation": 7, "fitness": 0.8751805233706498, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.875 with standard deviation 0.017. And the mean value of best solutions found was 0.183 (0. is the best) with standard deviation 0.008.", "error": "", "parent_id": "578fdfba-2b08-4a53-b77d-f344185d4ad1", "metadata": {"aucs": [0.8763962970250815, 0.8535810055613198, 0.8955642675255481], "final_y": [0.18612126778997984, 0.19051041035138183, 0.17221669949367024]}, "mutation_prompt": null}
{"id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved hybrid algorithm with adaptive parameter tuning, leveraging chaotic initialization and enhanced local search integration for better convergence.", "configspace": "", "generation": 7, "fitness": 0.9041815589514511, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.904 with standard deviation 0.040. And the mean value of best solutions found was 0.174 (0. is the best) with standard deviation 0.012.", "error": "", "parent_id": "b4058d9f-0fbc-4e4b-9a54-c28a5c319c42", "metadata": {"aucs": [0.9556315854508775, 0.8995659025336571, 0.8573471888698185], "final_y": [0.16485681162322952, 0.16636910592890752, 0.19023521443469815]}, "mutation_prompt": null}
{"id": "d819cf4e-a568-4056-aa7d-957abf07809d", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def levy_flight(self, size, dim):\n        return np.random.standard_cauchy(size=(size, dim))\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR_base=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2) + 0.01 * self.levy_flight(1, self.dim), lb, ub)\n                CR = CR_base * (1 - 0.5 * (self.eval_count / self.budget))\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration and exploitation using Lévy Flight distribution for mutation and adaptive crossover rate for improved performance.", "configspace": "", "generation": 8, "fitness": -Infinity, "feedback": "An exception occurred: ValueError('setting an array element with a sequence.').", "error": "ValueError('setting an array element with a sequence.')", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "e22509f5-e80c-462e-a919-57340d3aadf8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                CR_adaptive = CR * (1 - self.eval_count/self.budget)  # Adaptive crossover rate\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            x0_stochastic = best + np.random.randn(self.dim) * 0.05  # Stochastic local search initialization\n            best = self.local_search(func, x0_stochastic, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced DE with adaptive crossover rate and stochastic local search to balance exploration and exploitation effectively.", "configspace": "", "generation": 8, "fitness": 0.7446974367036003, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.745 with standard deviation 0.125. And the mean value of best solutions found was 0.248 (0. is the best) with standard deviation 0.070.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.888370119955131, 0.5847002509754406, 0.7610219391802293], "final_y": [0.1672731317424856, 0.3384638866810149, 0.23850129326636005]}, "mutation_prompt": null}
{"id": "e88fddbe-05a9-4eee-80d1-d92fd3b86cd5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                CR_dynamic = CR * (0.5 + 0.5 * np.cos(np.pi * self.eval_count / self.budget))  # Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR_dynamic\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution with dynamic crossover rate and periodicity encouragement for superior convergence.", "configspace": "", "generation": 8, "fitness": 0.7387574083295844, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.739 with standard deviation 0.177. And the mean value of best solutions found was 0.252 (0. is the best) with standard deviation 0.112.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.8532500307391093, 0.8740549638918931, 0.48896723035775036], "final_y": [0.16846832125286415, 0.1770786109539354, 0.41112486021822714]}, "mutation_prompt": null}
{"id": "5d6e5469-e428-4fc2-a60e-4d79dff19c5b", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR * (1 + 0.1 * np.sin(2 * np.pi * i / self.dim))  # Adaptive CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploration using adaptive crossover rate in Differential Evolution and integrating a periodicity bias to improve solution quality.", "configspace": "", "generation": 8, "fitness": 0.6732767339264987, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.673 with standard deviation 0.127. And the mean value of best solutions found was 0.287 (0. is the best) with standard deviation 0.081.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.8529291628724947, 0.5795025686233803, 0.5873984702836212], "final_y": [0.1718378653970345, 0.3467623057140232, 0.3413131306201349]}, "mutation_prompt": null}
{"id": "3aa3da9c-d227-44c1-a14d-49721b56f6dc", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                CR_adaptive = CR * (1 - (self.eval_count / self.budget))  # Adaptive crossover rate\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution with adaptive crossover rate and guided local search for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.7528285179954451, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.753 with standard deviation 0.155. And the mean value of best solutions found was 0.242 (0. is the best) with standard deviation 0.091.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.8301219778343216, 0.5360117153991647, 0.8923518607528491], "final_y": [0.16926975102171038, 0.3698120067212266, 0.18665472415700413]}, "mutation_prompt": null}
{"id": "c27e5da2-3fc4-4c6c-89e5-bf7fbb231907", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([self.modified_cost(func, ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget) \n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = self.modified_cost(func, trial)\n                self.eval_count += 1\n                if f_trial < self.modified_cost(func, pop[i]):\n                    pop[i] = trial\n                    if f_trial < self.modified_cost(func, best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def modified_cost(self, func, solution):\n        penalty = np.sum((solution[1:] - solution[:-1])**2)  # Encourage periodicity\n        return func(solution) + penalty\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Introduced periodicity encouragement through modified cost function and enhanced DE mutation strategy for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.8205201603310134, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.821 with standard deviation 0.095. And the mean value of best solutions found was 0.216 (0. is the best) with standard deviation 0.049.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.6858064184283681, 0.8845437969816292, 0.8912102655830426], "final_y": [0.2844752706787509, 0.17846906316360733, 0.1847820762346959]}, "mutation_prompt": null}
{"id": "ad1fd42b-8bb2-4402-977c-337ea1367e88", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range) * (1 - self.eval_count / self.budget)  # Enhanced mutation strategy\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n            pop_size = max(10, int(pop_size * (1 - self.eval_count / self.budget)))  # Adaptive population size\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced mutation strategy with adaptive population size for improved convergence and diversity.", "configspace": "", "generation": 8, "fitness": 0.7066955972319399, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.707 with standard deviation 0.201. And the mean value of best solutions found was 0.291 (0. is the best) with standard deviation 0.092.", "error": "", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {"aucs": [0.9856737762218308, 0.616146164510304, 0.518266850963685], "final_y": [0.16597515331365098, 0.3207044175712598, 0.3858525241429598]}, "mutation_prompt": null}
{"id": "eb9e0b01-e68e-44e5-b76e-e3b0f021c5a9", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR_initial=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        CR = CR_initial\n        \n        while self.eval_count < self.budget:\n            CR = CR_initial * (1 - self.eval_count / self.budget)  # Dynamic CR\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved hybrid algorithm with adaptive parameter tuning, leveraging chaotic initialization and enhanced local search integration for better convergence, now with dynamic crossover rate for adaptive exploration.", "configspace": "", "generation": 8, "fitness": 0.8061657405789141, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.806 with standard deviation 0.115. And the mean value of best solutions found was 0.215 (0. is the best) with standard deviation 0.065.", "error": "", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {"aucs": [0.8873452952939882, 0.8880030659908655, 0.6431488604518887], "final_y": [0.16564945365344974, 0.1730600719271942, 0.3075695824188789]}, "mutation_prompt": null}
{"id": "f2793a2b-ab9c-4172-823c-8f5dea2f3355", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            inertia_weight = 0.5 + np.random.rand() * 0.5  # Changed line\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + inertia_weight * F * (x1 - x2), lb, ub)  # Changed line\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        if res.success:  # Changed line\n            return res.x  # Changed line\n        return x0  # Changed line\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced differential evolution with adaptive inertia weight and selective local search integration for improved convergence efficiency.", "configspace": "", "generation": 8, "fitness": 0.722724455839305, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.723 with standard deviation 0.206. And the mean value of best solutions found was 0.280 (0. is the best) with standard deviation 0.111.", "error": "", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {"aucs": [0.9575274198610217, 0.45548294886527396, 0.7551629987916193], "final_y": [0.16485577324600875, 0.4301934689547684, 0.24621537333579635]}, "mutation_prompt": null}
{"id": "30a34571-41d0-496d-b3b4-d91e115b7d76", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def quasi_oppositional_initialization(self, lb, ub, size):\n        pop = lb + (ub - lb) * np.random.rand(size, self.dim)\n        opp_pop = ub + lb - pop\n        return np.concatenate((pop, opp_pop), axis=0)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR_range=(0.7, 0.9)):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.quasi_oppositional_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                CR = np.random.uniform(*CR_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced hybrid algorithm with periodicity encouragement, leveraging quasi-oppositional initialization and adaptive crossover for improved convergence.", "configspace": "", "generation": 8, "fitness": 0.778124232469852, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.778 with standard deviation 0.084. And the mean value of best solutions found was 0.223 (0. is the best) with standard deviation 0.057.", "error": "", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {"aucs": [0.8404535536395128, 0.8350059464937394, 0.6589131972763038], "final_y": [0.16498267764221997, 0.20371520771019302, 0.2997394715544496]}, "mutation_prompt": null}
{"id": "0776d6ea-6671-4437-b710-78feaf44dc58", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def levy_flight(self, size):\n        beta = 1.5\n        sigma = (np.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2) + 0.1 * self.levy_flight(self.dim), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced hybrid optimizer integrating Levy flight for better exploration and adaptive local search for improved exploitation.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "0bd1d0d5-bf69-4733-9717-73c22dd38dcb", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (np.sin(np.pi * self.eval_count / (2 * self.budget)))  # Adjusted mutation factor formula\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced adaptive mutation factor formula in Differential Evolution for improved convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {}, "mutation_prompt": null}
{"id": "9c0fc93e-637e-49ce-97fb-63bb8a67a0f4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR * (1 + 0.1 * np.sin(2 * np.pi * self.eval_count / self.budget))  # Adaptive crossover\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial = np.mean(trial.reshape(-1, 2), axis=1).repeat(2)  # Enforcing periodicity constraint\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution by incorporating periodicity constraints and adaptive crossover to improve solution quality.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {}, "mutation_prompt": null}
{"id": "606ed595-3f2f-4f63-ad61-2e8486ab5839", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.85):  # Changed CR from 0.9 to 0.85\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced balance between exploration and exploitation by fine-tuning crossover rate in Differential Evolution.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "1f001123-b22b-4400-ad94-ed9a21fb7e51", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        options = {'gtol': 1e-6}  # Changed line: Adaptive learning rate in BFGS\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS', options=options)\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced local exploitation by utilizing adaptive learning rate in BFGS for improved fine-tuning.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "7fd861b7-8c99-4792-b32a-454ea1a706e8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                # Periodicity-driven mutation for better performance\n                if i % 2 == 0:\n                    mutant = np.clip(mutant + np.sin(2 * np.pi * mutant), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced hybrid algorithm leveraging chaotic initialization and periodicity-driven mutation in DE for improved convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "b023aff4-df08-4427-bff5-bfbaeb0ea3e5", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.7):  # Changed CR from 0.9 to 0.7\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced differential evolution by adjusting the crossover rate for improved exploration and exploitation balance.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "5b000bdc-8dd6-4695-b013-94cc7a7ef017", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2 + best - x0), lb, ub)  # Modified line for improved mutation\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Improved convergence by modifying the mutation strategy in the differential evolution process.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "a4287232-a02b-4e38-9c20-861e0e72003e", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR_range=(0.8, 1.0)):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                CR = np.random.uniform(*CR_range)  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced hybrid algorithm with improved mutation strategy and adaptive crossover in Differential Evolution for better convergence.", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "0691a978-45ae-40da-8c84-48f331e43107", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                fitness_std = np.std(fitness)\n                CR = 0.5 + 0.5 * (fitness_std / np.mean(fitness))  # Adaptive CR based on fitness diversity\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Introduced adaptive crossover rate based on fitness diversity to enhance exploration and exploitation balance.  ", "configspace": "", "generation": 9, "fitness": -Infinity, "feedback": "An exception occurred: AttributeError(\"module 'numpy' has no attribute 'gamma'\").", "error": "AttributeError(\"module 'numpy' has no attribute 'gamma'\")", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {}, "mutation_prompt": null}
{"id": "a4ac16fe-7b83-4e58-99d6-a67f14ddf144", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                CR_adaptive = CR * (1 - self.eval_count/self.budget)  # Dynamically adjust crossover rate\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploitation by dynamically adjusting the crossover rate (CR) in Differential Evolution based on evaluation count.", "configspace": "", "generation": 10, "fitness": 0.8720033365791248, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.872 with standard deviation 0.061. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.001.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.8342985793113202, 0.8231679220814797, 0.9585435083445742], "final_y": [0.16630276633089935, 0.16499650547044187, 0.1648566877419969]}, "mutation_prompt": null}
{"id": "4079c355-5cbb-42f4-8625-f7aee44e6636", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                CR_adaptive = CR + 0.1 * (self.eval_count/self.budget)  # Adaptively increase crossover rate\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced exploitation using an adaptively increasing crossover rate in Differential Evolution for improved solution diversity.", "configspace": "", "generation": 10, "fitness": 0.876171542534205, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.876 with standard deviation 0.035. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.8795655041067799, 0.9177529578320941, 0.8311961656637413], "final_y": [0.18445939433391167, 0.16485745279494435, 0.16489274828803036]}, "mutation_prompt": null}
{"id": "f87e78a6-0384-4ea4-9c73-0f324b2340d4", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                CR_chaos = CR * (1 - np.sin(self.eval_count/self.budget * np.pi/2))  # Chaos-based adaptive CR\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR_chaos\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhancing diversity by incorporating chaos-based adaptive crossover rate in Differential Evolution.", "configspace": "", "generation": 10, "fitness": 0.8827233199042803, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.883 with standard deviation 0.025. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.847489236828239, 0.9065296567491816, 0.8941510661354206], "final_y": [0.16586750893947977, 0.1648572774873901, 0.16487049359814787]}, "mutation_prompt": null}
{"id": "7aa78e96-56ba-4ae3-90df-7c0825fc43b8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                CR_dynamic = CR * (self.eval_count/self.budget)  # Dynamic crossover rate based on progress\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR_dynamic  # Updated dynamic crossover rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution by incorporating dynamic crossover rate based on progress to improve diversity and convergence.", "configspace": "", "generation": 10, "fitness": 0.8776685377978529, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.878 with standard deviation 0.086. And the mean value of best solutions found was 0.171 (0. is the best) with standard deviation 0.009.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.7575678435207689, 0.9547038070296844, 0.9207339628431053], "final_y": [0.1845317528782241, 0.16485633254429366, 0.16485824588238174]}, "mutation_prompt": null}
{"id": "9fcca4e6-79f5-486b-9a2b-60780cdeb3b8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F_adaptive = F * (1 - self.eval_count/self.budget)  # Adjust mutation factor based on evaluations\n                CR_adaptive = CR + 0.1 * np.sin(2 * np.pi * self.eval_count/self.budget)  # Adaptive crossover\n                mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial = np.clip(trial - (trial % 2), lb, ub)  # Enforce periodicity\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced Differential Evolution with adaptive crossover probability and periodicity constraint for robust convergence.", "configspace": "", "generation": 10, "fitness": 0.8852934690475164, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.885 with standard deviation 0.072. And the mean value of best solutions found was 0.177 (0. is the best) with standard deviation 0.018.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.7964080617519711, 0.9725266109786271, 0.886945734411951], "final_y": [0.20265535695017212, 0.16485595803160968, 0.16485912827201177]}, "mutation_prompt": null}
{"id": "3f509473-713e-49c2-a295-2fc757dc66de", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def symmetric_initialization(self, lb, ub, size):\n        center = (lb + ub) / 2\n        return center + (np.random.rand(size, self.dim) - 0.5) * (ub - lb)\n\n    def levy_flight(self, step_size=0.01):\n        u = np.random.normal(0, 1, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        return step_size * u / np.abs(v) ** (1 / 3)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F=0.5, CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.symmetric_initialization(lb, ub, pop_size)\n        best_idx = np.argmin([func(ind) for ind in pop])\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                if np.random.rand() < 0.3:  # Introduce levy flight exploration\n                    trial = pop[i] + self.levy_flight()\n                    trial = np.clip(trial, lb, ub)\n                else:\n                    idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                    x0, x1, x2 = pop[idxs]\n                    F_adaptive = F * (1 - self.eval_count/self.budget)  \n                    mutant = np.clip(x0 + F_adaptive * (x1 - x2), lb, ub)\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < func(pop[i]):\n                    pop[i] = trial\n                    if f_trial < func(best):\n                        best = trial\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Hybrid algorithm enhancing exploration with Lévy flights and adaptive DE mutation for improved convergence in complex landscapes.", "configspace": "", "generation": 10, "fitness": 0.915481778759785, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.915 with standard deviation 0.062. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.005.", "error": "", "parent_id": "687ed113-37c6-4f1e-95ea-96eb5253dfa2", "metadata": {"aucs": [0.828823081016107, 0.9463648275441588, 0.9712574277190891], "final_y": [0.1750221586369266, 0.16485582286859934, 0.1648559108757255]}, "mutation_prompt": null}
{"id": "e3edc3e5-aa55-4a7d-9b75-6ff9e02a4b87", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def enforce_periodicity(self, arr):\n        period = self.dim // 2\n        return np.tile(arr[:period], 2)[:self.dim]\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(self.enforce_periodicity(ind)) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range) * 0.9  # Adaptive mutation factor\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                trial = self.enforce_periodicity(trial)\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced hybrid optimizer using frequency matching for periodicity enforcement and adaptive mutation in DE.", "configspace": "", "generation": 10, "fitness": 0.9533983188296409, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.953 with standard deviation 0.012. And the mean value of best solutions found was 0.168 (0. is the best) with standard deviation 0.004.", "error": "", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {"aucs": [0.966932197083782, 0.9549007896718485, 0.938361969733292], "final_y": [0.17285165452499607, 0.1648572048852257, 0.16485719543632138]}, "mutation_prompt": null}
{"id": "9f5b6cc3-fd08-45ea-bded-da45147de6a8", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, int(pop_size * (1 + self.eval_count / self.budget)))  # Adjusted population size\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop.shape[0]):  # Adjusted to reflect dynamic population size\n                idxs = np.random.choice(np.delete(np.arange(pop.shape[0]), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Enhanced hybrid algorithm using dynamic population size in Differential Evolution for improved convergence.", "configspace": "", "generation": 10, "fitness": 0.9272147929037899, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.927 with standard deviation 0.048. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {"aucs": [0.8600913023670761, 0.9722974874179389, 0.9492555889263546], "final_y": [0.16486776021906846, 0.16485687581842956, 0.16486136628892079]}, "mutation_prompt": null}
{"id": "9ffc5572-182e-474a-b2f1-73d69409c72c", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def periodic_initialization(self, lb, ub, size):\n        # Create a periodic pattern for initialization\n        period = 2 * np.pi / self.dim\n        return lb + (ub - lb) * (0.5 * (1 + np.cos(period * np.arange(size * self.dim)))).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR_range=(0.7, 0.9)):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.periodic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                CR = np.random.uniform(*CR_range)  # Dynamic crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='L-BFGS-B')  # Change method to L-BFGS-B\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Advanced hybrid optimization using periodic initialization, dynamic crossover, and strategic local search for enhanced convergence.", "configspace": "", "generation": 10, "fitness": 0.7224464344577343, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.722 with standard deviation 0.291. And the mean value of best solutions found was 0.207 (0. is the best) with standard deviation 0.060.", "error": "", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {"aucs": [0.31243833361335505, 0.9045589705777431, 0.9503419991821046], "final_y": [0.2920225029775452, 0.16488859981328996, 0.16485606233771966]}, "mutation_prompt": null}
{"id": "37265faa-3f47-46d2-88ac-2be7f3ec07b2", "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BraggMirrorOptimizer:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.eval_count = 0\n\n    def chaotic_initialization(self, lb, ub, size):\n        return lb + (ub - lb) * np.sin(np.linspace(0, np.pi, size * self.dim)).reshape(size, self.dim)\n\n    def differential_evolution(self, func, bounds, pop_size=20, F_range=(0.5, 1.0), CR=0.9):\n        lb, ub = bounds.lb, bounds.ub\n        pop = self.chaotic_initialization(lb, ub, pop_size)\n        fitness = np.array([func(ind) for ind in pop])\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                idxs = np.random.choice(np.delete(np.arange(pop_size), i), 3, replace=False)\n                x0, x1, x2 = pop[idxs]\n                F = np.random.uniform(*F_range)\n                mutant = np.clip(x0 + F * (x1 - x2), lb, ub)\n                CR = 0.7 + 0.3 * np.random.rand()  # Adaptive crossover rate\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, pop[i])\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    pop[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < fitness[best_idx]:\n                        best = trial\n                        best_idx = i\n                if self.eval_count >= self.budget:\n                    break\n        return best\n\n    def local_search(self, func, x0, bounds):\n        res = minimize(func, x0, bounds=[(lb, ub) for lb, ub in zip(bounds.lb, bounds.ub)], method='BFGS')\n        self.eval_count += res.nfev\n        return res.x\n\n    def __call__(self, func):\n        bounds = func.bounds\n        best = self.differential_evolution(func, bounds)\n        if self.eval_count < self.budget:\n            best = self.local_search(func, best, bounds)\n        return best", "name": "BraggMirrorOptimizer", "description": "Utilize adaptive crossover rate in Differential Evolution to enhance exploration and solution diversity.", "configspace": "", "generation": 10, "fitness": 0.9374042455393391, "feedback": "The algorithm BraggMirrorOptimizer got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.937 with standard deviation 0.024. And the mean value of best solutions found was 0.165 (0. is the best) with standard deviation 0.000.", "error": "", "parent_id": "a9d065a5-6545-4a3e-964b-f997af18a50b", "metadata": {"aucs": [0.9715345614515164, 0.9225825420469542, 0.9180956331195467], "final_y": [0.16485771771003976, 0.16485667279654992, 0.16485845719780878]}, "mutation_prompt": null}
